Directory structure:
â””â”€â”€ gpt-researcher/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ citation.cff
    â”œâ”€â”€ cli.py
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ Dockerfile.fullstack
    â”œâ”€â”€ json_schema_generator.py
    â”œâ”€â”€ langgraph.json
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ main.py
    â”œâ”€â”€ poetry.toml
    â”œâ”€â”€ Procfile
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ requirements_minimal.txt
    â”œâ”€â”€ setup.py
    â”œâ”€â”€ .cursorignore
    â”œâ”€â”€ .dockerignore
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ Procfile
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ run_server.py
    â”‚   â”œâ”€â”€ runtime.txt
    â”‚   â”œâ”€â”€ utils.py
    â”‚   â”œâ”€â”€ chat/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ chat.py
    â”‚   â”œâ”€â”€ memory/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ draft.py
    â”‚   â”‚   â””â”€â”€ research.py
    â”‚   â”œâ”€â”€ report_type/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ basic_report/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ basic_report.py
    â”‚   â”‚   â”œâ”€â”€ deep_research/
    â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ example.py
    â”‚   â”‚   â”‚   â””â”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ detailed_report/
    â”‚   â”‚       â”œâ”€â”€ README.md
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â””â”€â”€ detailed_report.py
    â”‚   â”œâ”€â”€ server/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ app.py
    â”‚   â”‚   â”œâ”€â”€ logging_config.py
    â”‚   â”‚   â”œâ”€â”€ server_utils.py
    â”‚   â”‚   â””â”€â”€ websocket_manager.py
    â”‚   â””â”€â”€ styles/
    â”‚       â””â”€â”€ pdf_styles.css
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ babel.config.js
    â”‚   â”œâ”€â”€ CNAME
    â”‚   â”œâ”€â”€ docusaurus.config.js
    â”‚   â”œâ”€â”€ package.json
    â”‚   â”œâ”€â”€ pydoc-markdown.yml
    â”‚   â”œâ”€â”€ sidebars.js
    â”‚   â”œâ”€â”€ blog/
    â”‚   â”‚   â”œâ”€â”€ authors.yml
    â”‚   â”‚   â”œâ”€â”€ 2023-09-22-gpt-researcher/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ 2023-11-12-openai-assistant/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ 2024-05-19-gptr-langgraph/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ 2024-09-7-hybrid-research/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â”œâ”€â”€ 2025-02-26-deep-research/
    â”‚   â”‚   â”‚   â””â”€â”€ index.md
    â”‚   â”‚   â””â”€â”€ 2025-03-10-stepping-into-the-story/
    â”‚   â”‚       â””â”€â”€ index.md
    â”‚   â”œâ”€â”€ discord-bot/
    â”‚   â”‚   â”œâ”€â”€ deploy-commands.js
    â”‚   â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.dev
    â”‚   â”‚   â”œâ”€â”€ gptr-webhook.js
    â”‚   â”‚   â”œâ”€â”€ index.js
    â”‚   â”‚   â”œâ”€â”€ package.json
    â”‚   â”‚   â”œâ”€â”€ server.js
    â”‚   â”‚   â””â”€â”€ commands/
    â”‚   â”‚       â””â”€â”€ ask.js
    â”‚   â”œâ”€â”€ docs/
    â”‚   â”‚   â”œâ”€â”€ contribute.md
    â”‚   â”‚   â”œâ”€â”€ faq.md
    â”‚   â”‚   â”œâ”€â”€ roadmap.md
    â”‚   â”‚   â”œâ”€â”€ welcome.md
    â”‚   â”‚   â”œâ”€â”€ examples/
    â”‚   â”‚   â”‚   â”œâ”€â”€ custom_prompt.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ detailed_report.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ examples.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ examples.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ hybrid_research.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ pip-run.ipynb
    â”‚   â”‚   â”‚   â”œâ”€â”€ sample_report.py
    â”‚   â”‚   â”‚   â””â”€â”€ sample_sources_only.py
    â”‚   â”‚   â”œâ”€â”€ gpt-researcher/
    â”‚   â”‚   â”‚   â”œâ”€â”€ context/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ azure-storage.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data-ingestion.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ filtering-by-domain.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ local-docs.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tailored-research.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vector-stores.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ img/
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ nextjs-filter-by-domain.JPG
    â”‚   â”‚   â”‚   â”œâ”€â”€ frontend/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ discord-bot.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ embed-script.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ introduction.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nextjs-frontend.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ react-package.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vanilla-js-frontend.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ visualizing-websockets.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ getting-started/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ cli.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ getting-started-with-docker.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ getting-started.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ how-to-choose.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ introduction.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ linux-deployment.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ gptr/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ automated-tests.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ config.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ deep_research.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ example.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ npm-package.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pip-package.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ querying-the-backend.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ scraping.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ troubleshooting.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ handling-logs/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ all-about-logs.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ langsmith-logs.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ simple-logs-example.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ llms/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ llms.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ running-with-azure.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ running-with-ollama.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ supported-llms.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ testing-your-llm.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ mcp-server/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ advanced-usage.md
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ claude-integration.md
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ getting-started.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ multi_agents/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ langgraph.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ retrievers/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ mcp-configs.mdx
    â”‚   â”‚   â”‚   â””â”€â”€ search-engines/
    â”‚   â”‚   â”‚       â”œâ”€â”€ search-engines.md
    â”‚   â”‚   â”‚       â””â”€â”€ test-your-retriever.md
    â”‚   â”‚   â””â”€â”€ reference/
    â”‚   â”‚       â”œâ”€â”€ sidebar.json
    â”‚   â”‚       â”œâ”€â”€ config/
    â”‚   â”‚       â”‚   â”œâ”€â”€ config.md
    â”‚   â”‚       â”‚   â””â”€â”€ singleton.md
    â”‚   â”‚       â””â”€â”€ processing/
    â”‚   â”‚           â”œâ”€â”€ html.md
    â”‚   â”‚           â””â”€â”€ text.md
    â”‚   â”œâ”€â”€ npm/
    â”‚   â”‚   â”œâ”€â”€ Readme.md
    â”‚   â”‚   â”œâ”€â”€ index.js
    â”‚   â”‚   â””â”€â”€ package.json
    â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”‚   â”œâ”€â”€ HomepageFeatures.js
    â”‚   â”‚   â”‚   â””â”€â”€ HomepageFeatures.module.css
    â”‚   â”‚   â”œâ”€â”€ css/
    â”‚   â”‚   â”‚   â””â”€â”€ custom.css
    â”‚   â”‚   â””â”€â”€ pages/
    â”‚   â”‚       â”œâ”€â”€ index.js
    â”‚   â”‚       â””â”€â”€ index.module.css
    â”‚   â””â”€â”€ static/
    â”‚       â”œâ”€â”€ CNAME
    â”‚       â””â”€â”€ .nojekyll
    â”œâ”€â”€ frontend/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ index.html
    â”‚   â”œâ”€â”€ pdf_styles.css
    â”‚   â”œâ”€â”€ scripts.js
    â”‚   â”œâ”€â”€ styles.css
    â”‚   â”œâ”€â”€ nextjs/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.dev
    â”‚   â”‚   â”œâ”€â”€ next.config.mjs
    â”‚   â”‚   â”œâ”€â”€ package.json
    â”‚   â”‚   â”œâ”€â”€ package.lib.json
    â”‚   â”‚   â”œâ”€â”€ postcss.config.mjs
    â”‚   â”‚   â”œâ”€â”€ rollup.config.js
    â”‚   â”‚   â”œâ”€â”€ tailwind.config.ts
    â”‚   â”‚   â”œâ”€â”€ tsconfig.json
    â”‚   â”‚   â”œâ”€â”€ tsconfig.lib.json
    â”‚   â”‚   â”œâ”€â”€ .babelrc.build.json
    â”‚   â”‚   â”œâ”€â”€ .dockerignore
    â”‚   â”‚   â”œâ”€â”€ .eslintrc.json
    â”‚   â”‚   â”œâ”€â”€ .example.env
    â”‚   â”‚   â”œâ”€â”€ .prettierrc
    â”‚   â”‚   â”œâ”€â”€ .python-version
    â”‚   â”‚   â”œâ”€â”€ actions/
    â”‚   â”‚   â”‚   â””â”€â”€ apiActions.ts
    â”‚   â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css
    â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat/
    â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ route.ts
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ reports/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ route.ts
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ [id]/
    â”‚   â”‚   â”‚   â”‚           â”œâ”€â”€ route.ts
    â”‚   â”‚   â”‚   â”‚           â””â”€â”€ chat/
    â”‚   â”‚   â”‚   â”‚               â””â”€â”€ route.ts
    â”‚   â”‚   â”‚   â””â”€â”€ research/
    â”‚   â”‚   â”‚       â””â”€â”€ [id]/
    â”‚   â”‚   â”‚           â””â”€â”€ page.tsx
    â”‚   â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”‚   â”œâ”€â”€ Footer.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Header.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Hero.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ HumanFeedback.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ LoadingDots.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchResults.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchSidebar.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ SimilarTopics.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ TypeAnimation.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Images/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ImageModal.tsx
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ImagesAlbum.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Langgraph/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Langgraph.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ layouts/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CopilotLayout.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MobileLayout.tsx
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ResearchPageLayout.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ mobile/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MobileChatPanel.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MobileHomeScreen.tsx
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MobileResearchContent.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ research/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CopilotPanel.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CopilotResearchContent.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ NotFoundContent.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchContent.tsx
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ResearchPanel.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchBlocks/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ AccessReport.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatResponse.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ImageSection.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ LogsSection.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Question.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Report.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Sources.tsx
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ elements/
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ ChatInput.tsx
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ InputArea.tsx
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ LogMessage.tsx
    â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ SourceCard.tsx
    â”‚   â”‚   â”‚   â”‚       â””â”€â”€ SubQuestions.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ Settings/
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ChatBox.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ LayoutSelector.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MCPSelector.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Modal.tsx
    â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.css
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ToneSelector.tsx
    â”‚   â”‚   â”‚   â””â”€â”€ Task/
    â”‚   â”‚   â”‚       â”œâ”€â”€ Accordion.tsx
    â”‚   â”‚   â”‚       â”œâ”€â”€ AgentLogs.tsx
    â”‚   â”‚   â”‚       â”œâ”€â”€ Report.tsx
    â”‚   â”‚   â”‚       â””â”€â”€ ResearchForm.tsx
    â”‚   â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”‚   â””â”€â”€ task.ts
    â”‚   â”‚   â”œâ”€â”€ helpers/
    â”‚   â”‚   â”‚   â”œâ”€â”€ findDifferences.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ getHost.ts
    â”‚   â”‚   â”‚   â””â”€â”€ markdownHelper.ts
    â”‚   â”‚   â”œâ”€â”€ hooks/
    â”‚   â”‚   â”‚   â”œâ”€â”€ ResearchHistoryContext.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ useAnalytics.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ useResearchHistory.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ useScrollHandler.ts
    â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts
    â”‚   â”‚   â”œâ”€â”€ nginx/
    â”‚   â”‚   â”‚   â””â”€â”€ default.conf
    â”‚   â”‚   â”œâ”€â”€ public/
    â”‚   â”‚   â”‚   â”œâ”€â”€ embed.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ manifest.json
    â”‚   â”‚   â”‚   â”œâ”€â”€ sw.js
    â”‚   â”‚   â”‚   â”œâ”€â”€ workbox-f1770938.js
    â”‚   â”‚   â”‚   â””â”€â”€ img/
    â”‚   â”‚   â”‚       â””â”€â”€ agents/
    â”‚   â”‚   â”‚           â””â”€â”€ defaultAgentAvatar.JPG
    â”‚   â”‚   â”œâ”€â”€ src/
    â”‚   â”‚   â”‚   â”œâ”€â”€ GPTResearcher.tsx
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.css
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.d.ts
    â”‚   â”‚   â”‚   â”œâ”€â”€ index.ts
    â”‚   â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚   â”‚       â””â”€â”€ imageTransformPlugin.js
    â”‚   â”‚   â”œâ”€â”€ styles/
    â”‚   â”‚   â”‚   â””â”€â”€ markdown.css
    â”‚   â”‚   â”œâ”€â”€ types/
    â”‚   â”‚   â”‚   â”œâ”€â”€ data.ts
    â”‚   â”‚   â”‚   â””â”€â”€ react-ga4.d.ts
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ consolidateBlocks.ts
    â”‚   â”‚       â”œâ”€â”€ dataProcessing.ts
    â”‚   â”‚       â””â”€â”€ getLayout.tsx
    â”‚   â””â”€â”€ static/
    â”‚       â””â”€â”€ defaultAgentAvatar.JPG
    â”œâ”€â”€ gpt_researcher/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ agent.py
    â”‚   â”œâ”€â”€ prompts.py
    â”‚   â”œâ”€â”€ actions/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ agent_creator.py
    â”‚   â”‚   â”œâ”€â”€ markdown_processing.py
    â”‚   â”‚   â”œâ”€â”€ query_processing.py
    â”‚   â”‚   â”œâ”€â”€ report_generation.py
    â”‚   â”‚   â”œâ”€â”€ retriever.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â””â”€â”€ web_scraping.py
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ config.py
    â”‚   â”‚   â””â”€â”€ variables/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ base.py
    â”‚   â”‚       â”œâ”€â”€ default.py
    â”‚   â”‚       â””â”€â”€ test_local.json
    â”‚   â”œâ”€â”€ context/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ compression.py
    â”‚   â”‚   â””â”€â”€ retriever.py
    â”‚   â”œâ”€â”€ document/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ azure_document_loader.py
    â”‚   â”‚   â”œâ”€â”€ document.py
    â”‚   â”‚   â”œâ”€â”€ langchain_document.py
    â”‚   â”‚   â””â”€â”€ online_document.py
    â”‚   â”œâ”€â”€ llm_provider/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ generic/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â””â”€â”€ base.py
    â”‚   â”œâ”€â”€ mcp/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ client.py
    â”‚   â”‚   â”œâ”€â”€ research.py
    â”‚   â”‚   â”œâ”€â”€ streaming.py
    â”‚   â”‚   â””â”€â”€ tool_selector.py
    â”‚   â”œâ”€â”€ memory/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ embeddings.py
    â”‚   â”œâ”€â”€ retrievers/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ arxiv/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ arxiv.py
    â”‚   â”‚   â”œâ”€â”€ bing/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ bing.py
    â”‚   â”‚   â”œâ”€â”€ custom/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ custom.py
    â”‚   â”‚   â”œâ”€â”€ duckduckgo/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ duckduckgo.py
    â”‚   â”‚   â”œâ”€â”€ exa/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ exa.py
    â”‚   â”‚   â”œâ”€â”€ google/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ google.py
    â”‚   â”‚   â”œâ”€â”€ mcp/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ retriever.py
    â”‚   â”‚   â”œâ”€â”€ pubmed_central/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ pubmed_central.py
    â”‚   â”‚   â”œâ”€â”€ searchapi/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ searchapi.py
    â”‚   â”‚   â”œâ”€â”€ searx/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ searx.py
    â”‚   â”‚   â”œâ”€â”€ semantic_scholar/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ semantic_scholar.py
    â”‚   â”‚   â”œâ”€â”€ serpapi/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ serpapi.py
    â”‚   â”‚   â”œâ”€â”€ serper/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ serper.py
    â”‚   â”‚   â””â”€â”€ tavily/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â””â”€â”€ tavily_search.py
    â”‚   â”œâ”€â”€ scraper/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ scraper.py
    â”‚   â”‚   â”œâ”€â”€ utils.py
    â”‚   â”‚   â”œâ”€â”€ arxiv/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ arxiv.py
    â”‚   â”‚   â”œâ”€â”€ beautiful_soup/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ beautiful_soup.py
    â”‚   â”‚   â”œâ”€â”€ browser/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ browser.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ nodriver_scraper.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ js/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ overlay.js
    â”‚   â”‚   â”‚   â””â”€â”€ processing/
    â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚       â”œâ”€â”€ html.py
    â”‚   â”‚   â”‚       â””â”€â”€ scrape_skills.py
    â”‚   â”‚   â”œâ”€â”€ firecrawl/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ firecrawl.py
    â”‚   â”‚   â”œâ”€â”€ pymupdf/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ pymupdf.py
    â”‚   â”‚   â”œâ”€â”€ tavily_extract/
    â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”‚   â””â”€â”€ tavily_extract.py
    â”‚   â”‚   â””â”€â”€ web_base_loader/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â””â”€â”€ web_base_loader.py
    â”‚   â”œâ”€â”€ skills/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ browser.py
    â”‚   â”‚   â”œâ”€â”€ context_manager.py
    â”‚   â”‚   â”œâ”€â”€ curator.py
    â”‚   â”‚   â”œâ”€â”€ deep_research.py
    â”‚   â”‚   â”œâ”€â”€ researcher.py
    â”‚   â”‚   â””â”€â”€ writer.py
    â”‚   â”œâ”€â”€ utils/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ costs.py
    â”‚   â”‚   â”œâ”€â”€ enum.py
    â”‚   â”‚   â”œâ”€â”€ llm.py
    â”‚   â”‚   â”œâ”€â”€ logger.py
    â”‚   â”‚   â”œâ”€â”€ logging_config.py
    â”‚   â”‚   â”œâ”€â”€ tools.py
    â”‚   â”‚   â”œâ”€â”€ validators.py
    â”‚   â”‚   â””â”€â”€ workers.py
    â”‚   â””â”€â”€ vector_store/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ vector_store.py
    â”œâ”€â”€ mcp-server/
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ multi_agents/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ agent.py
    â”‚   â”œâ”€â”€ langgraph.json
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ package.json
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ task.json
    â”‚   â”œâ”€â”€ agents/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ editor.py
    â”‚   â”‚   â”œâ”€â”€ human.py
    â”‚   â”‚   â”œâ”€â”€ orchestrator.py
    â”‚   â”‚   â”œâ”€â”€ publisher.py
    â”‚   â”‚   â”œâ”€â”€ researcher.py
    â”‚   â”‚   â”œâ”€â”€ reviewer.py
    â”‚   â”‚   â”œâ”€â”€ reviser.py
    â”‚   â”‚   â”œâ”€â”€ writer.py
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ __init__.py
    â”‚   â”‚       â”œâ”€â”€ file_formats.py
    â”‚   â”‚       â”œâ”€â”€ llms.py
    â”‚   â”‚       â”œâ”€â”€ pdf_styles.css
    â”‚   â”‚       â”œâ”€â”€ utils.py
    â”‚   â”‚       â””â”€â”€ views.py
    â”‚   â””â”€â”€ memory/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ draft.py
    â”‚       â””â”€â”€ research.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ documents-report-source.py
    â”‚   â”œâ”€â”€ gptr-logs-handler.py
    â”‚   â”œâ”€â”€ report-types.py
    â”‚   â”œâ”€â”€ research_test.py
    â”‚   â”œâ”€â”€ test-loaders.py
    â”‚   â”œâ”€â”€ test-openai-llm.py
    â”‚   â”œâ”€â”€ test-your-embeddings.py
    â”‚   â”œâ”€â”€ test-your-llm.py
    â”‚   â”œâ”€â”€ test-your-retriever.py
    â”‚   â”œâ”€â”€ test_logging.py
    â”‚   â”œâ”€â”€ test_logging_output.py
    â”‚   â”œâ”€â”€ test_logs.py
    â”‚   â”œâ”€â”€ test_mcp.py
    â”‚   â”œâ”€â”€ test_researcher_logging.py
    â”‚   â”œâ”€â”€ test_security_fix.py
    â”‚   â””â”€â”€ vector-store.py
    â””â”€â”€ .github/
        â”œâ”€â”€ dependabot.yml
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ bug_report.md
        â”‚   â””â”€â”€ feature_request.md
        â””â”€â”€ workflows/
            â”œâ”€â”€ docker-build.yml
            â””â”€â”€ docker-push.yml

================================================
FILE: README.md
================================================
<div align="center" id="top">

<img src="https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3" alt="Logo" width="80">

####

[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)

[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)
![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)
[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)
[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)
[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)

[English](README.md) | [ä¸­æ–‡](README-zh_CN.md) | [æ—¥æœ¬èª](README-ja_JP.md) | [í•œêµ­ì–´](README-ko_KR.md)

</div>

# ğŸ” GPT Researcher

**GPT Researcher is an open deep research agent designed for both web and local research on any given task.** 

The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.

**Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.**

## Why GPT Researcher?

- Objective conclusions for manual research can take weeks, requiring vast resources and time.
- LLMs trained on outdated information can hallucinate, becoming irrelevant for current research tasks.
- Current LLMs have token limitations, insufficient for generating long research reports.
- Limited web sources in existing services lead to misinformation and shallow results.
- Selective web sources can introduce bias into research tasks.

## Demo
<a href="https://www.youtube.com/watch?v=f60rlc_QCxE" target="_blank" rel="noopener">
  <img src="https://github.com/user-attachments/assets/ac2ec55f-b487-4b3f-ae6f-b8743ad296e4" alt="Demo video" width="800" target="_blank" />
</a>


## Architecture

The core idea is to utilize 'planner' and 'execution' agents. The planner generates research questions, while the execution agents gather relevant information. The publisher then aggregates all findings into a comprehensive report.

<div align="center">
<img align="center" height="600" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527">
</div>

Steps:
* Create a task-specific agent based on a research query.
* Generate questions that collectively form an objective opinion on the task.
* Use a crawler agent for gathering information for each question.
* Summarize and source-track each resource.
* Filter and aggregate summaries into a final research report.

## Tutorials
 - [How it Works](https://docs.gptr.dev/blog/building-gpt-researcher)
 - [How to Install](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)
 - [Live Demo](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)

## Features

- ğŸ“ Generate detailed research reports using web and local documents.
- ğŸ–¼ï¸ Smart image scraping and filtering for reports.
- ğŸ“œ Generate detailed reports exceeding 2,000 words.
- ğŸŒ Aggregate over 20 sources for objective conclusions.
- ğŸ–¥ï¸ Frontend available in lightweight (HTML/CSS/JS) and production-ready (NextJS + Tailwind) versions.
- ğŸ” JavaScript-enabled web scraping.
- ğŸ“‚ Maintains memory and context throughout research.
- ğŸ“„ Export reports to PDF, Word, and other formats.

## ğŸ“– Documentation

See the [Documentation](https://docs.gptr.dev/docs/gpt-researcher/getting-started) for:
- Installation and setup guides
- Configuration and customization options
- How-To examples
- Full API references

## âš™ï¸ Getting Started

### Installation

1. Install Python 3.11 or later. [Guide](https://www.tutorialsteacher.com/python/install-python).
2. Clone the project and navigate to the directory:

    ```bash
    git clone https://github.com/assafelovic/gpt-researcher.git
    cd gpt-researcher
    ```

3. Set up API keys by exporting them or storing them in a `.env` file.

    ```bash
    export OPENAI_API_KEY={Your OpenAI API Key here}
    export TAVILY_API_KEY={Your Tavily API Key here}
    ```

    For custom OpenAI-compatible APIs (e.g., local models, other providers), you can also set:
    
    ```bash
    export OPENAI_BASE_URL={Your custom API base URL here}
    ```

4. Install dependencies and start the server:

    ```bash
    pip install -r requirements.txt
    python -m uvicorn main:app --reload
    ```

Visit [http://localhost:8000](http://localhost:8000) to start.

For other setups (e.g., Poetry or virtual environments), check the [Getting Started page](https://docs.gptr.dev/docs/gpt-researcher/getting-started).

## Run as PIP package
```bash
pip install gpt-researcher

```
### Example Usage:
```python
...
from gpt_researcher import GPTResearcher

query = "why is Nvidia stock going up?"
researcher = GPTResearcher(query=query)
# Conduct research on the given query
research_result = await researcher.conduct_research()
# Write the report
report = await researcher.write_report()
...
```

**For more examples and configurations, please refer to the [PIP documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package) page.**

### ğŸ”§ MCP Client
GPT Researcher supports MCP integration to connect with specialized data sources like GitHub repositories, databases, and custom APIs. This enables research from data sources alongside web search.

```bash
export RETRIEVER=tavily,mcp  # Enable hybrid web + MCP research
```

```python
from gpt_researcher import GPTResearcher
import asyncio
import os

async def mcp_research_example():
    # Enable MCP with web search
    os.environ["RETRIEVER"] = "tavily,mcp"
    
    researcher = GPTResearcher(
        query="What are the top open source web research agents?",
        mcp_configs=[
            {
                "name": "github",
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")}
            }
        ]
    )
    
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    return report
```

> For comprehensive MCP documentation and advanced examples, visit the [MCP Integration Guide](https://docs.gptr.dev/docs/gpt-researcher/retrievers/mcp-configs).

## âœ¨ Deep Research

GPT Researcher now includes Deep Research - an advanced recursive research workflow that explores topics with agentic depth and breadth. This feature employs a tree-like exploration pattern, diving deeper into subtopics while maintaining a comprehensive view of the research subject.

- ğŸŒ³ Tree-like exploration with configurable depth and breadth
- âš¡ï¸ Concurrent processing for faster results
- ğŸ¤ Smart context management across research branches
- â±ï¸ Takes ~5 minutes per deep research
- ğŸ’° Costs ~$0.4 per research (using `o3-mini` on "high" reasoning effort)

[Learn more about Deep Research](https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research) in our documentation.

## Run with Docker

> **Step 1** - [Install Docker](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)

> **Step 2** - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'

> **Step 3** - Within the docker-compose file comment out services that you don't want to run with Docker.

```bash
docker-compose up --build
```

If that doesn't work, try running it without the dash:
```bash
docker compose up --build
```

> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:
 - the Python server running on localhost:8000<br>
 - the React app running on localhost:3000<br>

Visit localhost:3000 on any browser and enjoy researching!


## ğŸ“„ Research on Local Documents

You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.

Step 1: Add the env variable `DOC_PATH` pointing to the folder where your documents are located.

```bash
export DOC_PATH="./my-docs"
```

Step 2: 
 - If you're running the frontend app on localhost:8000, simply select "My Documents" from the "Report Source" Dropdown Options.
 - If you're running GPT Researcher with the [PIP package](https://docs.tavily.com/guides/gpt-researcher/gpt-researcher#pip-package), pass the `report_source` argument as "local" when you instantiate the `GPTResearcher` class [code sample here](https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research).


## ğŸ¤– MCP Server

We've moved our MCP server to a dedicated repository: [gptr-mcp](https://github.com/assafelovic/gptr-mcp).

The GPT Researcher MCP Server enables AI applications like Claude to conduct deep research. While LLM apps can access web search tools with MCP, GPT Researcher MCP delivers deeper, more reliable research results.

Features:
- Deep research capabilities for AI assistants
- Higher quality information with optimized context usage
- Comprehensive results with better reasoning for LLMs
- Claude Desktop integration

For detailed installation and usage instructions, please visit the [official repository](https://github.com/assafelovic/gptr-mcp).


## ğŸ‘ª Multi-Agent Assistant
As AI evolves from prompt engineering and RAG to multi-agent systems, we're excited to introduce our new multi-agent assistant built with [LangGraph](https://python.langchain.com/v0.1/docs/langgraph/).

By using LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Inspired by the recent [STORM](https://arxiv.org/abs/2402.14207) paper, this project showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.

An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.

Check it out [here](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents) or head over to our [documentation](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph) for more information.

## ğŸ–¥ï¸ Frontend Applications

GPT-Researcher now features an enhanced frontend to improve the user experience and streamline the research process. The frontend offers:

- An intuitive interface for inputting research queries
- Real-time progress tracking of research tasks
- Interactive display of research findings
- Customizable settings for tailored research experiences

Two deployment options are available:
1. A lightweight static frontend served by FastAPI
2. A feature-rich NextJS application for advanced functionality

For detailed setup instructions and more information about the frontend features, please visit our [documentation page](https://docs.gptr.dev/docs/gpt-researcher/frontend/introduction).

## ğŸš€ Contributing
We highly welcome contributions! Please check out [contributing](https://github.com/assafelovic/gpt-researcher/blob/master/CONTRIBUTING.md) if you're interested.

Please check out our [roadmap](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) page and reach out to us via our [Discord community](https://discord.gg/QgZXvJAccX) if you're interested in joining our mission.
<a href="https://github.com/assafelovic/gpt-researcher/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=assafelovic/gpt-researcher" />
</a>
## âœ‰ï¸ Support / Contact us
- [Community Discord](https://discord.gg/spBgZmm3Xe)
- Author Email: assaf.elovic@gmail.com

## ğŸ›¡ Disclaimer

This project, GPT Researcher, is an experimental application and is provided "as-is" without any warranty, express or implied. We are sharing codes for academic purposes under the Apache 2 license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.

Our view on unbiased research claims:
1. The main goal of GPT Researcher is to reduce incorrect and biased facts. How? We assume that the more sites we scrape the less chances of incorrect data. By scraping multiple sites per research, and choosing the most frequent information, the chances that they are all wrong is extremely low.
2. We do not aim to eliminate biases; we aim to reduce it as much as possible. **We are here as a community to figure out the most effective human/llm interactions.**
3. In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.

---

<p align="center">
<a href="https://star-history.com/#assafelovic/gpt-researcher">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date" />
    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=assafelovic/gpt-researcher&type=Date" />
  </picture>
</a>
</p>


<p align="right">
  <a href="#top">â¬†ï¸ Back to Top</a>
</p>



================================================
FILE: citation.cff
================================================
cff-version: 1.0.0
message: "If you use this software, please cite it as below."
authors:
  - family-names: Elovic
    given-names: Assaf
title: gpt-researcher
version: 0.5.4
date-released: 2023-07-23
repository-code: https://github.com/assafelovic/gpt-researcher
url: https://gptr.dev


================================================
FILE: cli.py
================================================
"""
Provides a command line interface for the GPTResearcher class.

Usage:

```shell
python cli.py "<query>" --report_type <report_type> --tone <tone> --query_domains <foo.com,bar.com>
```

"""
import asyncio
import argparse
from argparse import RawTextHelpFormatter
from uuid import uuid4
import os

from dotenv import load_dotenv

from gpt_researcher import GPTResearcher
from gpt_researcher.utils.enum import ReportType, Tone
from backend.report_type import DetailedReport

# =============================================================================
# CLI
# =============================================================================

cli = argparse.ArgumentParser(
    description="Generate a research report.",
    # Enables the use of newlines in the help message
    formatter_class=RawTextHelpFormatter)

# =====================================
# Arg: Query
# =====================================

cli.add_argument(
    # Position 0 argument
    "query",
    type=str,
    help="The query to conduct research on.")

# =====================================
# Arg: Report Type
# =====================================

choices = [report_type.value for report_type in ReportType]

report_type_descriptions = {
    ReportType.ResearchReport.value: "Summary - Short and fast (~2 min)",
    ReportType.DetailedReport.value: "Detailed - In depth and longer (~5 min)",
    ReportType.ResourceReport.value: "",
    ReportType.OutlineReport.value: "",
    ReportType.CustomReport.value: "",
    ReportType.SubtopicReport.value: "",
    ReportType.DeepResearch.value: "Deep Research"
}

cli.add_argument(
    "--report_type",
    type=str,
    help="The type of report to generate. Options:\n" + "\n".join(
        f"  {choice}: {report_type_descriptions[choice]}" for choice in choices
    ),
    # Deserialize ReportType as a List of strings:
    choices=choices,
    required=True)

# =====================================
# Arg: Tone
# =====================================

cli.add_argument(
    "--tone",
    type=str,
    help="The tone of the report (optional).",
    choices=["objective", "formal", "analytical", "persuasive", "informative",
            "explanatory", "descriptive", "critical", "comparative", "speculative",
            "reflective", "narrative", "humorous", "optimistic", "pessimistic"],
    default="objective"
)

# =====================================
# Arg: Encoding
# =====================================

cli.add_argument(
    "--encoding",
    type=str,
    help="The encoding to use for the output file (default: utf-8).",
    default="utf-8"
)

# =====================================
# Arg: Query Domains
# =====================================

cli.add_argument(
    "--query_domains",
    type=str,
    help="A comma-separated list of domains to search for the query.",
    default=""
)

# =============================================================================
# Main
# =============================================================================

async def main(args):
    """
    Conduct research on the given query, generate the report, and write
    it as a markdown file to the output directory.
    """
    query_domains = args.query_domains.split(",") if args.query_domains else []

    if args.report_type == 'detailed_report':
        detailed_report = DetailedReport(
            query=args.query,
            query_domains=query_domains,
            report_type="research_report",
            report_source="web_search",
        )

        report = await detailed_report.run()
    else:
        # Convert the simple keyword to the full Tone enum value
        tone_map = {
            "objective": Tone.Objective,
            "formal": Tone.Formal,
            "analytical": Tone.Analytical,
            "persuasive": Tone.Persuasive,
            "informative": Tone.Informative,
            "explanatory": Tone.Explanatory,
            "descriptive": Tone.Descriptive,
            "critical": Tone.Critical,
            "comparative": Tone.Comparative,
            "speculative": Tone.Speculative,
            "reflective": Tone.Reflective,
            "narrative": Tone.Narrative,
            "humorous": Tone.Humorous,
            "optimistic": Tone.Optimistic,
            "pessimistic": Tone.Pessimistic
        }

        researcher = GPTResearcher(
            query=args.query,
            query_domains=query_domains,
            report_type=args.report_type,
            tone=tone_map[args.tone],
            encoding=args.encoding
        )

        await researcher.conduct_research()

        report = await researcher.write_report()

    # Write the report to a file
    artifact_filepath = f"outputs/{uuid4()}.md"
    os.makedirs("outputs", exist_ok=True)
    with open(artifact_filepath, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"Report written to '{artifact_filepath}'")

if __name__ == "__main__":
    load_dotenv()
    args = cli.parse_args()
    asyncio.run(main(args))



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We, as members, contributors, and leaders, pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, sexual identity, or
orientation.

We commit to acting and interacting in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

- Demonstrating empathy and kindness toward others
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience
- Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

- The use of sexualized language or imagery, and sexual attention or
  advances of any kind
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or email address, without their explicit permission
- Other conduct that could reasonably be considered inappropriate in a professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior deemed inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that do not
align with this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies to all community spaces and also applies when
an individual is officially representing the community in public spaces.
Examples include using an official email address, posting via an official
social media account, or acting as an appointed representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[Assaf.elovic@gmail.com](mailto:Assaf.elovic@gmail.com).
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period. This includes
avoiding interactions in community spaces and external channels like social media.
Violating these terms may lead to a temporary or permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any interaction or public
communication with the community for a specified period. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of groups of individuals.

**Consequence**: A permanent ban from any public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.0, available at
https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see the FAQ at
https://www.contributor-covenant.org/faq. Translations are available at
https://www.contributor-covenant.org/translations.



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to GPT Researcher

First off, we'd like to welcome you and thank you for your interest and effort in contributing to our open-source project â¤ï¸. Contributions of all forms are welcomeâ€”from new features and bug fixes to documentation and more.

We are on a mission to build the #1 AI agent for comprehensive, unbiased, and factual research online, and we need your support to achieve this grand vision.

Please take a moment to review this document to make the contribution process easy and effective for everyone involved.

## Reporting Issues

If you come across any issue or have an idea for an improvement, don't hesitate to create an issue on GitHub. Describe your problem in sufficient detail, providing as much relevant information as possible. This way, we can reproduce the issue before attempting to fix it or respond appropriately.

## Contributing Code

1. **Fork the repository and create your branch from `master`.**  
   If itâ€™s not an urgent bug fix, branch from `master` and work on the feature or fix there.

2. **Make your changes.**  
   Implement your changes following best practices for coding in the project's language.

3. **Test your changes.**  
   Ensure that your changes pass all tests if any exist. If the project doesnâ€™t have automated tests, test your changes manually to confirm they behave as expected.

4. **Follow the coding style.**  
   Ensure your code adheres to the coding conventions used throughout the project, including indentation, accurate comments, etc.

5. **Commit your changes.**  
   Make your Git commits informative and concise. This is very helpful for others when they look at the Git log.

6. **Push to your fork and submit a pull request.**  
   When your work is ready and passes tests, push your branch to your fork of the repository and submit a pull request from there.

7. **Pat yourself on the back and wait for review.**  
   Your work is done, congratulations! Now sit tight. The project maintainers will review your submission as soon as possible. They might suggest changes or ask for improvements. Both constructive conversation and patience are key to the collaboration process.

## Documentation

If you would like to contribute to the project's documentation, please follow the same steps: fork the repository, make your changes, test them, and submit a pull request.

Documentation is a vital part of any software. It's not just about having good code; ensuring that users and contributors understand what's going on, how to use the software, or how to contribute is crucial.

We're grateful for all our contributors, and we look forward to building the world's leading AI research agent hand-in-hand with you. Let's harness the power of open source and AI to change the world together!



================================================
FILE: docker-compose.yml
================================================
services:
  gpt-researcher:
    pull_policy: build
    image: gptresearcher/gpt-researcher
    build: ./
    environment: 
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL}
      TAVILY_API_KEY: ${TAVILY_API_KEY}
      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}
      LOGGING_LEVEL: INFO
    volumes:
      - ${PWD}/my-docs:/usr/src/app/my-docs:rw
      - ${PWD}/outputs:/usr/src/app/outputs:rw
      - ${PWD}/logs:/usr/src/app/logs:rw
    user: root
    restart: always
    ports:
      - 8000:8000
      
  gptr-nextjs:
    pull_policy: build
    image: gptresearcher/gptr-nextjs
    stdin_open: true
    environment:
      CHOKIDAR_USEPOLLING: "true"
      LOGGING_LEVEL: INFO
      NEXT_PUBLIC_GA_MEASUREMENT_ID: ${NEXT_PUBLIC_GA_MEASUREMENT_ID}
      NEXT_PUBLIC_GPTR_API_URL: ${NEXT_PUBLIC_GPTR_API_URL}
    build:
      dockerfile: Dockerfile.dev
      context: frontend/nextjs
    volumes:
      - /app/node_modules
      - ./frontend/nextjs:/app
      - ./frontend/nextjs/.next:/app/.next
      - ./outputs:/app/outputs
    restart: always
    ports:
      - 3000:3000

  gpt-researcher-tests:
    image: gptresearcher/gpt-researcher-tests
    build: ./
    environment: 
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL}
      TAVILY_API_KEY: ${TAVILY_API_KEY}
      LANGCHAIN_API_KEY: ${LANGCHAIN_API_KEY}
      LOGGING_LEVEL: INFO
    profiles: ["test"]
    command: >
      /bin/sh -c "
      pip install pytest pytest-asyncio faiss-cpu &&
      python -m pytest tests/report-types.py &&
      python -m pytest tests/vector-store.py
      "
  
  discord-bot:
    build:
      context: ./docs/discord-bot
      dockerfile: Dockerfile.dev
    environment:
      - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN}
      - DISCORD_CLIENT_ID=${DISCORD_CLIENT_ID}
    volumes:
      - ./docs/discord-bot:/app
      - /app/node_modules
    ports:
      - 3001:3000
    profiles: ["discord"]
    restart: always



================================================
FILE: Dockerfile
================================================
# Stage 1: Browser and build tools installation
#FROM python:3.13.3-slim-bookworm AS install-browser
FROM python:3.11.4-slim-bullseye AS install-browser

# Install Chromium, Chromedriver, Firefox, Geckodriver, and build tools in one layer
RUN apt-get update \
    && apt-get install -y gnupg wget ca-certificates --no-install-recommends \
    && ARCH=$(dpkg --print-architecture) \
    && wget -qO - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=${ARCH}] http://dl.google.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google-chrome.list \
    && apt-get update \
    && apt-get install -y chromium chromium-driver \
    && chromium --version && chromedriver --version \
    && apt-get install -y --no-install-recommends firefox-esr build-essential \
    && GECKO_ARCH=$(case ${ARCH} in amd64) echo "linux64" ;; arm64) echo "linux-aarch64" ;; *) echo "linux64" ;; esac) \
    && wget https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
    && tar -xvzf geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
    && chmod +x geckodriver \
    && mv geckodriver /usr/local/bin/ \
    && rm geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
    && rm -rf /var/lib/apt/lists/*  # Clean up apt lists to reduce image size

# Stage 2: Python dependencies installation
FROM install-browser AS gpt-researcher-install

ENV PIP_ROOT_USER_ACTION=ignore
WORKDIR /usr/src/app

# Copy and install Python dependencies in a single layer to optimize cache usage
COPY ./requirements.txt ./requirements.txt
COPY ./multi_agents/requirements.txt ./multi_agents/requirements.txt

RUN pip install --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt --upgrade --prefer-binary && \
    pip install --no-cache-dir -r multi_agents/requirements.txt --upgrade --prefer-binary

# Stage 3: Final stage with non-root user and app
FROM gpt-researcher-install AS gpt-researcher

# Basic server configuration
ARG HOST=0.0.0.0
ENV HOST=${HOST}
ARG PORT=8000
ENV PORT=${PORT}
EXPOSE ${PORT}

# Uvicorn parameters used in CMD
ARG WORKERS=1
ENV WORKERS=${WORKERS}

# Create a non-root user for security
# NOTE: Don't use this if you are relying on `_check_pkg` to pip install packages dynamically.
RUN useradd -ms /bin/bash gpt-researcher && \
    chown -R gpt-researcher:gpt-researcher /usr/src/app && \
    # Add these lines to create and set permissions for outputs directory
    mkdir -p /usr/src/app/outputs && \
    chown -R gpt-researcher:gpt-researcher /usr/src/app/outputs && \
    chmod 777 /usr/src/app/outputs
USER gpt-researcher
WORKDIR /usr/src/app

# Copy the rest of the application files with proper ownership
COPY --chown=gpt-researcher:gpt-researcher ./ ./
CMD uvicorn main:app --host ${HOST} --port ${PORT} --workers ${WORKERS}



================================================
FILE: Dockerfile.fullstack
================================================
########################################################################
# Stage 1: Frontend build
########################################################################
FROM node:slim AS frontend-builder
WORKDIR /app/frontend/nextjs

# Copy package files and install dependencies
COPY frontend/nextjs/package.json frontend/nextjs/package-lock.json* ./
RUN npm install --legacy-peer-deps

# Copy the rest of the frontend application and build it
COPY frontend/nextjs/ ./
RUN npm run build

########################################################################
# Stage 2: Browser and backend build tools installation
########################################################################
FROM python:3.13.3-slim-bookworm AS install-browser

# Install Chromium, Chromedriver, Firefox, Geckodriver, and build tools in one layer
RUN echo 'Acquire::Retries "3";' > /etc/apt/apt.conf.d/80-retries \
  && echo 'Acquire::http::Timeout "60";' >> /etc/apt/apt.conf.d/80-retries \
  && echo 'Acquire::https::Timeout "60";' >> /etc/apt/apt.conf.d/80-retries \
  && echo 'Acquire::ftp::Timeout "60";' >> /etc/apt/apt.conf.d/80-retries \
  && apt-get update \
  && apt-get install -y gnupg wget ca-certificates --no-install-recommends \
  && ARCH=$(dpkg --print-architecture) \
  && if [ "$ARCH" = "arm64" ]; then \
  apt-get install -y chromium chromium-driver \
  && chromium --version && chromedriver --version; \
  else \
  wget -qO - https://dl.google.com/linux/linux_signing_key.pub | apt-key add - \
  && echo "deb [arch=${ARCH}] http://dl.google.com/linux/chrome/deb/ stable main" \
  > /etc/apt/sources.list.d/google-chrome.list \
  && apt-get update \
  && apt-get install -y google-chrome-stable; \
  fi \
  && apt-get install -y --no-install-recommends firefox-esr build-essential \
  && GECKO_ARCH=$(case ${ARCH} in amd64) echo "linux64" ;; arm64) echo "linux-aarch64" ;; *) echo "linux64" ;; esac) \
  && wget https://github.com/mozilla/geckodriver/releases/download/v0.36.0/geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
  && tar -xvzf geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
  && chmod +x geckodriver \
  && mv geckodriver /usr/local/bin/ \
  && rm geckodriver-v0.36.0-${GECKO_ARCH}.tar.gz \
  && rm -rf /var/lib/apt/lists/*

########################################################################
# Stage 3: Python dependencies installation
########################################################################
FROM install-browser AS backend-builder
WORKDIR /usr/src/app

ENV PIP_ROOT_USER_ACTION=ignore

COPY ./requirements.txt ./requirements.txt
COPY ./multi_agents/requirements.txt ./multi_agents/requirements.txt

# Install Python packages with retry logic and timeout configuration
RUN pip config set global.timeout 60 && \
  pip config set global.retries 3 && \
  pip install --upgrade pip && \
  pip install --no-cache-dir -r requirements.txt --upgrade --prefer-binary && \
  pip install --no-cache-dir -r multi_agents/requirements.txt --upgrade --prefer-binary

########################################################################
# Stage 4: Final image with backend, frontend
########################################################################
FROM backend-builder AS final

WORKDIR /usr/src/app

# Install Node.js and supervisord with retry logic
RUN apt-get update && \
  apt-get install -y curl supervisor nginx && \
  curl -fsSL --retry 3 --retry-delay 10 https://deb.nodesource.com/setup_20.x | bash - && \
  apt-get install -y nodejs && \
  rm -rf /var/lib/apt/lists/*

# Set backend server configuration
ARG HOST=0.0.0.0
ENV HOST=${HOST}

ARG PORT=8000
ENV PORT=${PORT}
EXPOSE ${PORT}

ARG NEXT_PORT=3000
ENV NEXT_PORT=${NEXT_PORT}
EXPOSE ${NEXT_PORT}

# Internal Next.js port (not exposed)
ARG NEXT_INTERNAL_PORT=3001
ENV NEXT_INTERNAL_PORT=${NEXT_INTERNAL_PORT}

# Copy application files
COPY ./ ./

# Copy built frontend from the frontend-builder stage
COPY --from=frontend-builder /app/frontend/nextjs/.next ./frontend/nextjs/.next
COPY --from=frontend-builder /app/frontend/nextjs/node_modules ./frontend/nextjs/node_modules
COPY --from=frontend-builder /app/frontend/nextjs/public ./frontend/nextjs/public
COPY --from=frontend-builder /app/frontend/nextjs/package.json ./frontend/nextjs/package.json
# Ensure next.config.mjs and other necessary files are present
COPY --from=frontend-builder /app/frontend/nextjs/next.config.mjs ./frontend/nextjs/next.config.mjs

# Create nginx configuration
RUN echo 'events {' > /etc/nginx/nginx.conf && \
  echo '    worker_connections 1024;' >> /etc/nginx/nginx.conf && \
  echo '}' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo 'http {' >> /etc/nginx/nginx.conf && \
  echo '    include /etc/nginx/mime.types;' >> /etc/nginx/nginx.conf && \
  echo '    default_type application/octet-stream;' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '    # Logging' >> /etc/nginx/nginx.conf && \
  echo '    access_log /var/log/nginx/access.log;' >> /etc/nginx/nginx.conf && \
  echo '    error_log /var/log/nginx/error.log;' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '    # Gzip compression' >> /etc/nginx/nginx.conf && \
  echo '    gzip on;' >> /etc/nginx/nginx.conf && \
  echo '    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '    # WebSocket support' >> /etc/nginx/nginx.conf && \
  echo '    map $http_upgrade $connection_upgrade {' >> /etc/nginx/nginx.conf && \
  echo '        default upgrade;' >> /etc/nginx/nginx.conf && \
  echo '        '"'"''"'"' close;' >> /etc/nginx/nginx.conf && \
  echo '    }' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '    server {' >> /etc/nginx/nginx.conf && \
  echo '        listen 3000;' >> /etc/nginx/nginx.conf && \
  echo '        server_name _;' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '        # Proxy backend routes to FastAPI server' >> /etc/nginx/nginx.conf && \
  echo '        location /outputs {' >> /etc/nginx/nginx.conf && \
  echo '            proxy_pass http://127.0.0.1:8000;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Host $host;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Real-IP $remote_addr;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-Proto $scheme;' >> /etc/nginx/nginx.conf && \
  echo '        }' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '        location /reports {' >> /etc/nginx/nginx.conf && \
  echo '            proxy_pass http://127.0.0.1:8000;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Host $host;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Real-IP $remote_addr;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-Proto $scheme;' >> /etc/nginx/nginx.conf && \
  echo '        }' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '        location /ws {' >> /etc/nginx/nginx.conf && \
  echo '            proxy_pass http://127.0.0.1:8000;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_http_version 1.1;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Upgrade $http_upgrade;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Connection $connection_upgrade;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Host $host;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Real-IP $remote_addr;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-Proto $scheme;' >> /etc/nginx/nginx.conf && \
  echo '        }' >> /etc/nginx/nginx.conf && \
  echo '' >> /etc/nginx/nginx.conf && \
  echo '        # Proxy all other requests to Next.js' >> /etc/nginx/nginx.conf && \
  echo '        location / {' >> /etc/nginx/nginx.conf && \
  echo '            proxy_pass http://127.0.0.1:3001;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header Host $host;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Real-IP $remote_addr;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;' >> /etc/nginx/nginx.conf && \
  echo '            proxy_set_header X-Forwarded-Proto $scheme;' >> /etc/nginx/nginx.conf && \
  echo '        }' >> /etc/nginx/nginx.conf && \
  echo '    }' >> /etc/nginx/nginx.conf && \
  echo '}' >> /etc/nginx/nginx.conf

# Create supervisord configuration
# stdout/stderr_maxbytes prevents log file rotation and ensures continuous output
RUN echo '[supervisord]' > /etc/supervisor/conf.d/supervisord.conf && \
  echo 'nodaemon=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'user=root' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'logfile=/dev/stdout' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '[program:backend]' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'command=uvicorn main:app --host %(ENV_HOST)s --port %(ENV_PORT)s' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'directory=/usr/src/app' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autostart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autorestart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile=/dev/stdout' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile=/dev/stderr' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '[program:frontend]' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'command=npm run start -- -p %(ENV_NEXT_INTERNAL_PORT)s' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'directory=/usr/src/app/frontend/nextjs' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autostart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autorestart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile=/dev/stdout' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile=/dev/stderr' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo '[program:nginx]' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'command=nginx -g "daemon off;"' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autostart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'autorestart=true' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile=/dev/stdout' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stdout_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile=/dev/stderr' >> /etc/supervisor/conf.d/supervisord.conf && \
  echo 'stderr_logfile_maxbytes=0' >> /etc/supervisor/conf.d/supervisord.conf

# Start supervisord to manage both services
CMD ["/usr/bin/supervisord", "-c", "/etc/supervisor/conf.d/supervisord.conf"]



================================================
FILE: json_schema_generator.py
================================================
import json
from typing import Dict, Any
from pydantic import BaseModel

class UserSchema(BaseModel):
    id: int
    name: str
    email: str
    age: int
    is_active: bool

def generate_structured_json(schema: BaseModel, data: Dict[str, Any]) -> str:
    """
    Generate structured JSON output based on provided schema
    
    Args:
        schema: Pydantic model defining the schema structure
        data: Dictionary containing the data to be structured
    
    Returns:
        str: JSON string with structured data
    """
    try:
        # Create instance of schema with provided data
        structured_data = schema(**data)
        # Convert to JSON string
        return json.dumps(structured_data.dict(), indent=2)
    except Exception as e:
        return f"Error generating JSON: {str(e)}"

# Example usage
if __name__ == "__main__":
    sample_data = {
        "id": 1,
        "name": "John Doe",
        "email": "john@example.com",
        "age": 30,
        "is_active": True
    }
    
    json_output = generate_structured_json(UserSchema, sample_data)
    print("Structured JSON Output:")
    print(json_output)



================================================
FILE: langgraph.json
================================================
{
  "python_version": "3.11",
  "dependencies": [
    "./multi_agents"
  ],
  "graphs": {
    "agent": "./multi_agents/agent.py:graph"
  },
  "env": ".env"
}


================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: main.py
================================================
from dotenv import load_dotenv
import logging
from pathlib import Path

# Create logs directory if it doesn't exist
logs_dir = Path("logs")
logs_dir.mkdir(exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        # File handler for general application logs
        logging.FileHandler('logs/app.log'),
        # Stream handler for console output
        logging.StreamHandler()
    ]
)

# Suppress verbose fontTools logging
logging.getLogger('fontTools').setLevel(logging.WARNING)
logging.getLogger('fontTools.subset').setLevel(logging.WARNING)
logging.getLogger('fontTools.ttLib').setLevel(logging.WARNING)

# Create logger instance
logger = logging.getLogger(__name__)

load_dotenv()

from backend.server.app import app

if __name__ == "__main__":
    import uvicorn
    
    logger.info("Starting server...")
    uvicorn.run(app, host="0.0.0.0", port=8000)



================================================
FILE: poetry.toml
================================================
[virtualenvs]
in-project = true


================================================
FILE: Procfile
================================================
web: python -m uvicorn backend.server.server:app --host=0.0.0.0 --port=${PORT}


================================================
FILE: pyproject.toml
================================================
[tool.poetry]
name = "gpt-researcher"
version = "0.14.4"
description = "GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks."
authors = ["Assaf Elovic <assaf.elovic@gmail.com>"]
license = "MIT"
readme = "README.md"

[tool.poetry.dependencies]
python = ">=3.11"
aiofiles = ">=23.2.1"
arxiv = ">=2.0.0"
beautifulsoup4 = ">=4.12.2"
colorama = ">=0.4.6"
duckduckgo_search = ">=4.1.1"
fastapi = ">=0.104.1"
htmldocx = "^0.0.6"
jinja2 = ">=3.1.2"
json-repair = "^0.29.8"
json5 = "^0.9.25"
langchain = "^0.3.18"
langchain_community = "^0.3.17"
langchain-openai = "^0.3.6"
langgraph = ">=0.2.73,<0.3"
loguru = "^0.7.2"
lxml = { version = ">=4.9.2", extras = ["html_clean"] }
markdown = ">=3.5.1"
md2pdf = ">=1.0.1"
mistune = "^3.0.2"
openai = ">=1.3.3"
pydantic = ">=2.5.1"
PyMuPDF = ">=1.23.6"
python-docx = "^1.1.0"
python-dotenv = ">=1.0.0"
python-multipart = ">=0.0.6"
pyyaml = ">=6.0.1"
requests = ">=2.31.0"
SQLAlchemy = ">=2.0.28"
tiktoken = ">=0.7.0"
unstructured = ">=0.13"
uvicorn = ">=0.24.0.post1"
websockets = "^13.1"
# Model Context Protocol support
mcp = { version = ">=1.0.0", markers = "platform_system != 'Windows'" }
langchain-mcp-adapters = ">=0.1.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
asyncio_mode = "strict"
addopts = "-v"
testpaths = ["tests"]
python_files = "test_*.py"
asyncio_fixture_loop_scope = "function"

[tool.uv.sources]
gpt-researcher = { workspace = true }

[project]
name = "gpt-researcher"
version = "0.14.4"
description = "GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks."
authors = [{ name = "Assaf Elovic", email = "assaf.elovic@gmail.com" }]
license = { text = "MIT" }
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "aiofiles>=23.2.1",
    # Core dependencies
    "aiohappyeyeballs>=2.6.1",
    "aiohttp>=3.12.0",
    "aiosignal>=1.3.2",
    "annotated-types>=0.7.0",
    "anyio>=4.9.0",
    "arxiv>=2.0.0",
    "attrs>=25.3.0",
    "backoff>=2.2.1",
    "beautifulsoup4>=4.12.2",
    "brotli>=1.1.0",
    "certifi>=2025.4.26",
    "cffi>=1.17.1",
    "chardet>=5.2.0",
    "charset-normalizer>=3.4.2",
    "click>=8.2.1",
    "colorama>=0.4.6",
    "cryptography>=45.0.2",
    "cssselect2>=0.8.0",
    "dataclasses-json>=0.6.7",
    "distro>=1.9.0",
    "docopt>=0.6.2",
    "duckduckgo-search>=4.1.1",
    "emoji>=2.14.1",
    "fastapi>=0.104.1",
    "feedparser>=6.0.11",
    "filelock>=3.18.0",
    "filetype>=1.2.0",
    "fonttools>=4.58.0",
    "frozenlist>=1.6.0",
    "fsspec>=2025.5.1",
    "greenlet>=3.2.2",
    "h11>=0.16.0",
    "html5lib>=1.1",
    "htmldocx>=0.0.6",
    "httpcore>=1.0.9",
    "httpx>=0.28.1",
    "httpx-aiohttp>=0.1.4",
    "httpx-sse>=0.4.0",
    "huggingface-hub>=0.32.0",
    "idna>=3.10",
    "importlib-metadata>=8.7.0",
    "jinja2>=3.1.6",
    "jiter>=0.10.0",
    "joblib>=1.5.1",
    "json-repair>=0.44.0",
    "json5>=0.12.0",
    "jsonpatch>=1.33",
    "jsonpointer>=3.0.0",
    "jsonschema>=4.23.0",
    "jsonschema-specifications>=2025.4.1",
    "kiwisolver>=1.4.8",
    "langchain-community>=0.3.17",
    "langchain-core>=0.3.61",
    "langchain-ollama>=0.3.3",
    "langchain-openai>=0.3.6",
    "langchain-text-splitters>=0.3.8",
    "langdetect>=1.0.9",
    "langgraph>=0.2.76",
    "langgraph-checkpoint>=2.0.26",
    "langgraph-cli>=0.2.10",
    "langgraph-sdk>=0.1.70",
    "langsmith>=0.3.42",
    "litellm>=1.71.0",
    "loguru>=0.7.3",
    "lxml>=5.4.0",
    "markdown>=3.8",
    "markdown2>=2.5.3",
    "markupsafe>=3.0.2",
    "marshmallow>=3.26.1",
    "mcp>=1.9.1",
    "md2pdf>=1.0.1",
    "mistune>=3.1.3",
    "multidict>=6.4.4",
    "mypy-extensions>=1.1.0",
    "nest-asyncio>=1.6.0",
    "nltk>=3.9.1",
    "numpy>=2.2.6",
    "olefile>=0.47",
    "ollama>=0.4.8",
    "openai>=1.82.0",
    "orjson>=3.10.18",
    "ormsgpack>=1.10.0",
    "packaging>=24.2",
    "pillow>=11.2.1",
    "primp>=0.15.0",
    "propcache>=0.3.1",
    "psutil>=7.0.0",
    "pycparser>=2.22",
    "pydantic>=2.11.5",
    "pydantic-core>=2.33.2",
    "pydantic-settings>=2.9.1",
    "pydyf>=0.11.0",
    "pymupdf>=1.26.0",
    "pypdf>=5.5.0",
    "pyphen>=0.17.2",
    "python-docx>=1.1.2",
    "python-dotenv>=1.1.0",
    "python-iso639>=2025.2.18",
    "python-magic>=0.4.27",
    "python-multipart>=0.0.20",
    "python-oxmsg>=0.0.2",
    "pyyaml>=6.0.2",
    "rapidfuzz>=3.13.0",
    "referencing>=0.36.2",
    "regex>=2024.11.6",
    "requests>=2.32.3",
    "requests-toolbelt>=1.0.0",
    "rpds-py>=0.25.1",
    "sgmllib3k>=1.0.0",
    "six>=1.17.0",
    "sniffio>=1.3.1",
    "soupsieve>=2.7",
    "sqlalchemy>=2.0.41",
    "sse-starlette>=2.3.5",
    "starlette>=0.46.2",
    "tenacity>=9.1.2",
    "tiktoken>=0.9.0",
    "tinycss2>=1.4.0",
    "tinyhtml5>=2.0.0",
    "tokenizers>=0.21.1",
    "tqdm>=4.67.1",
    "typing-extensions>=4.13.2",
    "typing-inspect>=0.9.0",
    "typing-inspection>=0.4.1",
    "unstructured>=0.17.2",
    "unstructured-client>=0.35.0",
    "urllib3>=2.4.0",
    "uvicorn>=0.34.2",
    "weasyprint>=65.1 ; sys_platform != 'win32'",
    "webencodings>=0.5.1",
    "websockets>=15.0.1",
    "win32-setctime>=1.2.0",
    "wrapt>=1.17.2",
    "yarl>=1.20.0",
    "zipp>=3.21.0",
    "zopfli>=0.2.3.post1",
    "zstandard>=0.23.0",
]

[project.optional-dependencies]
requirements-txt = [
    "arxiv_client",
    "azure-storage-blob",
    "duckduckgo_search",
    "exa_py",
    "firecrawl",
    "langchain-anthropic",
    "langchain-cohere",
    "langchain-dashscope",
    "langchain-fireworks",
    "langchain-gigachat",
    "langchain-google-genai",
    "langchain-google-vertexai",
    "langchain-groq",
    "langchain-huggingface",
    "langchain-mistralai",
    "langchain-together",
    "langchain-xai",
    "playwright",
    "scrapy",
    "selenium",
]

[dependency-groups]
dev = [
    "types-aiofiles>=24.1.0.20250516",
    "types-beautifulsoup4>=4.12.0.20250516",
    "types-colorama>=0.4.15.20240311",
    "types-markdown>=3.8.0.20250415",
    "types-requests>=2.32.0.20250515",
]



================================================
FILE: requirements.txt
================================================
aiofiles>=23.2.1
aiohappyeyeballs>=2.6.1
aiohttp>=3.12.0
aiosignal>=1.3.2
annotated-types>=0.7.0
anyio>=4.9.0
arxiv>=2.0.0
attrs>=25.3.0
beautifulsoup4>=4.12.2
brotli>=1.1.0
certifi>=2025.4.26
cffi>=1.17.1
chardet>=5.2.0
charset-normalizer>=3.4.2
click>=8.2.1
colorama>=0.4.6
cryptography>=45.0.2
cssselect2>=0.8.0
dataclasses-json>=0.6.7
distro>=1.9.0
docopt>=0.6.2
duckduckgo-search>=4.1.1
fastapi>=0.104.1
feedparser>=6.0.11
filelock>=3.18.0
filetype>=1.2.0
fonttools>=4.58.0
frozenlist>=1.6.0
fsspec>=2025.5.1
greenlet>=3.2.2
h11>=0.16.0
htmldocx>=0.0.6
httpcore>=1.0.9
httpx>=0.28.1
httpx-aiohttp>=0.1.4
httpx-sse>=0.4.0
huggingface-hub>=0.32.0
idna>=3.10
importlib-metadata>=8.7.0
jinja2>=3.1.6
jiter>=0.10.0
joblib>=1.5.1
json-repair>=0.29.8
json5>=0.9.25
jsonpatch>=1.33
jsonpointer>=3.0.0
jsonschema>=4.23.0
jsonschema-specifications>=2025.4.1
kiwisolver>=1.4.8
langchain-community>=0.3.17
langchain-core>=0.3.61
langchain-ollama>=0.3.3
langchain-openai>=0.3.6
langchain-text-splitters>=0.3.8
langgraph>=0.2.76
langgraph-checkpoint>=2.0.26
langgraph-cli>=0.2.10
langgraph-sdk>=0.1.70
langsmith>=0.3.42
litellm>=1.71.0
loguru>=0.7.2
lxml>=4.9.2
markdown>=3.5.1
markdown2>=2.5.3
markupsafe>=3.0.2
marshmallow>=3.26.1
mcp>=1.9.1
md2pdf>=1.0.1
mistune>=3.0.2
multidict>=6.4.4
mypy-extensions>=1.1.0
nest-asyncio>=1.6.0
numpy>=2.2.6
olefile>=0.47
ollama>=0.4.8
openai>=1.3.3
orjson>=3.10.18
ormsgpack>=1.10.0
packaging>=24.2
pillow>=11.2.1
primp>=0.15.0
propcache>=0.3.1
psutil>=7.0.0
pycparser>=2.22
pydantic>=2.5.1
pydantic-core>=2.33.2
pydantic-settings>=2.9.1
pydyf>=0.11.0
pymupdf>=1.23.6
python-docx>=1.1.0
python-dotenv>=1.0.0
python-multipart>=0.0.6
pyyaml>=6.0.1
rapidfuzz>=3.13.0
referencing>=0.36.2
regex>=2024.11.6
requests>=2.31.0
requests-toolbelt>=1.0.0
rpds-py>=0.25.1
sgmllib3k>=1.0.0
six>=1.17.0
sniffio>=1.3.1
soupsieve>=2.7
sqlalchemy>=2.0.28
sse-starlette>=2.3.5
starlette>=0.46.2
tenacity>=9.1.2
tiktoken>=0.7.0
tinycss2>=1.4.0
tinyhtml5>=2.0.0
tokenizers>=0.21.1
tqdm>=4.67.1
typing-extensions>=4.13.2
typing-inspect>=0.9.0
typing-inspection>=0.4.1
unstructured>=0.13
unstructured-client>=0.35.0
urllib3>=2.4.0
uvicorn>=0.24.0.post1
webencodings>=0.5.1
websockets>=13.1
win32-setctime>=1.2.0
wrapt>=1.17.2
yarl>=1.20.0
zipp>=3.21.0
zopfli>=0.2.3.post1
zstandard>=0.23.0
# Model Context Protocol support (optional)
mcp>=1.0.0
langchain-mcp-adapters>=0.1.0  # LangChain MCP adapters for tool integration
tavily-python>=0.7.12



================================================
FILE: requirements_minimal.txt
================================================
aiofiles>=23.2.1
aiohappyeyeballs>=2.6.1
aiohttp>=3.12.0
aiosignal>=1.3.2
annotated-types>=0.7.0
anyio>=4.9.0
arxiv>=2.0.0
attrs>=25.3.0
backoff>=2.2.1
beautifulsoup4>=4.12.2
brotli>=1.1.0
certifi>=2025.4.26
cffi>=1.17.1
chardet>=5.2.0
charset-normalizer>=3.4.2
click>=8.2.1
colorama>=0.4.6
cryptography>=45.0.2
cssselect2>=0.8.0
dataclasses-json>=0.6.7
distro>=1.9.0
docopt>=0.6.2
duckduckgo_search>=4.1.1
emoji>=2.14.1
fastapi>=0.104.1
feedparser>=6.0.11
filelock>=3.18.0
filetype>=1.2.0
fonttools>=4.58.0
frozenlist>=1.6.0
fsspec>=2025.5.1
greenlet>=3.2.2
h11>=0.16.0
html5lib>=1.1
htmldocx>=0.0.6
httpcore>=1.0.9
httpx-aiohttp>=0.1.4
httpx-sse>=0.4.0
httpx>=0.28.1
huggingface-hub>=0.32.0
idna>=3.10
importlib-metadata>=8.7.0
jinja2>=3.1.2
jiter>=0.10.0
joblib>=1.5.1
json-repair>=0.29.8
json5>=0.9.25
jsonpatch>=1.33
jsonpointer>=3.0.0
jsonschema-specifications>=2025.4.1
jsonschema>=4.23.0
kiwisolver>=1.4.8
langchain_community>=0.3.17
langchain-core>=0.3.60
langchain-ollama>=0.3.3
langchain-openai>=0.3.6
langchain-text-splitters>=0.3.8
langdetect>=1.0.9
langgraph-checkpoint>=2.0.26
langgraph-cli>=0.2.10
langgraph-sdk>=0.1.70
langgraph>=0.2.73,<0.3
langsmith>=0.3.42
litellm>=1.71.0
loguru>=0.7.3
lxml>=5.4.0
markdown>=3.8
markdown2>=2.5.3
markupsafe>=3.0.2
marshmallow>=3.26.1
mcp>=1.9.1
md2pdf>=1.0.1
mistune>=3.1.3
multidict>=6.4.4
mypy-extensions>=1.1.0
nest-asyncio>=1.6.0
nltk>=3.9.1
numpy>=2.2.6
olefile>=0.47
ollama>=0.4.8
openai>=1.82.0
orjson>=3.10.18
ormsgpack>=1.10.0
packaging>=24.2
pillow>=11.2.1
primp>=0.15.0
propcache>=0.3.1
psutil>=7.0.0
pycparser>=2.22
pydantic-core>=2.33.2
pydantic-settings>=2.9.1
pydantic>=2.11.5
pydyf>=0.11.0
pymupdf>=1.26.0
pypdf>=5.5.0
pyphen>=0.17.2
python-docx>=1.1.2
python-dotenv>=1.1.0
python-iso639>=2025.2.18
python-magic>=0.4.27
python-multipart>=0.0.20
python-oxmsg>=0.0.2
pyyaml>=6.0.2
rapidfuzz>=3.13.0
referencing>=0.36.2
regex>=2024.11.6
requests-toolbelt>=1.0.0
requests>=2.32.3
rpds-py>=0.25.1
sgmllib3k>=1.0.0
six>=1.17.0
sniffio>=1.3.1
soupsieve>=2.7
sqlalchemy>=2.0.41
sse-starlette>=2.3.5
starlette>=0.46.2
tenacity>=9.1.2
tiktoken>=0.9.0
tinycss2>=1.4.0
tinyhtml5>=2.0.0
tokenizers>=0.21.1
tqdm>=4.67.1
typing-extensions>=4.13.2
typing-inspect>=0.9.0
typing-inspection>=0.4.1
unstructured-client>=0.35.0
unstructured>=0.17.2
urllib3>=2.4.0
uvicorn>=0.34.2
weasyprint>=65.1; os.name != "nt"
webencodings>=0.5.1
websockets>=15.0.1
win32-setctime>=1.2.0
wrapt>=1.17.2
yarl>=1.20.0
zipp>=3.21.0
zopfli>=0.2.3.post1
zstandard>=0.23.0
tavily-python>=0.7.12


================================================
FILE: setup.py
================================================
from setuptools import find_packages, setup

LATEST_VERSION = "0.14.4"

exclude_packages = [
    "selenium",
    "webdriver",
    "fastapi",
    "fastapi.*",
    "uvicorn",
    "jinja2",
    "gpt-researcher",
    "langgraph"
]

with open(r"README.md", "r", encoding="utf-8") as f:
    long_description = f.read()

with open("requirements.txt", "r") as f:
    reqs = [line.strip() for line in f if not any(pkg in line for pkg in exclude_packages)]

setup(
    name="gpt-researcher",
    version=LATEST_VERSION,
    description="GPT Researcher is an autonomous agent designed for comprehensive web research on any task",
    package_dir={'gpt_researcher': 'gpt_researcher'},
    packages=find_packages(exclude=exclude_packages),
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/assafelovic/gpt-researcher",
    author="Assaf Elovic",
    author_email="assaf.elovic@gmail.com",
    license="MIT",
    classifiers=[
        "License :: OSI Approved :: MIT License",
        "Intended Audience :: Developers",
        "Intended Audience :: Education",
        "Intended Audience :: Science/Research",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Programming Language :: Python :: 3.13",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires='>=3.11',
    install_requires=reqs,


)


================================================
FILE: .cursorignore
================================================
.venv
__pycache__
outputs
.github


================================================
FILE: .dockerignore
================================================
.git
output/



================================================
FILE: .env.example
================================================
OPENAI_API_KEY=
OPENAI_BASE_URL=
TAVILY_API_KEY=
DOC_PATH=./my-docs

# NEXT_PUBLIC_GPTR_API_URL=http://0.0.0.0:8000  # Defaults to localhost:8000 if not set


================================================
FILE: .python-version
================================================
3.11



================================================
FILE: backend/__init__.py
================================================



================================================
FILE: backend/Dockerfile
================================================
FROM python:3.11-slim

WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application
COPY . .

# Expose the port the app will run on
EXPOSE 8000

# Start the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"] 


================================================
FILE: backend/Procfile
================================================
web: uvicorn server.app:app --host 0.0.0.0 --port $PORT --workers 1 


================================================
FILE: backend/requirements.txt
================================================
fastapi>=0.104.1
uvicorn>=0.24.0
pydantic>=2.5.1
httpx>=0.28.1
python-dotenv>=1.0.0
websockets>=13.1
openai>=1.3.3
langchain>=0.3.18
tavily-python
gpt-researcher>=0.14.4
# Note: gpt-researcher should be installed in development mode from the root directory
# Run: pip install -e . from the project root

# Added based on import scan (Please Verify)
aiofiles
mistune
md2pdf
python-docx
htmldocx
langchain-community
python-multipart
Jinja2

# Add any other dependencies your backend requires below
# aiohttp # Example if needed for async requests
# requests # Example if needed for sync requests
# multi_agents # Verify if this is a local module or pip package 


================================================
FILE: backend/run_server.py
================================================
#!/usr/bin/env python3
"""
GPT-Researcher Backend Server Startup Script

Run this to start the research API server.
"""

import uvicorn
import os
import sys

# Add the backend directory to Python path
backend_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, backend_dir)

if __name__ == "__main__":
    # Change to backend directory
    os.chdir(backend_dir)
    
    # Start the server
    uvicorn.run(
        "server.app:app",
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    )






================================================
FILE: backend/runtime.txt
================================================
python-3.11


================================================
FILE: backend/utils.py
================================================
import aiofiles
import urllib
import mistune

async def write_to_file(filename: str, text: str) -> None:
    """Asynchronously write text to a file in UTF-8 encoding.

    Args:
        filename (str): The filename to write to.
        text (str): The text to write.
    """
    # Ensure text is a string
    if not isinstance(text, str):
        text = str(text)

    # Convert text to UTF-8, replacing any problematic characters
    text_utf8 = text.encode('utf-8', errors='replace').decode('utf-8')

    async with aiofiles.open(filename, "w", encoding='utf-8') as file:
        await file.write(text_utf8)

async def write_text_to_md(text: str, filename: str = "") -> str:
    """Writes text to a Markdown file and returns the file path.

    Args:
        text (str): Text to write to the Markdown file.

    Returns:
        str: The file path of the generated Markdown file.
    """
    file_path = f"outputs/{filename[:60]}.md"
    await write_to_file(file_path, text)
    return urllib.parse.quote(file_path)

async def write_md_to_pdf(text: str, filename: str = "") -> str:
    """Converts Markdown text to a PDF file and returns the file path.

    Args:
        text (str): Markdown text to convert.

    Returns:
        str: The encoded file path of the generated PDF.
    """
    file_path = f"outputs/{filename[:60]}.pdf"

    try:
        from md2pdf.core import md2pdf
        md2pdf(file_path,
               md_content=text,
               # md_file_path=f"{file_path}.md",
               css_file_path="./styles/pdf_styles.css",
               base_url=None)
        print(f"Report written to {file_path}")
    except Exception as e:
        print(f"Error in converting Markdown to PDF: {e}")
        return ""

    encoded_file_path = urllib.parse.quote(file_path)
    return encoded_file_path

async def write_md_to_word(text: str, filename: str = "") -> str:
    """Converts Markdown text to a DOCX file and returns the file path.

    Args:
        text (str): Markdown text to convert.

    Returns:
        str: The encoded file path of the generated DOCX.
    """
    file_path = f"outputs/{filename[:60]}.docx"

    try:
        from docx import Document
        from htmldocx import HtmlToDocx
        # Convert report markdown to HTML
        html = mistune.html(text)
        # Create a document object
        doc = Document()
        # Convert the html generated from the report to document format
        HtmlToDocx().add_html_to_document(html, doc)

        # Saving the docx document to file_path
        doc.save(file_path)

        print(f"Report written to {file_path}")

        encoded_file_path = urllib.parse.quote(file_path)
        return encoded_file_path

    except Exception as e:
        print(f"Error in converting Markdown to DOCX: {e}")
        return ""


================================================
FILE: backend/chat/__init__.py
================================================


# Chat package initialization


================================================
FILE: backend/chat/chat.py
================================================
import logging
import os
import uuid
import json
from fastapi import WebSocket
from typing import List, Dict, Any

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import InMemoryVectorStore
from gpt_researcher.memory import Memory
from gpt_researcher.config.config import Config
from gpt_researcher.utils.llm import create_chat_completion
from gpt_researcher.utils.tools import create_chat_completion_with_tools, create_search_tool
from tavily import TavilyClient
from datetime import datetime

# Setup logging
# Get logger instance
logger = logging.getLogger(__name__)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler()  # Only log to console
    ]
)

# Note: LLM client is now handled through GPT Researcher's unified LLM system
# This supports all configured providers (OpenAI, Google Gemini, Anthropic, etc.)

def get_tools():
    """Define tools for LLM function calling (primarily for OpenAI-compatible providers)"""
    tools = [
        {
            "type": "function",
            "function": {
                "name": "quick_search",
                "description": "Search for current events or online information when you need new knowledge that doesn't exist in the current context",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The search query"
                        }
                    },
                    "required": ["query"]
                }
            }
        }
    ]
    return tools

class ChatAgentWithMemory:
    def __init__(
        self,
        report: str,
        config_path="default",
        headers=None,
        vector_store=None
    ):
        self.report = report
        self.headers = headers
        self.config = Config(config_path)
        self.vector_store = vector_store
        self.retriever = None
        self.search_metadata = None
        
        # Initialize Tavily client
        self.tavily_client = TavilyClient(api_key=os.environ.get("TAVILY_API_KEY"))
        
        # Process document and create vector store if not provided
        if not self.vector_store and False:
            self._setup_vector_store()
    
    def _setup_vector_store(self):
        """Setup vector store for document retrieval"""
        # Process document into chunks
        documents = self._process_document(self.report)
        
        # Create unique thread ID
        self.thread_id = str(uuid.uuid4())
        
        # Setup embeddings and vector store
        cfg = Config()
        self.embedding = Memory(
            cfg.embedding_provider,
            cfg.embedding_model,
            **cfg.embedding_kwargs
        ).get_embeddings()
        
        # Create vector store and retriever
        self.vector_store = InMemoryVectorStore(self.embedding)
        self.vector_store.add_texts(documents)
        self.retriever = self.vector_store.as_retriever(k=4)
        
    def _process_document(self, report):
        """Split Report into Chunks"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1024,
            chunk_overlap=20,
            length_function=len,
            is_separator_regex=False,
        )
        documents = text_splitter.split_text(report)
        return documents

    def quick_search(self, query):
        """Perform a web search for current information using Tavily"""
        try:
            logger.info(f"Performing web search for: {query}")
            results = self.tavily_client.search(query=query, max_results=5)
            
            # Store search metadata for frontend
            self.search_metadata = {
                "query": query,
                "sources": [
                    {"title": result.get("title", ""), 
                     "url": result.get("url", ""),
                     "content": result.get("content", "")[:200] + "..." if len(result.get("content", "")) > 200 else result.get("content", "")}
                    for result in results.get("results", [])
                ]
            }
            
            return results
        except Exception as e:
            logger.error(f"Error performing web search: {str(e)}", exc_info=True)
            return {
                "error": str(e),
                "results": []
            }


    async def process_chat_completion(self, messages: List[Dict[str, str]]):
        """Process chat completion using configured LLM provider with tool calling support"""
        # Create a search tool using the utility function
        search_tool = create_search_tool(self.quick_search)
        
        # Use the tool-enabled chat completion utility
        response, tool_calls_metadata = await create_chat_completion_with_tools(
            messages=messages,
            tools=[search_tool],
            model=self.config.smart_llm_model,
            llm_provider=self.config.smart_llm_provider,
            llm_kwargs=self.config.llm_kwargs,
        )
        
        # Process metadata to match the expected format for the chat system
        processed_metadata = []
        for metadata in tool_calls_metadata:
            if metadata.get("tool") == "search_tool":
                # Extract query from args
                query = metadata.get("args", {}).get("query", "")
                
                # Trigger search again to get metadata (the search was already executed by LangChain)
                if query:
                    self.quick_search(query)  # This populates self.search_metadata
                    
                processed_metadata.append({
                    "tool": "quick_search",
                    "query": query,
                    "search_metadata": self.search_metadata
                })
        
        return response, processed_metadata


    async def chat(self, messages, websocket=None):
        """Chat with configured LLM provider (supports OpenAI, Google Gemini, Anthropic, etc.)
        
        Args:
            messages: List of chat messages with role and content
            websocket: Optional websocket for streaming responses
        
        Returns:
            tuple: (str: The AI response message, dict: metadata about tool usage)
        """
        try:
            
            # Format system prompt with the report context
            system_prompt = f"""
            You are GPT Researcher, an autonomous research agent created by an open source community at https://github.com/assafelovic/gpt-researcher, homepage: https://gptr.dev. 
            To learn more about GPT Researcher you can suggest to check out: https://docs.gptr.dev.
            
            This is a chat about a research report that you created. Answer based on the given context and report.
            You must include citations to your answer based on the report.
            
            You may use the quick_search tool when the user asks about information that might require current data 
            not found in the report, such as recent events, updated statistics, or news. If there's no report available,
            you can use the quick_search tool to find information online.
            
            You must respond in markdown format. You must make it readable with paragraphs, tables, etc when possible. 
            Remember that you're answering in a chat not a report.
            
            Assume the current time is: {datetime.now()}.
            
            Report: {self.report}
            
            """
            
            # Format message history for OpenAI input
            formatted_messages = []
            
            # Add system message first
            formatted_messages.append({
                "role": "system", 
                "content": system_prompt
            })
            
            # Add user/assistant message history - filter out non-essential fields
            for msg in messages:
                if 'role' in msg and 'content' in msg:
                    formatted_messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
                else:
                    logger.warning(f"Skipping message with missing role or content: {msg}")
            
            # Process the chat using configured LLM provider
            ai_message, tool_calls_metadata = await self.process_chat_completion(formatted_messages)
            
            # Provide fallback response if message is empty
            if not ai_message:
                logger.warning("No AI message content found in response, using fallback message")
                ai_message = "I apologize, but I couldn't generate a proper response. Please try asking your question again."
            
            logger.info(f"Generated response: {ai_message[:100]}..." if len(ai_message) > 100 else f"Generated response: {ai_message}")
            
            # Return both the message and any metadata about tools used
            return ai_message, tool_calls_metadata
            
        except Exception as e:
            logger.error(f"Error in chat: {str(e)}", exc_info=True)
            raise

    def get_context(self):
        """return the current context of the chat"""
        return self.report



================================================
FILE: backend/memory/__init__.py
================================================



================================================
FILE: backend/memory/draft.py
================================================
from typing import TypedDict, List, Annotated
import operator


class DraftState(TypedDict):
    task: dict
    topic: str
    draft: dict
    review: str
    revision_notes: str


================================================
FILE: backend/memory/research.py
================================================
from typing import TypedDict, List, Annotated
import operator


class ResearchState(TypedDict):
    task: dict
    initial_research: str
    sections: List[str]
    research_data: List[dict]
    # Report layout
    title: str
    headers: dict
    date: str
    table_of_contents: str
    introduction: str
    conclusion: str
    sources: List[str]
    report: str





================================================
FILE: backend/report_type/__init__.py
================================================
from .basic_report.basic_report import BasicReport
from .detailed_report.detailed_report import DetailedReport

__all__ = [
    "BasicReport",
    "DetailedReport"
]


================================================
FILE: backend/report_type/basic_report/__init__.py
================================================



================================================
FILE: backend/report_type/basic_report/basic_report.py
================================================
from fastapi import WebSocket
from typing import Any

from gpt_researcher import GPTResearcher


class BasicReport:
    def __init__(
        self,
        query: str,
        query_domains: list,
        report_type: str,
        report_source: str,
        source_urls,
        document_urls,
        tone: Any,
        config_path: str,
        websocket: WebSocket,
        headers=None,
        mcp_configs=None,
        mcp_strategy=None,
    ):
        self.query = query
        self.query_domains = query_domains
        self.report_type = report_type
        self.report_source = report_source
        self.source_urls = source_urls
        self.document_urls = document_urls
        self.tone = tone
        self.config_path = config_path
        self.websocket = websocket
        self.headers = headers or {}

        # Initialize researcher with optional MCP parameters
        gpt_researcher_params = {
            "query": self.query,
            "query_domains": self.query_domains,
            "report_type": self.report_type,
            "report_source": self.report_source,
            "source_urls": self.source_urls,
            "document_urls": self.document_urls,
            "tone": self.tone,
            "config_path": self.config_path,
            "websocket": self.websocket,
            "headers": self.headers,
        }

        # Add MCP parameters if provided
        if mcp_configs is not None:
            gpt_researcher_params["mcp_configs"] = mcp_configs
        if mcp_strategy is not None:
            gpt_researcher_params["mcp_strategy"] = mcp_strategy

        self.gpt_researcher = GPTResearcher(**gpt_researcher_params)

    async def run(self):
        await self.gpt_researcher.conduct_research()
        report = await self.gpt_researcher.write_report()
        return report



================================================
FILE: backend/report_type/deep_research/README.md
================================================
# Deep Research âœ¨ NEW âœ¨

With the latest "Deep Research" trend in the AI community, we're excited to implement our own Open source deep research capability! Introducing GPT Researcher's Deep Research - an advanced recursive research system that explores topics with unprecedented depth and breadth.

## How It Works

Deep Research employs a fascinating tree-like exploration pattern:

1. **Breadth**: At each level, it generates multiple search queries to explore different aspects of your topic
2. **Depth**: For each branch, it recursively dives deeper, following leads and uncovering connections
3. **Concurrent Processing**: Utilizes async/await patterns to run multiple research paths simultaneously
4. **Smart Context Management**: Automatically aggregates and synthesizes findings across all branches
5. **Progress Tracking**: Real-time updates on research progress across both breadth and depth dimensions

Think of it as deploying a team of AI researchers, each following their own research path while collaborating to build a comprehensive understanding of your topic.

## Process Flow
![deep research](https://github.com/user-attachments/assets/eba2d94b-bef3-4f8d-bbc0-f15bd0a40968)


## Quick Start

```python
from gpt_researcher import GPTResearcher
from gpt_researcher.utils.enum import ReportType, Tone
import asyncio

async def main():
    # Initialize researcher with deep research type
    researcher = GPTResearcher(
        query="What are the latest developments in quantum computing?",
        report_type="deep",  # This triggers deep research modd
    )
    
    # Run research
    research_data = await researcher.conduct_research()
    
    # Generate report
    report = await researcher.write_report()
    print(report)

if __name__ == "__main__":
    asyncio.run(main())
```

## Configuration

Deep Research behavior can be customized through several parameters:

- `deep_research_breadth`: Number of parallel research paths at each level (default: 4)
- `deep_research_depth`: How many levels deep to explore (default: 2)
- `deep_research_concurrency`: Maximum number of concurrent research operations (default: 2)

You can configure these in your config file, pass as environment variables or pass them directly:

```python
researcher = GPTResearcher(
    query="your query",
    report_type="deep",
    config_path="path/to/config.yaml"  # Configure deep research parameters here
)
```

## Progress Tracking

The `on_progress` callback provides real-time insights into the research process:

```python
class ResearchProgress:
    current_depth: int       # Current depth level
    total_depth: int         # Maximum depth to explore
    current_breadth: int     # Current number of parallel paths
    total_breadth: int       # Maximum breadth at each level
    current_query: str       # Currently processing query
    completed_queries: int   # Number of completed queries
    total_queries: int       # Total queries to process
```

## Advanced Usage

### Custom Research Flow

```python
researcher = GPTResearcher(
    query="your query",
    report_type="deep",
    tone=Tone.Objective,
    headers={"User-Agent": "your-agent"},  # Custom headers for web requests
    verbose=True  # Enable detailed logging
)

# Get raw research context
context = await researcher.conduct_research()

# Access research sources
sources = researcher.get_research_sources()

# Get visited URLs
urls = researcher.get_source_urls()

# Generate formatted report
report = await researcher.write_report()
```

### Error Handling

The deep research system is designed to be resilient:

- Failed queries are automatically skipped
- Research continues even if some branches fail
- Progress tracking helps identify any issues

## Best Practices

1. **Start Broad**: Begin with a general query and let the system explore specifics
2. **Monitor Progress**: Use the progress callback to understand the research flow
3. **Adjust Parameters**: Tune breadth and depth based on your needs:
   - More breadth = wider coverage
   - More depth = deeper insights
4. **Resource Management**: Consider concurrency limits based on your system capabilities

## Limitations

- Usage of reasoning LLM models such as `o3-mini`. This means that permissions for reasoning are required and the overall run will be significantly slower.
- Deep research may take longer than standard research
- Higher API usage and costs due to multiple concurrent queries
- May require more system resources for parallel processing

Happy researching! ğŸ‰ 



================================================
FILE: backend/report_type/deep_research/__init__.py
================================================



================================================
FILE: backend/report_type/deep_research/example.py
================================================
from typing import List, Dict, Any, Optional, Set
from fastapi import WebSocket
import asyncio
import logging
from gpt_researcher import GPTResearcher
from gpt_researcher.llm_provider.generic.base import ReasoningEfforts
from gpt_researcher.utils.llm import create_chat_completion
from gpt_researcher.utils.enum import ReportType, ReportSource, Tone

logger = logging.getLogger(__name__)

# Constants for models
GPT4_MODEL = "gpt-4o"  # For standard tasks
O3_MINI_MODEL = "o3-mini"  # For reasoning tasks
LLM_PROVIDER = "openai"

class ResearchProgress:
    def __init__(self, total_depth: int, total_breadth: int):
        self.current_depth = total_depth
        self.total_depth = total_depth
        self.current_breadth = total_breadth
        self.total_breadth = total_breadth
        self.current_query: Optional[str] = None
        self.total_queries = 0
        self.completed_queries = 0

class DeepResearch:
    def __init__(
        self,
        query: str,
        breadth: int = 4,
        depth: int = 2,
        websocket: Optional[WebSocket] = None,
        tone: Tone = Tone.Objective,
        config_path: Optional[str] = None,
        headers: Optional[Dict] = None,
        concurrency_limit: int = 2  # Match TypeScript version
    ):
        self.query = query
        self.breadth = breadth
        self.depth = depth
        self.websocket = websocket
        self.tone = tone
        self.config_path = config_path
        self.headers = headers or {}
        self.visited_urls: Set[str] = set()
        self.learnings: List[str] = []
        self.concurrency_limit = concurrency_limit

    async def generate_feedback(self, query: str, num_questions: int = 3) -> List[str]:
        """Generate follow-up questions to clarify research direction"""
        messages = [
            {"role": "system", "content": "You are an expert researcher helping to clarify research directions."},
            {"role": "user", "content": f"Given the following query from the user, ask some follow up questions to clarify the research direction. Return a maximum of {num_questions} questions, but feel free to return less if the original query is clear. Format each question on a new line starting with 'Question: ': {query}"}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=LLM_PROVIDER,
            model=O3_MINI_MODEL,  # Using reasoning model for better question generation
            temperature=0.7,
            max_tokens=500,
            reasoning_effort=ReasoningEfforts.High.value
        )

        # Parse questions from response
        questions = [q.replace('Question:', '').strip()
                    for q in response.split('\n')
                    if q.strip().startswith('Question:')]
        return questions[:num_questions]

    async def generate_serp_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:
        """Generate SERP queries for research"""
        messages = [
            {"role": "system", "content": "You are an expert researcher generating search queries."},
            {"role": "user", "content": f"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as 'Query: <query>' followed by 'Goal: <goal>' for each pair: {query}"}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=LLM_PROVIDER,
            model=GPT4_MODEL,  # Using GPT-4 for general task
            temperature=0.7,
            max_tokens=1000
        )

        # Parse queries and goals from response
        lines = response.split('\n')
        queries = []
        current_query = {}

        for line in lines:
            line = line.strip()
            if line.startswith('Query:'):
                if current_query:
                    queries.append(current_query)
                current_query = {'query': line.replace('Query:', '').strip()}
            elif line.startswith('Goal:') and current_query:
                current_query['researchGoal'] = line.replace('Goal:', '').strip()

        if current_query:
            queries.append(current_query)

        return queries[:num_queries]

    async def process_serp_result(self, query: str, context: str, num_learnings: int = 3) -> Dict[str, List[str]]:
        """Process research results to extract learnings and follow-up questions"""
        messages = [
            {"role": "system", "content": "You are an expert researcher analyzing search results."},
            {"role": "user", "content": f"Given the following research results for the query '{query}', extract key learnings and suggest follow-up questions. For each learning, include a citation to the source URL if available. Format each learning as 'Learning [source_url]: <insight>' and each question as 'Question: <question>':\n\n{context}"}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=LLM_PROVIDER,
            model=O3_MINI_MODEL,  # Using reasoning model for analysis
            temperature=0.7,
            max_tokens=1000,
            reasoning_effort=ReasoningEfforts.High.value
        )

        # Parse learnings and questions with citations
        lines = response.split('\n')
        learnings = []
        questions = []
        citations = {}

        for line in lines:
            line = line.strip()
            if line.startswith('Learning'):
                # Extract URL if present in square brackets
                import re
                url_match = re.search(r'\[(.*?)\]:', line)
                if url_match:
                    url = url_match.group(1)
                    learning = line.split(':', 1)[1].strip()
                    learnings.append(learning)
                    citations[learning] = url
                else:
                    learnings.append(line.replace('Learning:', '').strip())
            elif line.startswith('Question:'):
                questions.append(line.replace('Question:', '').strip())

        return {
            'learnings': learnings[:num_learnings],
            'followUpQuestions': questions[:num_learnings],
            'citations': citations
        }

    async def deep_research(
        self,
        query: str,
        breadth: int,
        depth: int,
        learnings: List[str] = None,
        citations: Dict[str, str] = None,
        visited_urls: Set[str] = None,
        on_progress = None
    ) -> Dict[str, Any]:
        """Conduct deep iterative research"""
        if learnings is None:
            learnings = []
        if citations is None:
            citations = {}
        if visited_urls is None:
            visited_urls = set()

        progress = ResearchProgress(depth, breadth)

        if on_progress:
            on_progress(progress)

        # Generate search queries
        serp_queries = await self.generate_serp_queries(query, num_queries=breadth)
        progress.total_queries = len(serp_queries)

        all_learnings = learnings.copy()
        all_citations = citations.copy()
        all_visited_urls = visited_urls.copy()

        # Process queries with concurrency limit
        semaphore = asyncio.Semaphore(self.concurrency_limit)

        async def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:
            async with semaphore:
                try:
                    progress.current_query = serp_query['query']
                    if on_progress:
                        on_progress(progress)

                    # Initialize researcher for this query
                    researcher = GPTResearcher(
                        query=serp_query['query'],
                        report_type=ReportType.ResearchReport.value,
                        report_source=ReportSource.Web.value,
                        tone=self.tone,
                        websocket=self.websocket,
                        config_path=self.config_path,
                        headers=self.headers
                    )

                    # Conduct research
                    await researcher.conduct_research()

                    # Get results
                    context = researcher.context
                    visited = set(researcher.visited_urls)

                    # Process results
                    results = await self.process_serp_result(
                        query=serp_query['query'],
                        context=context
                    )

                    # Update progress
                    progress.completed_queries += 1
                    if on_progress:
                        on_progress(progress)

                    return {
                        'learnings': results['learnings'],
                        'visited_urls': visited,
                        'followUpQuestions': results['followUpQuestions'],
                        'researchGoal': serp_query['researchGoal'],
                        'citations': results['citations']
                    }

                except Exception as e:
                    logger.error(f"Error processing query '{serp_query['query']}': {str(e)}")
                    return None

        # Process queries concurrently with limit
        tasks = [process_query(query) for query in serp_queries]
        results = await asyncio.gather(*tasks)
        results = [r for r in results if r is not None]  # Filter out failed queries

        # Collect all results
        for result in results:
            all_learnings.extend(result['learnings'])
            all_visited_urls.update(set(result['visited_urls']))
            all_citations.update(result['citations'])

            # Continue deeper if needed
            if depth > 1:
                new_breadth = max(2, breadth // 2)
                new_depth = depth - 1

                # Create next query from research goal and follow-up questions
                next_query = f"""
                Previous research goal: {result['researchGoal']}
                Follow-up questions: {' '.join(result['followUpQuestions'])}
                """

                # Recursive research
                deeper_results = await self.deep_research(
                    query=next_query,
                    breadth=new_breadth,
                    depth=new_depth,
                    learnings=all_learnings,
                    citations=all_citations,
                    visited_urls=all_visited_urls,
                    on_progress=on_progress
                )

                all_learnings = deeper_results['learnings']
                all_visited_urls = set(deeper_results['visited_urls'])
                all_citations.update(deeper_results['citations'])

        return {
            'learnings': list(set(all_learnings)),
            'visited_urls': list(all_visited_urls),
            'citations': all_citations
        }

    async def run(self, on_progress=None) -> str:
        """Run the deep research process and generate final report"""
        # Get initial feedback
        follow_up_questions = await self.generate_feedback(self.query)

        # Collect answers (this would normally come from user interaction)
        answers = ["Automatically proceeding with research"] * len(follow_up_questions)

        # Combine query and Q&A
        combined_query = f"""
        Initial Query: {self.query}
        Follow-up Questions and Answers:
        {' '.join([f'Q: {q}\nA: {a}' for q, a in zip(follow_up_questions, answers)])}
        """

        # Run deep research
        results = await self.deep_research(
            query=combined_query,
            breadth=self.breadth,
            depth=self.depth,
            on_progress=on_progress
        )

        # Generate final report
        researcher = GPTResearcher(
            query=self.query,
            report_type=ReportType.DetailedReport.value,
            report_source=ReportSource.Web.value,
            tone=self.tone,
            websocket=self.websocket,
            config_path=self.config_path,
            headers=self.headers
        )

        # Prepare context with citations
        context_with_citations = []
        for learning in results['learnings']:
            citation = results['citations'].get(learning, '')
            if citation:
                context_with_citations.append(f"{learning} [Source: {citation}]")
            else:
                context_with_citations.append(learning)

        # Set enhanced context for final report
        researcher.context = "\n".join(context_with_citations)
        researcher.visited_urls = set(results['visited_urls'])

        # Generate report
        report = await researcher.write_report()
        return report


================================================
FILE: backend/report_type/deep_research/main.py
================================================
from gpt_researcher import GPTResearcher
from backend.utils import write_md_to_pdf
import asyncio


async def main(task: str):
    # Progress callback
    def on_progress(progress):
        print(f"Depth: {progress.current_depth}/{progress.total_depth}")
        print(f"Breadth: {progress.current_breadth}/{progress.total_breadth}")
        print(f"Queries: {progress.completed_queries}/{progress.total_queries}")
        if progress.current_query:
            print(f"Current query: {progress.current_query}")
    
    # Initialize researcher with deep research type
    researcher = GPTResearcher(
        query=task,
        report_type="deep",  # This will trigger deep research
    )
    
    # Run research with progress tracking
    print("Starting deep research...")
    context = await researcher.conduct_research(on_progress=on_progress)
    print("\nResearch completed. Generating report...")
    
    # Generate the final report
    report = await researcher.write_report()
    await write_md_to_pdf(report, "deep_research_report")
    print(f"\nFinal Report: {report}")

if __name__ == "__main__":
    query = "What are the most effective ways for beginners to start investing?"
    asyncio.run(main(query))


================================================
FILE: backend/report_type/detailed_report/README.md
================================================
## Detailed Reports

Introducing long and detailed reports, with a completely new architecture inspired by the latest [STORM](https://arxiv.org/abs/2402.14207) paper.

In this method we do the following:

1. Trigger Initial GPT Researcher report based on task
2. Generate subtopics from research summary
3. For each subtopic the headers of the subtopic report are extracted and accumulated
4. For each subtopic a report is generated making sure that any information about the headers accumulated until now are not re-generated.
5. An additional introduction section is written along with a table of contents constructed from the entire report.
6. The final report is constructed by appending these : Intro + Table of contents + Subsection reports


================================================
FILE: backend/report_type/detailed_report/__init__.py
================================================



================================================
FILE: backend/report_type/detailed_report/detailed_report.py
================================================
import asyncio
from typing import List, Dict, Set, Optional, Any
from fastapi import WebSocket

from gpt_researcher import GPTResearcher


class DetailedReport:
    def __init__(
        self,
        query: str,
        report_type: str,
        report_source: str,
        source_urls: List[str] = [],
        document_urls: List[str] = [],
        query_domains: List[str] = [],
        config_path: str = None,
        tone: Any = "",
        websocket: WebSocket = None,
        subtopics: List[Dict] = [],
        headers: Optional[Dict] = None,
        complement_source_urls: bool = False,
        mcp_configs=None,
        mcp_strategy=None,
    ):
        self.query = query
        self.report_type = report_type
        self.report_source = report_source
        self.source_urls = source_urls
        self.document_urls = document_urls
        self.query_domains = query_domains
        self.config_path = config_path
        self.tone = tone
        self.websocket = websocket
        self.subtopics = subtopics
        self.headers = headers or {}
        self.complement_source_urls = complement_source_urls
        
        # Initialize researcher with optional MCP parameters
        gpt_researcher_params = {
            "query": self.query,
            "query_domains": self.query_domains,
            "report_type": "research_report",
            "report_source": self.report_source,
            "source_urls": self.source_urls,
            "document_urls": self.document_urls,
            "config_path": self.config_path,
            "tone": self.tone,
            "websocket": self.websocket,
            "headers": self.headers,
            "complement_source_urls": self.complement_source_urls,
        }

        # Add MCP parameters if provided
        if mcp_configs is not None:
            gpt_researcher_params["mcp_configs"] = mcp_configs
        if mcp_strategy is not None:
            gpt_researcher_params["mcp_strategy"] = mcp_strategy

        self.gpt_researcher = GPTResearcher(**gpt_researcher_params)
        self.existing_headers: List[Dict] = []
        self.global_context: List[str] = []
        self.global_written_sections: List[str] = []
        self.global_urls: Set[str] = set(
            self.source_urls) if self.source_urls else set()

    async def run(self) -> str:
        await self._initial_research()
        subtopics = await self._get_all_subtopics()
        report_introduction = await self.gpt_researcher.write_introduction()
        _, report_body = await self._generate_subtopic_reports(subtopics)
        self.gpt_researcher.visited_urls.update(self.global_urls)
        report = await self._construct_detailed_report(report_introduction, report_body)
        return report

    async def _initial_research(self) -> None:
        await self.gpt_researcher.conduct_research()
        self.global_context = self.gpt_researcher.context
        self.global_urls = self.gpt_researcher.visited_urls

    async def _get_all_subtopics(self) -> List[Dict]:
        subtopics_data = await self.gpt_researcher.get_subtopics()

        all_subtopics = []
        if subtopics_data and subtopics_data.subtopics:
            for subtopic in subtopics_data.subtopics:
                all_subtopics.append({"task": subtopic.task})
        else:
            print(f"Unexpected subtopics data format: {subtopics_data}")

        return all_subtopics

    async def _generate_subtopic_reports(self, subtopics: List[Dict]) -> tuple:
        subtopic_reports = []
        subtopics_report_body = ""

        for subtopic in subtopics:
            result = await self._get_subtopic_report(subtopic)
            if result["report"]:
                subtopic_reports.append(result)
                subtopics_report_body += f"\n\n\n{result['report']}"

        return subtopic_reports, subtopics_report_body

    async def _get_subtopic_report(self, subtopic: Dict) -> Dict[str, str]:
        current_subtopic_task = subtopic.get("task")
        subtopic_assistant = GPTResearcher(
            query=current_subtopic_task,
            query_domains=self.query_domains,
            report_type="subtopic_report",
            report_source=self.report_source,
            websocket=self.websocket,
            headers=self.headers,
            parent_query=self.query,
            subtopics=self.subtopics,
            visited_urls=self.global_urls,
            agent=self.gpt_researcher.agent,
            role=self.gpt_researcher.role,
            tone=self.tone,
            complement_source_urls=self.complement_source_urls,
            source_urls=self.source_urls
        )

        subtopic_assistant.context = list(set(self.global_context))
        await subtopic_assistant.conduct_research()

        draft_section_titles = await subtopic_assistant.get_draft_section_titles(current_subtopic_task)

        if not isinstance(draft_section_titles, str):
            draft_section_titles = str(draft_section_titles)

        parse_draft_section_titles = self.gpt_researcher.extract_headers(draft_section_titles)
        parse_draft_section_titles_text = [header.get(
            "text", "") for header in parse_draft_section_titles]

        relevant_contents = await subtopic_assistant.get_similar_written_contents_by_draft_section_titles(
            current_subtopic_task, parse_draft_section_titles_text, self.global_written_sections
        )

        subtopic_report = await subtopic_assistant.write_report(self.existing_headers, relevant_contents)

        self.global_written_sections.extend(self.gpt_researcher.extract_sections(subtopic_report))
        self.global_context = list(set(subtopic_assistant.context))
        self.global_urls.update(subtopic_assistant.visited_urls)

        self.existing_headers.append({
            "subtopic task": current_subtopic_task,
            "headers": self.gpt_researcher.extract_headers(subtopic_report),
        })

        return {"topic": subtopic, "report": subtopic_report}

    async def _construct_detailed_report(self, introduction: str, report_body: str) -> str:
        toc = self.gpt_researcher.table_of_contents(report_body)
        conclusion = await self.gpt_researcher.write_report_conclusion(report_body)
        conclusion_with_references = self.gpt_researcher.add_references(
            conclusion, self.gpt_researcher.visited_urls)
        report = f"{introduction}\n\n{toc}\n\n{report_body}\n\n{conclusion_with_references}"
        return report



================================================
FILE: backend/server/__init__.py
================================================



================================================
FILE: backend/server/app.py
================================================
import json
import os
from typing import Dict, List, Any
import time
import logging
import sys
import warnings

# Suppress Pydantic V2 migration warnings
warnings.filterwarnings("ignore", message="Valid config keys have changed in V2")
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect, File, UploadFile, BackgroundTasks, HTTPException
from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, HTMLResponse
from pydantic import BaseModel, ConfigDict

# Add the parent directory to sys.path to make sure we can import from server
sys.path.insert(0, os.path.abspath(os.path.dirname(os.path.dirname(__file__))))

from server.websocket_manager import WebSocketManager
from server.server_utils import (
    get_config_dict, sanitize_filename,
    update_environment_variables, handle_file_upload, handle_file_deletion,
    execute_multi_agents, handle_websocket_communication
)

from server.websocket_manager import run_agent
from utils import write_md_to_word, write_md_to_pdf
from gpt_researcher.utils.enum import Tone
from chat.chat import ChatAgentWithMemory

# MongoDB services removed - no database persistence needed

# Setup logging
logger = logging.getLogger(__name__)

# Don't override parent logger settings
logger.propagate = True

# Silence uvicorn reload logs
logging.getLogger("uvicorn.supervisors.ChangeReload").setLevel(logging.WARNING)

# Models


class ResearchRequest(BaseModel):
    task: str
    report_type: str
    report_source: str
    tone: str
    headers: dict | None = None
    repo_name: str
    branch_name: str
    generate_in_background: bool = True


class ChatRequest(BaseModel):
    model_config = ConfigDict(extra="allow")  # Allow extra fields in the request
    
    report: str
    messages: List[Dict[str, Any]]


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    os.makedirs("outputs", exist_ok=True)
    app.mount("/outputs", StaticFiles(directory="outputs"), name="outputs")
    
    # Mount frontend static files
    frontend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "frontend")
    if os.path.exists(frontend_path):
        app.mount("/site", StaticFiles(directory=frontend_path), name="frontend")
        logger.debug(f"Frontend mounted from: {frontend_path}")
        
        # Also mount the static directory directly for assets referenced as /static/
        static_path = os.path.join(frontend_path, "static")
        if os.path.exists(static_path):
            app.mount("/static", StaticFiles(directory=static_path), name="static")
            logger.debug(f"Static assets mounted from: {static_path}")
    else:
        logger.warning(f"Frontend directory not found: {frontend_path}")
    
    logger.info("GPT Researcher API ready - local mode (no database persistence)")
    yield
    # Shutdown
    logger.info("Research API shutting down")

# App initialization
app = FastAPI(lifespan=lifespan)

# Configure allowed origins for CORS
ALLOWED_ORIGINS = [
    "http://localhost:3000",   # Local development
    "http://127.0.0.1:3000",   # Local development alternative
    "https://app.gptr.dev",    # Production frontend
    "*",                      # Allow all origins for testing
]

# Standard JSON response - no custom MongoDB encoding needed

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Use default JSON response class

# Mount static files for frontend
# Get the absolute path to the frontend directory
frontend_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "frontend"))

# Mount static directories
app.mount("/static", StaticFiles(directory=os.path.join(frontend_dir, "static")), name="static")
app.mount("/site", StaticFiles(directory=frontend_dir), name="site")

# WebSocket manager
manager = WebSocketManager()

# Constants
DOC_PATH = os.getenv("DOC_PATH", "./my-docs")

# Startup event


# Lifespan events now handled in the lifespan context manager above


# Routes
@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    """Serve the main frontend HTML page."""
    frontend_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "frontend"))
    index_path = os.path.join(frontend_dir, "index.html")
    
    if not os.path.exists(index_path):
        raise HTTPException(status_code=404, detail="Frontend index.html not found")
    
    with open(index_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    return HTMLResponse(content=content)

@app.get("/report/{research_id}")
async def read_report(request: Request, research_id: str):
    docx_path = os.path.join('outputs', f"{research_id}.docx")
    if not os.path.exists(docx_path):
        return {"message": "Report not found."}
    return FileResponse(docx_path)


# Simplified API routes - no database persistence
@app.get("/api/reports")
async def get_all_reports(report_ids: str = None):
    """Get research reports - returns empty list since no database."""
    logger.debug("No database configured - returning empty reports list")
    return {"reports": []}


@app.get("/api/reports/{research_id}")
async def get_report_by_id(research_id: str):
    """Get a specific research report by ID - no database configured."""
    logger.debug(f"No database configured - cannot retrieve report {research_id}")
    raise HTTPException(status_code=404, detail="Report not found")


@app.post("/api/reports")
async def create_or_update_report(request: Request):
    """Create or update a research report - no database persistence."""
    try:
        data = await request.json()
        research_id = data.get("id", "temp_id")
        logger.debug(f"Report creation requested for ID: {research_id} - no database configured, not persisted")
        return {"success": True, "id": research_id}
    except Exception as e:
        logger.error(f"Error processing report creation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def write_report(research_request: ResearchRequest, research_id: str = None):
    report_information = await run_agent(
        task=research_request.task,
        report_type=research_request.report_type,
        report_source=research_request.report_source,
        source_urls=[],
        document_urls=[],
        tone=Tone[research_request.tone],
        websocket=None,
        stream_output=None,
        headers=research_request.headers,
        query_domains=[],
        config_path="",
        return_researcher=True
    )

    docx_path = await write_md_to_word(report_information[0], research_id)
    pdf_path = await write_md_to_pdf(report_information[0], research_id)
    if research_request.report_type != "multi_agents":
        report, researcher = report_information
        response = {
            "research_id": research_id,
            "research_information": {
                "source_urls": researcher.get_source_urls(),
                "research_costs": researcher.get_costs(),
                "visited_urls": list(researcher.visited_urls),
                "research_images": researcher.get_research_images(),
                # "research_sources": researcher.get_research_sources(),  # Raw content of sources may be very large
            },
            "report": report,
            "docx_path": docx_path,
            "pdf_path": pdf_path
        }
    else:
        response = { "research_id": research_id, "report": "", "docx_path": docx_path, "pdf_path": pdf_path }

    return response

@app.post("/report/")
async def generate_report(research_request: ResearchRequest, background_tasks: BackgroundTasks):
    research_id = sanitize_filename(f"task_{int(time.time())}_{research_request.task}")

    if research_request.generate_in_background:
        background_tasks.add_task(write_report, research_request=research_request, research_id=research_id)
        return {"message": "Your report is being generated in the background. Please check back later.",
                "research_id": research_id}
    else:
        response = await write_report(research_request, research_id)
        return response


@app.get("/files/")
async def list_files():
    if not os.path.exists(DOC_PATH):
        os.makedirs(DOC_PATH, exist_ok=True)
    files = os.listdir(DOC_PATH)
    print(f"Files in {DOC_PATH}: {files}")
    return {"files": files}


@app.post("/api/multi_agents")
async def run_multi_agents():
    return await execute_multi_agents(manager)


@app.post("/upload/")
async def upload_file(file: UploadFile = File(...)):
    return await handle_file_upload(file, DOC_PATH)


@app.delete("/files/{filename}")
async def delete_file(filename: str):
    return await handle_file_deletion(filename, DOC_PATH)


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        await handle_websocket_communication(websocket, manager)
    except WebSocketDisconnect as e:
        # Disconnect with more detailed logging about the WebSocket disconnect reason
        logger.info(f"WebSocket disconnected with code {e.code} and reason: '{e.reason}'")
        await manager.disconnect(websocket)
    except Exception as e:
        # More general exception handling
        logger.error(f"Unexpected WebSocket error: {str(e)}")
        await manager.disconnect(websocket)

@app.post("/api/chat")
async def chat(chat_request: ChatRequest):
    """Process a chat request with a report and message history.

    Args:
        chat_request: ChatRequest object containing report text and message history

    Returns:
        JSON response with the assistant's message and any tool usage metadata
    """
    try:
        logger.info(f"Received chat request with {len(chat_request.messages)} messages")

        # Create chat agent with the report
        chat_agent = ChatAgentWithMemory(
            report=chat_request.report,
            config_path="default",
            headers=None
        )

        # Process the chat and get response with metadata
        response_content, tool_calls_metadata = await chat_agent.chat(chat_request.messages, None)
        logger.info(f"response_content: {response_content}")
        logger.info(f"Got chat response of length: {len(response_content) if response_content else 0}")
        
        if tool_calls_metadata:
            logger.info(f"Tool calls used: {json.dumps(tool_calls_metadata)}")

        # Format response as a ChatMessage object with role, content, timestamp and metadata
        response_message = {
            "role": "assistant",
            "content": response_content,
            "timestamp": int(time.time() * 1000),  # Current time in milliseconds
            "metadata": {
                "tool_calls": tool_calls_metadata
            } if tool_calls_metadata else None
        }

        logger.info(f"Returning formatted response: {json.dumps(response_message)[:100]}...")
        return {"response": response_message}
    except Exception as e:
        logger.error(f"Error processing chat request: {str(e)}", exc_info=True)
        return {"error": str(e)}

@app.post("/api/reports/{research_id}/chat")
async def research_report_chat(research_id: str, request: Request):
    """Handle chat requests for a specific research report.
    Directly processes the raw request data to avoid validation errors.
    """
    try:
        # Get raw JSON data from request
        data = await request.json()
        
        # Create chat agent with the report
        chat_agent = ChatAgentWithMemory(
            report=data.get("report", ""),
            config_path="default",
            headers=None
        )

        # Process the chat and get response with metadata
        response_content, tool_calls_metadata = await chat_agent.chat(data.get("messages", []), None)
        
        if tool_calls_metadata:
            logger.info(f"Tool calls used: {json.dumps(tool_calls_metadata)}")

        # Format response as a ChatMessage object
        response_message = {
            "role": "assistant",
            "content": response_content,
            "timestamp": int(time.time() * 1000),
            "metadata": {
                "tool_calls": tool_calls_metadata
            } if tool_calls_metadata else None
        }

        return {"response": response_message}
    except Exception as e:
        logger.error(f"Error in research report chat: {str(e)}", exc_info=True)
        return {"error": str(e)}

@app.put("/api/reports/{research_id}")
async def update_report(research_id: str, request: Request):
    """Update a specific research report by ID - no database configured."""
    logger.debug(f"Update requested for report {research_id} - no database configured, not persisted")
    return {"success": True, "id": research_id}

@app.delete("/api/reports/{research_id}")
async def delete_report(research_id: str):
    """Delete a specific research report by ID - no database configured."""
    logger.debug(f"Delete requested for report {research_id} - no database configured, nothing to delete")
    return {"success": True, "id": research_id}



================================================
FILE: backend/server/logging_config.py
================================================
import logging
import json
import os
from datetime import datetime
from pathlib import Path

class JSONResearchHandler:
    def __init__(self, json_file):
        self.json_file = json_file
        self.research_data = {
            "timestamp": datetime.now().isoformat(),
            "events": [],
            "content": {
                "query": "",
                "sources": [],
                "context": [],
                "report": "",
                "costs": 0.0
            }
        }

    def log_event(self, event_type: str, data: dict):
        self.research_data["events"].append({
            "timestamp": datetime.now().isoformat(),
            "type": event_type,
            "data": data
        })
        self._save_json()

    def update_content(self, key: str, value):
        self.research_data["content"][key] = value
        self._save_json()

    def _save_json(self):
        with open(self.json_file, 'w') as f:
            json.dump(self.research_data, f, indent=2)

def setup_research_logging():
    # Create logs directory if it doesn't exist
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # Generate timestamp for log files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create log file paths
    log_file = logs_dir / f"research_{timestamp}.log"
    json_file = logs_dir / f"research_{timestamp}.json"
    
    # Configure file handler for research logs
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    
    # Get research logger and configure it
    research_logger = logging.getLogger('research')
    research_logger.setLevel(logging.INFO)
    
    # Remove any existing handlers to avoid duplicates
    research_logger.handlers.clear()
    
    # Add file handler
    research_logger.addHandler(file_handler)
    
    # Add stream handler for console output
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    research_logger.addHandler(console_handler)
    
    # Prevent propagation to root logger to avoid duplicate logs
    research_logger.propagate = False
    
    # Create JSON handler
    json_handler = JSONResearchHandler(json_file)
    
    return str(log_file), str(json_file), research_logger, json_handler

# Create a function to get the logger and JSON handler
def get_research_logger():
    return logging.getLogger('research')

def get_json_handler():
    return getattr(logging.getLogger('research'), 'json_handler', None)


================================================
FILE: backend/server/server_utils.py
================================================
import asyncio
import json
import os
import re
import time
import shutil
import traceback
from typing import Awaitable, Dict, List, Any
from fastapi.responses import JSONResponse, FileResponse
from gpt_researcher.document.document import DocumentLoader
from gpt_researcher import GPTResearcher
from utils import write_md_to_pdf, write_md_to_word, write_text_to_md
from pathlib import Path
from datetime import datetime
from fastapi import HTTPException
import logging

logger = logging.getLogger(__name__)

class CustomLogsHandler:
    """Custom handler to capture streaming logs from the research process"""
    def __init__(self, websocket, task: str):
        self.logs = []
        self.websocket = websocket
        sanitized_filename = sanitize_filename(f"task_{int(time.time())}_{task}")
        self.log_file = os.path.join("outputs", f"{sanitized_filename}.json")
        self.timestamp = datetime.now().isoformat()
        # Initialize log file with metadata
        os.makedirs("outputs", exist_ok=True)
        with open(self.log_file, 'w') as f:
            json.dump({
                "timestamp": self.timestamp,
                "events": [],
                "content": {
                    "query": "",
                    "sources": [],
                    "context": [],
                    "report": "",
                    "costs": 0.0
                }
            }, f, indent=2)

    async def send_json(self, data: Dict[str, Any]) -> None:
        """Store log data and send to websocket"""
        # Send to websocket for real-time display
        if self.websocket:
            await self.websocket.send_json(data)
            
        # Read current log file
        with open(self.log_file, 'r') as f:
            log_data = json.load(f)
            
        # Update appropriate section based on data type
        if data.get('type') == 'logs':
            log_data['events'].append({
                "timestamp": datetime.now().isoformat(),
                "type": "event",
                "data": data
            })
        else:
            # Update content section for other types of data
            log_data['content'].update(data)
            
        # Save updated log file
        with open(self.log_file, 'w') as f:
            json.dump(log_data, f, indent=2)


class Researcher:
    def __init__(self, query: str, report_type: str = "research_report"):
        self.query = query
        self.report_type = report_type
        # Generate unique ID for this research task
        self.research_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(query)}"
        # Initialize logs handler with research ID
        self.logs_handler = CustomLogsHandler(None, self.research_id)
        self.researcher = GPTResearcher(
            query=query,
            report_type=report_type,
            websocket=self.logs_handler
        )

    async def research(self) -> dict:
        """Conduct research and return paths to generated files"""
        await self.researcher.conduct_research()
        report = await self.researcher.write_report()
        
        # Generate the files
        sanitized_filename = sanitize_filename(f"task_{int(time.time())}_{self.query}")
        file_paths = await generate_report_files(report, sanitized_filename)
        
        # Get the JSON log path that was created by CustomLogsHandler
        json_relative_path = os.path.relpath(self.logs_handler.log_file)
        
        return {
            "output": {
                **file_paths,  # Include PDF, DOCX, and MD paths
                "json": json_relative_path
            }
        }

def sanitize_filename(filename: str) -> str:
    # Split into components
    prefix, timestamp, *task_parts = filename.split('_')
    task = '_'.join(task_parts)
    
    # Calculate max length for task portion
    # 255 - len(os.getcwd()) - len("\\gpt-researcher\\outputs\\") - len("task_") - len(timestamp) - len("_.json") - safety_margin
    max_task_length = 255 - len(os.getcwd()) - 24 - 5 - 10 - 6 - 5  # ~189 chars for task
    
    # Truncate task if needed
    truncated_task = task[:max_task_length] if len(task) > max_task_length else task
    
    # Reassemble and clean the filename
    sanitized = f"{prefix}_{timestamp}_{truncated_task}"
    return re.sub(r"[^\w\s-]", "", sanitized).strip()


async def handle_start_command(websocket, data: str, manager):
    json_data = json.loads(data[6:])
    (
        task,
        report_type,
        source_urls,
        document_urls,
        tone,
        headers,
        report_source,
        query_domains,
        mcp_enabled,
        mcp_strategy,
        mcp_configs,
    ) = extract_command_data(json_data)

    if not task or not report_type:
        print("Error: Missing task or report_type")
        return

    # Create logs handler with websocket and task
    logs_handler = CustomLogsHandler(websocket, task)
    # Initialize log content with query
    await logs_handler.send_json({
        "query": task,
        "sources": [],
        "context": [],
        "report": ""
    })

    sanitized_filename = sanitize_filename(f"task_{int(time.time())}_{task}")

    report = await manager.start_streaming(
        task,
        report_type,
        report_source,
        source_urls,
        document_urls,
        tone,
        websocket,
        headers,
        query_domains,
        mcp_enabled,
        mcp_strategy,
        mcp_configs,
    )
    report = str(report)
    file_paths = await generate_report_files(report, sanitized_filename)
    # Add JSON log path to file_paths
    file_paths["json"] = os.path.relpath(logs_handler.log_file)
    await send_file_paths(websocket, file_paths)


async def handle_human_feedback(data: str):
    feedback_data = json.loads(data[14:])  # Remove "human_feedback" prefix
    print(f"Received human feedback: {feedback_data}")
    # TODO: Add logic to forward the feedback to the appropriate agent or update the research state

async def generate_report_files(report: str, filename: str) -> Dict[str, str]:
    pdf_path = await write_md_to_pdf(report, filename)
    docx_path = await write_md_to_word(report, filename)
    md_path = await write_text_to_md(report, filename)
    return {"pdf": pdf_path, "docx": docx_path, "md": md_path}


async def send_file_paths(websocket, file_paths: Dict[str, str]):
    await websocket.send_json({"type": "path", "output": file_paths})


def get_config_dict(
    langchain_api_key: str, openai_api_key: str, tavily_api_key: str,
    google_api_key: str, google_cx_key: str, bing_api_key: str,
    searchapi_api_key: str, serpapi_api_key: str, serper_api_key: str, searx_url: str
) -> Dict[str, str]:
    return {
        "LANGCHAIN_API_KEY": langchain_api_key or os.getenv("LANGCHAIN_API_KEY", ""),
        "OPENAI_API_KEY": openai_api_key or os.getenv("OPENAI_API_KEY", ""),
        "TAVILY_API_KEY": tavily_api_key or os.getenv("TAVILY_API_KEY", ""),
        "GOOGLE_API_KEY": google_api_key or os.getenv("GOOGLE_API_KEY", ""),
        "GOOGLE_CX_KEY": google_cx_key or os.getenv("GOOGLE_CX_KEY", ""),
        "BING_API_KEY": bing_api_key or os.getenv("BING_API_KEY", ""),
        "SEARCHAPI_API_KEY": searchapi_api_key or os.getenv("SEARCHAPI_API_KEY", ""),
        "SERPAPI_API_KEY": serpapi_api_key or os.getenv("SERPAPI_API_KEY", ""),
        "SERPER_API_KEY": serper_api_key or os.getenv("SERPER_API_KEY", ""),
        "SEARX_URL": searx_url or os.getenv("SEARX_URL", ""),
        "LANGCHAIN_TRACING_V2": os.getenv("LANGCHAIN_TRACING_V2", "true"),
        "DOC_PATH": os.getenv("DOC_PATH", "./my-docs"),
        "RETRIEVER": os.getenv("RETRIEVER", ""),
        "EMBEDDING_MODEL": os.getenv("OPENAI_EMBEDDING_MODEL", "")
    }


def update_environment_variables(config: Dict[str, str]):
    for key, value in config.items():
        os.environ[key] = value


async def handle_file_upload(file, DOC_PATH: str) -> Dict[str, str]:
    file_path = os.path.join(DOC_PATH, os.path.basename(file.filename))
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    print(f"File uploaded to {file_path}")

    document_loader = DocumentLoader(DOC_PATH)
    await document_loader.load()

    return {"filename": file.filename, "path": file_path}


async def handle_file_deletion(filename: str, DOC_PATH: str) -> JSONResponse:
    file_path = os.path.join(DOC_PATH, os.path.basename(filename))
    if os.path.exists(file_path):
        os.remove(file_path)
        print(f"File deleted: {file_path}")
        return JSONResponse(content={"message": "File deleted successfully"})
    else:
        print(f"File not found: {file_path}")
        return JSONResponse(status_code=404, content={"message": "File not found"})


async def execute_multi_agents(manager) -> Any:
    websocket = manager.active_connections[0] if manager.active_connections else None
    if websocket:
        report = await run_research_task("Is AI in a hype cycle?", websocket, stream_output)
        return {"report": report}
    else:
        return JSONResponse(status_code=400, content={"message": "No active WebSocket connection"})


async def handle_websocket_communication(websocket, manager):
    running_task: asyncio.Task | None = None

    def run_long_running_task(awaitable: Awaitable) -> asyncio.Task:
        async def safe_run():
            try:
                await awaitable
            except asyncio.CancelledError:
                logger.info("Task cancelled.")
                raise
            except Exception as e:
                logger.error(f"Error running task: {e}\n{traceback.format_exc()}")
                await websocket.send_json(
                    {
                        "type": "logs",
                        "content": "error",
                        "output": f"Error: {e}",
                    }
                )

        return asyncio.create_task(safe_run())

    try:
        while True:
            try:
                data = await websocket.receive_text()
                logger.info(f"Received WebSocket message: {data[:50]}..." if len(data) > 50 else data)
                
                if data == "ping":
                    await websocket.send_text("pong")
                elif running_task and not running_task.done():
                    # discard any new request if a task is already running
                    logger.warning(
                        f"Received request while task is already running. Request data preview: {data[: min(20, len(data))]}..."
                    )
                    await websocket.send_json(
                        {
                            "type": "logs",
                            "content": "warning",
                            "output": "Task already running. Please wait.",
                        }
                    )
                # Normalize command detection by checking startswith after stripping whitespace
                elif data.strip().startswith("start"):
                    logger.info(f"Processing start command")
                    running_task = run_long_running_task(
                        handle_start_command(websocket, data, manager)
                    )
                elif data.strip().startswith("human_feedback"):
                    logger.info(f"Processing human_feedback command")
                    running_task = run_long_running_task(handle_human_feedback(data))
                else:
                    error_msg = f"Error: Unknown command or not enough parameters provided. Received: '{data[:100]}...'" if len(data) > 100 else f"Error: Unknown command or not enough parameters provided. Received: '{data}'"
                    logger.error(error_msg)
                    print(error_msg)
                    await websocket.send_json({
                        "type": "error",
                        "content": "error",
                        "output": "Unknown command received by server"
                    })
            except Exception as e:
                logger.error(f"WebSocket error: {str(e)}\n{traceback.format_exc()}")
                print(f"WebSocket error: {e}")
                break
    finally:
        if running_task and not running_task.done():
            running_task.cancel()

def extract_command_data(json_data: Dict) -> tuple:
    return (
        json_data.get("task"),
        json_data.get("report_type"),
        json_data.get("source_urls"),
        json_data.get("document_urls"),
        json_data.get("tone"),
        json_data.get("headers", {}),
        json_data.get("report_source"),
        json_data.get("query_domains", []),
        json_data.get("mcp_enabled", False),
        json_data.get("mcp_strategy", "fast"),
        json_data.get("mcp_configs", []),
    )



================================================
FILE: backend/server/websocket_manager.py
================================================
import asyncio
import datetime
import json
import logging
import traceback
from typing import Dict, List

from fastapi import WebSocket

from report_type import BasicReport, DetailedReport

from gpt_researcher.utils.enum import ReportType, Tone
from gpt_researcher.actions import stream_output  # Import stream_output
from .server_utils import CustomLogsHandler

logger = logging.getLogger(__name__)

class WebSocketManager:
    """Manage websockets"""

    def __init__(self):
        """Initialize the WebSocketManager class."""
        self.active_connections: List[WebSocket] = []
        self.sender_tasks: Dict[WebSocket, asyncio.Task] = {}
        self.message_queues: Dict[WebSocket, asyncio.Queue] = {}

    async def start_sender(self, websocket: WebSocket):
        """Start the sender task."""
        queue = self.message_queues.get(websocket)
        if not queue:
            return

        while True:
            try:
                message = await queue.get()
                if message is None:  # Shutdown signal
                    break
                    
                if websocket in self.active_connections:
                    if message == "ping":
                        await websocket.send_text("pong")
                    else:
                        await websocket.send_text(message)
                else:
                    break
            except Exception as e:
                print(f"Error in sender task: {e}")
                break

    async def connect(self, websocket: WebSocket):
        """Connect a websocket."""
        try:
            await websocket.accept()
            self.active_connections.append(websocket)
            self.message_queues[websocket] = asyncio.Queue()
            self.sender_tasks[websocket] = asyncio.create_task(
                self.start_sender(websocket))
        except Exception as e:
            print(f"Error connecting websocket: {e}")
            if websocket in self.active_connections:
                await self.disconnect(websocket)

    async def disconnect(self, websocket: WebSocket):
        """Disconnect a websocket."""
        try:
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
                
                # Cancel sender task if it exists
                if websocket in self.sender_tasks:
                    try:
                        self.sender_tasks[websocket].cancel()
                        await self.message_queues[websocket].put(None)
                    except Exception as e:
                        logger.error(f"Error canceling sender task: {e}")
                    finally:
                        # Always try to clean up regardless of errors
                        if websocket in self.sender_tasks:
                            del self.sender_tasks[websocket]
                
                # Clean up message queue
                if websocket in self.message_queues:
                    del self.message_queues[websocket]
                
                # Finally close the WebSocket
                try:
                    await websocket.close()
                except Exception as e:
                    logger.info(f"WebSocket already closed: {e}")
        except Exception as e:
            logger.error(f"Error during WebSocket disconnection: {e}")
            # Still try to close the connection if possible
            try:
                await websocket.close()
            except:
                pass  # If this fails too, there's nothing more we can do

    async def start_streaming(self, task, report_type, report_source, source_urls, document_urls, tone, websocket, headers=None, query_domains=[], mcp_enabled=False, mcp_strategy="fast", mcp_configs=[]):
        """Start streaming the output."""
        tone = Tone[tone]
        # add customized JSON config file path here
        config_path = "default"

        # Pass MCP parameters to run_agent
        report = await run_agent(
            task, report_type, report_source, source_urls, document_urls, tone, websocket, 
            headers=headers, query_domains=query_domains, config_path=config_path,
            mcp_enabled=mcp_enabled, mcp_strategy=mcp_strategy, mcp_configs=mcp_configs
        )
        return report

async def run_agent(task, report_type, report_source, source_urls, document_urls, tone: Tone, websocket, stream_output=stream_output, headers=None, query_domains=[], config_path="", return_researcher=False, mcp_enabled=False, mcp_strategy="fast", mcp_configs=[]):
    """Run the agent."""    
    # Create logs handler for this research task
    logs_handler = CustomLogsHandler(websocket, task)

    # Set up MCP configuration if enabled
    if mcp_enabled and mcp_configs:
        import os
        current_retriever = os.getenv("RETRIEVER", "tavily")
        if "mcp" not in current_retriever:
            # Add MCP to existing retrievers
            os.environ["RETRIEVER"] = f"{current_retriever},mcp"

        # Set MCP strategy
        os.environ["MCP_STRATEGY"] = mcp_strategy

        print(f"ğŸ”§ MCP enabled with strategy '{mcp_strategy}' and {len(mcp_configs)} server(s)")
        await logs_handler.send_json({
            "type": "logs",
            "content": "mcp_init",
            "output": f"ğŸ”§ MCP enabled with strategy '{mcp_strategy}' and {len(mcp_configs)} server(s)"
        })

    # Initialize researcher based on report type
    if report_type == "multi_agents":
        report = await run_research_task(
            query=task, 
            websocket=logs_handler,  # Use logs_handler instead of raw websocket
            stream_output=stream_output, 
            tone=tone, 
            headers=headers
        )
        report = report.get("report", "")

    elif report_type == ReportType.DetailedReport.value:
        researcher = DetailedReport(
            query=task,
            query_domains=query_domains,
            report_type=report_type,
            report_source=report_source,
            source_urls=source_urls,
            document_urls=document_urls,
            tone=tone,
            config_path=config_path,
            websocket=logs_handler,  # Use logs_handler instead of raw websocket
            headers=headers,
            mcp_configs=mcp_configs if mcp_enabled else None,
            mcp_strategy=mcp_strategy if mcp_enabled else None,
        )
        report = await researcher.run()
        
    else:
        researcher = BasicReport(
            query=task,
            query_domains=query_domains,
            report_type=report_type,
            report_source=report_source,
            source_urls=source_urls,
            document_urls=document_urls,
            tone=tone,
            config_path=config_path,
            websocket=logs_handler,  # Use logs_handler instead of raw websocket
            headers=headers,
            mcp_configs=mcp_configs if mcp_enabled else None,
            mcp_strategy=mcp_strategy if mcp_enabled else None,
        )
        report = await researcher.run()

    if report_type != "multi_agents" and return_researcher:
        return report, researcher.gpt_researcher
    else:
        return report



================================================
FILE: backend/styles/pdf_styles.css
================================================
body {
    font-family: 'Libre Baskerville', serif;
    font-size: 12pt; /* standard size for academic papers */
    line-height: 1.6; /* for readability */
    color: #333; /* softer on the eyes than black */
    background-color: #fff; /* white background */
    margin: 0;
    padding: 0;
}

h1, h2, h3, h4, h5, h6 {
    font-family: 'Libre Baskerville', serif;
    color: #000; /* darker than the body text */
    margin-top: 1em; /* space above headers */
}

h1 {
    font-size: 2em; /* make h1 twice the size of the body text */
}

h2 {
    font-size: 1.5em;
}

/* Add some space between paragraphs */
p {
    margin-bottom: 1em;
}

/* Style for blockquotes, often used in academic papers */
blockquote {
    font-style: italic;
    margin: 1em 0;
    padding: 1em;
    background-color: #f9f9f9; /* a light grey background */
}

/* You might want to style tables, figures, etc. too */
table {
    border-collapse: collapse;
    width: 100%;
}

table, th, td {
    border: 1px solid #ddd;
    text-align: left;
    padding: 8px;
}

th {
    background-color: #f2f2f2;
    color: black;
}


================================================
FILE: docs/README.md
================================================
# Website

This website is built using [Docusaurus 2](https://docusaurus.io/), a modern static website generator.

## Prerequisites

To build and test documentation locally, begin by downloading and installing [Node.js](https://nodejs.org/en/download/), and then installing [Yarn](https://classic.yarnpkg.com/en/).
On Windows, you can install via the npm package manager (npm) which comes bundled with Node.js:

```console
npm install --global yarn
```

## Installation

```console
pip install pydoc-markdown
cd website
yarn install
```

## Local Development

Navigate to the website folder and run:

```console
pydoc-markdown
yarn start
```

This command starts a local development server and opens up a browser window. Most changes are reflected live without having to restart the server.



================================================
FILE: docs/babel.config.js
================================================
module.exports = {
  presets: [require.resolve('@docusaurus/core/lib/babel/preset')],
};



================================================
FILE: docs/CNAME
================================================
docs.gptr.dev


================================================
FILE: docs/docusaurus.config.js
================================================
/** @type {import('@docusaurus/types').DocusaurusConfig} */
const math = require('remark-math');
const katex = require('rehype-katex');

module.exports = {
  title: 'GPT Researcher',
  tagline: 'The leading autonomous AI research agent',
  url: 'https://docs.gptr.dev',
  baseUrl: '/',
  onBrokenLinks: 'ignore',
  //deploymentBranch: 'master',
  onBrokenMarkdownLinks: 'warn',
  favicon: 'img/gptr-logo.png',
  organizationName: 'assafelovic',
  trailingSlash: false,
  projectName: 'gpt-researcher',
  themeConfig: {
    navbar: {
      title: 'GPT Researcher',
      logo: {
        alt: 'GPT Researcher',
        src: 'img/gptr-logo.png',
      },
      items: [
        {
          type: 'doc',
          docId: 'welcome',
          position: 'left',
          label: 'Docs',
        },

        {to: 'blog', label: 'Blog', position: 'left'},
        {
          type: 'doc',
          docId: 'faq',
          position: 'left',
          label: 'FAQ',
        },
        {
            href: 'mailto:assaf.elovic@gmail.com',
            position: 'left',
            label: 'Contact',
        },
        {
          href: 'https://github.com/assafelovic/gpt-researcher',
          label: 'GitHub',
          position: 'right',
        },
      ],
    },
    footer: {
      style: 'dark',
      links: [
        {
          title: 'Community',
          items: [
            {
              label: 'Discord',
              href: 'https://discord.gg/8YkBcCED5y',
            },
            {
              label: 'Twitter',
              href: 'https://twitter.com/assaf_elovic',
            },
            {
              label: 'LinkedIn',
              href: 'https://www.linkedin.com/in/assafe/',
            },
          ],
        },
        {
          title: 'Company',
          items: [
            {
              label: 'Homepage',
              href: 'https://gptr.dev',
            },
            {
              label: 'Contact',
              href: 'mailto:assafelovic@gmail.com',
            },
          ],
        },
      ],
      copyright: `Copyright Â© ${new Date().getFullYear()} GPT Researcher.`,
    },
  },
  presets: [
    [
      '@docusaurus/preset-classic',
      {
        docs: {
          sidebarPath: require.resolve('./sidebars.js'),
          // Please change this to your repo.
          editUrl:
            'https://github.com/assafelovic/gpt-researcher/tree/master/docs',
          remarkPlugins: [math],
          rehypePlugins: [katex],
        },
        blog: {
          onUntruncatedBlogPosts: 'ignore',
        },
        theme: {
          customCss: require.resolve('./src/css/custom.css'),
        },
      },
    ],
  ],
  stylesheets: [
    {
        href: "https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css",
        integrity: "sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc",
        crossorigin: "anonymous",
    },
  ],

  plugins: [
    // ... Your other plugins.
    [
      require.resolve("@easyops-cn/docusaurus-search-local"),
      {
        // ... Your options.
        // `hashed` is recommended as long-term-cache of index file is possible.
        hashed: true,
        blogDir:"./blog/"
        // For Docs using Chinese, The `language` is recommended to set to:
        // ```
        // language: ["en", "zh"],
        // ```
        // When applying `zh` in language, please install `nodejieba` in your project.
      },
    ],
  ],
};



================================================
FILE: docs/package.json
================================================
{
  "name": "website",
  "version": "0.0.0",
  "private": true,
  "resolutions": {
    "nth-check": "2.0.1",
    "trim": "0.0.3",
    "got": "11.8.5",
    "node-forge": "1.3.0",
    "minimatch": "3.0.5",
    "loader-utils": "2.0.4",
    "eta": "2.0.0",
    "@sideway/formula": "3.0.1",
    "http-cache-semantics": "4.1.1"
  },
  "scripts": {
    "docusaurus": "docusaurus",
    "start": "docusaurus start",
    "build": "docusaurus build",
    "swizzle": "docusaurus swizzle",
    "deploy": "docusaurus deploy",
    "clear": "docusaurus clear",
    "serve": "docusaurus serve",
    "write-translations": "docusaurus write-translations",
    "write-heading-ids": "docusaurus write-heading-ids"
  },
  "dependencies": {
    "@docusaurus/core": "3.7.0",
    "@docusaurus/preset-classic": "3.7.0",
    "@easyops-cn/docusaurus-search-local": "^0.49.2",
    "@mdx-js/react": "^3.1.0",
    "@svgr/webpack": "^8.1.0",
    "clsx": "^1.1.1",
    "file-loader": "^6.2.0",
    "hast-util-is-element": "1.1.0",
    "minimatch": "3.0.5",
    "react": "^18.0.1",
    "react-dom": "^18.0.1",
    "rehype-katex": "^7.0.1",
    "remark-math": "3",
    "trim": "^0.0.3",
    "url-loader": "^4.1.1"
  },
  "browserslist": {
    "production": [
      ">0.5%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}



================================================
FILE: docs/pydoc-markdown.yml
================================================
loaders:
   - type: python
     search_path: [../docs]
processors:
  - type: filter
    skip_empty_modules: true
  - type: smart
  - type: crossref
renderer:
  type: docusaurus
  docs_base_path: docs
  relative_output_path: reference
  relative_sidebar_path: sidebar.json
  sidebar_top_level_label: Reference
  markdown:
    escape_html_in_docstring: false



================================================
FILE: docs/sidebars.js
================================================
/**
 * Creating a sidebar enables you to:
 - create an ordered group of docs
 - render a sidebar for each doc of that group
 - provide next/previous navigation

 The sidebars can be generated from the filesystem, or explicitly defined here.

 Create as many sidebars as you want.
 */

 module.exports = {
  docsSidebar: [
    'welcome',
    {
      type: 'category',
      label: 'Getting Started',
      collapsible: true,
      collapsed: false,
      items: [
        'gpt-researcher/getting-started/introduction',
        'gpt-researcher/getting-started/how-to-choose',
        'gpt-researcher/getting-started/getting-started',
        'gpt-researcher/getting-started/cli',
        'gpt-researcher/getting-started/getting-started-with-docker',
        'gpt-researcher/getting-started/linux-deployment',
      ]
    },
    {
      type: 'category',
      label: 'GPT Researcher',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/gptr/pip-package',
        'gpt-researcher/gptr/npm-package',
        'gpt-researcher/gptr/example',
        'gpt-researcher/gptr/deep_research',
        'gpt-researcher/gptr/config',
        'gpt-researcher/gptr/scraping',
        'gpt-researcher/gptr/querying-the-backend',
        'gpt-researcher/gptr/automated-tests',
        'gpt-researcher/gptr/troubleshooting'
      ],
    },
    {
      type: 'category',
      label: 'Frontend',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/frontend/introduction',
        'gpt-researcher/frontend/nextjs-frontend',
        'gpt-researcher/frontend/react-package',
        'gpt-researcher/frontend/embed-script',
        'gpt-researcher/frontend/vanilla-js-frontend',
        'gpt-researcher/frontend/discord-bot',
        'gpt-researcher/frontend/visualizing-websockets'
      ],
    },
    {
      type: 'category',
      label: 'Custom Context',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/context/tailored-research',
        'gpt-researcher/context/local-docs',
        'gpt-researcher/context/azure-storage',
        'gpt-researcher/context/filtering-by-domain',
        'gpt-researcher/context/vector-stores',
        'gpt-researcher/context/data-ingestion'
        ]
    },
    {
      type: 'category',
      label: 'Handling Logs',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/handling-logs/all-about-logs',
        'gpt-researcher/handling-logs/simple-logs-example',
        'gpt-researcher/handling-logs/langsmith-logs'
        ]
    },
    {
      type: 'category',
      label: 'LLM Providers',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/llms/llms',
        'gpt-researcher/llms/supported-llms',
        'gpt-researcher/llms/testing-your-llm',
        'gpt-researcher/llms/running-with-azure',
        'gpt-researcher/llms/running-with-ollama'
      ]
    },
    {
      type: 'category',
      label: 'Retrievers',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/search-engines/search-engines',
        'gpt-researcher/retrievers/mcp-configs',
        'gpt-researcher/search-engines/test-your-retriever',
        ]
    },
    {
      type: 'category',
      label: 'Multi-Agent Frameworks',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/multi_agents/langgraph',
        ]
    },
    {
      type: 'category',
      label: 'MCP Server',
      collapsible: true,
      collapsed: true,
      items: [
        'gpt-researcher/mcp-server/getting-started',
        'gpt-researcher/mcp-server/advanced-usage',
        'gpt-researcher/mcp-server/claude-integration',
        ]
    },
    {'Examples': [{type: 'autogenerated', dirName: 'examples'}]},
    'contribute',
    'roadmap',
    'faq',
  ],
  // Removing empty Reference category that was causing the build error
  referenceSideBar: []
};



================================================
FILE: docs/blog/authors.yml
================================================
assafe:
  name: Assaf Elovic
  title: Creator @ GPT Researcher and Tavily
  url: https://github.com/assafelovic
  image_url: https://lh3.googleusercontent.com/a/ACg8ocJtrLku69VG_2Y0sJa5mt66gIGNaEBX5r_mgE6CRPEb7A=s96-c

elishakay:
  name: Elisha Kramer
  title: Core Contributor @ GPT Researcher
  url: https://github.com/ElishaKay
  image_url: https://avatars.githubusercontent.com/u/16700452



================================================
FILE: docs/blog/2023-09-22-gpt-researcher/index.md
================================================
---
slug: building-gpt-researcher
title: How we built GPT Researcher
authors: [assafe]
tags: [gpt-researcher, autonomous-agent, opensource, github]
---

After [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) was published, we immediately took it for a spin. The first use case that came to mind was autonomous online research. Forming objective conclusions for manual research tasks can take time, sometimes weeks, to find the right resources and information. Seeing how well AutoGPT created tasks and executed them got me thinking about the great potential of using AI to conduct comprehensive research and what it meant for the future of online research.

But the problem with AutoGPT was that it usually ran into never-ending loops, required human interference for almost every step, constantly lost track of its progress, and almost never actually completed the task.

Nonetheless, the information and context gathered during the research task were lost (such as keeping track of sources), and sometimes hallucinated.

The passion for leveraging AI for online research and the limitations I found put me on a mission to try and solve it while sharing my work with the world. This is when I created [GPT Researcher](https://github.com/assafelovic/gpt-researcher) â€” an open source autonomous agent for online comprehensive research.

In this article, we will share the steps that guided me toward the proposed solution.

### Moving from infinite loops to deterministic results
The first step in solving these issues was to seek a more deterministic solution that could ultimately guarantee completing any research task within a fixed time frame, without human interference.

This is when we stumbled upon the recent paper [Plan and Solve](https://arxiv.org/abs/2305.04091). The paper aims to provide a better solution for the challenges stated above. The idea is quite simple and consists of two components: first, devising a plan to divide the entire task into smaller subtasks and then carrying out the subtasks according to the plan.

![Planner-Excutor-Model](./planner.jpeg)

As it relates to research, first create an outline of questions to research related to the task, and then deterministically execute an agent for every outline item. This approach eliminates the uncertainty in task completion by breaking the agent steps into a deterministic finite set of tasks. Once all tasks are completed, the agent concludes the research.

Following this strategy has improved the reliability of completing research tasks to 100%. Now the challenge is, how to improve quality and speed?

### Aiming for objective and unbiased results
The biggest challenge with LLMs is the lack of factuality and unbiased responses caused by hallucinations and out-of-date training sets (GPT is currently trained on datasets from 2021). But the irony is that for research tasks, it is crucial to optimize for these exact two criteria: factuality and bias.

To tackle this challenges, we assumed the following:

- Law of large numbers â€” More content will lead to less biased results. Especially if gathered properly.
- Leveraging LLMs for the summarization of factual information can significantly improve the overall better factuality of results.

After experimenting with LLMs for quite some time, we can say that the areas where foundation models excel are in the summarization and rewriting of given content. So, in theory, if LLMs only review given content and summarize and rewrite it, potentially it would reduce hallucinations significantly.

In addition, assuming the given content is unbiased, or at least holds opinions and information from all sides of a topic, the rewritten result would also be unbiased. So how can content be unbiased? The [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). In other words, if enough sites that hold relevant information are scraped, the possibility of biased information reduces greatly. So the idea would be to scrape just enough sites together to form an objective opinion on any topic.

Great! Sounds like, for now, we have an idea for how to create both deterministic, factual, and unbiased results. But what about the speed problem?

### Speeding up the research process
Another issue with AutoGPT is that it works synchronously. The main idea of it is to create a list of tasks and then execute them one by one. So if, letâ€™s say, a research task requires visiting 20 sites, and each site takes around one minute to scrape and summarize, the overall research task would take a minimum of +20 minutes. Thatâ€™s assuming it ever stops. But what if we could parallelize agent work?

By levering Python libraries such as asyncio, the agent tasks have been optimized to work in parallel, thus significantly reducing the time to research.

```python
# Create a list to hold the coroutine agent tasks
tasks = [async_browse(url, query, self.websocket) for url in await new_search_urls]

# Gather the results as they become available
responses = await asyncio.gather(*tasks, return_exceptions=True)
```

In the example above, we trigger scraping for all URLs in parallel, and only once all is done, continue with the task. Based on many tests, an average research task takes around three minutes (!!). Thatâ€™s 85% faster than AutoGPT.

### Finalizing the research report
Finally, after aggregating as much information as possible about a given research task, the challenge is to write a comprehensive report about it.

After experimenting with several OpenAI models and even open source, Iâ€™ve concluded that the best results are currently achieved with GPT-4. The task is straightforward â€” provide GPT-4 as context with all the aggregated information, and ask it to write a detailed report about it given the original research task.

The prompt is as follows:
```commandline
"{research_summary}" Using the above information, answer the following question or topic: "{question}" in a detailed report â€” The report should focus on the answer to the question, should be well structured, informative, in depth, with facts and numbers if available, a minimum of 1,200 words and with markdown syntax and apa format. Write all source urls at the end of the report in apa format. You should write your report only based on the given information and nothing else.
```

The results are quite impressive, with some minor hallucinations in very few samples, but itâ€™s fair to assume that as GPT improves over time, results will only get better.

### The final architecture
Now that weâ€™ve reviewed the necessary steps of GPT Researcher, letâ€™s break down the final architecture, as shown below:

<div align="center">
<img align="center" height="500" src="https://cowriter-images.s3.amazonaws.com/architecture.png"/>
</div>

More specifically:
- Generate an outline of research questions that form an objective opinion on any given task.
- For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.
- For each scraped resource, keep track, filter, and summarize only if it includes relevant information.
- Finally, aggregate all summarized sources and generate a final research report.

### Going forward
The future of online research automation is heading toward a major disruption. As AI continues to improve, it is only a matter of time before AI agents can perform comprehensive research tasks for any of our day-to-day needs. AI research can disrupt areas of finance, legal, academia, health, and retail, reducing our time for each research by 95% while optimizing for factual and unbiased reports within an influx and overload of ever-growing online information.

Imagine if an AI can eventually understand and analyze any form of online content â€” videos, images, graphs, tables, reviews, text, audio. And imagine if it could support and analyze hundreds of thousands of words of aggregated information within a single prompt. Even imagine that AI can eventually improve in reasoning and analysis, making it much more suitable for reaching new and innovative research conclusions. And that it can do all that in minutes, if not seconds.

Itâ€™s all a matter of time and what [GPT Researcher](https://github.com/assafelovic/gpt-researcher) is all about.



================================================
FILE: docs/blog/2023-11-12-openai-assistant/index.md
================================================
---
slug: building-openai-assistant
title: How to build an OpenAI Assistant with Internet access
authors: [assafe]
tags: [tavily, search-api, openai, assistant-api]
---

OpenAI has done it again with a [groundbreaking DevDay](https://openai.com/blog/new-models-and-developer-products-announced-at-devday) showcasing some of the latest improvements to the OpenAI suite of tools, products and services. One major release was the new [Assistants API](https://platform.openai.com/docs/assistants/overview) that makes it easier for developers to build their own assistive AI apps that have goals and can call models and tools.

The new Assistants API currently supports three types of tools: Code Interpreter, Retrieval, and Function calling. Although you might expect the Retrieval tool to support online information retrieval (such as search APIs or as ChatGPT plugins), it only supports raw data for now such as text or CSV files.

This blog will demonstrate how to leverage the latest Assistants API with online information using the function calling tool.

To skip the tutorial below, feel free to check out the full [Github Gist here](https://gist.github.com/assafelovic/579822cd42d52d80db1e1c1ff82ffffd).

At a high level, a typical integration of the Assistants API has the following steps:

- Create an [Assistant](https://platform.openai.com/docs/api-reference/assistants/createAssistant) in the API by defining its custom instructions and picking a model. If helpful, enable tools like Code Interpreter, Retrieval, and Function calling.
- Create a [Thread](https://platform.openai.com/docs/api-reference/threads) when a user starts a conversation.
- Add [Messages](https://platform.openai.com/docs/api-reference/messages) to the Thread as the user ask questions.
- [Run](https://platform.openai.com/docs/api-reference/runs) the Assistant on the Thread to trigger responses. This automatically calls the relevant tools.

As you can see below, an Assistant object includes Threads for storing and handling conversation sessions between the assistant and users, and Run for invocation of an Assistant on a Thread.

![OpenAI Assistant Object](./diagram-assistant.jpeg)

Letâ€™s go ahead and implement these steps one by one! For the example, we will build a finance GPT that can provide insights about financial questions. We will use the [OpenAI Python SDK v1.2](https://github.com/openai/openai-python/tree/main#installation) and [Tavily Search API](https://tavily.com).

First things first, letâ€™s define the assistantâ€™s instructions:

```python
assistant_prompt_instruction = """You are a finance expert. 
Your goal is to provide answers based on information from the internet. 
You must use the provided Tavily search API function to find relevant online information. 
You should never use your own knowledge to answer questions.
Please include relevant url sources in the end of your answers.
"""
```
Next, letâ€™s finalize step 1 and create an assistant using the latest [GPT-4 Turbo model](https://github.com/openai/openai-python/tree/main#installation) (128K context), and the call function using the [Tavily web search API](https://tavily.com/):

```python
# Create an assistant
assistant = client.beta.assistants.create(
    instructions=assistant_prompt_instruction,
    model="gpt-4-1106-preview",
    tools=[{
        "type": "function",
        "function": {
            "name": "tavily_search",
            "description": "Get information on recent events from the web.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The search query to use. For example: 'Latest news on Nvidia stock performance'"},
                },
                "required": ["query"]
            }
        }
    }]
)
```

Step 2+3 are quite straight forward, weâ€™ll initiate a new thread and update it with a user message:

```python
thread = client.beta.threads.create()
user_input = input("You: ")
message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content=user_input,
)
```

Finally, weâ€™ll run the assistant on the thread to trigger the function call and get the response:

```python
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=assistant_id,
)
```

So far so good! But this is where it gets a bit messy. Unlike with the regular GPT APIs, the Assistants API doesnâ€™t return a synchronous response, but returns a status. This allows for asynchronous operations across assistants, but requires more overhead for fetching statuses and dealing with each manually.

![Status Diagram](./diagram-1.png)

To manage this status lifecycle, letâ€™s build a function that can be reused and handles waiting for various statuses (such as â€˜requires_actionâ€™):

```python
# Function to wait for a run to complete
def wait_for_run_completion(thread_id, run_id):
    while True:
        time.sleep(1)
        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)
        print(f"Current run status: {run.status}")
        if run.status in ['completed', 'failed', 'requires_action']:
            return run
```

This function will sleep as long as the run has not been finalized such as in cases where itâ€™s completed or requires an action from a function call.

Weâ€™re almost there! Lastly, letâ€™s take care of when the assistant wants to call the web search API:

```python
# Function to handle tool output submission
def submit_tool_outputs(thread_id, run_id, tools_to_call):
    tool_output_array = []
    for tool in tools_to_call:
        output = None
        tool_call_id = tool.id
        function_name = tool.function.name
        function_args = tool.function.arguments

        if function_name == "tavily_search":
            output = tavily_search(query=json.loads(function_args)["query"])

        if output:
            tool_output_array.append({"tool_call_id": tool_call_id, "output": output})

    return client.beta.threads.runs.submit_tool_outputs(
        thread_id=thread_id,
        run_id=run_id,
        tool_outputs=tool_output_array
    )
```

As seen above, if the assistant has reasoned that a function call should trigger, we extract the given required function params and pass back to the runnable thread. We catch this status and call our functions as seen below:

```python
if run.status == 'requires_action':
    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)
    run = wait_for_run_completion(thread.id, run.id)
```

Thatâ€™s it! We now have a working OpenAI Assistant that can be used to answer financial questions using real time online information. Below is the full runnable code:

```python
import os
import json
import time
from openai import OpenAI
from tavily import TavilyClient

# Initialize clients with API keys
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

assistant_prompt_instruction = """You are a finance expert. 
Your goal is to provide answers based on information from the internet. 
You must use the provided Tavily search API function to find relevant online information. 
You should never use your own knowledge to answer questions.
Please include relevant url sources in the end of your answers.
"""

# Function to perform a Tavily search
def tavily_search(query):
    search_result = tavily_client.get_search_context(query, search_depth="advanced", max_tokens=8000)
    return search_result

# Function to wait for a run to complete
def wait_for_run_completion(thread_id, run_id):
    while True:
        time.sleep(1)
        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)
        print(f"Current run status: {run.status}")
        if run.status in ['completed', 'failed', 'requires_action']:
            return run

# Function to handle tool output submission
def submit_tool_outputs(thread_id, run_id, tools_to_call):
    tool_output_array = []
    for tool in tools_to_call:
        output = None
        tool_call_id = tool.id
        function_name = tool.function.name
        function_args = tool.function.arguments

        if function_name == "tavily_search":
            output = tavily_search(query=json.loads(function_args)["query"])

        if output:
            tool_output_array.append({"tool_call_id": tool_call_id, "output": output})

    return client.beta.threads.runs.submit_tool_outputs(
        thread_id=thread_id,
        run_id=run_id,
        tool_outputs=tool_output_array
    )

# Function to print messages from a thread
def print_messages_from_thread(thread_id):
    messages = client.beta.threads.messages.list(thread_id=thread_id)
    for msg in messages:
        print(f"{msg.role}: {msg.content[0].text.value}")

# Create an assistant
assistant = client.beta.assistants.create(
    instructions=assistant_prompt_instruction,
    model="gpt-4-1106-preview",
    tools=[{
        "type": "function",
        "function": {
            "name": "tavily_search",
            "description": "Get information on recent events from the web.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The search query to use. For example: 'Latest news on Nvidia stock performance'"},
                },
                "required": ["query"]
            }
        }
    }]
)
assistant_id = assistant.id
print(f"Assistant ID: {assistant_id}")

# Create a thread
thread = client.beta.threads.create()
print(f"Thread: {thread}")

# Ongoing conversation loop
while True:
    user_input = input("You: ")
    if user_input.lower() == 'exit':
        break

    # Create a message
    message = client.beta.threads.messages.create(
        thread_id=thread.id,
        role="user",
        content=user_input,
    )

    # Create a run
    run = client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=assistant_id,
    )
    print(f"Run ID: {run.id}")

    # Wait for run to complete
    run = wait_for_run_completion(thread.id, run.id)

    if run.status == 'failed':
        print(run.error)
        continue
    elif run.status == 'requires_action':
        run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)
        run = wait_for_run_completion(thread.id, run.id)

    # Print messages from the thread
    print_messages_from_thread(thread.id)
```

The assistant can be further customized and improved using additional retrieval information, OpenAIâ€™s coding interpreter and more. Also, you can go ahead and add more function tools to make the assistant even smarter.

Feel free to drop a comment below if you have any further questions!



================================================
FILE: docs/blog/2024-05-19-gptr-langgraph/index.md
================================================
---
slug: gptr-langgraph
title: How to Build the Ultimate Research Multi-Agent Assistant
authors: [assafe]
tags: [multi-skills, gpt-researcher, langchain, langgraph]
---
![Header](./blog-langgraph.jpeg)
# Introducing the GPT Researcher Multi-Agent Assistant
### Learn how to build an autonomous research assistant using LangGraph with a team of specialized AI agents

It has only been a year since the initial release of GPT Researcher, but methods for building, testing, and deploying AI agents have already evolved significantly. Thatâ€™s just the nature and speed of the current AI progress. What started as simple zero-shot or few-shot prompting, has quickly evolved to agent function calling, RAG and now finally agentic workflows (aka â€œflow engineeringâ€).

Andrew Ng has [recently stated](https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/), â€œI think AI agent workflows will drive massive AI progress this year â€” perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.â€

In this article you will learn why multi-agent workflows are the current best standard and how to build the optimal autonomous research multi-agent assistant using LangGraph.

To skip this tutorial, feel free to check out the Github repo of [GPT Researcher x LangGraph](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents).

## Introducing LangGraph
LangGraph is an extension of LangChain aimed at creating agent and multi-agent flows. It adds in the ability to create cyclical flows and comes with memory built in â€” both important attributes for creating agents.

LangGraph provides developers with a high degree of controllability and is important for creating custom agents and flows. Nearly all agents in production are customized towards the specific use case they are trying solve. LangGraph gives you the flexibility to create arbitrary customized agents, while providing an intuitive developer experience for doing so.

Enough with the smalltalk, letâ€™s start building!

## Building the Ultimate Autonomous Research Agent
By leveraging LangGraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. Having every agent focus and specialize only a specific skill, allows for better separation of concerns, customizability, and further development at scale as the project grows.

Inspired by the recent STORM paper, this example showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication. This example will also leverage the leading autonomous research agent GPT Researcher.

### The Research Agent Team
The research team consists of seven LLM agents:

* **Chief Editor** â€” Oversees the research process and manages the team. This is the â€œmasterâ€ agent that coordinates the other agents using LangGraph. This agent acts as the main LangGraph interface.
* **GPT Researcher** â€” A specialized autonomous agent that conducts in depth research on a given topic.
* **Editor** â€” Responsible for planning the research outline and structure.
* **Reviewer** â€” Validates the correctness of the research results given a set of criteria.
* **Reviser** â€” Revises the research results based on the feedback from the reviewer.
* **Writer** â€” Responsible for compiling and writing the final report.
* **Publisher** â€” Responsible for publishing the final report in various formats.

### Architecture
As seen below, the automation process is based on the following stages: Planning the research, data collection and analysis, review and revision, writing the report and finally publication:

![Architecture](./architecture.jpeg)

More specifically the process is as follows:

* **Browser (gpt-researcher)** â€” Browses the internet for initial research based on the given research task. This step is crucial for LLMs to plan the research process based on up to date and relevant information, and not rely solely on pre-trained data for a given task or topic.
* **Editor** â€” Plans the report outline and structure based on the initial research. The Editor is also responsible for triggering the parallel research tasks based on the planned outline.
* For each outline topic (in parallel):
  * **Researcher (gpt-researcher)** â€” Runs an in depth research on the subtopics and writes a draft. This agent leverages the GPT Researcher Python package under the hood, for optimized, in depth and factual research report.
  * **Reviewer** â€” Validates the correctness of the draft given a set of guidelines and provides feedback to the reviser (if any).
  * **Reviser** â€” Revises the draft until it is satisfactory based on the reviewer feedback.
* **Writer** â€” Compiles and writes the final report including an introduction, conclusion and references section from the given research findings.
* **Publisher** â€” Publishes the final report to multi formats such as PDF, Docx, Markdown, etc.

* We will not dive into all the code since thereâ€™s a lot of it, but focus mostly on the interesting parts Iâ€™ve found valuable to share.

## Define the Graph State
One of my favorite features with LangGraph is state management. States in LangGraph are facilitated through a structured approach where developers define a GraphState that encapsulates the entire state of the application. Each node in the graph can modify this state, allowing for dynamic responses based on the evolving context of the interaction.

Like in every start of a technical design, considering the data schema throughout the application is key. In this case weâ€™ll define a ResearchState like so:

```python
class ResearchState(TypedDict):
    task: dict
    initial_research: str
    sections: List[str]
    research_data: List[dict]
    # Report layout
    title: str
    headers: dict
    date: str
    table_of_contents: str
    introduction: str
    conclusion: str
    sources: List[str]
    report: str
```

As seen above, the state is divided into two main areas: the research task and the report layout content. As data circulates through the graph agents, each agent will, in turn, generate new data based on the existing state and update it for subsequent processing further down the graph with other agents.

We can then initialize the graph with the following:


```python
from langgraph.graph import StateGraph
workflow = StateGraph(ResearchState)
```

Initializing the graph with LangGraph
As stated above, one of the great things about multi-agent development is building each agent to have specialized and scoped skills. Letâ€™s take an example of the Researcher agent using GPT Researcher python package:

```python
from gpt_researcher import GPTResearcher

class ResearchAgent:
    def __init__(self):
        pass
  
    async def research(self, query: str):
        # Initialize the researcher
        researcher = GPTResearcher(parent_query=parent_query, query=query, report_type=research_report, config_path=None)
        # Conduct research on the given query
        await researcher.conduct_research()
        # Write the report
        report = await researcher.write_report()
  
        return report
```

As you can see above, weâ€™ve created an instance of the Research agent. Now letâ€™s assume weâ€™ve done the same for each of the teamâ€™s agent. After creating all of the agents, weâ€™d initialize the graph with LangGraph:

```python
def init_research_team(self):
    # Initialize skills
    editor_agent = EditorAgent(self.task)
    research_agent = ResearchAgent()
    writer_agent = WriterAgent()
    publisher_agent = PublisherAgent(self.output_dir)
    
    # Define a Langchain StateGraph with the ResearchState
    workflow = StateGraph(ResearchState)
    
    # Add nodes for each agent
    workflow.add_node("browser", research_agent.run_initial_research)
    workflow.add_node("planner", editor_agent.plan_research)
    workflow.add_node("researcher", editor_agent.run_parallel_research)
    workflow.add_node("writer", writer_agent.run)
    workflow.add_node("publisher", publisher_agent.run)
    
    workflow.add_edge('browser', 'planner')
    workflow.add_edge('planner', 'researcher')
    workflow.add_edge('researcher', 'writer')
    workflow.add_edge('writer', 'publisher')
    
    # set up start and end nodes
    workflow.set_entry_point("browser")
    workflow.add_edge('publisher', END)
    
    return workflow
```

As seen above, creating the LangGraph graph is very straight forward and consists of three main functions: add_node, add_edge and set_entry_point. With these main functions you can first add the nodes to the graph, connect the edges and finally set the starting point.

Focus check: If youâ€™ve been following the code and architecture properly, youâ€™ll notice that the Reviewer and Reviser agents are missing in the initialization above. Letâ€™s dive into it!

## A Graph within a Graph to support stateful Parallelization
This was the most exciting part of my experience working with LangGraph! One exciting feature of this autonomous assistant is having a parallel run for each research task, that would be reviewed and revised based on a set of predefined guidelines.

Knowing how to leverage parallel work within a process is key for optimizing speed. But how would you trigger parallel agent work if all agents report to the same state? This can cause race conditions and inconsistencies in the final data report. To solve this, you can create a sub graph, that would be triggered from the main LangGraph instance. This sub graph would hold its own state for each parallel run, and that would solve the issues that were raised.

As weâ€™ve done before, letâ€™s define the LangGraph state and its agents. Since this sub graph basically reviews and revises a research draft, weâ€™ll define the state with draft information:

```python
class DraftState(TypedDict):
    task: dict
    topic: str
    draft: dict
    review: str
    revision_notes: str
```

As seen in the DraftState, we mostly care about the topic discussed, and the reviewer and revision notes as they communicate between each other to finalize the subtopic research report. To create the circular condition weâ€™ll take advantage of the last important piece of LangGraph which is conditional edges:

```python
async def run_parallel_research(self, research_state: dict):
    workflow = StateGraph(DraftState)
    
    workflow.add_node("researcher", research_agent.run_depth_research)
    workflow.add_node("reviewer", reviewer_agent.run)
    workflow.add_node("reviser", reviser_agent.run)
    
    # set up edges researcher->reviewer->reviser->reviewer...
    workflow.set_entry_point("researcher")
    workflow.add_edge('researcher', 'reviewer')
    workflow.add_edge('reviser', 'reviewer')
    workflow.add_conditional_edges('reviewer',
                                   (lambda draft: "accept" if draft['review'] is None else "revise"),
                                   {"accept": END, "revise": "reviser"})
```

By defining the conditional edges, the graph would direct to reviser if there exists review notes by the reviewer, or the cycle would end with the final draft. If you go back to the main graph weâ€™ve built, youâ€™ll see that this parallel work is under a node named â€œresearcherâ€ called by ChiefEditor agent.

Running the Research Assistant
After finalizing the agents, states and graphs, itâ€™s time to run our research assistant! To make it easier to customize, the assistant runs with a given task.json file:

```json
{
  "query": "Is AI in a hype cycle?",
  "max_sections": 3,
  "publish_formats": {
    "markdown": true,
    "pdf": true,
    "docx": true
  },
  "follow_guidelines": false,
  "model": "gpt-4-turbo",
  "guidelines": [
    "The report MUST be written in APA format",
    "Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section",
    "The report MUST be written in spanish"
  ]
}
```

The task object is pretty self explanatory, however please notice that follow_guidelines if false would cause the graph to ignore the revision step and defined guidelines. Also, the max_sections field defines how many subheaders to research for. Having less will generate a shorter report.

Running the assistant will result in a final research report in formats such as Markdown, PDF and Docx.

To download and run the example check out the GPT Researcher x LangGraph [open source page](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents).

## Whatâ€™s Next?
Going forward, there are super exciting things to think about. Human in the loop is key for optimized AI experiences. Having a human help the assistant revise and focus on just the right research plan, topics and outline, would enhance the overall quality and experience. Also generally, aiming for relying on human intervention throughout the AI flow ensures correctness, sense of control and deterministic results. Happy to see that LangGraph already supports this out of the box as seen here.

In addition, having support for research about both web and local data would be key for many types of business and personal use cases.

Lastly, more efforts can be done to improve the quality of retrieved sources and making sure the final report is built in the optimal storyline.

A step forward in LangGraph and multi-agent collaboration in a whole would be where assistants can plan and generate graphs dynamically based on given tasks. This vision would allow assistants to choose only a subset of agents for a given task and plan their strategy based on the graph fundamentals as presented in this article and open a whole new world of possibilities. Given the pace of innovation in the AI space, it wonâ€™t be long before a new disruptive version of GPT Researcher is launched. Looking forward to what the future brings!

To keep track of this projectâ€™s ongoing progress and updates please join our Discord community. And as always, if you have any feedback or further questions, please comment below!


================================================
FILE: docs/blog/2024-09-7-hybrid-research/index.md
================================================
---
slug: gptr-hybrid
title: The Future of Research is Hybrid
authors: [assafe]
tags: [hybrid-research, gpt-researcher, langchain, langgraph, tavily]
image: https://miro.medium.com/v2/resize:fit:1400/1*NgVIlZVSePqrK5EkB1wu4Q.png
---
![Hyrbrid Research with GPT Researcher](https://miro.medium.com/v2/resize:fit:1400/1*MaauY1ecsD05nL8JqW0Zdg.jpeg)

Over the past few years, we've seen an explosion of new AI tools designed to disrupt research. Some, like [ChatPDF](https://www.chatpdf.com/) and [Consensus](https://consensus.app), focus on extracting insights from documents. Others, such as [Perplexity](https://www.perplexity.ai/), excel at scouring the web for information. But here's the thing: none of these tools combine both web and local document search within a single contextual research pipeline.

This is why I'm excited to introduce the latest advancements of **[GPT Researcher](https://gptr.dev)** â€” now able to conduct hybrid research on any given task and documents.

Web driven research often lacks specific context, risks information overload, and may include outdated or unreliable data. On the flip side, local driven research is limited to historical data and existing knowledge, potentially creating organizational echo chambers and missing out on crucial market trends or competitor moves. Both approaches, when used in isolation, can lead to incomplete or biased insights, hampering your ability to make fully informed decisions.

Today, we're going to change the game. By the end of this guide, you'll learn how to conduct hybrid research that combines the best of both worlds â€” web and local â€” enabling you to conduct more thorough, relevant, and insightful research.

## Why Hybrid Research Works Better

By combining web and local sources, hybrid research addresses these limitations and offers several key advantages:

1. **Grounded context**: Local documents provide a foundation of verified, organization specific information. This grounds the research in established knowledge, reducing the risk of straying from core concepts or misinterpreting industry specific terminology.
   
   *Example*: A pharmaceutical company researching a new drug development opportunity can use its internal research papers and clinical trial data as a base, then supplement this with the latest published studies and regulatory updates from the web.

2. **Enhanced accuracy**: Web sources offer up-to-date information, while local documents provide historical context. This combination allows for more accurate trend analysis and decision-making.
   
   *Example*: A financial services firm analyzing market trends can combine their historical trading data with real-time market news and social media sentiment analysis to make more informed investment decisions.

3. **Reduced bias**: By drawing from both web and local sources, we mitigate the risk of bias that might be present in either source alone.
   
   *Example*: A tech company evaluating its product roadmap can balance internal feature requests and usage data with external customer reviews and competitor analysis, ensuring a well-rounded perspective.

4. **Improved planning and reasoning**: LLMs can leverage the context from local documents to better plan their web research strategies and reason about the information they find online.
   
   *Example*: An AI-powered market research tool can use a company's past campaign data to guide its web search for current marketing trends, resulting in more relevant and actionable insights.

5. **Customized insights**: Hybrid research allows for the integration of proprietary information with public data, leading to unique, organization-specific insights.
   
   *Example*: A retail chain can combine its sales data with web-scraped competitor pricing and economic indicators to optimize its pricing strategy in different regions.

These are just a few examples for business use cases that can leverage hybrid research, but enough with the small talk â€” let's build!

## Building the Hybrid Research Assistant

Before we dive into the details, it's worth noting that GPT Researcher has the capability to conduct hybrid research out of the box! However, to truly appreciate how this works and to give you a deeper understanding of the process, we're going to take a look under the hood.

![GPT Researcher hybrid research](./gptr-hybrid.png)

GPT Researcher conducts web research based on an auto-generated plan from local documents, as seen in the architecture above. It then retrieves relevant information from both local and web data for the final research report.

We'll explore how local documents are processed using LangChain, which is a key component of GPT Researcher's document handling. Then, we'll show you how to leverage GPT Researcher to conduct hybrid research, combining the advantages of web search with your local document knowledge base.

### Processing Local Documents with Langchain

LangChain provides a variety of document loaders that allow us to process different file types. This flexibility is crucial when dealing with diverse local documents. Here's how to set it up:

```python
from langchain_community.document_loaders import (
    PyMuPDFLoader, 
    TextLoader, 
    UnstructuredCSVLoader, 
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader, 
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

def load_local_documents(file_paths):
    documents = []
    for file_path in file_paths:
        if file_path.endswith('.pdf'):
            loader = PyMuPDFLoader(file_path)
        elif file_path.endswith('.txt'):
            loader = TextLoader(file_path)
        elif file_path.endswith('.csv'):
            loader = UnstructuredCSVLoader(file_path)
        elif file_path.endswith('.xlsx'):
            loader = UnstructuredExcelLoader(file_path)
        elif file_path.endswith('.md'):
            loader = UnstructuredMarkdownLoader(file_path)
        elif file_path.endswith('.pptx'):
            loader = UnstructuredPowerPointLoader(file_path)
        elif file_path.endswith('.docx'):
            loader = UnstructuredWordDocumentLoader(file_path)
        else:
            raise ValueError(f"Unsupported file type: {file_path}")
        
        documents.extend(loader.load())
    
    return documents

# Use the function to load your local documents
local_docs = load_local_documents(['company_report.pdf', 'meeting_notes.docx', 'data.csv'])

# Split the documents into smaller chunks for more efficient processing
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(local_docs)

# Create embeddings and store them in a vector database for quick retrieval
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

# Example of how to perform a similarity search
query = "What were the key points from our last strategy meeting?"
relevant_docs = vectorstore.similarity_search(query, k=3)

for doc in relevant_docs:
    print(doc.page_content)
```

### Conducting Web Research with GPT Researcher

Now that we've learned how to work with local documents, let's take a quick look at how GPT Researcher works under the hood:

![GPT Researcher Architecture](https://miro.medium.com/v2/resize:fit:1400/1*yFtT43N0GxL0TMKvjtYjug.png)

As seen above, GPT Researcher creates a research plan based on the given task by generating potential research queries that can collectively provide an objective and broad overview of the topic. Once these queries are generated, GPT Researcher uses a search engine like Tavily to find relevant results. Each scraped result is then saved in a vector database. Finally, the top k chunks most related to the research task are retrieved to generate a final research report.

GPT Researcher supports hybrid research, which involves an additional step of chunking local documents (implemented using Langchain) before retrieving the most related information. After numerous evaluations conducted by the community, we've found that hybrid research improved the correctness of final results by over 40%!

### Running the Hybrid Research with GPT Researcher

Now that you have a better understanding of how hybrid research works, let's demonstrate how easy this can be achieved with GPT Researcher.

#### Step 1: Install GPT Researcher with PIP

```bash
pip install gpt-researcher
```

#### Step 2: Setting up the environment

We will run GPT Researcher with OpenAI as the LLM vendor and Tavily as the search engine. You'll need to obtain API keys for both before moving forward. Then, export the environment variables in your CLI as follows:

```bash
export OPENAI_API_KEY={your-openai-key}
export TAVILY_API_KEY={your-tavily-key}
```

#### Step 3: Initialize GPT Researcher with hybrid research configuration

GPT Researcher can be easily initialized with params that signal it to run a hybrid research. You can conduct many forms of research, head to the documentation page to learn more.

To get GPT Researcher to run a hybrid research, you need to include all relevant files in my-docs directory (create it if it doesn't exist), and set the instance report_source to "hybrid" as seen below. Once the report source is set to hybrid, GPT Researcher will look for existing documents in the my-docs directory and include them in the research. If no documents exist, it will ignore it.

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_research_report(query: str, report_type: str, report_source: str) -> str:
    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source)
    research = await researcher.conduct_research()
    report = await researcher.write_report()
    return report
    
if __name__ == "__main__":
    query = "How does our product roadmap compare to emerging market trends in our industry?"
    report_source = "hybrid"

    report = asyncio.run(get_research_report(query=query, report_type="research_report", report_source=report_source))
    print(report)
```

As seen above, we can run the research on the following example:

- Research task: "How does our product roadmap compare to emerging market trends in our industry?"
- Web: Current market trends, competitor announcements, and industry forecasts
- Local: Internal product roadmap documents and feature prioritization lists

After various community evaluations we've found that the results of this research improve quality and correctness of research by over 40% and remove hallucinations by 50%. Moreover as stated above, local information helps the LLM improve planning reasoning allowing it to make better decisions and researching more relevant web sources.

But wait, there's more! GPT Researcher also includes a sleek front-end app using NextJS and Tailwind. To learn how to get it running check out the documentation page. You can easily use drag and drop for documents to run hybrid research.

## Conclusion

Hybrid research represents a significant advancement in data gathering and decision making. By leveraging tools like [GPT Researcher](https://gptr.dev), teams can now conduct more comprehensive, context-aware, and actionable research. This approach addresses the limitations of using web or local sources in isolation, offering benefits such as grounded context, enhanced accuracy, reduced bias, improved planning and reasoning, and customized insights.

The automation of hybrid research can enable teams to make faster, more data-driven decisions, ultimately enhancing productivity and offering a competitive advantage in analyzing an expanding pool of unstructured and dynamic information.


================================================
FILE: docs/blog/2025-02-26-deep-research/index.md
================================================
# Introducing Deep Research: The Open Source Alternative

## The Dawn of Deep Research in AI

The AI research landscape is witnessing a revolutionary shift with the emergence of "Deep Research" capabilities. But what exactly is deep research, and why should you care? 

Deep research represents the next evolution in AI-powered information retrieval - going far beyond simple search to deliver comprehensive, multi-layered analysis of complex topics. Unlike traditional search engines that return a list of links, or even first-generation AI assistants that provide surface-level summaries, deep research tools deploy sophisticated algorithms to explore topics with unprecedented depth and breadth, mimicking the way human researchers would tackle complex subjects.

The key features that define true deep research capabilities include iterative analysis that refines queries and results dynamically ([InfoQ, 2025](https://www.infoq.com/news/2025/02/perplexity-deep-research/)), multimodal processing that integrates diverse data formats ([Observer, 2025](https://observer.com/2025/01/openai-google-gemini-agi/)), real-time data retrieval for up-to-date insights ([WinBuzzer, 2025](https://winbuzzer.com/2025/02/15/perplexity-deep-research-challenges-openai-and-googles-ai-powered-information-retrieval-xcxwbn/)), and structured outputs with proper citations for academic and technical applications ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)).

In recent months, we've seen major players launch their own deep research solutions, each with its unique approach and positioning in the market:

- **Perplexity AI** focuses on speed, delivering research results in under three minutes with real-time data retrieval ([Analytics Vidhya, 2025](https://www.analyticsvidhya.com/blog/2025/02/perplexity-deep-research/)). Their cost-effective model (starting at free tier) makes advanced research accessible to a broader audience, though some analysts note potential accuracy trade-offs in favor of speed ([Medium, 2025](https://medium.com/towards-agi/perplexity-ai-deep-research-vs-openai-deep-research-an-in-depth-comparison-6784c814fc4a)).

- **OpenAI's Deep Research** (built on the O3 model) prioritizes depth and precision, excelling in technical and academic applications with advanced reasoning capabilities ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)). Their structured outputs include detailed citations, ensuring reliability and verifiability. However, at $200/month ([Opentools, 2025](https://opentools.ai/news/openai-unveils-groundbreaking-deep-research-chatgpt-for-pro-users)), it represents a significant investment, and comprehensive reports can take 5-30 minutes to generate ([ClickItTech, 2025](https://www.clickittech.com/ai/perplexity-deep-research-vs-openai-deep-research/)).

- **Google's Gemini 2.0** emphasizes multimodal integration across text, images, audio, and video, with particular strength in enterprise applications ([Adyog, 2024](https://blog.adyog.com/2024/12/31/the-ai-titans-face-off-openais-o3-vs-googles-gemini-2-0/)). At $20/month, it offers a more affordable alternative to OpenAI's solution, though some users note limitations in customization flexibility ([Helicone, 2025](https://www.helicone.ai/blog/openai-deep-research)).

What makes deep research truly exciting is its potential to democratize advanced knowledge synthesis ([Medium, 2025](https://medium.com/@greeshmamshajan/the-evolution-of-ai-powered-research-perplexitys-disruption-and-the-battle-for-cognitive-87af682cc8e6)), dramatically enhance productivity by automating time-intensive research tasks ([The Mobile Indian, 2025](https://www.themobileindian.com/news/perplexity-deep-research-vs-openai-deep-research-vs-gemini-1-5-pro-deep-research-ai-fight)), and open new avenues for interdisciplinary research through advanced reasoning capabilities ([Observer, 2025](https://observer.com/2025/01/openai-google-gemini-agi/)).

However, a key limitation in the current market is accessibility - the most powerful deep research tools remain locked behind expensive paywalls or closed systems, putting them out of reach for many researchers, students, and smaller organizations who could benefit most from these capabilities.

## Introducing GPT Researcher Deep Research âœ¨

We're thrilled to announce our answer to this trend: **GPT Researcher Deep Research** - an advanced open-source recursive research system that explores topics with depth and breadth, all while maintaining cost-effectiveness and transparency.

[GPT Researcher](https://github.com/assafelovic/gpt-researcher) Deep Research not only matches the capabilities of the industry giants but exceeds them in several key metrics:

- **Cost-effective**: Each deep research operation costs approximately $0.40 (using `o3-mini` on `"high"` reasoning effort)
- **Time-efficient**: Complete research in around 5 minutes
- **Fully customizable**: Adjust parameters to match your specific research needs
- **Transparent**: Full visibility into the research process and methodology
- **Open source**: Free to use, modify, and integrate into your workflows

## How It Works: The Recursive Research Tree

What makes GPT Researcher's deep research so powerful is its tree-like exploration pattern that combines breadth and depth in an intelligent, recursive approach:

![Research Flow Diagram](https://github.com/user-attachments/assets/eba2d94b-bef3-4f8d-bbc0-f15bd0a40968)

1. **Breadth Exploration**: At each level, it generates multiple search queries to explore different aspects of your topic
2. **Depth Diving**: For each branch, it recursively goes deeper, following promising leads and uncovering hidden connections
3. **Concurrent Processing**: Utilizing async/await patterns to run multiple research paths simultaneously
4. **Context Management**: Automatically aggregates and synthesizes findings across all branches
5. **Real-time Tracking**: Provides updates on research progress across both breadth and depth dimensions

Imagine deploying a team of AI researchers, each following their own research path while collaborating to build a comprehensive understanding of your topic. That's the power of GPT Researcher's deep research approach.

## Getting Started in Minutes

Integrating deep research into your projects is remarkably straightforward:

```python
from gpt_researcher import GPTResearcher
import asyncio

async def main():
    # Initialize researcher with deep research type
    researcher = GPTResearcher(
        query="What are the latest developments in quantum computing?",
        report_type="deep",  # This triggers deep research mode
    )
    
    # Run research
    research_data = await researcher.conduct_research()
    
    # Generate report
    report = await researcher.write_report()
    print(report)

if __name__ == "__main__":
    asyncio.run(main())
```

## Under the Hood: How Deep Research Works

Looking at the codebase reveals the sophisticated system that powers GPT Researcher's deep research capabilities:

### 1. Query Generation and Planning

The system begins by generating a set of diverse search queries based on your initial question:

```python
async def generate_search_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:
    """Generate SERP queries for research"""
    messages = [
        {"role": "system", "content": "You are an expert researcher generating search queries."},
        {"role": "user",
         "content": f"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as 'Query: <query>' followed by 'Goal: <goal>' for each pair: {query}"}
    ]
```

This process creates targeted queries, each with a specific research goal. For example, a query about quantum computing might generate:
- "Latest quantum computing breakthroughs 2024-2025"
- "Quantum computing practical applications in finance"
- "Quantum error correction advancements"

### 2. Concurrent Research Execution

The system then executes these queries concurrently, with intelligent resource management:

```python
# Process queries with concurrency limit
semaphore = asyncio.Semaphore(self.concurrency_limit)

async def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:
    async with semaphore:
        # Research execution logic
```

This approach maximizes efficiency while ensuring system stability - like having multiple researchers working in parallel.

### 3. Recursive Exploration

The magic happens with recursive exploration:

```python
# Continue deeper if needed
if depth > 1:
    new_breadth = max(2, breadth // 2)
    new_depth = depth - 1
    progress.current_depth += 1

    # Create next query from research goal and follow-up questions
    next_query = f"""
    Previous research goal: {result['researchGoal']}
    Follow-up questions: {' '.join(result['followUpQuestions'])}
    """

    # Recursive research
    deeper_results = await self.deep_research(
        query=next_query,
        breadth=new_breadth,
        depth=new_depth,
        # Additional parameters
    )
```

This creates a tree-like exploration pattern that follows promising leads deeper while maintaining breadth of coverage.

### 4. Context Management and Synthesis

Managing the vast amount of gathered information requires sophisticated tracking:

```python
# Trim context to stay within word limits
trimmed_context = trim_context_to_word_limit(all_context)
logger.info(f"Trimmed context from {len(all_context)} items to {len(trimmed_context)} items to stay within word limit")
```

This ensures the most relevant information is retained while respecting model context limitations.

## Customizing Your Research Experience

One of the key advantages of GPT Researcher's open-source approach is full customizability. You can tailor the research process to your specific needs through several configuration options:

```yaml
deep_research_breadth: 4    # Number of parallel research paths
deep_research_depth: 2      # How many levels deep to explore
deep_research_concurrency: 4  # Maximum concurrent operations
total_words: 2500           # Word count for final report
reasoning_effort: medium
```

Apply these configurations through environment variables, a config file, or directly in code:

```python
researcher = GPTResearcher(
    query="your query",
    report_type="deep",
    config_path="path/to/config.yaml"
)
```

## Real-time Progress Tracking

For applications requiring visibility into the research process, GPT Researcher provides detailed progress tracking:

```python
class ResearchProgress:
    current_depth: int       # Current depth level
    total_depth: int         # Maximum depth to explore
    current_breadth: int     # Current number of parallel paths
    total_breadth: int       # Maximum breadth at each level
    current_query: str       # Currently processing query
    completed_queries: int   # Number of completed queries
    total_queries: int       # Total queries to process
```

This allows you to build interfaces that show research progress in real-time - perfect for applications where users want visibility into the process.

## Why This Matters: The Impact of Deep Research

The democratization of deep research capabilities through open-source tools like GPT Researcher represents a paradigm shift in how we process and analyze information. Benefits include:

1. **Deeper insights**: Uncover connections and patterns that surface-level research would miss
2. **Time savings**: Automate hours or days of manual research into minutes
3. **Reduced costs**: Enterprise-grade research capabilities at a fraction of the cost
4. **Accessibility**: Bringing advanced research tools to individuals and small organizations
5. **Transparency**: Full visibility into the research methodology and sources

## Getting Started Today

Ready to experience the power of deep research in your projects? Here's how to get started:

1. **Installation**: `pip install gpt-researcher`
2. **API Key**: Set up your API key for the LLM provider and search engine of your choice
3. **Configuration**: Customize parameters based on your research needs
4. **Implementation**: Use the example code to integrate into your application

More detailed instructions and examples can be found in the [GPT Researcher documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/deep_research)

Whether you're a developer building the next generation of research tools, an academic seeking deeper insights, or a business professional needing comprehensive analysis, GPT Researcher's deep research capabilities offer an accessible, powerful solution that rivals - and in many ways exceeds - the offerings from major AI companies.

The future of AI-powered research is here, and it's open source. ğŸ‰

Happy researching!


================================================
FILE: docs/blog/2025-03-10-stepping-into-the-story/index.md
================================================
---
slug: stepping-into-the-story
title: Stepping Into the Story of GPT Researcher
authors: [elishakay]
tags: [ai, gpt-researcher, prompts, dreams, community]
image: https://github.com/user-attachments/assets/f6e8a6b5-12f8-4faa-ae99-6a2fbaf23cc1
---
![GPTR reflecting ourselves](https://github.com/user-attachments/assets/f6e8a6b5-12f8-4faa-ae99-6a2fbaf23cc1)

## The Barnes & Noble Dream

As a teenager, I remember stepping into Barnes & Noble, the scent of fresh pages filling the air, my fingers tracing the spines of books that had shaped minds and captured hearts. I'd whisper to myself: One day, my name will be here.

To me, books weren't just storiesâ€”they were reflections of the human experience, ways for people to see themselves more clearly. Shakespeare once said, â€œThe purpose of art is to hold a mirror up to nature.â€ That idea stuck with me. Art, writing, and storytelling weren't just about entertainment; they were about understanding ourselves in new ways.

But the world changed. The bookstores faded, attention shifted, and the novelâ€”once the pinnacle of deep thought and reflectionâ€”gave way to new forms of engagement. The long, immersive experience of reading was replaced with something more dynamic, more interactive.

## The Journey into Coding: A Simba Moment

About 9 years ago, [much like Simba in The Lion King](https://open.spotify.com/track/3BUT32qmBXmlqp3EJkgRfp?si=0935ef6eedf247ed), I embarked on a new journey filled with doubt and uncertainty. Leaving my known world of writing, I stepped into the unknown realm of coding. It was a foreign language at firstâ€”endless lines of syntax, debugging errors that made no sense, and moments of frustration where I felt like an imposter in a world of developers.

The journey was toughâ€”I struggled to find my place, faced canceled contracts, and got my butt handed to me more times than I could count. Every rejection, every missed opportunity made me question if I had taken the wrong path. Maybe I wasn't meant to buildâ€”maybe I was meant to stay in the world of stories.

Even when I finally landed a job at Fiverr, working with JavaScript, MySQL, HTML, and CSS, I still felt like I had abandoned my identity as a writer.

## Discovering GPT Researcher

One night, about a year ago, deep into a rabbit hole of AI research, I stumbled upon GPT Researcher. The concept struck me instantlyâ€”AI wasn't just a tool; it was a means of expanding human knowledge, refining our questions, and reshaping how we approach research itself.

I reached out to Assaf, not expecting much. But instead of a polite acknowledgment, he welcomed me in. That momentâ€”seeing my first commit mergedâ€”felt like an echo of my old dream. Only this time, I wasn't just writing stories. I was building something that helped others uncover their own.

## The Wicked Witch of the Researcher's Mirror

Around that time, I found myself repeatedly asking GPT Researcher the same question:

"Who is Elisha Kramer?"

At first, it was like the Magic Mirror in Snow White, responding with something generic like, "Elisha Kramer is a software engineer with experience in web development." It pulled information from my LinkedIn, GitHub, and Udemy profiles, painting a picture of who I was professionally. But then, things got weird.

I made more commits to GPT Researcher. More contributions. And as I coded, I asked a different question.

"Who is ElishaKay on Github?"

As time went on, the answer changed since the Researcher was pulling new sources fresh off web search results.

"ElishaKay is an active open source contributor with multiple repositories and over 500 commits in the past year."

Holy Shnikes! It was learning. Another commit. Another feature. Another line of documentation. Time to get more specific.

"Who is ElishaKay of gpt-researcher?"

"ElishaKay is a core contributor of GPT Researcher, improving research workflows and enhancing AI retrieval through significant code and documentation contributions."

Now we were talking. But I wasn't done. Like the Wicked Witch, I kept coming back. More commits. More improvements. More features.

Until finally, I asked:

"Tell me about gpt-researcher and tips to improve it"

And GPT Researcher looked back at me and said:

"GPTR is a thriving open-source community. The best path forward is to continue investing in that community - through code contributions, documentation improvements, and helping new contributors get started. The project's strength lies in its collaborative nature."

And that's when I knewâ€”I wasn't just using GPT Researcher. I was becoming part of its story.

## AI as a mirror of ourselves

This evolving feedback helped me frame my own self-narrative. GPT Researcher wasn't just reflecting what was already knownâ€”it was pulling in context from both my work and the broader internet.

It was reflecting back my own journey, refining it with each step, blurring the illusion of a fixed identity, and embracing an evolving one.

Every query, every commit, every improvement shaped the toolâ€”and in turn, it shaped me.

## Building as a Community

GPT Researcher isn't just a tool. It's a reflection of the open-source spirit, a living, evolving ecosystem where knowledge isn't static but constantly refined. It isn't just answering questions; it's engaging in a dialogue, shaping and reshaping narratives based on the latest contributions, research, and discoveries.
It isn't just about me anymore. It's about us.
A network of 138 contributors. An open-source project watched by 20,000 stars. A collective movement pushing the boundaries of AI-driven research.

Every researcher, every developer, every curious mind who refines their questions, contributes a feature, or engages with the tool is part of something bigger. AI isn't just some black box spitting out answersâ€”it's a tool that helps us refine our own thinking, challenge assumptions, and expand our understanding.
It's an iterative process, just like life itself.
The more context we provide, the better the insights we get. The more we engage, the more it reflects back not just who we were but who we are becoming.

## A Story Still Being Written

So while I once dreamed of seeing my name on a book spine in Barnes & Noble, I now see something even greater.
My words aren't bound to a single bookâ€”they live within every line of code, every contribution, every researcher refining their questions.
We are not just users. We are builders.
And this isn't just my story.
It's our story.
And it's still being written.


================================================
FILE: docs/discord-bot/deploy-commands.js
================================================
const { Client, GatewayIntentBits, REST, Routes } = require('discord.js');
require('dotenv').config();

// Create a new REST client and set your bot token
const rest = new REST({ version: '10' }).setToken(process.env.DISCORD_BOT_TOKEN);

// Define commands
const commands = [
    {
        name: 'ping',
        description: 'Replies with Pong!',
    },
    {
        name: 'ask',
        description: 'Ask a question to the bot',
    },
];

// Deploy commands to Discord
(async () => {
    try {
        console.log('Started refreshing application (/) commands.');

        await rest.put(Routes.applicationCommands(process.env.DISCORD_CLIENT_ID), {
            body: commands,
        });

        console.log('Successfully reloaded application (/) commands.');
    } catch (error) {
        console.error(error);
    }
})();



================================================
FILE: docs/discord-bot/Dockerfile
================================================
FROM node:18.17.0-alpine
WORKDIR /app
COPY ./package.json ./
RUN npm install --legacy-peer-deps
COPY . .
CMD ["node", "index.js"]


================================================
FILE: docs/discord-bot/Dockerfile.dev
================================================
FROM node:18.17.0-alpine
WORKDIR /app
COPY ./package.json ./
RUN npm install --legacy-peer-deps
RUN npm install -g nodemon
COPY . .
CMD ["nodemon", "index.js"]


================================================
FILE: docs/discord-bot/gptr-webhook.js
================================================
// gptr-webhook.js
const WebSocket = require('ws');

let socket = null;
const responseCallbacks = new Map(); // Using Map for multiple callbacks

async function initializeWebSocket() {
  if (!socket) {
    const host = 'gpt-researcher:8000';
    const ws_uri = `ws://${host}/ws`;

    socket = new WebSocket(ws_uri);

    socket.onopen = () => {
      console.log('WebSocket connection established');
    };

    socket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      console.log('WebSocket data received:', data);

      // Get the callback for this request
      const callback = responseCallbacks.get('current');
      
      if (data.type === 'report') {
        // Send progress updates
        if (callback && callback.onProgress) {
          callback.onProgress(data.output);
        }
      } else if (data.content === 'dev_team_result') {
        // Send final result
        if (callback && callback.onComplete) {
          callback.onComplete(data.output);
          responseCallbacks.delete('current'); // Clean up after completion
        }
      }
    };

    socket.onclose = () => {
      console.log('WebSocket connection closed');
      socket = null;
    };

    socket.onerror = (error) => {
      console.error('WebSocket error:', error);
    };
  }
}

async function sendWebhookMessage({query, moreContext}) {
  return new Promise((resolve, reject) => {
    if (!socket || socket.readyState !== WebSocket.OPEN) {
      initializeWebSocket();
    }

    const data = {
      task: `${query}. Additional context: ${moreContext}`,
      report_type: 'research_report',
      report_source: 'web',
      tone: 'Objective',
      headers: {},
      repo_name: typeof repoName === 'undefined' || repoName === '' ? 'assafelovic/gpt-researcher' : repoName,
      branch_name: typeof branchName === 'undefined' || branchName === '' ? 'master' : branchName
    };

    const payload = "start " + JSON.stringify(data);

    // Store both progress and completion callbacks
    responseCallbacks.set('current', {
      onProgress: (progressData) => {
        resolve({ type: 'progress', data: progressData });
      },
      onComplete: (finalData) => {
        resolve({ type: 'complete', data: finalData });
      }
    });

    if (socket.readyState === WebSocket.OPEN) {
      socket.send(payload);
      console.log('Message sent:', payload);
    } else {
      socket.onopen = () => {
        socket.send(payload);
        console.log('Message sent after connection:', payload);
      };
    }
  });
}

module.exports = {
  sendWebhookMessage
};


================================================
FILE: docs/discord-bot/index.js
================================================
require('dotenv').config();
const { Client, GatewayIntentBits, ActionRowBuilder, Events, ModalBuilder, TextInputBuilder, TextInputStyle, ChannelType } = require('discord.js');
const keepAlive = require('./server');
const { sendWebhookMessage } = require('./gptr-webhook');
const { jsonrepair } = require('jsonrepair');
const { EmbedBuilder } = require('discord.js');

const client = new Client({
  intents: [
    GatewayIntentBits.Guilds,
    GatewayIntentBits.GuildMessages,
    GatewayIntentBits.MessageContent,
    GatewayIntentBits.DirectMessages
  ],
});

function splitMessage(message, chunkSize = 1500) {
  const chunks = [];
  for (let i = 0; i < message.length; i += chunkSize) {
    chunks.push(message.slice(i, i + chunkSize));
  }
  return chunks;
}

client.on('ready', () => {
  console.log(`Logged in as ${client.user.tag}!`);
});

// Cooldown object to store the last message time for each channel
const cooldowns = {};

client.on('messageCreate', async message => {
  if (message.author.bot) return;
  // only share the /ask guide when a new message is posted in the help forum -  limit to every 30 minutes per post
  console.log(`Channel Data: ${message.channel.id}`);
  console.log(`Message Channel Data: ${console.log(JSON.stringify(message.channel, null, 2))}`);
  
  const channelId = message.channel.id;
  const channelParentId = message.channel.parentId;
  //return if its not posted in the help forum
  if(channelParentId != '1129339320562626580') return
  
  const now = Date.now();
  const cooldownAmount = 30 * 60 * 1000; // 30 minutes in milliseconds

  if (!cooldowns[channelId] || (now - cooldowns[channelId]) > cooldownAmount) {
    // await message.reply('please use the /ask command to launch a report by typing `/ask` into the chatbox & hitting ENTER.');

    const exampleEmbed = new EmbedBuilder()
      .setTitle('please use the /ask command to launch a report by typing `/ask` into the chatbox & hitting ENTER.')
      .setImage('https://media.discordapp.net/attachments/1127851779573420053/1285577932353568902/ask.webp?ex=66eb6fff&is=66ea1e7f&hm=32bc8335ed4c09c15a8541c058bbd513cf2ce757221a116d9c248c39a12d75df&=&format=webp&width=1740&height=704');
    
    message.channel.send({ embeds: [exampleEmbed] });
    cooldowns[channelId] = now;
  }
});


client.on(Events.InteractionCreate, async interaction => {
  if (interaction.isChatInputCommand()) {
    if (interaction.commandName === 'ask') {
      const modal = new ModalBuilder()
        .setCustomId('myModal')
        .setTitle('Ask the AI Researcher');

      const queryInput = new TextInputBuilder()
        .setCustomId('queryInput')
        .setLabel('Your question')
        .setStyle(TextInputStyle.Paragraph)
        .setPlaceholder('What are you exploring today / what tickles your mind?');

      const moreContextInput = new TextInputBuilder()
        .setCustomId('moreContextInput')
        .setLabel('Additional context (optional)')
        .setStyle(TextInputStyle.Paragraph)
        .setPlaceholder('Any additional context or details that would help us understand your question better?')
        .setRequired(false);

      const firstActionRow = new ActionRowBuilder().addComponents(queryInput);
      const secondActionRow = new ActionRowBuilder().addComponents(moreContextInput);

      modal.addComponents(firstActionRow, secondActionRow);

      await interaction.showModal(modal);
    }
  } else if (interaction.isModalSubmit()) {
    if (interaction.customId === 'myModal') {
      const query = interaction.fields.getTextInputValue('queryInput');
      const moreContext = interaction.fields.getTextInputValue('moreContextInput');

      let thread;
      if (interaction?.channel?.type === ChannelType.GuildText) {
        thread = await interaction.channel.threads.create({
          name: `Discussion: ${query.slice(0, 30)}...`,
          autoArchiveDuration: 60,
          reason: 'Discussion thread for the query',
        });
      }

      await interaction.deferUpdate();

      runDevTeam({ interaction, query, moreContext, thread })
        .catch(console.error);
    }
  }
});

async function runDevTeam({ interaction, query, moreContext, thread }) {
  const queryToDisplay = `**user query**: ${query}. 
                          ${moreContext ? '\n**more context**: ' + moreContext : ''} 
                          \nBrowsing the web to investigate your query... give me a minute or so`;

  if (!thread) {
    await interaction.followUp({ content: queryToDisplay });
  } else {
    await thread.send(queryToDisplay);
  }

  try {
    while (true) {
      const response = await sendWebhookMessage({ query, moreContext });
      
      if (response.type === 'progress') {
        // Handle progress updates
        const progressChunks = splitMessage(response.data);
        for (const chunk of progressChunks) {
          if (!thread) {
            await interaction.followUp({ content: chunk });
          } else {
            await thread.send(chunk);
          }
        }
      } else if (response.type === 'complete') {
        // Handle final result
        if (response.data && response.data.rubber_ducker_thoughts) {
          let rubberDuckerChunks = '';
          let theGuidance = response.data.rubber_ducker_thoughts;

          try {
            rubberDuckerChunks = splitMessage(theGuidance);
          } catch (error) {
            console.error('Error splitting messages:', error);
            rubberDuckerChunks = splitMessage(typeof theGuidance === 'object' ? JSON.stringify(theGuidance) : theGuidance);
          }

          for (const chunk of rubberDuckerChunks) {
            if (!thread) {
              await interaction.followUp({ content: chunk });
            } else {
              await thread.send(chunk);
            }
          }
        }
        break; // Exit the loop when we get the final result
      }
    }

    return true;
  } catch (error) {
    console.error({ content: 'Error handling message:', error });
    if (!thread) {
      return await interaction.followUp({ content: 'There was an error processing your request.' });
    } else {
      return await thread.send('There was an error processing your request.');
    }
  }
}

keepAlive();
client.login(process.env.DISCORD_BOT_TOKEN);


================================================
FILE: docs/discord-bot/package.json
================================================
{
  "name": "Discord-Bot-JS",
  "version": "1.0.0",
  "description": "",
  "main": "index.js",
  "dependencies": {
    "discord.js": "^14.16.1",
    "dotenv": "^16.4.5",
    "express": "^4.17.1",
    "jsonrepair": "^3.8.0",
    "nodemon": "^3.1.4",
    "ws": "^8.18.0"
  },
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "dev": "nodemon --legacy-watch index.js"
  },
  "keywords": [],
  "author": "",
  "license": "ISC"
}



================================================
FILE: docs/discord-bot/server.js
================================================
const express = require("express")

const server = express()

server.all("/", (req, res) => {
  res.send("Bot is running!")
})

function keepAlive() {
  server.listen(5000, () => {
    console.log("Server is ready.")
  })

  // Handle uncaught exceptions
  process.on("uncaughtException", (err) => {
    console.error("Uncaught Exception:", err);
    // Graceful shutdown logic
    // process.exit(1); // Exit process to trigger Docker's restart policy
  });

  // Handle unhandled promise rejections
  process.on("unhandledRejection", (reason, promise) => {
    console.error("Unhandled Rejection at:", promise, "reason:", reason);
    // Graceful shutdown logic
    // process.exit(1); // Exit process to trigger Docker's restart policy
  });
}

module.exports = keepAlive


================================================
FILE: docs/discord-bot/commands/ask.js
================================================
const { SlashCommandBuilder } = require('discord.js');

module.exports = {
    data: new SlashCommandBuilder()
        .setName('ask')
        .setDescription('Ask a question to the bot'),
    async execute(interaction) {
        await interaction.reply('Please provide your question.');
    }
};



================================================
FILE: docs/docs/contribute.md
================================================
# Contribute

We highly welcome contributions! Please check out [contributing](https://github.com/assafelovic/gpt-researcher/blob/master/CONTRIBUTING.md) if you're interested.

Please check out our [roadmap](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap) page and reach out to us via our [Discord community](https://discord.gg/QgZXvJAccX) if you're interested in joining our mission.


================================================
FILE: docs/docs/faq.md
================================================
# FAQ

### How do I get started?
It really depends on what you're aiming for. 

If you're looking to connect your AI application to the internet with Tavily tailored API, check out the [Tavily API](https://docs.tavily.com/docs/tavily-api/introductionn) documentation. 
If you're looking to build and deploy our open source autonomous research agent GPT Researcher, please see [GPT Researcher](/docs/gpt-researcher/getting-started/introduction) documentation.
You can also check out demos and examples for inspiration [here](/docs/examples/examples).

### What is GPT Researcher?

GPT Researcher is a popular open source autonomous research agent that takes care of the tedious task of research for you, by scraping, filtering and aggregating over 20+ web sources per a single research task.

GPT Researcher is built with best practices for leveraging LLMs (prompt engineering, RAG, chains, embeddings, etc), and is optimized for quick and efficient research. It is also fully customizable and can be tailored to your specific needs.

To learn more about GPT Researcher, check out the [documentation page](/docs/gpt-researcher/getting-started/introduction).

### How much does each research run cost?

A research task using GPT Researcher costs around $0.01 per a single run (for GPT-4 usage). We're constantly optimizing LLM calls to reduce costs and improve performance. 

### How do you ensure the report is factual and accurate?

we do our best to ensure that the information we provide is factual and accurate. We do this by using multiple sources, and by using proprietary AI to score and rank the most relevant and accurate information. We also use proprietary AI to filter out irrelevant information and sources.

Lastly, by using RAG and other techniques, we ensure that the information is relevant to the context of the research task, leading to more accurate generative AI content and reduced hallucinations.

### What are your plans for the future?

We're constantly working on improving our products and services. We're currently working on improving our search API together with design partners, and adding more data sources to our search engine. We're also working on improving our research agent GPT Researcher, and adding more features to it while growing our amazing open source community.

If you're interested in our roadmap or looking to collaborate, check out our [roadmap page](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap). 

Feel free to [contact us](mailto:assafelovic@gmail.com) if you have any further questions or suggestions!


================================================
FILE: docs/docs/roadmap.md
================================================
# Roadmap

We're constantly working on additional features and improvements to our products and services. We're also working on new products and services to help you build better AI applications using [GPT Researcher](https://gptr.dev).

Our vision is to build the #1 autonomous research agent for AI developers and researchers, and we're excited to have you join us on this journey!

The roadmap is prioritized based on the following goals: Performance, Quality, Modularity and Conversational flexibility. The roadmap is public and can be found [here](https://trello.com/b/3O7KBePw/gpt-researcher-roadmap). 

Interested in collaborating or contributing? Check out our [contributing page](/docs/contribute) for more information.


================================================
FILE: docs/docs/welcome.md
================================================
# Welcome

Hey there! ğŸ‘‹

We're a team of AI researchers and developers who are passionate about building the next generation of AI assistants. 
Our mission is to empower individuals and organizations with accurate, unbiased, and factual information.

### GPT Researcher
Quickly accessing relevant and trustworthy information is more crucial than ever. However, we've learned that none of today's search engines provide a suitable tool that provides factual, explicit and objective answers without the need to continuously click and explore multiple sites for a given research task. 

This is why we've built the trending open source **[GPT Researcher](https://github.com/assafelovic/gpt-researcher)**. GPT Researcher is an autonomous agent that takes care of the tedious task of research for you, by scraping, filtering and aggregating over 20+ web sources per a single research task. 

To learn more about GPT Researcher, check out the [documentation page](/docs/gpt-researcher/getting-started/introduction).



================================================
FILE: docs/docs/examples/custom_prompt.py
================================================
"""
Custom Prompt Example for GPT Researcher

This example demonstrates how to use the custom_prompt parameter to customize report generation
based on specific formatting requirements or content needs.
"""

import asyncio
import nest_asyncio  # Required for notebooks/interactive environments

# Apply nest_asyncio to allow for nested event loops (needed in notebooks)
nest_asyncio.apply()

from gpt_researcher import GPTResearcher


async def custom_report_example():
    """Demonstrate various custom prompt examples with GPT Researcher."""
    
    # Define your research query
    query = "What are the latest advancements in renewable energy?"
    report_type = "research_report"
    
    # Initialize the researcher
    researcher = GPTResearcher(
        query=query,
        report_type=report_type,
        verbose=True  # Set to True to see detailed logs
    )
    
    # Conduct the research (this step is the same regardless of custom prompts)
    print("ğŸ” Conducting research...")
    await researcher.conduct_research()
    print("âœ… Research completed!\n")
    
    # Example 1: Standard report (no custom prompt)
    print("\nğŸ“ EXAMPLE 1: STANDARD REPORT\n" + "="*40)
    standard_report = await researcher.write_report()
    print(f"Standard Report Length: {len(standard_report.split())} words\n")
    print(standard_report[:500] + "...\n")  # Print first 500 chars
    
    # Example 2: Short summary with custom prompt
    print("\nğŸ“ EXAMPLE 2: SHORT SUMMARY\n" + "="*40)
    short_prompt = "Provide a brief summary of the research findings in 2-3 paragraphs without citations."
    short_report = await researcher.write_report(custom_prompt=short_prompt)
    print(f"Short Report Length: {len(short_report.split())} words\n")
    print(short_report + "\n")
    
    # Example 3: Bullet point format
    print("\nğŸ“ EXAMPLE 3: BULLET POINT FORMAT\n" + "="*40)
    bullet_prompt = "List the top 5 advancements in renewable energy as bullet points with a brief explanation for each."
    bullet_report = await researcher.write_report(custom_prompt=bullet_prompt)
    print(bullet_report + "\n")
    
    # Example 4: Question and answer format
    print("\nğŸ“ EXAMPLE 4: Q&A FORMAT\n" + "="*40)
    qa_prompt = "Present the research as a Q&A session with 5 important questions and detailed answers about renewable energy advancements."
    qa_report = await researcher.write_report(custom_prompt=qa_prompt)
    print(qa_report[:500] + "...\n")  # Print first 500 chars
    
    # Example 5: Technical audience
    print("\nğŸ“ EXAMPLE 5: TECHNICAL AUDIENCE\n" + "="*40)
    technical_prompt = "Create a technical summary focusing on engineering challenges and solutions in renewable energy. Use appropriate technical terminology."
    technical_report = await researcher.write_report(custom_prompt=technical_prompt)
    print(technical_report[:500] + "...\n")  # Print first 500 chars
    
    # Show research costs
    print("\nğŸ’° RESEARCH COSTS")
    print(f"Total tokens used: {researcher.get_costs()}")


if __name__ == "__main__":
    asyncio.run(custom_report_example())



================================================
FILE: docs/docs/examples/detailed_report.md
================================================
# Detailed Report

## Overview

The `DetailedReport` class inspired by the recent STORM paper, is a powerful component of GPT Researcher, designed to generate comprehensive reports on complex topics. It's particularly useful for creating long-form content that exceeds the typical limits of LLM outputs. This class orchestrates the research process, breaking down the main query into subtopics, conducting in-depth research on each, and combining the results into a cohesive, detailed report.

Located in `backend/report_types/detailed_report.py` in the [GPT Researcher GitHub repository](https://github.com/assafelovic/gpt-researcher), this class leverages the capabilities of the `GPTResearcher` agent to perform targeted research and generate content.

## Key Features

- Breaks down complex topics into manageable subtopics
- Conducts in-depth research on each subtopic
- Generates a comprehensive report with introduction, table of contents, and body
- Avoids redundancy by tracking previously written content
- Supports asynchronous operations for improved performance

## Class Structure

### Initialization

The `DetailedReport` class is initialized with the following parameters:

- `query`: The main research query
- `report_type`: Type of the report
- `report_source`: Source of the report
- `source_urls`: Initial list of source URLs
- `config_path`: Path to the configuration file
- `tone`: Tone of the report (using the `Tone` enum)
- `websocket`: WebSocket for real-time communication
- `subtopics`: Optional list of predefined subtopics
- `headers`: Optional headers for HTTP requests

## How It Works

1. The `DetailedReport` class starts by conducting initial research on the main query.
2. It then breaks down the topic into subtopics.
3. For each subtopic, it:
   - Conducts focused research
   - Generates draft section titles
   - Retrieves relevant previously written content to avoid redundancy
   - Writes a report section
4. Finally, it combines all subtopic reports, adds a table of contents, and includes source references to create the final detailed report.

## Usage Example

Here's how you can use the `DetailedReport` class in your project:

```python
import asyncio
from fastapi import WebSocket
from gpt_researcher.utils.enum import Tone
from backend.report_type import DetailedReport

async def generate_report(websocket: WebSocket):
    detailed_report = DetailedReport(
        query="The impact of artificial intelligence on modern healthcare",
        report_type="research_report",
        report_source="web_search",
        source_urls=[],  # You can provide initial source URLs if available
        config_path="path/to/config.yaml",
        tone=Tone.FORMAL,
        websocket=websocket,
        subtopics=[],  # You can provide predefined subtopics if desired
        headers={}  # Add any necessary HTTP headers
    )

    final_report = await detailed_report.run()
    return final_report

# In your FastAPI app
@app.websocket("/generate_report")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    report = await generate_report(websocket)
    await websocket.send_text(report)
```

This example demonstrates how to create a `DetailedReport` instance and run it to generate a comprehensive report on the impact of AI on healthcare.

## Conclusion

The `DetailedReport` class is a sophisticated tool for generating in-depth, well-structured reports on complex topics. By breaking down the main query into subtopics and leveraging the power of GPT Researcher, it can produce content that goes beyond the typical limitations of LLM outputs. This makes it an invaluable asset for researchers, content creators, and anyone needing detailed, well-researched information on a given topic.


================================================
FILE: docs/docs/examples/examples.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Tavily Samples
"""

"""
## Setup
"""

# install tavily
!pip install tavily-python

# import and connect
from tavily import TavilyClient
client = TavilyClient(api_key="")

# simple query using tavily's advanced search
client.search("What happend in the latest burning man floods?", search_depth="advanced")
# Output:
#   {'query': 'What happend in the latest burning man floods?',

#    'follow_up_questions': ['How severe were the floods at Burning Man?',

#     'What were the impacts of the floods?',

#     'How did the organizers handle the floods at Burning Man?'],

#    'answer': None,

#    'images': None,

#    'results': [{'content': "This yearâ€™s rains opened the floodgates for Burning Man criticism  Give Newsletters Site search Vox main menu Filed under: The Burning Man flameout, explained Climate change â€” and schadenfreude\xa0â€” finally caught up to the survivalist cosplayers. Share this story Share  Has Burning Man finally lost its glamour?  September 1, after most of the scheduled events and live performances were canceled due to the weather, Burning Man organizers closed routes in and out of the area, forcing attendees to stay behindShare Attendees look at a rainbow over flooding on a desert plain on September 1, 2023, after heavy rains turned the annual Burning Man festival site in Nevada's Black Rock desert into a mud...",

#      'url': 'https://www.vox.com/culture/2023/9/6/23861675/burning-man-2023-mud-stranded-climate-change-playa-foot',

#      'score': 0.9797,

#      'raw_content': None},

#     {'content': 'Tens of thousands of Burning Man festivalgoers are slowly making their way home from the Nevada desert after muddy conditions from heavy rains made it nearly impossible to leave over the weekend.  according to burningman.org.  Though the death at this year\'s Burning Man is still being investigated, a social media hoax was blamed for spreading rumors that it\'s due to a breakout of Ebola.  "Thank goodness this community knows how to take care of each other," the Instagram page for Burning Man Information Radio wrote on a post predicting more rain.News Burning Man attendees make mass exodus after being stranded in the mud at festival A caravan of festivalgoers were backed up as much as eight hours when they were finally allowed to leave...',

#      'url': 'https://www.today.com/news/what-is-burning-man-flood-death-rcna103231',

#      'score': 0.9691,

#      'raw_content': None},

#     {'content': 'â€œIt was a perfect, typical Burning Man weather until Friday â€” then the rain started coming down hard," said Phillip Martin, 37. "Then it turned into Mud Fest."  After more than a half-inch (1.3 centimeters) of rain fell Friday, flooding turned the playa to foot-deep mud â€” closing roads and forcing burners to lean on each other for help.  ABC News Video Live Shows Election 2024 538 Stream on No longer stranded, tens of thousands clean up and head home after Burning Man floods  Mark Fromson, 54, who goes by the name â€œStuffyâ€ on the playa, had been staying in an RV, but the rains forced him to find shelter at another camp, where fellow burners provided him food and cover.RENO, Nev. -- The traffic jam leaving the Burning Man festival eased up considerably Tuesday as the exodus from the mud-caked Nevada desert entered another day following massive rain that left tens of thousands of partygoers stranded for days.',

#      'url': 'https://abcnews.go.com/US/wireStory/wait-times-exit-burning-man-drop-after-flooding-102936473',

#      'score': 0.9648,

#      'raw_content': None},

#     {'content': 'Burning Man hit by heavy rains, now mud soaked.People there told to conserve food and water as they shelter in place.(Video: Josh Keppel) pic.twitter.com/DuBj0Ejtb8  More on this story Burning Man revelers begin exodus from festival after road reopens Officials investigate death at Burning Man as thousands stranded by floods  Burning Man festival-goers trapped in desert as rain turns site to mud Tens of thousands of â€˜burnersâ€™ urged to conserve food and water as rain and flash floods sweep Nevada  Burning Man festivalgoers surrounded by mud in Nevada desert â€“ video Burning Man attendees roadblocked by climate activists: â€˜They have a privileged mindsetâ€™Last year, Burning Man drew approximately 80,000 people. This year, only about 60,000 were expected - with many citing the usual heat and dust and eight-hour traffic jams when they tried to leave.',

#      'url': 'https://www.theguardian.com/culture/2023/sep/02/burning-man-festival-mud-trapped-shelter-in-place',

#      'score': 0.9618,

#      'raw_content': None},

#     {'content': 'Skip links Live Navigation menu Live Death at Burning Man investigated in US, thousands stranded by flooding  Attendees trudged through mud, many barefoot or wearing plastic bags on their feet. The revellers were urged to shelter in place and conserve food, water and other supplies.  Thousands of festivalgoers remain stranded as organisers close vehicular traffic to the festival site following storm flooding in Nevadaâ€™s desert.  Authorities in Nevada are investigating a death at the site of the Burning Man festival, where thousands of attendees remained stranded after flooding from storms swept through the Nevada desert in3 Sep 2023. Authorities in Nevada are investigating a death at the site of the Burning Man festival, where thousands of attendees remained stranded after flooding from storms swept through the ...',

#      'url': 'https://www.aljazeera.com/news/2023/9/3/death-under-investigation-after-storm-flooding-at-burning-man-festival',

#      'score': 0.9612,

#      'raw_content': None}],

#    'response_time': 6.23}

"""
## Sample 1: Reseach Report using Tavily and GPT-4 with Langchain
"""

# install lanchain
!pip install langchain

# set up openai api key
openai_api_key = ""

# libraries
from langchain.adapters.openai import convert_openai_messages
from langchain_community.chat_models import ChatOpenAI

# setup query
query = "What happend in the latest burning man floods?"

# run tavily search
content = client.search(query, search_depth="advanced")["results"]

# setup prompt
prompt = [{
    "role": "system",
    "content":  f'You are an AI critical thinker research assistant. '\
                f'Your sole purpose is to write well written, critically acclaimed,'\
                f'objective and structured reports on given text.'
}, {
    "role": "user",
    "content": f'Information: """{content}"""\n\n' \
               f'Using the above information, answer the following'\
               f'query: "{query}" in a detailed report --'\
               f'Please use MLA format and markdown syntax.'
}]

# run gpt-4
lc_messages = convert_openai_messages(prompt)
report = ChatOpenAI(model='gpt-4',openai_api_key=openai_api_key).invoke(lc_messages).content

# print report
print(report)

# Output:
#   # The Burning Man Festival 2023: A Festival Turned Mud Fest

#   

#   **Abstract:** The Burning Man Festival of 2023 in Nevadaâ€™s Black Rock desert will be remembered for a significant event: a heavy rainfall that turned the festival site into a muddy mess, testing the community spirit of the annual event attendees and stranding tens of thousands of festival-goers. 

#   

#   **Keywords:** Burning Man Festival, flooding, rainfall, mud, community spirit, Nevada, Black Rock desert, stranded attendees, shelter

#   

#   ---

#   ## 1. Introduction

#   

#   The Burning Man Festival, an annual event known for its art installations, free spirit, and community ethos, faced an unprecedented challenge in 2023 due to heavy rains that flooded the festival site, turning it into a foot-deep mud pit[^1^][^2^]. The festival, held in Nevada's Black Rock desert, is known for its harsh weather conditions, including heat and dust, but this was the first time the event was affected to such an extent by rainfall[^4^].

#   

#   ## 2. Impact of the Rain

#   

#   The heavy rains started on Friday, and more than a half-inch of rain fell, leading to flooding that turned the playa into a foot-deep mud pit[^2^]. The roads were closed due to the muddy conditions, stranding tens of thousands of festival-goers[^2^][^5^]. The burners, as the attendees are known, were forced to lean on each other for help[^2^].

#   

#   ## 3. Community Spirit Tested

#   

#   The unexpected weather conditions put the Burning Man community spirit to the test[^1^]. Festival-goers found themselves sheltering in place, conserving food and water, and helping each other out[^3^]. For instance, Mark Fromson, who had been staying in an RV, was forced to find shelter at another camp due to the rains, where fellow burners provided him with food and cover[^2^].

#   

#   ## 4. Exodus After Rain

#   

#   Despite the challenges, the festival-goers made the best of the situation. Once the rain stopped and things dried up a bit, the party quickly resumed[^3^]. A day later than scheduled, the massive wooden effigy known as the Man was set ablaze[^5^]. As the situation improved, thousands of Burning Man attendees began their mass exodus from the festival site[^5^].

#   

#   ## 5. Conclusion

#   

#   The Burning Man Festival of 2023 will be remembered for the community spirit shown by the attendees in the face of heavy rainfall and flooding. Although the event was marred by the weather, the festival-goers managed to make the best of the situation, demonstrating the resilience and camaraderie that the Burning Man Festival is known for.

#   

#   ---

#   **References**

#   

#   [^1^]: "Attendees walk through a muddy desert plain..." NPR. 2023. https://www.npr.org/2023/09/02/1197441202/burning-man-festival-rains-floods-stranded-nevada.

#   

#   [^2^]: â€œ'It was a perfect, typical Burning Man weather until Friday...'" ABC News. 2023. https://abcnews.go.com/US/wireStory/wait-times-exit-burning-man-drop-after-flooding-102936473.

#   

#   [^3^]: "The latest on the Burning Man flooding..." WUNC. 2023. https://www.wunc.org/2023-09-03/the-latest-on-the-burning-man-flooding.

#   

#   [^4^]: "Burning Man hit by heavy rains, now mud soaked..." The Guardian. 2023. https://www.theguardian.com/culture/2023/sep/02/burning-man-festival-mud-trapped-shelter-in-place.

#   

#   [^5^]: "One day later than scheduled, the massive wooden effigy known as the Man was set ablaze..." CNN. 2023. https://www.cnn.com/2023/09/05/us/burning-man-storms-shelter-exodus-tuesday/index.html.




================================================
FILE: docs/docs/examples/examples.md
================================================
# Simple Run

### Run PIP Package
```python
from gpt_researcher import GPTResearcher
import asyncio

### Using Quick Run
async def main():
    """
    This is a sample script that shows how to run a research report.
    """
    # Query
    query = "What happened in the latest burning man floods?"

    # Report Type
    report_type = "research_report"

    # Initialize the researcher
    researcher = GPTResearcher(query=query, report_type=report_type, config_path=None)
    # Conduct research on the given query
    await researcher.conduct_research()
    # Write the report
    report = await researcher.write_report()
    
    return report


if __name__ == "__main__":
    asyncio.run(main())

# Custom Report Formatting

### Using Custom Prompts
```python
from gpt_researcher import GPTResearcher
import asyncio


async def main():
    """
    This example shows how to use custom prompts to control report formatting.
    """
    # Query
    query = "What are the latest advancements in renewable energy?"

    # Report Type
    report_type = "research_report"

    # Initialize the researcher
    researcher = GPTResearcher(query=query, report_type=report_type)
    
    # Conduct research on the given query
    await researcher.conduct_research()
    
    # Generate a standard report
    standard_report = await researcher.write_report()
    print("Standard Report Generated")
    
    # Generate a short, concise report using custom_prompt
    custom_prompt = "Provide a concise summary in 2 paragraphs without citations."
    short_report = await researcher.write_report(custom_prompt=custom_prompt)
    print("Short Report Generated")
    
    # Generate a bullet-point format report
    bullet_prompt = "List the top 5 advancements as bullet points with brief explanations."
    bullet_report = await researcher.write_report(custom_prompt=bullet_prompt)
    print("Bullet-Point Report Generated")
    
    return standard_report, short_report, bullet_report


if __name__ == "__main__":
    asyncio.run(main())

For more comprehensive examples of using custom prompts, see the `custom_prompt.py` file included in the examples directory.
```


================================================
FILE: docs/docs/examples/hybrid_research.md
================================================
# Hybrid Research

## Introduction

GPT Researcher can combine web search capabilities with local document analysis to provide comprehensive, context-aware research results. 

This guide will walk you through the process of setting up and running hybrid research using GPT Researcher.

## Prerequisites

Before you begin, ensure you have the following:

- Python 3.10 or higher installed on your system
- pip (Python package installer)
- An OpenAI API key (you can also choose other supported [LLMs](../gpt-researcher/llms/llms.md))
- A Tavily API key (you can also choose other supported [Retrievers](../gpt-researcher/search-engines/retrievers.md))

## Installation

```bash
pip install gpt-researcher
```

## Setting Up the Environment

Export your API keys as environment variables:

```bash
export OPENAI_API_KEY=your_openai_api_key_here
export TAVILY_API_KEY=your_tavily_api_key_here
```

For custom OpenAI-compatible APIs, you can also set:

```bash
export OPENAI_BASE_URL=your_custom_api_base_url_here
```

Alternatively, you can set these in your Python script:

```python
import os
os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'
os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key_here'
os.environ['OPENAI_BASE_URL'] = 'your_custom_api_base_url_here'  # Optional
```
Set the environment variable REPORT_SOURCE to an empty string "" in default.py
## Preparing Documents

### 1. Local Documents
1. Create a directory named `my-docs` in your project folder.
2. Place all relevant local documents (PDFs, TXTs, DOCXs, etc.) in this directory.

### 2. Online Documents
1. Here is an example of your online document URL example: https://xxxx.xxx.pdf (supports file formats like PDFs, TXTs, DOCXs, etc.) 


## Running Hybrid Research By "Local Documents"

Here's a basic script to run hybrid research:

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_research_report(query: str, report_type: str, report_source: str) -> str:
    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source)
    research = await researcher.conduct_research()
    report = await researcher.write_report()
    return report

if __name__ == "__main__":
    query = "How does our product roadmap compare to emerging market trends in our industry?"
    report_source = "hybrid"

    report = asyncio.run(get_research_report(query=query, report_type="research_report", report_source=report_source))
    print(report)
```

## Running Hybrid Research By "Online Documents"

Here's a basic script to run hybrid research:

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_research_report(query: str, report_type: str, report_source: str) -> str:
    researcher = GPTResearcher(query=query, report_type=report_type, document_urls=document_urls, report_source=report_source)
    research = await researcher.conduct_research()
    report = await researcher.write_report()
    return report

if __name__ == "__main__":
    query = "How does our product roadmap compare to emerging market trends in our industry?"
    report_source = "hybrid"
    document_urls = ["https://xxxx.xxx.pdf", "https://xxxx.xxx.doc"]

    report = asyncio.run(get_research_report(query=query, report_type="research_report", document_urls=document_urls, report_source=report_source))
    print(report)
```

To run the script:

1. Save it as `run_research.py`
2. Execute it with: `python run_research.py`

## Understanding the Results

The output will be a comprehensive research report that combines insights from both web sources and your local documents. The report typically includes an executive summary, key findings, detailed analysis, comparisons between your internal data and external trends, and recommendations based on the combined insights.

## Troubleshooting

1. **API Key Issues**: Ensure your API keys are correctly set and have the necessary permissions.
2. **Document Loading Errors**: Check that your local documents are in supported formats and are not corrupted.
3. **Memory Issues**: For large documents or extensive research, you may need to increase your system's available memory or adjust the `chunk_size` in the document processing step.

## FAQ

**Q: How long does a typical research session take?**
A: The duration varies based on the complexity of the query and the amount of data to process. It can range from 1-5 minutes for very comprehensive research.

**Q: Can I use GPT Researcher with other language models?**
A: Currently, GPT Researcher is optimized for OpenAI's models. Support for other models can be found [here](../gpt-researcher/llms/llms.md).

**Q: How does GPT Researcher handle conflicting information between local and web sources?**
A: The system attempts to reconcile differences by providing context and noting discrepancies in the final report. It prioritizes more recent or authoritative sources when conflicts arise.

**Q: Is my local data sent to external servers during the research process?**
A: No, your local documents are processed on your machine. Only the generated queries and synthesized information (not raw data) are sent to external services for web research.

For more information and updates, please visit the [GPT Researcher GitHub repository](https://github.com/assafelovic/gpt-researcher).



================================================
FILE: docs/docs/examples/pip-run.ipynb
================================================
# Jupyter notebook converted to Python script.

import os
os.environ['OPENAI_API_KEY'] = 'your_openai_api_key'
os.environ['TAVILY_API_KEY'] = 'your_tavily_api_key' # Get a free key here: https://app.tavily.com

!pip install -U gpt-researcher nest_asyncio

import nest_asyncio # required for notebooks
nest_asyncio.apply()

from gpt_researcher import GPTResearcher
import asyncio

async def get_report(query: str, report_type: str) -> str:
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    
    # Get additional information
    research_context = researcher.get_research_context()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()
    
    return report, research_context, research_costs, research_images, research_sources

if __name__ == "__main__":
    query = "Should I invest in Nvidia?"
    report_type = "research_report"

    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))
    
    print("Report:")
    print(report)
    print("\nResearch Costs:")
    print(costs)
    print("\nResearch Images:")
    print(images)
    print("\nResearch Sources:")
    print(sources)



================================================
FILE: docs/docs/examples/sample_report.py
================================================
import nest_asyncio  # required for notebooks

nest_asyncio.apply()

from gpt_researcher import GPTResearcher
import asyncio


async def get_report(query: str, report_type: str, custom_prompt: str = None):
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    
    # Generate report with optional custom prompt
    report = await researcher.write_report(custom_prompt=custom_prompt)

    # Get additional information
    research_context = researcher.get_research_context()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()

    return report, research_context, research_costs, research_images, research_sources


if __name__ == "__main__":
    query = "Should I invest in Nvidia?"
    report_type = "research_report"

    # Standard report
    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))

    print("Standard Report:")
    print(report)
    
    # Custom report with specific formatting requirements
    custom_prompt = "Answer in short, 2 paragraphs max without citations. Focus on the most important facts for investors."
    custom_report, _, _, _, _ = asyncio.run(get_report(query, report_type, custom_prompt))
    
    print("\nCustomized Short Report:")
    print(custom_report)
    
    print("\nResearch Costs:")
    print(costs)
    print("\nNumber of Research Images:")
    print(len(images))
    print("\nNumber of Research Sources:")
    print(len(sources))


================================================
FILE: docs/docs/examples/sample_sources_only.py
================================================
from gpt_researcher import GPTResearcher
import asyncio


async def get_report(query: str, report_source: str, sources: list) -> str:
    researcher = GPTResearcher(query=query, report_source=report_source, source_urls=sources)
    research_context = await researcher.conduct_research()
    return await researcher.write_report()

if __name__ == "__main__":
    query = "What are the biggest trends in AI lately?"
    report_source = "static"
    sources = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://www.ibm.com/think/insights/artificial-intelligence-trends",
        "https://www.forbes.com/advisor/business/ai-statistics"
    ]

    report = asyncio.run(get_report(query=query, report_source=report_source, sources=sources))
    print(report)



================================================
FILE: docs/docs/gpt-researcher/context/azure-storage.md
================================================
# Azure Storage

If you want to use Azure Blob Storage as the source for your GPT Researcher report context, follow these steps:

> **Step 1** - Set these environment variables with a .env file in the root folder

```bash
AZURE_CONNECTION_STRING=
AZURE_CONTAINER_NAME=
```

> **Step 2** - Add the `azure-storage-blob` dependency to your requirements.txt file

```bash
azure-storage-blob
```

> **Step 3** - When running the GPTResearcher class, pass the `report_source` as `azure`

```python
report = GPTResearcher(
    query="What happened in the latest burning man floods?",
    report_type="research_report",
    report_source="azure",
)
```


================================================
FILE: docs/docs/gpt-researcher/context/data-ingestion.md
================================================
# Data Ingestion

When you're dealing with a large amount of context data, you may want to start meditating upon a standalone process for data ingestion.

Some signs that the system is telling you to move to a custom data ingestion process:

- Your embedding model is hitting API rate limits
- Your Langchain VectorStore's underlying database needs rate limiting
- You sense you need to add custom pacing/throttling logic in your Python code

As mentioned in our [YouTube Tutorial Series](https://www.youtube.com/watch?v=yRuduRCblbg), GPTR is using [Langchain Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) and [Langchain VectorStores](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/) under the hood.

These are 2 beautiful abstractions that make the GPTR architecture highly configurable.

The current research flow, whether you're generating reports on web or local documents, is:

```bash
Step 1: transform your content (web results or local documents) into Langchain Documents
```

```bash
Step 2: Insert your Langchain Documents into a Langchain VectorStore
```

```bash
Step 3: Pass your Langchain Vectorstore into your GPTR report ([more on that here](https://docs.gptr.dev/docs/gpt-researcher/context/vector-stores) and below)
```

Code samples below:

Assuming your .env variables are like so:

```bash
OPENAI_API_KEY={Your OpenAI API Key here}
TAVILY_API_KEY={Your Tavily API Key here}
PGVECTOR_CONNECTION_STRING=postgresql://username:password...
```

Below is a custom data ingestion process that you can use to ingest your data into a Langchain VectorStore. See a [full working example here](https://github.com/assafelovic/gpt-researcher/pull/819#issue-2501632831).
In this example, we're using a Postgres VectorStore to embed data of a Github Branch, but you can use [any supported Langchain VectorStore](https://python.langchain.com/v0.2/docs/integrations/vectorstores/).

Note that when you create the Langchain Documents, you should include as metadata the `source` and `title` fields in order for GPTR to leverage your Documents seamlessly. In the example below, we're splitting the documents list into chunks of 100 & then inserting 1 chunk at a time into the vector store.

### Step 1: Transform your content into Langchain Documents

```python
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

async def transform_to_langchain_docs(self, directory_structure):
    documents = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)
    run_timestamp = datetime.utcnow().strftime('%Y%m%d%H%M%S')

    for file_name in directory_structure:
        if not file_name.endswith('/'):
            try:
                content = self.repo.get_contents(file_name, ref=self.branch_name)
                try:
                    decoded_content = base64.b64decode(content.content).decode()
                except Exception as e:
                    print(f"Error decoding content: {e}")
                    print("the problematic file_name is", file_name)
                    continue
                print("file_name", file_name)
                print("content", decoded_content)

                # Split each document into smaller chunks
                chunks = splitter.split_text(decoded_content)

                # Extract metadata for each chunk
                for index, chunk in enumerate(chunks):
                    metadata = {
                        "id": f"{run_timestamp}_{uuid4()}",  # Generate a unique UUID for each document
                        "source": file_name,
                        "title": file_name,
                        "extension": os.path.splitext(file_name)[1],
                        "file_path": file_name
                    }
                    document = Document(
                        page_content=chunk,
                        metadata=metadata
                    )
                    documents.append(document)

            except Exception as e:
                print(f"Error saving to vector store: {e}")
                return None

    await save_to_vector_store(documents)
```

### Step 2: Insert your Langchain Documents into a Langchain VectorStore

```python
from langchain_postgres import PGVector
from langchain_postgres.vectorstores import PGVector
from sqlalchemy.ext.asyncio import create_async_engine

from langchain_community.embeddings import OpenAIEmbeddings

async def save_to_vector_store(self, documents):
    # The documents are already Document objects, so we don't need to convert them
    embeddings = OpenAIEmbeddings()
    # self.vector_store = FAISS.from_documents(documents, embeddings)
    pgvector_connection_string = os.environ["PGVECTOR_CONNECTION_STRING"]

    collection_name = "my_docs"

    vector_store = PGVector(
        embeddings=embeddings,
        collection_name=collection_name,
        connection=pgvector_connection_string,
        use_jsonb=True
    )

    # for faiss
    # self.vector_store = vector_store.add_documents(documents, ids=[doc.metadata["id"] for doc in documents])

    # Split the documents list into chunks of 100
    for i in range(0, len(documents), 100):
        chunk = documents[i:i+100]
        # Insert the chunk into the vector store
        vector_store.add_documents(chunk, ids=[doc.metadata["id"] for doc in chunk])
```

### Step 3: Pass your Langchain Vectorstore into your GPTR report

```python
async_connection_string = pgvector_connection_string.replace("postgresql://", "postgresql+psycopg://")

# Initialize the async engine with the psycopg3 driver
async_engine = create_async_engine(
    async_connection_string,
    echo=True
)

async_vector_store = PGVector(
    embeddings=embeddings,
    collection_name=collection_name,
    connection=async_engine,
    use_jsonb=True
)


researcher = GPTResearcher(
    query=query,
    report_type="research_report",
    report_source="langchain_vectorstore",
    vector_store=async_vector_store,
)
await researcher.conduct_research()
report = await researcher.write_report()
```   


================================================
FILE: docs/docs/gpt-researcher/context/filtering-by-domain.md
================================================
# Filtering by Domain

You can filter web search results by specific domains when using either the Tavily or Google Search retrievers. This functionality is available across all interfaces - pip package, NextJS frontend, and vanilla JS frontend.

> Note: We welcome contributions to add domain filtering to other retrievers!

To set Tavily as a retriever, you'll need to set the `RETRIEVER` environment variable to `tavily` and set the `TAVILY_API_KEY` environment variable to your Tavily API key.

```bash
RETRIEVER=tavily
TAVILY_API_KEY=your_tavily_api_key
```

To set Google as a retriever, you'll need to set the `RETRIEVER` environment variable to `google` and set the `GOOGLE_API_KEY` and `GOOGLE_CX_KEY` environment variables to your Google API key and Google Custom Search Engine ID.

```bash
RETRIEVER=google
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CX_KEY=your_google_custom_search_engine_id
```

## Using the Pip Package

When using the pip package, you can pass a list of domains to filter results:

```python
report = GPTResearcher(
    query="Latest AI Startups",
    report_type="research_report",
    report_source="web",
    domains=["forbes.com", "techcrunch.com"]
)
```

## Using the NextJS Frontend

When using the NextJS frontend, you can pass a list of domains to filter results via the Settings Modal:

![Settings Modal](./img/nextjs-filter-by-domain.JPG)

## Using the Vanilla JS Frontend

When using the Vanilla JS frontend, you can pass a list of domains to filter results via the relevant input field:

![Filter by Domain](./img/vanilla-filter-by-domains.png)

## Filtering by Domain based on URL Param

If you'd like to show off for your work pals how GPTR is the ultra-customizable Deep Research Agent, you can send them a link to your hosted GPTR app with the domain filter included in the URL itself.

This can be handle for demonstrating a proof of concept of the Research Agent tailored to a specific domain. Some examples below:

### Single Domain:

https://app.gptr.dev/?domains=wikipedia.org

### Multiple Domains:

https://app.gptr.dev/?domains=wired.com,forbes.com,wikipedia.org

The `https://app.gptr.dev` part of the URL can be replaces with [the domain that you deployed GPTR on](https://docs.gptr.dev/docs/gpt-researcher/getting-started/linux-deployment).



================================================
FILE: docs/docs/gpt-researcher/context/local-docs.md
================================================
# Local Documents

## Just Local Docs

You can instruct the GPT Researcher to run research tasks based on your local documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.

Step 1: Add the env variable `DOC_PATH` pointing to the folder where your documents are located.

```bash
export DOC_PATH="./my-docs"
```

Step 2: 
 - If you're running the frontend app on localhost:8000, simply select "My Documents" from the "Report Source" Dropdown Options.
 - If you're running GPT Researcher with the [PIP package](https://docs.tavily.com/docs/gpt-researcher/gptr/pip-package), pass the `report_source` argument as "local" when you instantiate the `GPTResearcher` class [code sample here](https://docs.gptr.dev/docs/gpt-researcher/context/tailored-research).

## Local Docs + Web (Hybrid)

![GPT Researcher hybrid research](./img/gptr-hybrid.png)

Check out the blog post on [Hybrid Research](https://docs.gptr.dev/blog/gptr-hybrid) to learn more about how to combine local documents with web research.
```



================================================
FILE: docs/docs/gpt-researcher/context/tailored-research.md
================================================
# Tailored Research

The GPT Researcher package allows you to tailor the research to your needs such as researching on specific sources (URLs) or local documents, and even specify the agent prompt instruction upon which the research is conducted.

### Research on Specific Sources ğŸ“š

You can specify the sources you want the GPT Researcher to research on by providing a list of URLs. The GPT Researcher will then conduct research on the provided sources via `source_urls`. 

If you want GPT Researcher to perform additional research outside of the URLs you provided, i.e., conduct research on various other websites that it finds suitable for the query/sub-query, you can set the parameter `complement_source_urls` as `True`. Default value of `False` will only scour the websites you provide via `source_urls`.


```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_report(query: str, report_type: str, sources: list) -> str:
    researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=False)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report

if __name__ == "__main__":
    query = "What are the biggest trends in AI lately?"
    report_source = "static"
    sources = [
        "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "https://www.ibm.com/think/insights/artificial-intelligence-trends",
        "https://www.forbes.com/advisor/business/ai-statistics"
    ]
    report = asyncio.run(get_report(query=query, report_source=report_source, sources=sources))
    print(report)
```

### Specify Agent Prompt ğŸ“

You can specify the agent prompt instruction upon which the research is conducted. This allows you to guide the research in a specific direction and tailor the report layout.
Simply pass the prompt as the `query` argument to the `GPTResearcher` class and the "custom_report" `report_type`.

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_report(prompt: str, report_type: str) -> str:
    researcher = GPTResearcher(query=prompt, report_type=report_type)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report
    
if __name__ == "__main__":
    report_type = "custom_report"
    prompt = "Research the latest advancements in AI and provide a detailed report in APA format including sources."

    report = asyncio.run(get_report(prompt=prompt, report_type=report_type))
    print(report)
```

### Research on Local Documents ğŸ“„
You can instruct the GPT Researcher to research on local documents by providing the path to those documents. Currently supported file formats are: PDF, plain text, CSV, Excel, Markdown, PowerPoint, and Word documents.

*Step 1*: Add the env variable `DOC_PATH` pointing to the folder where your documents are located.

For example:

```bash
export DOC_PATH="./my-docs"
```

*Step 2*: When you create an instance of the `GPTResearcher` class, pass the `report_source` argument as `"local"`.

GPT Researcher will then conduct research on the provided documents.

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_report(query: str, report_source: str) -> str:
    researcher = GPTResearcher(query=query, report_source=report_source)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report
    
if __name__ == "__main__":
    query = "What can you tell me about myself based on my documents?"
    report_source = "local" # "local" or "web"

    report = asyncio.run(get_report(query=query, report_source=report_source))
    print(report)
```

### Hybrid Research ğŸ”„
You can combine the above methods to conduct hybrid research. For example, you can instruct the GPT Researcher to research on both web sources and local documents.
Simply provide the sources and set the `report_source` argument as `"hybrid"` and watch the magic happen.

Please note! You should set the proper retrievers for the web sources and doc path for local documents for this to work.
To learn more about retrievers check out the [Retrievers](https://docs.gptr.dev/docs/gpt-researcher/search-engines/retrievers) documentation.


### Research on LangChain Documents ğŸ¦œï¸ğŸ”—
You can instruct the GPT Researcher to research on a list of langchain document instances.

For example:

```python
from langchain_core.documents import Document
from typing import List, Dict
from gpt_researcher import GPTResearcher
from langchain_postgres.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings
from sqlalchemy import create_engine
import asyncio



CONNECTION_STRING = 'postgresql://someuser:somepass@localhost:5432/somedatabase'

def get_retriever(collection_name: str, search_kwargs: Dict[str, str]):
    engine = create_engine(CONNECTION_STRING)
    embeddings =  OpenAIEmbeddings()

    index = PGVector.from_existing_index(
        use_jsonb=True,
        embedding=embeddings,
        collection_name=collection_name,
        connection=engine,
    )

    return index.as_retriever(search_kwargs=search_kwargs)


async def get_report(query: str, report_type: str, report_source: str, documents: List[Document]) -> str:
    researcher = GPTResearcher(query=query, report_type=report_type, report_source=report_source, documents=documents)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report

if __name__ == "__main__":
    query = "What can you tell me about blue cheese based on my documents?"
    report_type = "research_report"
    report_source = "langchain_documents"

    # using a LangChain retriever to get all the documents regarding cheese
    # https://api.python.langchain.com/en/latest/retrievers/langchain_core.retrievers.BaseRetriever.html#langchain_core.retrievers.BaseRetriever.invoke
    langchain_retriever = get_retriever("cheese_collection", { "k": 3 })
    documents = langchain_retriever.invoke("All the documents about cheese")
    report = asyncio.run(get_report(query=query, report_type=report_type, report_source=report_source, documents=documents))
    print(report)
```



================================================
FILE: docs/docs/gpt-researcher/context/vector-stores.md
================================================
# Vector Stores

The GPT Researcher package allows you to integrate with existing langchain vector stores that have been populated.
For a complete list of supported langchain vector stores, please refer to this [link](https://python.langchain.com/v0.2/docs/integrations/vectorstores/).

You can create a set of embeddings and langchain documents and store them in any supported vector store of your choosing.
GPT-Researcher will work with any langchain vector store that implements the `asimilarity_search` method.

**If you want to use the existing knowledge in your vector store, make sure to set `report_source="langchain_vectorstore"`. Any other settings will add additional information from scraped data and might contaminate your vectordb (See _How to add scraped data to your vector store_ for more context)**

## Faiss
```python
from gpt_researcher import GPTResearcher

from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# exerpt taken from - https://paulgraham.com/wealth.html
essay = """
May 2004

(This essay was originally published in Hackers & Painters.)

If you wanted to get rich, how would you do it? I think your best bet would be to start or join a startup.
That's been a reliable way to get rich for hundreds of years. The word "startup" dates from the 1960s,
but what happens in one is very similar to the venture-backed trading voyages of the Middle Ages.

Startups usually involve technology, so much so that the phrase "high-tech startup" is almost redundant.
A startup is a small company that takes on a hard technical problem.

Lots of people get rich knowing nothing more than that. You don't have to know physics to be a good pitcher.
But I think it could give you an edge to understand the underlying principles. Why do startups have to be small?
Will a startup inevitably stop being a startup as it grows larger?
And why do they so often work on developing new technology? Why are there so many startups selling new drugs or computer software,
and none selling corn oil or laundry detergent?


The Proposition

Economically, you can think of a startup as a way to compress your whole working life into a few years.
Instead of working at a low intensity for forty years, you work as hard as you possibly can for four.
This pays especially well in technology, where you earn a premium for working fast.

Here is a brief sketch of the economic proposition. If you're a good hacker in your mid twenties,
you can get a job paying about $80,000 per year. So on average such a hacker must be able to do at
least $80,000 worth of work per year for the company just to break even. You could probably work twice
as many hours as a corporate employee, and if you focus you can probably get three times as much done in an hour.[1]
You should get another multiple of two, at least, by eliminating the drag of the pointy-haired middle manager who
would be your boss in a big company. Then there is one more multiple: how much smarter are you than your job
description expects you to be? Suppose another multiple of three. Combine all these multipliers,
and I'm claiming you could be 36 times more productive than you're expected to be in a random corporate job.[2]
If a fairly good hacker is worth $80,000 a year at a big company, then a smart hacker working very hard without 
any corporate bullshit to slow him down should be able to do work worth about $3 million a year.
...
...
...
"""

document = [Document(page_content=essay)]
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30, separator="\n")
docs = text_splitter.split_documents(documents=document)

vector_store = FAISS.from_documents(documents, OpenAIEmbeddings())

query = """
    Summarize the essay into 3 or 4 succinct sections.
    Make sure to include key points regarding wealth creation.

    Include some recommendations for entrepreneurs in the conclusion.
"""


# Create an instance of GPTResearcher
researcher = GPTResearcher(
    query=query,
    report_type="research_report",
    report_source="langchain_vectorstore",
    vector_store=vector_store,
)

# Conduct research and write the report
await researcher.conduct_research()
report = await researcher.write_report()
```


## PGVector
```python
from gpt_researcher import GPTResearcher
from langchain_postgres.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings

CONNECTION_STRING = 'postgresql://someuser:somepass@localhost:5432/somedatabase'


# assuming the vector store exists and contains the relevent documents
# also assuming embeddings have been or will be generated
vector_store = PGVector.from_existing_index(
    use_jsonb=True,
    embedding=OpenAIEmbeddings(),
    collection_name='some collection name',
    connection=CONNECTION_STRING,
    async_mode=True,
)

query = """
    Create a short report about apples.
    Include a section about which apples are considered best
    during each season.
"""

# Create an instance of GPTResearcher
researcher = GPTResearcher(
    query=query,
    report_type="research_report",
    report_source="langchain_vectorstore",
    vector_store=vector_store, 
)

# Conduct research and write the report
await researcher.conduct_research()
report = await researcher.write_report()
```
## Adding Scraped Data to your vector store

In some cases in which you want to store the scraped data and documents into your own vector store for future usages, GPT-Researcher also allows you to do so seamlessly just by inputting your vector store (make sure to set `report_source` value to something other than `langchain_vectorstore`)

```python
from gpt_researcher import GPTResearcher

from langchain_community.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())

query = "The best LLM"

# Create an instance of GPTResearcher
researcher = GPTResearcher(
    query=query,
    report_type="research_report",
    report_source="web",
    vector_store=vector_store, 
)

# Conduct research, the context will be chunked and stored in the vector_store
await researcher.conduct_research()

# Query the 5 most relevant context in our vector store
related_contexts = await vector_store.asimilarity_search("GPT-4", k = 5) 
print(related_contexts)
print(len(related_contexts)) #Should be 5 
```



================================================
FILE: docs/docs/gpt-researcher/context/img/nextjs-filter-by-domain.JPG
================================================
[Non-text file]


================================================
FILE: docs/docs/gpt-researcher/frontend/discord-bot.md
================================================
# Discord Bot

## Intro

You can either leverage the official GPTR Discord bot or create your own custom bot.

To add the official GPTR Discord bot, simply [click here to invite GPTR to your Discord server](https://discord.com/oauth2/authorize?client_id=1281438963034361856&permissions=1689934339898432&integration_type=0&scope=bot).


## To create your own discord bot with GPTR functionality

Add a .env file in the root of the project and add the following:

```
DISCORD_BOT_TOKEN=
DISCORD_CLIENT_ID=
```
You can fetch the token from the Discord Developer Portal by following these steps:

1. Go to https://discord.com/developers/applications/
2. Click the "New Application" button and give your bot a name
3. Navigate to the OAuth2 tab to generate an invite URL for your bot
4. Under "Scopes", select "bot"

![OAuth2 URL Generator](./img/oath2-url-generator.png)

5. Select the appropriate bot permissions

![Bot Permissions](./img/bot-permissions.png)

6. Copy your bot's token and paste it into the `.env` file you created earlier


### Deploying the bot commands

```bash
node deploy-commands.js
```

In our case, this will make the "ask" and "ping" commands available to users of the bot.


### Running the bot via Docker

```bash
docker compose --profile discord run --rm discord-bot
```

### Running the bot via CLI

```bash
# install dependencies
npm install

# run the bot
npm run dev
```

### Installing NodeJS and NPM on Ubuntu

```bash
#install nvm
wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.4/install.sh | bash

export NVM_DIR="$([ -z "${XDG_CONFIG_HOME-}" ] && printf %s "${HOME}/.nvm" || printf %s "${XDG_CONFIG_HOME}/nvm")"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh" # This loads nvm

# install nodejs
nvm install 18.17.0

# install npm
sudo apt-get install npm
```



================================================
FILE: docs/docs/gpt-researcher/frontend/embed-script.md
================================================
# Embed Script

The embed script enables you to embed the latest GPTR NextJS app into your web app.

To achieve this, simply add these 2 script tags into your HTML:

```javascript
<script>localStorage.setItem("GPTR_API_URL", "http://localhost:8000");</script>
<script src="https://app.gptr.dev/embed.js"></script>
```

Here's a minmalistic HTML example (P.S. You can also save this as an index.html file and open it with your Web Browser)

```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT Researcher Embed Demo</title>
</head>
<body style="margin: 0; padding: 0;">
    <!-- GPT Researcher Embed -->
    <script>localStorage.setItem("GPTR_API_URL", "http://localhost:8000");</script>
    <script src="https://app.gptr.dev/embed.js"></script>
</body>
</html>
```

This example relies on setting a custom localstorage value for `GPTR_API_URL`. To point your embedded frontend at a custom GPTR API Server, feel free to edit `http://localhost:8000` to your custom GPTR server address.


================================================
FILE: docs/docs/gpt-researcher/frontend/introduction.md
================================================
# Intro to the Frontends

The frontends enhance GPT-Researcher by providing:

1. Intuitive Research Interface: Streamlined input for research queries.
2. Real-time Progress Tracking: Visual feedback on ongoing research tasks.
3. Interactive Results Display: Easy-to-navigate presentation of findings.
4. Customizable Settings: Adjust research parameters to suit specific needs.
5. Responsive Design: Optimal experience across various devices.

These features aim to make the research process more efficient and user-friendly, complementing GPT-Researcher's powerful agent capabilities.

## Choosing an Option

- Static Frontend: Quick setup, lightweight deployment.
- NextJS Frontend: Feature-rich, scalable, better performance and SEO (For production, NextJS is recommended)
- Discord Bot: Integrate GPT-Researcher into your Discord server.


================================================
FILE: docs/docs/gpt-researcher/frontend/nextjs-frontend.md
================================================
# NextJS Frontend

This frontend project aims to enhance the user experience of GPT Researcher, providing an intuitive and efficient interface for automated research. It offers two deployment options to suit different needs and environments.

#### Demo
<iframe height="400" width="700" src="https://github.com/user-attachments/assets/092e9e71-7e27-475d-8c4f-9dddd28934a3" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

View an in-depth Product Tutorial here: [GPT-Researcher Frontend Tutorial](https://www.youtube.com/watch?v=hIZqA6lPusk)


## NextJS Frontend App

The React app (located in the `frontend` directory) is our Frontend 2.0 which we hope will enable us to display the robustness of the backend on the frontend, as well.

It comes with loads of added features, such as: 
 - a drag-n-drop user interface for uploading and deleting files to be used as local documents by GPTResearcher.
 - a GUI for setting your GPTR environment variables.
 - the ability to trigger the multi_agents flow via the Backend Module or Langgraph Cloud Host (currently in closed beta).
 - stability fixes
 - and more coming soon!

### Run the NextJS React App with Docker

> **Step 1** - [Install Docker](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)

> **Step 2** - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'

> **Step 3** - Within the docker-compose file comment out services that you don't want to run with Docker.

```bash
docker compose up --build
```

If that doesn't work, try running it without the dash:
```bash
docker compose up --build
```

> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:
 - the Python server running on localhost:8000
 - the React app running on localhost:3000

Visit localhost:3000 on any browser and enjoy researching!

If, for some reason, you don't want to run the GPTR API Server on localhost:8000, no problem! You can set the `NEXT_PUBLIC_GPTR_API_URL` environment variable in your `.env` file to the URL of your GPTR API Server.

For example:
```
NEXT_PUBLIC_GPTR_API_URL=https://app.gptr.dev
```

Or: 
```
NEXT_PUBLIC_GPTR_API_URL=http://localhost:7000
```

## Running NextJS Frontend via CLI

A more robust solution with enhanced features and performance.

#### Prerequisites
- Node.js (v18.17.0 recommended)
- npm

#### Setup and Running

1. Navigate to NextJS directory:
   ```
   cd nextjs
   ```

2. Set up Node.js:
   ```
   nvm install 18.17.0
   nvm use v18.17.0
   ```

3. Install dependencies:
   ```
   npm install --legacy-peer-deps
   ```

4. Start development server:
   ```
   npm run dev
   ```

5. Access at `http://localhost:3000`

Note: Requires backend server on `localhost:8000` as detailed in option 1.


### Adding Google Analytics

To add Google Analytics to your NextJS frontend, simply add the following to your `.env` file:

```
NEXT_PUBLIC_GA_MEASUREMENT_ID="G-G2YVXKHJNZ"
```


================================================
FILE: docs/docs/gpt-researcher/frontend/react-package.md
================================================
# React Package

The GPTR React package is an abstraction on top of the NextJS app meant to empower users to easily import the GPTR frontend into any React App. The package is [available on npm](https://www.npmjs.com/package/gpt-researcher-ui).


## Installation

```bash
npm install gpt-researcher-ui
```

## Usage

```javascript
import React from 'react';
import { GPTResearcher } from 'gpt-researcher-ui';

function App() {
  return (
    <div className="App">
      <GPTResearcher 
        apiUrl="http://localhost:8000"
        defaultPrompt="What is quantum computing?"
        onResultsChange={(results) => console.log('Research results:', results)}
      />
    </div>
  );
}

export default App;
```


## Publishing to a private npm registry

If you'd like to build and publish the package into your own private npm registry, you can do so by running the following commands:

 ```bash
 cd frontend/nextjs/
 npm run build:lib
 npm run build:types
 npm publish
 ```
 



================================================
FILE: docs/docs/gpt-researcher/frontend/vanilla-js-frontend.md
================================================
# Vanilla JS Frontend

The VanillaJS frontend is a lightweight solution leveraging FastAPI to serve static files.

### Demo
<iframe height="400" width="700" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/dd6cf08f-b31e-40c6-9907-1915f52a7110" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

#### Prerequisites
- Python 3.11+
- pip

#### Setup and Running

1. Install required packages:
   ```
   pip install -r requirements.txt
   ```

2. Start the server:
   ```
   python -m uvicorn main:app
   ```

3. Access at `http://localhost:8000`



================================================
FILE: docs/docs/gpt-researcher/frontend/visualizing-websockets.md
================================================
# Visualizing Websockets

The GPTR Frontend is powered by Websockets streaming back from the Backend. This allows for real-time updates on the status of your research tasks, as well as the ability to interact with the Backend directly from the Frontend.


## Inspecting Websockets

When running reports via the frontend, you can inspect the websocket messages in the Network Tab.

Here's how: 

![image](https://github.com/user-attachments/assets/15fcb5a4-77ea-4b3b-87d7-55d4b6f80095)


## Am I polling the right URL?

If you're concerned that your frontend isn't hitting the right API Endpoint, you can check the URL in the Network Tab.

Click into the WS request & go to the "Headers" tab

![image](https://github.com/user-attachments/assets/dbd58c1d-3506-411a-852b-e1b133b6f5c8)

For debugging, have a look at the <a href="https://github.com/assafelovic/gpt-researcher/blob/master/frontend/nextjs/helpers/getHost.ts">getHost function.</a>


================================================
FILE: docs/docs/gpt-researcher/getting-started/cli.md
================================================
# Run with CLI

This command-line interface (CLI) tool allows you to generate research reports using the GPTResearcher class. It provides an easy way to conduct research on various topics and generate different types of reports.

## Installation

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/gpt-researcher.git
   cd gpt-researcher
   ```

2. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

3. Set up your environment variables:
   Create a `.env` file in the project root and add your API keys or other necessary configurations.

## Usage

The basic syntax for using the CLI is:

```
python cli.py "<query>" --report_type <report_type> [--tone <tone>]
```

### Arguments

- `query` (required): The research query you want to investigate.
- `--report_type` (required): The type of report to generate. Options include:
  - `research_report`: Summary - Short and fast (~2 min)
  - `detailed_report`: Detailed - In depth and longer (~5 min)
  - `resource_report`
  - `outline_report`
  - `custom_report`
  - `subtopic_report`
- `--tone` (optional): The tone of the report. Defaults to 'objective'. Options include:
  - `objective`: Impartial and unbiased presentation
  - `formal`: Academic standards with sophisticated language
  - `analytical`: Critical evaluation and examination
  - `persuasive`: Convincing viewpoint
  - `informative`: Clear and comprehensive information
  - `explanatory`: Clarifying complex concepts
  - `descriptive`: Detailed depiction
  - `critical`: Judging validity and relevance
  - `comparative`: Juxtaposing different theories
  - `speculative`: Exploring hypotheses
  - `reflective`: Personal insights
  - `narrative`: Story-based presentation
  - `humorous`: Light-hearted and engaging
  - `optimistic`: Highlighting positive aspects
  - `pessimistic`: Focusing on challenges

## Examples

1. Generate a quick research report on climate change:
   ```
   python cli.py "What are the main causes of climate change?" --report_type research_report
   ```

2. Create a detailed report on artificial intelligence with an analytical tone:
   ```
   python cli.py "The impact of artificial intelligence on job markets" --report_type detailed_report --tone analytical
   ```

3. Generate an outline report on renewable energy with a persuasive tone:
   ```
   python cli.py "Renewable energy sources and their potential" --report_type outline_report --tone persuasive
   ```

## Output

The generated report will be saved as a Markdown file in the `outputs` directory. The filename will be a unique UUID.

## Note

- The execution time may vary depending on the complexity of the query and the type of report requested.
- Make sure you have the necessary API keys and permissions set up in your `.env` file for the tool to function correctly.
- All tone options should be provided in lowercase.


================================================
FILE: docs/docs/gpt-researcher/getting-started/getting-started-with-docker.md
================================================
# Docker: Quickstart

> **Step 1** - Install & Open Docker Desktop

Follow instructions at https://www.docker.com/products/docker-desktop/


> **Step 2** - [Follow this flow](https://www.youtube.com/watch?v=x1gKFt_6Us4)

This mainly includes cloning the '.env.example' file, adding your API Keys to the cloned file and saving the file as '.env'

In `requirements.txt` add the relevant langchain packages for the LLM your choose (langchain-google-genai, langchain-deepseek, langchain_mistralai for example)

> **Step 3** - Within root, run with Docker.

```bash
docker-compose up --build
```

If that doesn't work, try running it without the dash:
```bash
docker compose up --build
```

> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:
 - the Python server running on localhost:8000
 - the React app running on localhost:3000

Visit localhost:3000 on any browser and enjoy researching!


## Running with the Docker CLI

If you want to run the Docker container without using docker-compose, you can use the following command:

```bash
docker run -it --name gpt-researcher -p 8000:8000 --env-file .env  -v /absolute/path/to/gptr_docs:/my-docs  gpt-researcher
```

This will run the Docker container and mount the `/gptr_docs` directory to the container's `/my-docs` directory for analysis by the GPTR API Server.



================================================
FILE: docs/docs/gpt-researcher/getting-started/getting-started.md
================================================
# Getting Started

> **Step 0** - Install Python 3.11 or later. [See here](https://www.tutorialsteacher.com/python/install-python) for a step-by-step guide.

> **Step 1** - Download the project and navigate to its directory

```bash
$ git clone https://github.com/assafelovic/gpt-researcher.git
$ cd gpt-researcher
```

> **Step 3** - Set up API keys using two methods: exporting them directly or storing them in a `.env` file.

For Linux/Temporary Windows Setup, use the export method:

```bash
export OPENAI_API_KEY={Your OpenAI API Key here}
export TAVILY_API_KEY={Your Tavily API Key here}
```

For custom OpenAI-compatible APIs (e.g., local models, other providers), you can also set:

```bash
export OPENAI_BASE_URL={Your custom API base URL here}
```

For a more permanent setup, create a `.env` file in the current `gpt-researcher` directory and input the env vars (without `export`).

- For LLM provider, we recommend **[OpenAI GPT](https://platform.openai.com/docs/guides/gpt)**, but you can use any other LLM model (including open sources). To learn how to change the LLM model, please refer to the [documentation](https://docs.gptr.dev/docs/gpt-researcher/llms/llms) page. 
- For web search API, we recommend **[Tavily Search API](https://app.tavily.com)**, but you can also refer to other search APIs of your choice by changing the search provider in config/config.py to `duckduckgo`, `google`, `bing`, `searchapi`, `serper`, `searx` and more. Then add the corresponding env API key.

## Quickstart

> **Step 1** - Install dependencies

```bash
$ pip install -r requirements.txt
```

> **Step 2** - Run the agent with FastAPI

```bash
$ uvicorn main:app --reload
```

> **Step 3** - Go to http://localhost:8000 on any browser and enjoy researching!

## Using Virtual Environment or Poetry
Select either based on your familiarity with each:

### Virtual Environment

#### *Establishing the Virtual Environment with Activate/Deactivate configuration*

Create a virtual environment using the `venv` package with the environment name `<your_name>`, for example, `env`. Execute the following command in the PowerShell/CMD terminal:

```bash
python -m venv env
```

To activate the virtual environment, use the following activation script in PowerShell/CMD terminal:

```bash
.\env\Scripts\activate
```

To deactivate the virtual environment, run the following deactivation script in PowerShell/CMD terminal:

```bash
deactivate
```

#### *Install the dependencies for a Virtual environment*

After activating the `env` environment, install dependencies using the `requirements.txt` file with the following command:

```bash
python -m pip install -r requirements.txt
```

<br />

### Poetry

#### *Establishing the Poetry dependencies and virtual environment with Poetry version `~1.7.1`*

Install project dependencies and simultaneously create a virtual environment for the specified project. By executing this command, Poetry reads the project's "pyproject.toml" file to determine the required dependencies and their versions, ensuring a consistent and isolated development environment. The virtual environment allows for a clean separation of project-specific dependencies, preventing conflicts with system-wide packages and enabling more straightforward dependency management throughout the project's lifecycle.

```bash
poetry install
```

#### *Activate the virtual environment associated with a Poetry project*

By running this command, the user enters a shell session within the isolated environment associated with the project, providing a dedicated space for development and execution. This virtual environment ensures that the project dependencies are encapsulated, avoiding conflicts with system-wide packages. Activating the Poetry shell is essential for seamlessly working on a project, as it ensures that the correct versions of dependencies are used and provides a controlled environment conducive to efficient development and testing.

```bash
poetry shell
```

### *Run the app*
> Launch the FastAPI application agent on a *Virtual Environment or Poetry* setup by executing the following command:
```bash
python -m uvicorn main:app --reload
```
> Visit http://localhost:8000 in any web browser and explore your research!

<br />





================================================
FILE: docs/docs/gpt-researcher/getting-started/how-to-choose.md
================================================
# How to Choose

GPT Researcher is a powerful autonomous research agent designed to enhance and streamline your research processes. Whether you're a developer looking to integrate research capabilities into your project or an end-user seeking a comprehensive research solution, GPT Researcher offers flexible options to meet your needs.

We envision a future where AI agents collaborate to complete complex tasks, with research being a critical step in the process. GPT Researcher aims to be your go-to agent for any research task, regardless of complexity. It can be easily integrated into existing agent workflows, eliminating the need to create your own research agent from scratch.

## Options

GPT Researcher offers multiple ways to leverage its capabilities:

<img src="https://github.com/user-attachments/assets/305fa3b9-60fa-42b6-a4b0-84740ab6c665" alt="Logo" width="568"></img>
<br></br>

1. **GPT Researcher PIP agent**: Ideal for integrating GPT Researcher into your existing projects and workflows.
2. **Backend**: A backend service to interact with the frontend user interfaces, offering advanced features like detailed reports.
3. **Multi Agent System**: An advanced setup using LangGraph, offering the most comprehensive research capabilities.
4. **Frontend**: Several front-end solutions depending on your needs, including a simple HTML/JS version and a more advanced NextJS version.

## Usage Options

### 1. PIP Package

The PIP package is ideal for leveraging GPT Researcher as an agent in your preferred environment and code.

**Pros:**
- Easy integration into existing projects
- Flexible usage in multi-agent systems, chains, or workflows
- Optimized for production performance

**Cons:**
- Requires some coding knowledge
- May need additional setup for advanced features

**Installation:**
```
pip install gpt-researcher
```

**System Requirements:**
- Python 3.10+
- pip package manager

**Learn More:** [PIP Documentation](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package)

### 2. End-to-End Application

For a complete out-of-the-box experience, including a sleek frontend, you can clone our repository.

**Pros:**
- Ready-to-use frontend and backend services
- Includes advanced use cases like detailed report generation
- Optimal user experience

**Cons:**
- Less flexible than the PIP package for custom integrations
- Requires setting up the entire application

**Getting Started:**
1. Clone the repository: `git clone https://github.com/assafelovic/gpt-researcher.git`
2. Follow the [installation instructions](https://docs.gptr.dev/docs/gpt-researcher/getting-started)

**System Requirements:**
- Git
- Python 3.10+
- Node.js and npm (for frontend)

**Advanced Usage Example:** [Detailed Report Implementation](https://github.com/assafelovic/gpt-researcher/tree/master/backend/report_type/detailed_report)

### 3. Multi Agent System with LangGraph

We've collaborated with LangChain to support multi-agents with LangGraph and GPT Researcher, offering the most complex and comprehensive version of GPT Researcher.

**Pros:**
- Very detailed, customized research reports
- Inner AI agent loops and reasoning

**Cons:**
- More expensive and time-consuming
- Heavyweight for production use

This version is recommended for local, experimental, and educational use. We're working on providing a lighter version soon!

**System Requirements:**
- Python 3.10+
- LangGraph library

**Learn More:** [GPT Researcher x LangGraph](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph)

## Comparison Table

| Feature | PIP Package | End-to-End Application | Multi Agent System |
|---------|-------------|------------------------|---------------------|
| Ease of Integration | High | Medium | Low |
| Customization | High | Medium | High |
| Out-of-the-box UI | No | Yes | No |
| Complexity | Low | Medium | High |
| Best for | Developers | End-users | Researchers/Experimenters |

Please note that all options have been optimized and refined for production use.

## Deep Dive

To learn more about each of the options, check out these docs and code snippets:

1. **PIP Package**: 
   - Install: `pip install gpt-researcher`
   - [Integration guide](https://docs.gptr.dev/docs/gpt-researcher/gptr/pip-package)

2. **End-to-End Application**: 
   - Clone the repository: `git clone https://github.com/assafelovic/gpt-researcher.git`
   - [Installation instructions](https://docs.gptr.dev/docs/gpt-researcher/getting-started)

3. **Multi-Agent System**: 
   - [Multi-Agents code](https://github.com/assafelovic/gpt-researcher/tree/master/multi_agents)
   - [LangGraph documentation](https://docs.gptr.dev/docs/gpt-researcher/multi_agents/langgraph)
   - [Blog](https://docs.gptr.dev/blog/gptr-langgraph)

## Versioning and Updates

GPT Researcher is actively maintained and updated. To ensure you're using the latest version:

- For the PIP package: `pip install --upgrade gpt-researcher`
- For the End-to-End Application: Pull the latest changes from the GitHub repository
- For the Multi-Agent System: Check the documentation for compatibility with the latest LangChain and LangGraph versions

## Troubleshooting and FAQs

For common issues and questions, please refer to our [FAQ section](https://docs.gptr.dev/docs/faq) in the documentation.



================================================
FILE: docs/docs/gpt-researcher/getting-started/introduction.md
================================================
# Introduction

[![Official Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white)](https://gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted)](https://discord.gg/QgZXvJAccX)

[![GitHub Repo stars](https://img.shields.io/github/stars/assafelovic/gpt-researcher?style=social)](https://github.com/assafelovic/gpt-researcher)
[![Twitter Follow](https://img.shields.io/twitter/follow/assaf_elovic?style=social)](https://twitter.com/assaf_elovic)
[![PyPI version](https://badge.fury.io/py/gpt-researcher.svg)](https://badge.fury.io/py/gpt-researcher)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)

**[GPT Researcher](https://gptr.dev) is an autonomous agent designed for comprehensive online research on a variety of tasks.** 

The agent can produce detailed, factual and unbiased research reports, with customization options for focusing on relevant resources, outlines, and lessons. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses issues of speed, determinism and reliability, offering a more stable performance and increased speed through parallelized agent work, as opposed to synchronous operations.

## Why GPT Researcher?

- To form objective conclusions for manual research tasks can take time, sometimes weeks to find the right resources and information.
- Current LLMs are trained on past and outdated information, with heavy risks of hallucinations, making them almost irrelevant for research tasks.
- Current LLMs are limited to short token outputs which are not sufficient for long detailed research reports (2k+ words).
- Solutions that enable web search (such as ChatGPT + Web Plugin), only consider limited resources and content that in some cases result in superficial conclusions or biased answers.
- Using only a selection of resources can create bias in determining the right conclusions for research questions or tasks. 

## Architecture
The main idea is to run "planner" and "execution" agents, whereas the planner generates questions to research, and the execution agents seek the most related information based on each generated research question. Finally, the planner filters and aggregates all related information and creates a research report. <br /> <br /> 
The agents leverage both gpt-4o-mini and gpt-4o (128K context) to complete a research task. We optimize for costs using each only when necessary. **The average research task takes around 3 minutes to complete, and costs ~$0.1.**

<div align="center">
<img align="center" height="600" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/4ac896fd-63ab-4b77-9688-ff62aafcc527" />
</div>


More specifically:
* Create a domain specific agent based on research query or task.
* Generate a set of research questions that together form an objective opinion on any given task. 
* For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.
* For each scraped resources, summarize based on relevant information and keep track of its sources.
* Finally, filter and aggregate all summarized sources and generate a final research report.

## Demo
<iframe height="400" width="700" src="https://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

## Tutorials
 - [Video Tutorial Series](https://www.youtube.com/playlist?list=PLUGOUZPIB0F-qv6MvKq3HGr0M_b3U2ATv)
 - [How it Works](https://medium.com/better-programming/how-i-built-an-autonomous-ai-agent-for-online-research-93435a97c6c)
 - [How to Install](https://www.loom.com/share/04ebffb6ed2a4520a27c3e3addcdde20?sid=da1848e8-b1f1-42d1-93c3-5b0b9c3b24ea)
 - [Live Demo](https://www.loom.com/share/6a3385db4e8747a1913dd85a7834846f?sid=a740fd5b-2aa3-457e-8fb7-86976f59f9b8)
 - [Homepage](https://gptr.dev)

## Features
- ğŸ“ Generate research, outlines, resources and lessons reports
- ğŸ“œ Can generate long and detailed research reports (over 2K words)
- ğŸŒ Aggregates over 20 web sources per research to form objective and factual conclusions
- ğŸ–¥ï¸ Includes an easy-to-use web interface (HTML/CSS/JS)
- ğŸ” Scrapes web sources with javascript support
- ğŸ“‚ Keeps track and context of visited and used web sources
- ğŸ“„ Export research reports to PDF, Word and more...

Let's get started [here](/docs/gpt-researcher/getting-started/getting-started)!



================================================
FILE: docs/docs/gpt-researcher/getting-started/linux-deployment.md
================================================
# Running on Linux

This guide will walk you through the process of deploying GPT Researcher on a Linux server.

## Server Requirements

The default Ubuntu droplet option on [DigitalOcean](https://m.do.co/c/1a2af257efba) works well, but this setup should work on any hosting service with similar specifications:

- 2 GB RAM
- 1 vCPU
- 50 GB SSD Storage

Here's a screenshot of the recommended Ubuntu machine specifications:

![Ubuntu Server Specifications](https://github.com/user-attachments/assets/035865c0-d1a2-4990-b7fb-544c229d5198)

## Deployment Steps

After setting up your server, follow these steps to install Docker, Docker Compose, and Nginx.


Some more commands to achieve that:

### Step 1: Update the System
### First, ensure your package index is up-to-date:

```bash
sudo apt update
### Step 2: Install Git
### Git is a version control system. Install it using:

sudo apt install git -y

### Verify the installation by checking the Git version:
git --version
### Step 3: Install Docker
### Docker is a platform for developing, shipping, and running applications inside containers.

### Install prerequisites:

sudo apt install apt-transport-https ca-certificates curl software-properties-common -y
### Add Dockerâ€™s official GPG key:

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
### Set up the stable repository:

echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
### Update the package index again and install Docker:

sudo apt update
sudo apt install docker-ce -y
### Verify Docker installation:

sudo systemctl status docker
### Optionally, add your user to the docker group to run Docker without sudo:

sudo usermod -aG docker ${USER}
### Log out and back in for the group change to take effect.

Step 4: Install Nginx
### Nginx is a high-performance web server.

### Install Nginx:

sudo apt install nginx -y
### Start and enable Nginx:

sudo systemctl start nginx
sudo systemctl enable nginx
### Verify Nginx installation:

sudo systemctl status nginx
```

Here's your nginx config file:

```bash
events {}

http {
   server {
       listen 80;
       server_name name.example;
       
       client_max_body_size 64M;

       location / {
           proxy_pass http://localhost:3000;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection 'upgrade';
           proxy_set_header Host $host;
           proxy_cache_bypass $http_upgrade;
       }

       location ~ ^/(ws|upload|files|outputs|getConfig|setConfig) {
           proxy_pass http://localhost:8000;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection "Upgrade";
           proxy_set_header Host $host;
       }
   }
}
```

And if you're using SSL:

```nginx
server {
    server_name name.example;
    
    client_max_body_size 64M;
    
    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
    
    location ~ ^/(ws|upload|files|outputs|getConfig|setConfig) {
        proxy_pass http://localhost:8000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "Upgrade";
        proxy_set_header Host $host;
    }
    
    listen 443 ssl; # managed by Certbot
    ssl_certificate /etc/letsencrypt/live/name.example/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/name.example/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot
}

server {
    if ($host = name.example) {
        return 301 https://$host$request_uri;
    } # managed by Certbot
    
    listen 80;
    server_name name.example;
    return 404; # managed by Certbot
}
```

And the relevant commands:


```bash
vim /etc/nginx/nginx.conf
### Edit it to reflect above. Then verify all is good with:

sudo nginx -t
# If there are no errors:

sudo systemctl restart nginx

# Clone .env.example as .env
# Run from root: 

docker-compose up --build

```


================================================
FILE: docs/docs/gpt-researcher/gptr/automated-tests.md
================================================
# Automated Tests

## Automated Testing with Github Actions

This repository contains the code for the automated testing of the GPT-Researcher Repo using Github Actions. 

The tests are triggered in a docker container which runs the tests via the `pytest` module.

## Running the Tests

You can run the tests:

### Via a docker command

```bash
docker-compose --profile test run --rm gpt-researcher-tests
```

### Via a Github Action

![image](https://github.com/user-attachments/assets/721fca20-01bb-4c10-9cf9-19d823bebbb0)

Attaching here the required settings & screenshots on the github repo level:

Step 1: Within the repo, press the "Settings" tab

Step 2: Create a new environment named "tests" (all lowercase)

Step 3: Click into the "tests" environment & add environment secrets of ```OPENAI_API_KEY``` & ```TAVILY_API_KEY```

Get the keys from here:

https://app.tavily.com/sign-in

https://platform.openai.com/api-keys


![Screen Shot 2024-07-28 at 9 00 19](https://github.com/user-attachments/assets/7cd341c6-d8d4-461f-ab5e-325abc9fe509)
![Screen Shot 2024-07-28 at 9 02 55](https://github.com/user-attachments/assets/a3744f01-06a6-4c9d-8aa0-1fc742d3e866)

If configured correctly, here's what the Github action should look like when opening a new PR or committing to an open PR:

![Screen Shot 2024-07-28 at 8 57 02](https://github.com/user-attachments/assets/30dbc668-4e6a-4b3b-a02e-dc859fc9bd3d)


================================================
FILE: docs/docs/gpt-researcher/gptr/config.md
================================================
# Configuration

The config.py enables you to customize GPT Researcher to your specific needs and preferences.

Thanks to our amazing community and contributions, GPT Researcher supports multiple LLMs and Retrievers.
In addition, GPT Researcher can be tailored to various report formats (such as APA), word count, research iterations depth, etc.

GPT Researcher defaults to our recommended suite of integrations: [OpenAI](https://platform.openai.com/docs/overview) for LLM calls and [Tavily API](https://app.tavily.com) for retrieving real-time web information.

As seen below, OpenAI still stands as the superior LLM. We assume it will stay this way for some time, and that prices will only continue to decrease, while performance and speed increase over time.

<div style={{ marginBottom: '10px' }}>
<img align="center" height="350" src="/img/leaderboard.png" />
</div>

The default config.py file can be found in `/gpt_researcher/config/`. It supports various options for customizing GPT Researcher to your needs.
You can also include your own external JSON file `config.json` by adding the path in the `config_path` param.
The config JSON should follow the format/keys in the default config. Below is a sample config.json file to help get you started:
'''bash
{
  "RETRIEVER": "tavily",
  "EMBEDDING": "openai:text-embedding-3-small",
  "SIMILARITY_THRESHOLD": 0.42,
  "FAST_LLM": "openai:gpt-4o-mini",
  "SMART_LLM": "openai:gpt-4.1",
  "STRATEGIC_LLM": "openai:o4-mini",
  "LANGUAGE": "english",
  "CURATE_SOURCES": false,
  "FAST_TOKEN_LIMIT": 2000,
  "SMART_TOKEN_LIMIT": 4000,
  "STRATEGIC_TOKEN_LIMIT": 4000,
  "BROWSE_CHUNK_MAX_LENGTH": 8192,
  "SUMMARY_TOKEN_LIMIT": 700,
  "TEMPERATURE": 0.4,
  "DOC_PATH": "./my-docs",
  "REPORT_SOURCE": "web"
}
'''


For example, to start GPT-Researcher and specify a specific config you would do this:
```bash
python gpt_researcher/main.py --config_path my_config.json
```




 **Please follow the config.py file for additional future support**.

Below is a list of current supported options:

- **`RETRIEVER`**: Web search engine used for retrieving sources. Defaults to `tavily`. Options: `duckduckgo`, `bing`, `google`, `searchapi`, `serper`, `searx`. [Check here](https://github.com/assafelovic/gpt-researcher/tree/master/gpt_researcher/retrievers) for supported retrievers
- **`EMBEDDING`**: Embedding model. Defaults to `openai:text-embedding-3-small`. Options: `ollama`, `huggingface`, `azure_openai`, `custom`.
- **`SIMILARITY_THRESHOLD`**: Threshold value for similarity comparison when processing documents. Defaults to `0.42`.
- **`FAST_LLM`**: Model name for fast LLM operations such summaries. Defaults to `openai:gpt-4o-mini`.
- **`SMART_LLM`**: Model name for smart operations like generating research reports and reasoning. Defaults to `openai:gpt-5`.
- **`STRATEGIC_LLM`**: Model name for strategic operations like generating research plans and strategies. Defaults to `openai:gpt-5-mini`.
- **`LANGUAGE`**: Language to be used for the final research report. Defaults to `english`.
- **`CURATE_SOURCES`**: Whether to curate sources for research. This step adds an LLM run which may increase costs and total run time but improves quality of source selection. Defaults to `False`.
- **`FAST_TOKEN_LIMIT`**: Maximum token limit for fast LLM responses. Defaults to `2000`.
- **`SMART_TOKEN_LIMIT`**: Maximum token limit for smart LLM responses. Defaults to `4000`.
- **`STRATEGIC_TOKEN_LIMIT`**: Maximum token limit for strategic LLM responses. Defaults to `4000`.
- **`BROWSE_CHUNK_MAX_LENGTH`**: Maximum length of text chunks to browse in web sources. Defaults to `8192`.
- **`SUMMARY_TOKEN_LIMIT`**: Maximum token limit for generating summaries. Defaults to `700`.
- **`TEMPERATURE`**: Sampling temperature for LLM responses, typically between 0 and 1. A higher value results in more randomness and creativity, while a lower value results in more focused and deterministic responses. Defaults to `0.4`.
- **`USER_AGENT`**: Custom User-Agent string for web crawling and web requests.
- **`MAX_SEARCH_RESULTS_PER_QUERY`**: Maximum number of search results to retrieve per query. Defaults to `5`.
- **`MEMORY_BACKEND`**: Backend used for memory operations, such as local storage of temporary data. Defaults to `local`.
- **`TOTAL_WORDS`**: Total word count limit for document generation or processing tasks. Defaults to `1200`.
- **`REPORT_FORMAT`**: Preferred format for report generation. Defaults to `APA`. Consider formats like `MLA`, `CMS`, `Harvard style`, `IEEE`, etc.
- **`MAX_ITERATIONS`**: Maximum number of iterations for processes like query expansion or search refinement. Defaults to `3`.
- **`AGENT_ROLE`**: Role of the agent. This configures the behavior of specialized research agents. Defaults to `None`. When set, it activates role-specific prompting and techniques tailored to particular research domains.
- **`MAX_SUBTOPICS`**: Maximum number of subtopics to generate or consider. Defaults to `3`.
- **`SCRAPER`**: Web scraper to use for gathering information. Defaults to `bs` (BeautifulSoup). You can also use [newspaper](https://github.com/codelucas/newspaper).
- **`MAX_SCRAPER_WORKERS`**: Maximum number of concurrent scraper workers per research. Defaults to `15`.
- **`REPORT_SOURCE`**: Source for the research report data. Defaults to `web` for online research. Can be set to `doc` for local document-based research. This determines where GPT Researcher gathers its primary information from.
- **`DOC_PATH`**: Path to read and research local documents. Defaults to `./my-docs`.
- **`PROMPT_FAMILY`**: The family of prompts and prompt formatting to use. Defaults to prompting optimized for GPT models. See the full list of options in [enum.py](https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/utils/enum.py#L56).
- **`LLM_KWARGS`**: Json formatted dict of additional keyword args to be passed to the LLM provider class when instantiating it. This is primarily useful for clients like Ollama that allow for additional keyword arguments such as `num_ctx` that influence the inference calls.
- **`EMBEDDING_KWARGS`**: Json formatted dict of additional keyword args to be passed to the embedding provider class when instantiating it.
- **`DEEP_RESEARCH_BREADTH`**: Controls the breadth of deep research, defining how many parallel paths to explore. Defaults to `3`.
- **`DEEP_RESEARCH_DEPTH`**: Controls the depth of deep research, defining how many sequential searches to perform. Defaults to `2`.
- **`DEEP_RESEARCH_CONCURRENCY`**: Controls the concurrency level for deep research operations. Defaults to `4`.
- **`REASONING_EFFORT`**: Controls the reasoning effort of strategic models. Default to `medium`.

## Deep Research Configuration

The deep research parameters allow you to fine-tune how GPT Researcher explores complex topics that require extensive knowledge gathering. These parameters work together to determine the thoroughness and efficiency of the research process:

- **`DEEP_RESEARCH_BREADTH`**: Controls how many parallel research paths are explored simultaneously. A higher value (e.g., 5) causes the researcher to investigate more diverse subtopics at each step, resulting in broader coverage but potentially less focus on core themes. The default value of `3` provides a balanced approach between breadth and depth.

- **`DEEP_RESEARCH_DEPTH`**: Determines how many sequential search iterations GPT Researcher performs for each research path. A higher value (e.g., 3-4) allows for following citation trails and diving deeper into specialized information, but increases research time substantially. The default value of `2` ensures reasonable depth while maintaining practical completion times.

- **`DEEP_RESEARCH_CONCURRENCY`**: Sets how many concurrent operations can run during deep research. Higher values speed up the research process on capable systems but may increase API rate limit issues or resource consumption. The default value of `4` is suitable for most environments, but can be increased on systems with more resources or decreased if you experience performance issues.

For academic or highly specialized research, consider increasing both breadth and depth (e.g., BREADTH=4, DEPTH=3). For quick exploratory research, lower values (e.g., BREADTH=2, DEPTH=1) will provide faster results with less detail.

To change the default configurations, you can simply add env variables to your `.env` file as named above or export manually in your local project directory.

For example, to manually change the search engine and report format:
```bash
export RETRIEVER=bing
export REPORT_FORMAT=IEEE
```
Please note that you might need to export additional env vars and obtain API keys for other supported search retrievers and LLM providers. Please follow your console logs for further assistance.
To learn more about additional LLM support you can check out the docs [here](/docs/gpt-researcher/llms/llms).




================================================
FILE: docs/docs/gpt-researcher/gptr/deep_research.md
================================================
# Deep Research âœ¨ NEW âœ¨

With the latest "Deep Research" trend in the AI community, we're excited to implement our own Open source deep research capability! Introducing GPT Researcher's Deep Research - an advanced recursive research system that explores topics with unprecedented depth and breadth. 

Each deep research takes around 5 minutes to complete and costs around $0.4 (using `o3-mini` on `"high" `reasoning effort)

## How It Works

Deep Research employs a fascinating tree-like exploration pattern:

1. **Breadth**: At each level, it generates multiple search queries to explore different aspects of your topic
2. **Depth**: For each branch, it recursively dives deeper, following leads and uncovering connections
3. **Concurrent Processing**: Utilizes async/await patterns to run multiple research paths simultaneously
4. **Smart Context Management**: Automatically aggregates and synthesizes findings across all branches
5. **Progress Tracking**: Real-time updates on research progress across both breadth and depth dimensions

Think of it as deploying a team of AI researchers, each following their own research path while collaborating to build a comprehensive understanding of your topic.

## Process Flow
<img src="https://github.com/user-attachments/assets/eba2d94b-bef3-4f8d-bbc0-f15bd0a40968" alt="Logo" width="568"></img>
<br></br>

## Quick Start

```python
from gpt_researcher import GPTResearcher
from gpt_researcher.utils.enum import ReportType, Tone
import asyncio

async def main():
    # Initialize researcher with deep research type
    researcher = GPTResearcher(
        query="What are the latest developments in quantum computing?",
        report_type="deep",  # This triggers deep research modd
    )

    # Run research
    research_data = await researcher.conduct_research()

    # Generate report
    report = await researcher.write_report()
    print(report)

if __name__ == "__main__":
    asyncio.run(main())
```

## Configuration

Deep Research behavior can be customized through several parameters:

- `deep_research_breadth`: Number of parallel research paths at each level (default: 4)
- `deep_research_depth`: How many levels deep to explore (default: 2)
- `deep_research_concurrency`: Maximum number of concurrent research operations (default: 4)
- `total_words`: Total words in the generated report (recommended: 2000)

You can configure these parameters in multiple ways:

1. **Environment Variables**:
```bash
export DEEP_RESEARCH_BREADTH=4
export DEEP_RESEARCH_DEPTH=2
export DEEP_RESEARCH_CONCURRENCY=4
export TOTAL_WORDS=2500
```

2. **Config File**:
```yaml
deep_research_breadth: 4
deep_research_depth: 2
deep_research_concurrency: 4
total_words: 2500
```

```python
researcher = GPTResearcher(
    query="your query",
    report_type="deep",
    config_path="path/to/config.yaml"  # Configure deep research parameters here
)
```

## Progress Tracking

The `on_progress` callback provides real-time insights into the research process:

```python
class ResearchProgress:
    current_depth: int       # Current depth level
    total_depth: int         # Maximum depth to explore
    current_breadth: int     # Current number of parallel paths
    total_breadth: int       # Maximum breadth at each level
    current_query: str       # Currently processing query
    completed_queries: int   # Number of completed queries
    total_queries: int       # Total queries to process
```

## Error Handling

The deep research workflow is designed to be resilient:

- Failed queries are automatically skipped
- Research continues even if some branches fail
- Progress tracking helps identify any issues

## Best Practices

1. **Start Broad**: Begin with a general query and let the system explore specifics
2. **Monitor Progress**: Use the progress callback to understand the research flow
3. **Adjust Parameters**: Tune breadth and depth based on your needs:
   - More breadth = wider coverage
   - More depth = deeper insights
4. **Resource Management**: Consider concurrency limits based on your system capabilities

## Limitations

- Usage of reasoning LLM models such as `o3-mini`
- Deep research may take longer than standard research
- Higher API usage and costs due to multiple concurrent queries
- May require more system resources for parallel processing

Happy researching! ğŸ‰ 


================================================
FILE: docs/docs/gpt-researcher/gptr/example.md
================================================
# Agent Example

If you're interested in using GPT Researcher as a standalone agent, you can easily import it into any existing Python project. Below, is an example of calling the agent to generate a research report:

```python
from gpt_researcher import GPTResearcher
import asyncio

async def fetch_report(query):
    """
    Fetch a research report based on the provided query and report type.
    """
    researcher = GPTResearcher(query=query)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report

async def generate_research_report(query):
    """
    This is a sample script that executes an async main function to run a research report.
    """
    report = await fetch_report(query)
    print(report)

if __name__ == "__main__":
    QUERY = "What happened in the latest burning man floods?"
    asyncio.run(generate_research_report(query=QUERY))
```

You can further enhance this example to use the returned report as context for generating valuable content such as news article, marketing content, email templates, newsletters, etc.

You can also use GPT Researcher to gather information about code documentation, business analysis, financial information and more. All of which can be used to complete much more complex tasks that require factual and high quality realtime information.



================================================
FILE: docs/docs/gpt-researcher/gptr/npm-package.md
================================================
# npm package

The [gpt-researcher npm package](https://www.npmjs.com/package/gpt-researcher) is a WebSocket client for interacting with GPT Researcher.

## Installation

```bash
npm install gpt-researcher
```

## Usage

```javascript
const GPTResearcher = require('gpt-researcher');

const researcher = new GPTResearcher({
  host: 'localhost:8000',
  logListener: (data) => console.log('logListener logging data: ',data)
});

researcher.sendMessage({
  query: 'Does providing better context reduce LLM hallucinations?',
  moreContext: 'Provide a detailed answer'
});
```



================================================
FILE: docs/docs/gpt-researcher/gptr/pip-package.md
================================================
# PIP Package
[![PyPI version](https://badge.fury.io/py/gpt-researcher.svg)](https://badge.fury.io/py/gpt-researcher)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)

ğŸŒŸ **Exciting News!** Now, you can integrate `gpt-researcher` with your apps seamlessly!

## Steps to Install GPT Researcher

Follow these easy steps to get started:

0. **Pre-requisite**: Ensure Python 3.10+ is installed on your machine ğŸ’»
1. **Install gpt-researcher**: Grab the official package from [PyPi](https://pypi.org/project/gpt-researcher/).

```bash
pip install gpt-researcher
```

2. **Environment Variables:** Create a .env file with your OpenAI API key or simply export it

```bash
export OPENAI_API_KEY={Your OpenAI API Key here}
```

```bash
export TAVILY_API_KEY={Your Tavily API Key here}
```

3. **Start using GPT Researcher in your own codebase**

## Example Usage

```python
from gpt_researcher import GPTResearcher
import asyncio

async def get_report(query: str, report_type: str):
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    
    # Get additional information
    research_context = researcher.get_research_context()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()
    
    return report, research_context, research_costs, research_images, research_sources

if __name__ == "__main__":
    query = "what team may win the NBA finals?"
    report_type = "research_report"

    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))
    
    print("Report:")
    print(report)
    print("\nResearch Costs:")
    print(costs)
    print("\nNumber of Research Images:")
    print(len(images))
    print("\nNumber of Research Sources:")
    print(len(sources))
```

## Specific Examples

### Example 1: Research Report

```python
query = "Latest developments in renewable energy technologies"
report_type = "research_report"
```

### Example 2: Resource Report

```python
query = "List of top AI conferences in 2023"
report_type = "resource_report"
```

### Example 3: Outline Report

```python
query = "Outline for an article on the impact of AI in education"
report_type = "outline_report"
```

## Integration with Web Frameworks

### FastAPI Example

```python
from fastapi import FastAPI
from gpt_researcher import GPTResearcher
import asyncio

app = FastAPI()

@app.get("/report/{report_type}")
async def get_report(query: str, report_type: str) -> dict:
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    
    source_urls = researcher.get_source_urls()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()
    
    return {
        "report": report,
        "source_urls": source_urls,
        "research_costs": research_costs,
        "num_images": len(research_images),
        "num_sources": len(research_sources)
    }

# Run the server
# uvicorn main:app --reload
```

### Flask Example

**Pre-requisite**: Install flask with the async extra.

```bash
pip install 'flask[async]'
```

```python
from flask import Flask, request, jsonify
from gpt_researcher import GPTResearcher

app = Flask(__name__)

@app.route('/report/<report_type>', methods=['GET'])
async def get_report(report_type):
    query = request.args.get('query')
    researcher = GPTResearcher(query, report_type)
    research_result = await researcher.conduct_research()
    report = await researcher.write_report()
    
    source_urls = researcher.get_source_urls()
    research_costs = researcher.get_costs()
    research_images = researcher.get_research_images()
    research_sources = researcher.get_research_sources()
    
    return jsonify({
        "report": report,
        "source_urls": source_urls,
        "research_costs": research_costs,
        "num_images": len(research_images),
        "num_sources": len(research_sources)
    })

# Run the server
# flask run
```

**Run the server**

```bash
flask run
```

**Example Request**

```bash
curl -X GET "http://localhost:5000/report/research_report?query=what team may win the nba finals?"
```

## Getters and Setters
GPT Researcher provides several methods to retrieve additional information about the research process:

### Get Research Sources
Sources are the URLs that were used to gather information for the research.
```python
source_urls = researcher.get_source_urls()
```

### Get Research Context
Context is all the retrieved information from the research. It includes the sources and their corresponding content.
```python
research_context = researcher.get_research_context()
```

### Get Research Costs
Costs are the number of tokens consumed during the research process.
```python
research_costs = researcher.get_costs()
```

### Get Research Images
Retrieves a list of images found during the research process.
```python
research_images = researcher.get_research_images()
```

### Get Research Sources
Retrieves a list of research sources, including title, content, and images.
```python
research_sources = researcher.get_research_sources()
```

### Set Verbose
You can set the verbose mode to get more detailed logs.
```python
researcher.set_verbose(True)
```

### Add Costs
You can also add costs to the research process if you want to track the costs from external usage.
```python
researcher.add_costs(0.22)
```

## Advanced Usage

### Customizing the Research Process

You can customize various aspects of the research process by passing additional parameters when initializing the GPTResearcher:

```python
researcher = GPTResearcher(
    query="Your research query",
    report_type="research_report",
    report_format="APA",
    tone="formal and objective",
    max_subtopics=5,
    verbose=True
)
```

### Handling Research Results

After conducting research, you can process the results in various ways:

```python
# Conduct research
research_result = await researcher.conduct_research()

# Generate a standard report
report = await researcher.write_report()

# Generate a customized report with specific formatting requirements
custom_report = await researcher.write_report(custom_prompt="Answer in short, 2 paragraphs max without citations.")

# Generate a focused report for a specific audience
executive_summary = await researcher.write_report(custom_prompt="Create an executive summary focused on business impact and ROI. Keep it under 500 words.")

# Generate a report with specific structure requirements
technical_report = await researcher.write_report(custom_prompt="Create a technical report with problem statement, methodology, findings, and recommendations sections.")

# Generate a conclusion
conclusion = await researcher.write_report_conclusion(report)

# Get subtopics
subtopics = await researcher.get_subtopics()

# Get draft section titles for a subtopic
draft_titles = await researcher.get_draft_section_titles("Subtopic name")
```

### Customizing Report Generation with Custom Prompts

The `write_report` method accepts a `custom_prompt` parameter that gives you complete control over how your research is presented:

```python
# After conducting research
research_result = await researcher.conduct_research()

# Generate a report with a custom prompt
report = await researcher.write_report(
    custom_prompt="Based on the research, provide a bullet-point summary of the key findings."
)
```

Custom prompts can be used for various purposes:

1. **Format Control**: Specify the structure, length, or style of your report
   ```python
   report = await researcher.write_report(
       custom_prompt="Write a blog post in a conversational tone using the research. Include headings and a conclusion."
   )
   ```

2. **Audience Targeting**: Tailor the content for specific readers
   ```python
   report = await researcher.write_report(
       custom_prompt="Create a report for technical stakeholders, focusing on methodologies and implementation details."
   )
   ```

3. **Specialized Outputs**: Generate specific types of content
   ```python
   report = await researcher.write_report(
       custom_prompt="Create a FAQ section based on the research with at least 5 questions and detailed answers."
   )
   ```

The custom prompt will be combined with the research context to generate your customized report.

### Working with Research Context

You can use the research context for further processing or analysis:

```python
# Get the full research context
context = researcher.get_research_context()

# Get similar written contents based on draft section titles
similar_contents = await researcher.get_similar_written_contents_by_draft_section_titles(
    current_subtopic="Subtopic name",
    draft_section_titles=["Title 1", "Title 2"],
    written_contents=some_written_contents,
    max_results=10
)
```

This comprehensive documentation should help users understand and utilize the full capabilities of the GPT Researcher package.



================================================
FILE: docs/docs/gpt-researcher/gptr/querying-the-backend.md
================================================
# Querying the Backend

## Introduction

In this section, we will discuss how to query the GPTR backend server. The GPTR backend server is a Python server that runs the GPTR Python package. The server listens for WebSocket connections and processes incoming messages to generate reports, streaming back logs and results to the client.

An example WebSocket client is implemented in the `gptr-webhook.js` file below.

This function sends a Webhook Message to the GPTR Python backend running on localhost:8000, but this example can also be modified to query a [GPTR Server hosted on Linux](https://docs.gptr.dev/docs/gpt-researcher/getting-started/linux-deployment).

// gptr-webhook.js

```javascript

const WebSocket = require('ws');

let socket = null;
let responseCallback = null;

async function initializeWebSocket() {
  if (!socket) {
    const host = 'localhost:8000';
    const ws_uri = `ws://${host}/ws`;

    socket = new WebSocket(ws_uri);

    socket.onopen = () => {
      console.log('WebSocket connection established');
    };

    socket.onmessage = (event) => {
      const data = JSON.parse(event.data);
      console.log('WebSocket data received:', data);

      if (data.content === 'dev_team_result' 
          && data.output.rubber_ducker_thoughts != undefined
          && data.output.tech_lead_review != undefined) {
        if (responseCallback) {
          responseCallback(data.output);
          responseCallback = null; // Clear callback after use
        }
      } else {
        console.log('Received data:', data);
      }
    };

    socket.onclose = () => {
      console.log('WebSocket connection closed');
      socket = null;
    };

    socket.onerror = (error) => {
      console.error('WebSocket error:', error);
    };
  }
}

async function sendWebhookMessage(message) {
  return new Promise((resolve, reject) => {
    if (!socket || socket.readyState !== WebSocket.OPEN) {
      initializeWebSocket();
    }

    const data = {
      task: message,
      report_type: 'dev_team',
      report_source: 'web',
      tone: 'Objective',
      headers: {},
      repo_name: 'elishakay/gpt-researcher'
    };

    const payload = "start " + JSON.stringify(data);

    responseCallback = (response) => {
      resolve(response); // Resolve the promise with the WebSocket response
    };

    if (socket.readyState === WebSocket.OPEN) {
      socket.send(payload);
      console.log('Message sent:', payload);
    } else {
      socket.onopen = () => {
        socket.send(payload);
        console.log('Message sent after connection:', payload);
      };
    }
  });
}

module.exports = {
  sendWebhookMessage
};
```

And here's how you can leverage this helper function:

```javascript
const { sendWebhookMessage } = require('./gptr-webhook');

async function main() {
  const message = 'How do I get started with GPT-Researcher Websockets?';
  const response = await sendWebhookMessage(message);
  console.log('Response:', response);
}
```


================================================
FILE: docs/docs/gpt-researcher/gptr/scraping.md
================================================
# Scraping Options

GPT Researcher now offers various methods for web scraping: static scraping with BeautifulSoup, dynamic scraping with Selenium, and High scale scraping with Tavily Extract. This document explains how to switch between these methods and the benefits of each approach.

## Configuring Scraping Method

You can choose your preferred scraping method by setting the `SCRAPER` environment variable:

1. For BeautifulSoup (static scraping):
   ```
   export SCRAPER="bs"
   ```

2. For dynamic browser scraping, either with Selenium:
   ```
   export SCRAPER="browser"
   ```
   Or with NoDriver (ZenDriver):
   ```
   export SCRAPER="nodriver"
   pip install zendriver
   ```

3. For **production** use cases, you can set the Scraper to `tavily_extract` or `firecrawl`. [Tavily](https://tavily.com) allows you to scrape sites at scale without the hassle of setting up proxies, managing cookies, or dealing with CAPTCHAs. Please note that you need to have a Tavily account and [API key](https://app.tavily.com) to use this option. To learn more about Tavily Extract [see here](https://docs.tavily.com/docs/python-sdk/tavily-extract/getting-started).
    Make sure to first install the pip package `tavily-python`. Then:
   ```
   export SCRAPER="tavily_extract"
   ```
   [FireCrawl](https://firecrawl.dev) is also allows you to scrape sites at scale. FireCrawl also provides open source code to self hosted server which provided better scrape quality compared to BeautifulSoup by passing markdown version of the scraped sites to LLMs. You will needs to have FireCrawl account (official service) to get API key or you needs self host URL and API key (if you set for your self host server) to use this option.
   Make sure to install the pip package `firecrawl-py`. Then:
   ```bash
   export SCRAPER="firecrawl"
   ```

Note: If not set, GPT Researcher will default to BeautifulSoup for scraping.

## Scraping Methods Explained

### BeautifulSoup (Static Scraping)

When `SCRAPER="bs"`, GPT Researcher uses BeautifulSoup for static scraping. This method:

- Sends a single HTTP request to fetch the page content
- Parses the static HTML content
- Extracts text and data from the parsed HTML

Benefits:
- Faster and more lightweight
- Doesn't require additional setup
- Works well for simple, static websites

Limitations:
- Cannot handle dynamic content loaded by JavaScript
- May miss content that requires user interaction to display

### Selenium (Browser Scraping)

When `SCRAPER="browser"`, GPT Researcher uses Selenium for dynamic scraping. This method:

- Opens a real browser instance (Chrome by default)
- Loads the page and executes JavaScript
- Waits for dynamic content to load
- Extracts text and data from the fully rendered page

Benefits:
- Can scrape dynamically loaded content
- Simulates real user interactions (scrolling, clicking, etc.)
- Works well for complex, JavaScript-heavy websites

Limitations:
- Slower than static scraping
- Requires more system resources
- Requires additional setup (Selenium and WebDriver installation)

### NoDriver (Browser Scraping)

Alternative to Selenium for potentially better performance.

Setup:
```bash
pip install zendriver
```

### Tavily Extract (Recommended for Production)

When `SCRAPER="tavily_extract"`, GPT Researcher uses Tavily's Extract API for web scraping. This method:

- Uses Tavily's robust infrastructure to handle web scraping at scale
- Automatically handles CAPTCHAs, JavaScript rendering, and anti-bot measures
- Provides clean, structured content extraction

Benefits:
- Production-ready and highly reliable
- No need to manage proxies or handle rate limiting
- Excellent success rate on most websites
- Handles both static and dynamic content
- Built-in content cleaning and formatting
- Fast response times through Tavily's distributed infrastructure

Setup:
1. Create a Tavily account at [app.tavily.com](https://app.tavily.com)
2. Get your API key from the dashboard
3. Install the Tavily Python SDK:
   ```bash
   pip install tavily-python
   ```
4. Set your Tavily API key:
   ```bash
   export TAVILY_API_KEY="your-api-key"
   ```

Usage Considerations:
- Requires a Tavily API key and account
- API calls are metered based on your Tavily plan
- Best for production environments where reliability is crucial
- Ideal for businesses and applications that need consistent scraping results

### FireCrawl (Recommended for Production)
When `SCRAPER="firecrawl"`, GPT Researcher uses FireCrawl Scrape API for web scraping in markdown format. This method:

- Uses FireCrawl's robust infrastructure to handle web scraping at scale
- Or uses self-hosted FireCrawl server.
- Automatically handles CAPTCHAs, JavaScript rendering, and anti-bot measures
- Provides clean, structured content extraction in markdown format.

Benefits:
- Production-ready and highly reliable
- No need to manage proxies or handle rate limiting
- Excellent success rate on most websites
- Handles both static and dynamic content
- Built-in content cleaning and formatting
- Fast response times through FireCrawl's distributed infrastructure
- Ease of setup with FireCrawl self-hosted

Setup (official service by FireCrawl):
1. Create a FireCrawl account at [firecrawl.dev/app](https://www.firecrawl.dev/app)
2. Get your API key from the dashboard
3. Install the FireCrawl Python SDK:
   ```bash
   pip install firecrawl-py
   ```
4. Set your FireCrawl API key:
   ```bash
   export FIRECRAWL_API_KEY=<your-firecrawl-api>
   ```
Setup (with self-hosted server):
1. Host your FireCrawl. Read their [self-hosted guidelines](https://docs.firecrawl.dev/contributing/self-host) or [run locally guidelines](https://docs.firecrawl.dev/contributing/guide)
2. Get your server URL and API key (if you set it).
3. Install the FireCrawl Python SDK:
   ```bash
   pip install firecrawl-py
   ```
4. Set your FireCrawl API key:
   ```bash
   export FIRECRAWL_API_KEY=<your-firecrawl-api>
   ```

Note: `FIRECRAWL_API_KEY` can be empty if you not setup authentication for your self host server (`FIRECRAWL_API_KEY=""`).
There will be some difference between their cloud service and open source service. To understand differences between FireCrawl option read [here](https://docs.firecrawl.dev/contributing/open-source-or-cloud).

Usage Considerations:
- Requires a FireCrawl API key and account or self-hosted server
- API calls are metered based on your FireCrawl plan (it can be basically free with self-hosted FireCrawl method)
- Best for production environments where reliability is crucial (for their cloud service)
- Ideal for businesses and applications that need consistent scraping results
- Need robust scraping option for personal use

## Additional Setup for Selenium

If you choose to use Selenium (SCRAPER="browser"), you'll need to:

1. Install the Selenium package:
   ```
   pip install selenium
   ```

2. Download the appropriate WebDriver for your browser:
   - For Chrome: [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads)
   - For Firefox: [GeckoDriver](https://github.com/mozilla/geckodriver/releases)
   - For Safari: Built-in, no download required

   Ensure the WebDriver is in your system's PATH.

## Choosing the Right Method

- Use BeautifulSoup (static) for:
  - Simple websites with mostly static content
  - Scenarios where speed is a priority
  - When you don't need to interact with the page

- Use Selenium (dynamic) for:
  - Websites with content loaded via JavaScript
  - Sites that require scrolling or clicking to load more content
  - When you need to simulate user interactions

## Troubleshooting

- If Selenium fails to start, ensure you have the correct WebDriver installed and it's in your system's PATH.
- If you encounter an `ImportError` related to Selenium, make sure you've installed the Selenium package.
- If the scraper misses expected content, try switching between static and dynamic scraping to see which works better for your target website.

Remember, the choice between static and dynamic scraping can significantly impact the quality and completeness of the data GPT Researcher can gather. Choose the method that best suits your research needs and the websites you're targeting.


================================================
FILE: docs/docs/gpt-researcher/gptr/troubleshooting.md
================================================
# Troubleshooting

We're constantly working to provide a more stable version. If you're running into any issues, please first check out the resolved issues or ask us via our [Discord community](https://discord.gg/QgZXvJAccX).

### model: gpt-4 does not exist
This relates to not having permission to use gpt-4 yet. Based on OpenAI, it will be [widely available for all by end of July](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4).

### cannot load library 'gobject-2.0-0'

The issue relates to the library WeasyPrint (which is used to generate PDFs from the research report). Please follow this guide to resolve it: https://doc.courtbouillon.org/weasyprint/stable/first_steps.html

Or you can install this package manually

In case of MacOS you can install this lib using
`brew install glib pango`
If you face an issue with linking afterward, you can try running `brew link glib`

In case of Linux you can install this lib using
`sudo apt install libglib2.0-dev`

### cannot load library 'pango'

In case of MacOS you can install this lib using
`brew install pango`

In case of Linux you can install this lib using
`sudo apt install libpango-1.0-0`

**Workaround for Mac M chip users**

If the above solutions don't work, you can try the following:
- Install a fresh version of Python 3.11 pointed to brew:
`brew install python@3.11`
- Install the required libraries:
`brew install pango glib gobject-introspection`
- Install the required GPT Researcher Python packages:
`pip3.11 install -r requirements.txt`
- Run the app with Python 3.11 (using brew):
`python3.11 -m uvicorn main:app --reload`

### Error processing the url

We're using [Selenium](https://www.selenium.dev) for site scraping. Some sites fail to be scraped. In these cases, restart and try running again.


### Chrome version issues

Many users have an issue with their chromedriver because the latest chrome browser version doesn't have a compatible chrome driver yet.

To downgrade your Chrome web browser using [slimjet](https://www.slimjet.com/chrome/google-chrome-old-version.php), follow these steps. First, visit the website and scroll down to find the list of available older Chrome versions. Choose the version you wish to install
making sure it's compatible with your operating system.
Once you've selected the desired version, click on the corresponding link to download the installer. Before proceeding with the installation, it's crucial to uninstall your current version of Chrome to avoid conflicts.

It's important to check if the version you downgrade to, has a chromedriver available in the official [chrome driver website](https://chromedriver.chromium.org/downloads)

**If none of the above work, you can [try out our hosted beta](https://app.tavily.com)**


================================================
FILE: docs/docs/gpt-researcher/handling-logs/all-about-logs.md
================================================
# All About Logs

This document explains how to interpret the log files generated for each report. These logs provide a detailed record of the research process, from initial task planning to the gathering of information, and finally, the report writing process. Reports may change over time as new features are developed. 
  
## Log File Overview  

The log file is a JSON file that contains a list of events that happened during the research process. Each event is an object with a timestamp, type, and data. The data contains the specific information about the event.

You can find the log file in the `outputs` folder.  

Or you can access the log file from the report page itself by clicking the "Download Logs" button.

For developers, there is an additional `logs` folder that may be useful. See description below for more details.  

## Key Components:

* `timestamp`: The timestamp is in the format `YYYY-MM-DDTHH:MM:SS.ffffff` which is an ISO format. The main timestamp is for the generation of the file itself. The timestamps for the events are when each specific event happened during the research process. 
* `events`: This is an array containing all the logged events during the research task. Each event object has the following structure.
* `timestamp`: The specific time when the event occurred, allowing you to follow the sequence of actions.
* `type`: This will always be "event" for now.
* `data`: Contains specific information about the event. Includes:
* `type`: This indicates the general kind of event (e.g., "logs").
* `content`: A descriptor of what the tool is doing (e.g., "starting\_research", "running\_subquery\_research", "scraping\_content").
* `output`: A more detailed message, which often includes visual indicators (emojis), that is sent to the user when the tool performs the task
* `metadata`: Additional data related to the event. This can be `null` or contain an array of relevant information like URLs.

## Types of Events & Their Significance
Here's a complete breakdown of all the unique `content` types and what they mean. This is a comprehensive list of all the different actions the research tool will perform.
1. **`starting_research`**:
* Indicates that the research process has begun for a given task.
* `output`: Includes the text of the research query.
2. **`agent_generated`**:
* This is an indicator of what the agent is used for this task
* `output`: Will show the name of the agent
3. **`planning_research`**:
* Shows the tool is initially browsing to understand the scope of the request and start planning.
* The `output` indicates the tool is either browsing or doing initial planning.
4. **`subqueries`**:
* Indicates that the tool has created subqueries that it will use for research
* `output`: Lists out all of the subqueries that the tool will be running to perform the research
* `metadata`: An array of strings that contain the subqueries to be run
5. **`running_subquery_research`**:
* Indicates that a specific subquery research is being performed.
* `output`: Shows the specific subquery being run.
6. **`added_source_url`**:
* Signifies a URL that was identified as a relevant source of information.
* `output`: Provides the URL with a checkmark emoji to indicate success.
* `metadata`: Contains the actual URL added.
7. **`researching`**:
* Indicates the tool is actively searching across multiple sources for information.
* `output`: A general message indicating research across multiple sources is happening.
8. **`scraping_urls`**:
* Shows the tool is beginning to scrape content from a group of URLs.
* `output`: Indicates how many URLs the tool will be scraping from.
9. **`scraping_content`**:
* Indicates the tool successfully scraped the content from the URLs.
* `output`: Shows the number of pages that have been successfully scraped.
10. **`scraping_images`**:
* Signifies that images were identified and selected during the scraping process.
* `output`: Shows the number of new images selected and the total images found
* `metadata`: An array containing URLs of the selected images.
11. **`scraping_complete`**:
* Indicates that the scraping process is complete for the URLs.
* `output`: A message stating that the scraping process is complete
12. **`fetching_query_content`**:
* Indicates that the tool is fetching content based on a specific query.
* `output`: The specific query for which content is being fetched
13. **`subquery_context_window`**:
* Indicates the tool is creating a context window for a given subquery to help with more detailed research.
* `output`: A message stating the context window for the subquery is created.
14. **`research_step_finalized`**:
* Indicates that the research portion of a step is finalized.
* `output`: A message stating that the research is complete.
15. **`generating_subtopics`**:
* Signifies that the tool is generating subtopics to guide the report.
* `output`: A message indicating that the tool is generating subtopics.
16. **`subtopics_generated`**:
* Indicates that subtopics have been generated.
* `output`: A message that subtopics have been generated.
17. **`writing_introduction`**:
* Indicates the tool is beginning to write the introduction to the report.
* `output`: A message to the user that the introduction writing has started.
18. **`introduction_written`**:
* Indicates the introduction to the report is finished
* `output`: A message to the user that the introduction writing is complete
19. **`generating_draft_sections`**:
* Shows that the tool is generating draft sections for the report.
* `output`: A message that the report is generating draft sections.
20. **`draft_sections_generated`**:
* Indicates the draft sections of the report are generated.
* `output`: A message to the user that the draft sections have been generated.
21. **`fetching_relevant_written_content`**:
* Indicates the tool is fetching relevant written content for the report.
* `output`: A message to the user that relevant content is being fetched
22. **`writing_report`**:
* Indicates that the tool is starting to compile the research into a report.
* `output`: A message to the user that the report generation has started.
23. **`report_written`**:
* Signifies that the report generation is complete.
* `output`: A message that the report generation is finished.
24. **`relevant_contents_context`**:
* Indicates that a context window for relevant content has been created.
* `output`: A message indicating a context window for relevant content has been created.
25. **`writing_conclusion`**:
* Indicates the tool has started writing the conclusion for the report
* `output`: A message to the user that the conclusion is being written
26. **`conclusion_written`**:
* Indicates the conclusion of the report has been written
* `output`: A message to the user that the conclusion has been written

## How to Use the Logs

* **Troubleshooting:** If the research results are unexpected, the log files can help you understand the exact steps the tool took, including the queries used, the sources it visited, and how the report was generated.
* **Transparency:** The logs provide transparency into the research process. You can see exactly which URLs were visited, which images were selected, and how the report was built.
* **Understanding the Process**: The logs will provide an overview of what the tool does and what each of the steps look like.
* **Reproducibility:** The log files allow users to trace the exact process.

## Example Usage
By looking at the timestamps, you can see the flow of the research task. The logs will show you the subqueries used by the tool to approach the main query, all the URLs used, if images were selected for the research, and all the steps the tool took to generate the report.

## Logs for Developers
In addition to the user-facing log files (detailed and summary reports), the application also generates two types of log files specifically for developers:
1. A `.log` file which is a basic log file format for logging events as they occur
2. A `.json` file which is more structured
Find the logs in the `logs` folder.

### Basic Log File (.log)

* **Format:** Plain text format. Each line represents a log entry.
* **Content:**
	* Timestamps with millisecond precision.
	* Log level: Usually `INFO`, but could include `DEBUG`, `WARNING`, or `ERROR` in a more complex setup.
	* Module name (e.g., "research").
	* Descriptive messages about various processes.
	* Includes data about:
		* Start and end of research tasks
		* Web searches being performed
		* Planning of the research
		* Subqueries generated and their results
		* The sizes of scraped data
		* The size of content found from subqueries
		* The final combined size of all context found
* **Use Cases for Developers:**
	* **Real-time Monitoring:** Can be used to monitor the tool's activity in real time.
	* **Debugging:** Helpful for pinpointing issues by seeing the chronological flow of operations, the size of content collected, etc.
	* **Performance Analysis:** Timestamps can help in identifying bottlenecks by measuring how long certain operations take.
	* **High-level overview**: Allows developers to easily see which steps of the tool were performed, and some basic information like sizes of collected content.
* **Key Differences from User Logs:**
	* Less structured, more for developers to review in real-time.
	* Contains technical information not usually relevant to a non-developer user.
	* Does not have emojis or simplified language.
	* No information on the images collected

### JSON Log File (.json)

* **Format**: Structured JSON format
* **Content**:
	* Timestamps, as in all log files
		* `type` field that can be:
		* `sub_query`: which contains the subquery string along with `scraped_data_size`
		* `content_found`: which includes the `sub_query` and the `content_size`
		* A `content` field which gives a snapshot of the overall research and can contain the final context and sources found from the research for that task
* **Use Cases for Developers**:
	* **Detailed Analysis**: Allows developers to view specific details of how the tool is running, particularly related to the subqueries and the results of the research.
	* **Process Understanding**: Developers can see the different subqueries run and how much content each generated which can lead to better debugging and understanding of the tool.
	* **Data Inspection**: Can be useful for reviewing the generated queries and content sizes.
* **Key Differences from User Logs**:
	* Highly structured and focused on subquery execution, and the results of this process, specifically the sizes of collected information.
	* Does not contain simplified language, emojis, or high-level explanations.
	* Does not contain information on the overall context or the images collected, it mainly focuses on the subquery process.



================================================
FILE: docs/docs/gpt-researcher/handling-logs/langsmith-logs.md
================================================
# Langsmith Logs

With the help of Langsmith, you can easily visualize logs on cost and errors within your Langsmith Dashboard (calculated per LLM call or grouped by project)

Here are the steps to setup Langsmith:

Step 1: Setup a Langsmith account at: [smith.langchain.com](https://smith.langchain.com)

Step 2: Create a new API key at: [smith.langchain.com/settings](https://smith.langchain.com/settings)

Step 3: Add these 2 environment variables:

```bash
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=Set this to your API key
```

Here's what this looks like in the Langsmith Dashboard:

![Langsmith Dashboard](./langsmith.png)

This can be helpful for: 

- Enabling users to visualize and inspect the backend data flow
- Quality assurance debugging - where can the input or output of our AI flows use improvement
- Cost analysis - where are we spending the most on LLM calls
- Error analysis - where are we getting the most errors
- Optimizing speed - which parts of the flow are taking the most time



================================================
FILE: docs/docs/gpt-researcher/handling-logs/simple-logs-example.md
================================================
# Simple Logs Example

Here is a snippet of code to help you handle the streaming logs of your Research tasks.

```python
from typing import Dict, Any
import asyncio
from gpt_researcher import GPTResearcher

class CustomLogsHandler:
    """A custom Logs handler class to handle JSON data."""
    def __init__(self):
        self.logs = []  # Initialize logs to store data

    async def send_json(self, data: Dict[str, Any]) -> None:
        """Send JSON data and log it."""
        self.logs.append(data)  # Append data to logs
        print(f"My custom Log: {data}")  # For demonstration, print the log

async def run():
    # Define the necessary parameters with sample values
    
    query = "What happened in the latest burning man floods?"
    report_type = "research_report"  # Type of report to generate
    report_source = "online"  # Could specify source like 'online', 'books', etc.
    tone = "informative"  # Tone of the report ('informative', 'casual', etc.)
    config_path = None  # Path to a config file, if needed
    
    # Initialize researcher with a custom WebSocket
    custom_logs_handler = CustomLogsHandler()

    researcher = GPTResearcher(
        query=query,
        report_type=report_type,
        report_source=report_source,
        tone=tone,
        config_path=config_path,
        websocket=custom_logs_handler
    )

    await researcher.conduct_research()  # Conduct the research
    report = await researcher.write_report()  # Write the research report

    return report

# Run the asynchronous function using asyncio
if __name__ == "__main__":
    asyncio.run(run())
```

The data from the research process will be logged and stored in the `CustomLogsHandler` instance. You can customize the logging behavior as needed for your application.

Here's a sample of the output:

```
{
    "type": "logs",
    "content": "added_source_url",
    "output": "âœ… Added source url to research: https://www.npr.org/2023/09/28/1202110410/how-rumors-and-conspiracy-theories-got-in-the-way-of-mauis-fire-recovery\n",
    "metadata": "https://www.npr.org/2023/09/28/1202110410/how-rumors-and-conspiracy-theories-got-in-the-way-of-mauis-fire-recovery"
}
```

The `metadata` field will include whatever metadata is relevant to the log entry. Let the script above run to completion for the full logs output of a given research task.


================================================
FILE: docs/docs/gpt-researcher/llms/llms.md
================================================
# Configure LLM

As described in the [introduction](/docs/gpt-researcher/gptr/config), the default LLM and embedding is OpenAI due to its superior performance and speed. 
With that said, GPT Researcher supports various open/closed source LLMs and embeddings, and you can easily switch between them by updating the `SMART_LLM`, `FAST_LLM` and `EMBEDDING` env variables. You might also need to include the provider API key and corresponding configuration params.

Current supported LLMs are `openai`, `anthropic`, `azure_openai`, `cohere`, `google_vertexai`, `google_genai`, `fireworks`, `ollama`, `together`, `mistralai`, `huggingface`, `groq`, `bedrock` and `litellm`.

Current supported embeddings are `openai`, `azure_openai`, `cohere`, `google_vertexai`, `google_genai`, `fireworks`, `ollama`, `together`, `mistralai`, `huggingface`, `nomic` ,`voyageai` and `bedrock`.

To learn more about support customization options see [here](/docs/gpt-researcher/gptr/config).

**Please note**: GPT Researcher is optimized and heavily tested on GPT models. Some other models might run into context limit errors, and unexpected responses.
Please provide any feedback in our [Discord community](https://discord.gg/DUmbTebB) channel, so we can better improve the experience and performance.

Below you can find examples for how to configure the various supported LLMs.

## OpenAI

```env
# set the custom OpenAI API key
OPENAI_API_KEY=[Your Key]

# specify llms
FAST_LLM=openai:gpt-5-mini
SMART_LLM=openai:gpt-5
STRATEGIC_LLM=openai:o4-mini

# specify embedding
EMBEDDING=openai:text-embedding-3-small
```


## Custom LLM

Create a local OpenAI API using [llama.cpp Server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md#quick-start).

For custom LLM, specify "openai:&#123;your-llm&#125;"
```env
# set the custom OpenAI API url
OPENAI_BASE_URL=http://localhost:1234/v1
# set the custom OpenAI API key
OPENAI_API_KEY=dummy_key

# specify custom llms  
FAST_LLM=openai:your_fast_llm
SMART_LLM=openai:your_smart_llm
STRATEGIC_LLM=openai:your_strategic_llm
```

For custom embedding, set "custom:&#123;your-embedding&#125;"
```env
# set the custom OpenAI API url
OPENAI_BASE_URL=http://localhost:1234/v1
# set the custom OpenAI API key
OPENAI_API_KEY=dummy_key

# specify the custom embedding model   
EMBEDDING=custom:your_embedding
```


## Azure OpenAI

In Azure OpenAI you have to chose which models you want to use and make deployments for each model. You do this on the [Azure OpenAI Portal](https://portal.azure.com/). 

In January 2025 the models that are recommended to use are: 

- gpt-4o-mini
- gpt-4o
- o1-preview or o1-mini (You might need to request access to these models before you can deploy them).

Please then specify the model names/deployment names in your `.env` file.

**Required Precondition** 

- Your endpoint can have any valid name.
- A model's deployment name *must be the same* as the model name.
- You need to deploy an *Embedding Model*: To ensure optimal performance, GPT Researcher requires the 'text-embedding-3-large' model. Please deploy this specific model to your Azure Endpoint.

**Recommended**:

- Quota increase: You should also request a quota increase especially for the embedding model, as the default quota is not sufficient. 

```env
# set the azure api key and deployment as you have configured it in Azure Portal. There is no default access point unless you configure it yourself!
AZURE_OPENAI_API_KEY=[Your Key]
AZURE_OPENAI_ENDPOINT=https://&#123;your-endpoint&#125;.openai.azure.com/
OPENAI_API_VERSION=2024-05-01-preview

# each string is "azure_openai:deployment_name". ensure that your deployment have the same name as the model you use!
FAST_LLM=azure_openai:gpt-4o-mini
SMART_LLM=azure_openai:gpt-4o
STRATEGIC_LLM=azure_openai:o1-preview

# specify embedding
EMBEDDING=azure_openai:text-embedding-3-large
```

Add `langchain-azure-dynamic-sessions` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Ollama

GPT Researcher supports both Ollama LLMs and embeddings. You can choose each or both.
To use [Ollama](http://www.ollama.com) you can set the following environment variables

```env
OLLAMA_BASE_URL=http://localhost:11434
FAST_LLM=ollama:llama3
SMART_LLM=ollama:llama3
STRATEGIC_LLM=ollama:llama3

EMBEDDING=ollama:nomic-embed-text
```

Add `langchain-ollama` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

### Granite with Ollama

GPT Researcher has custom prompt formatting for the [Granite family of models](https://ollama.com/search?q=granite). To use
the right formatting, you can set the following environment variables:

```env
OLLAMA_BASE_URL=http://localhost:11434
FAST_LLM=ollama:granite3.3:2b
SMART_LLM=ollama:granite3.3:8b
STRATEGIC_LLM=ollama:granite3.3:8b
PROMPT_FAMILY=granite
```

## Groq

GroqCloud provides advanced AI hardware and software solutions designed to deliver amazingly fast AI inference performance.
To leverage Groq in GPT-Researcher, you will need a GroqCloud account and an API Key. (__NOTE:__ Groq has a very _generous free tier_.)

### Sign up
- You can signup here: [https://console.groq.com/login](https://console.groq.com/login)
- Once you are logged in, you can get an API Key here: [https://console.groq.com/keys](https://console.groq.com/keys)

- Once you have an API key, you will need to add it to your `systems environment` using the variable name:
`GROQ_API_KEY=*********************`

### Update env vars
And finally, you will need to configure the GPT-Researcher Provider and Model variables:

```env
GROQ_API_KEY=[Your Key]

# Set one of the LLM models supported by Groq
FAST_LLM=groq:Mixtral-8x7b-32768
SMART_LLM=groq:Mixtral-8x7b-32768
STRATEGIC_LLM=groq:Mixtral-8x7b-32768
```

Add `langchain-groq` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

__NOTE:__ As of the writing of this Doc (May 2024), the available Language Models from Groq are:

* Llama3-70b-8192
* Llama3-8b-8192
* Mixtral-8x7b-32768
* Gemma-7b-it


## Anthropic

Refer to Anthropic [Getting started page](https://docs.anthropic.com/en/api/getting-started) to obtain Anthropic API key. Update the corresponding env vars, for example:
```env
ANTHROPIC_API_KEY=[Your Key]
FAST_LLM=anthropic:claude-2.1
SMART_LLM=anthropic:claude-3-opus-20240229
STRATEGIC_LLM=anthropic:claude-3-opus-20240229
```

Add `langchain-anthropic` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

Anthropic does not offer its own embedding model, therefore, you'll want to either default to the OpenAI embedding model, or find another.


## Mistral AI

Sign up for a [Mistral API key](https://console.mistral.ai/users/api-keys/). 
Then update the corresponding env vars, for example:
```env
MISTRAL_API_KEY=[Your Key]
FAST_LLM=mistralai:open-mistral-7b
SMART_LLM=mistralai:mistral-large-latest
STRATEGIC_LLM=mistralai:mistral-large-latest

EMBEDDING=mistralai:mistral-embed
```

Add `langchain-mistralai` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Together AI
[Together AI](https://www.together.ai/) offers an API to query [50+ leading open-source models](https://docs.together.ai/docs/inference-models) in a couple lines of code.
Then update corresponding env vars, for example:
```env
TOGETHER_API_KEY=[Your Key]
FAST_LLM=together:meta-llama/Llama-3-8b-chat-hf
SMART_LLM=together:meta-llama/Llama-3-70b-chat-hf
STRATEGIC_LLM=together:meta-llama/Llama-3-70b-chat-hf

EMBEDDING=mistralai:nomic-ai/nomic-embed-text-v1.5
```

Add `langchain-together` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## NetMind
[NetMind](https://netmind.ai/) provide a variety of [model API](https://www.netmind.ai/modelsLibrary) servicesâ€”including LLM, image, text, audio, and videoâ€”that add limitless possibilities for scaling your application.
```env
NETMIND_API_KEY=[Your Key]

FAST_LLM=netmind:deepseek-ai/DeepSeek-V3-0324
SMART_LLM=netmind:deepseek-ai/DeepSeek-R1-0528
STRATEGIC_LLM=netmind:deepseek-ai/DeepSeek-V3-0324

EMBEDDING=netmind:nvidia/NV-Embed-v2
```
Add langchain-netmind to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or pip install it

## HuggingFace

This integration requires a bit of extra work. Follow [this guide](https://python.langchain.com/v0.1/docs/integrations/chat/huggingface/) to learn more.
After you've followed the tutorial above, update the env vars:
```env
HUGGINGFACE_API_KEY=[Your Key]
FAST_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta
SMART_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta
STRATEGIC_LLM=huggingface:HuggingFaceH4/zephyr-7b-beta

EMBEDDING=huggingface:sentence-transformers/all-MiniLM-L6-v2
```

Add `langchain-huggingface` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Google Gemini

Sign up [here](https://ai.google.dev/gemini-api/docs/api-key) for obtaining a Google Gemini API Key and update the following env vars:
```env
GOOGLE_API_KEY=[Your Key]
FAST_LLM=google_genai:gemini-1.5-flash
SMART_LLM=google_genai:gemini-1.5-pro
STRATEGIC_LLM=google_genai:gemini-1.5-pro

EMBEDDING=google_genai:models/text-embedding-004
```

Add `langchain-google-genai` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Google VertexAI

```env
FAST_LLM=google_vertexai:gemini-1.5-flash-001
SMART_LLM=google_vertexai:gemini-1.5-pro-001
STRATEGIC_LLM=google_vertexai:gemini-1.5-pro-001

EMBEDDING=google_vertexai:text-embedding-004
```

Add `langchain-google-vertexai` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Cohere

```env
COHERE_API_KEY=[Your Key]
FAST_LLM=cohere:command
SMART_LLM=cohere:command-nightly
STRATEGIC_LLM=cohere:command-nightly

EMBEDDING=cohere:embed-english-v3.0
```

Add `langchain-cohere` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Fireworks

```env
FIREWORKS_API_KEY=[Your Key]
base_url=https://api.fireworks.ai/inference/v1/completions
FAST_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct
SMART_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct
STRATEGIC_LLM=fireworks:accounts/fireworks/models/mixtral-8x7b-instruct

EMBEDDING=fireworks:nomic-ai/nomic-embed-text-v1.5
```

Add `langchain-fireworks` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Bedrock

```env
FAST_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0
SMART_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0
STRATEGIC_LLM=bedrock:anthropic.claude-3-sonnet-20240229-v1:0

EMBEDDING=bedrock:amazon.titan-embed-text-v2:0
```

Add `langchain_aws` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## LiteLLM

```env
FAST_LLM=litellm:perplexity/pplx-7b-chat
SMART_LLM=litellm:perplexity/pplx-70b-chat
STRATEGIC_LLM=litellm:perplexity/pplx-70b-chat
```

Add `langchain_community` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## xAI

```env
FAST_LLM=xai:grok-beta
SMART_LLM=xai:grok-beta
STRATEGIC_LLM=xai:grok-beta
```

Add `langchain_xai` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## DeepSeek
```env
DEEPSEEK_API_KEY=[Your Key]
FAST_LLM=deepseek:deepseek-chat
SMART_LLM=deepseek:deepseek-chat
STRATEGIC_LLM=deepseek:deepseek-chat
```

## Dashscope

```envs
DASHSCOPE_API_KEY=[Your Key]
export FAST_LLM=dashscope:qwen3-32b
export SMART_LLM=dashscope:qwen-turbo-2025-04-28
export STRATEGIC_LLM=dashscope:qwen-plus-latest

export EMBEDDING=dashscope:text-embedding-v3
```

Add `dashscope` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it

## Openrouter.ai

```env
OPENROUTER_API_KEY=[Your openrouter.ai key]
OPENAI_BASE_URL=https://openrouter.ai/api/v1
FAST_LLM=openrouter:google/gemini-2.0-flash-lite-001
SMART_LLM=openrouter:google/gemini-2.0-flash-001
STRATEGIC_LLM=openrouter:google/gemini-2.5-pro-exp-03-25
OPENROUTER_LIMIT_RPS=1  # Ratelimit request per secound
EMBEDDING=google_genai:models/text-embedding-004 # openrouter doesn't support embedding models, use google instead its free
GOOGLE_API_KEY=[Your *google gemini* key]
```
## AI/ML API
#### AI/ML API provides 300+ AI models including Deepseek, Gemini, ChatGPT. The models run at enterprise-grade rate limits and uptimes.
You can check provider docs [_here_](https://docs.aimlapi.com/?utm_source=gptr&utm_medium=github&utm_campaign=integration)

And models overview is [_here_](https://aimlapi.com/models/?utm_source=gptr&utm_medium=github&utm_campaign=integration)

```env
AIMLAPI_API_KEY=[Your aimlapi.com key]
AIMLAPI_BASE_URL="https://api.aimlapi.com/v1"
FAST_LLM="aimlapi:claude-3-5-sonnet-20241022"
SMART_LLM="aimlapi:openai/o4-mini-2025-04-16"
STRATEGIC_LLM="aimlapi:x-ai/grok-3-mini-beta"
EMBEDDING="aimlapi:text-embedding-3-small"
```

## vLLM
```env
VLLM_OPENAI_API_KEY=[Your Key] # you can set this to 'EMPTY' or anything
VLLM_OPENAI_API_BASE=[Your base url] # for example http://localhost:8000/v1/
FAST_LLM=vllm_openai:Qwen/Qwen3-8B-AWQ
SMART_LLM=vllm_openai:Qwen/Qwen3-8B-AWQ
STRATEGIC_LLM=vllm_openai:Qwen/Qwen3-8B-AWQ
```

## Other Embedding Models

### Nomic

```env
EMBEDDING=nomic:nomic-embed-text-v1.5
```

### VoyageAI

```env
VOYAGE_API_KEY=[Your Key]
EMBEDDING=voyageai:voyage-law-2
```

Add `langchain-voyageai` to [requirements.txt](https://github.com/assafelovic/gpt-researcher/blob/master/requirements.txt) for Docker Support or `pip install` it



================================================
FILE: docs/docs/gpt-researcher/llms/running-with-azure.md
================================================
# Running with Azure

## Example: Azure OpenAI Configuration

If you are not using OpenAI's models, but other model providers, besides the general configuration above, also additional environment variables are required.

Here is an example for [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) configuration:

```bash
OPENAI_API_VERSION="2024-05-01-preview" # or whatever you are using
AZURE_OPENAI_ENDPOINT="https://CHANGEMEN.openai.azure.com/" # change to the name of your deployment
AZURE_OPENAI_API_KEY="[Your Key]" # change to your API key

EMBEDDING="azure_openai:text-embedding-ada-002" # change to the deployment of your embedding model

FAST_LLM="azure_openai:gpt-4o-mini" # change to the name of your deployment (not model-name)
FAST_TOKEN_LIMIT=4000

SMART_LLM="azure_openai:gpt-4o" # change to the name of your deployment (not model-name)
SMART_TOKEN_LIMIT=4000

RETRIEVER="bing" # if you are using Bing as your search engine (which is likely if you use Azure)
BING_API_KEY="[Your Key]"
```

For more details on what each variable does, you can check out the [GPTR Config Docs](https://docs.gptr.dev/docs/gpt-researcher/gptr/config)


================================================
FILE: docs/docs/gpt-researcher/llms/running-with-ollama.md
================================================
# Running with Ollama

Ollama is a platform that allows you to deploy and manage custom language models. This guide will walk you through deploying a custom language model on Ollama.

Read on to understand how to install a Custom LLM with the Ollama WebUI, and how to query it with GPT-Researcher.


## Fetching the Desired LLM Models

After deploying Ollama WebUI, you'll want to enter the [Open WebUI Admin App](https://github.com/open-webui/open-webui/tree/main) & download a custom LLM.

Choose a model from [Ollama's Library of LLM's](https://ollama.com/library?sort=popular)

Paste the model name & size into the Web UI:

<img width="1511" alt="Screen Shot 2024-08-27 at 23 26 28" src="https://github.com/user-attachments/assets/32abd048-745c-4232-9f1f-6af265cff250"></img>

For our example, let's choose to download the `qwen2:1.5b` from the chat completion model & `nomic-embed-text` for the embeddings model.

This model now automatically becomes available via your Server's out-of-the-box API - we'll leverage it within our GPT-Researcher .env file in the next step.


## Querying your Custom LLM with GPT-Researcher

If you deploy ollama locally, a .env like so, should enable powering GPT-Researcher with Ollama:

```bash
OPENAI_API_KEY="123"
OPENAI_API_BASE="http://127.0.0.1:11434/v1"
OLLAMA_BASE_URL="http://127.0.0.1:11434/"
FAST_LLM="ollama:qwen2:1.5b"
SMART_LLM="ollama:qwen2:1.5b"
STRATEGIC_LLM="ollama:qwen2:1.5b"
EMBEDDING_PROVIDER="ollama"
OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
```

Replace `FAST_LLM` & `SMART_LLM` with the model you downloaded from the Elestio Web UI in the previous step.


## Deploy Ollama on Elestio

Elestio is a platform that allows you to deploy and manage custom language models. This guide will walk you through deploying a custom language model on Elestio.

You can deploy an [Open WebUI](https://github.com/open-webui/open-webui/tree/main) server with [Elestio](https://elest.io/open-source/ollama)


## Run LLM Test Script for GPTR

You can leverage the global `test-your-llm` function with `tests/test-your-llm`.
Here are the steps to do so:

Step 1: Set the following values in your `.env`. Note: replace the base urls with the custom domain that your web app is available on - for example: if the web app is available on `https://ollama-2d52b-u21899.vm.elestio.app/` within the browser, that becomes the value to use in your .env file.

```bash
OPENAI_API_KEY="123"
OPENAI_API_BASE="https://ollama-2d52b-u21899.vm.elestio.app:57987/v1"
OLLAMA_BASE_URL="https://ollama-2d52b-u21899.vm.elestio.app:57987/"
FAST_LLM="openai:qwen2.5"
SMART_LLM="openai:qwen2.5"
STRATEGIC_LLM="openai:qwen2.5"
EMBEDDING_PROVIDER="ollama"
OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
```

Note: to verify you're pointing at the correct API URL, you can run something like this in your terminal:

```bash
nslookup ollama-2d52b-u21899.vm.elestio.app
```

Step 2:

```bash
cd tests
python -m test-your-llm
```

You should get an LLM response, such as:
```
Sup! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything.
```

#### Disable Elestio Authentication or Add Auth Headers

To remove the basic auth you have to follow the below steps:

Go to your service -> Security in your Elestio admin panel.

Step 1: Disable the Firewall.

Step 2: Edit your Nginx Configuration. You'll want to comment both these both these lines out:

```bash
auth_basic           "Authentication"; 
auth_basic_user_file /etc/nginx/conf.d/.htpasswd;
```

Step 2: Click the button "Update & Restart" to apply your nginx changes.



================================================
FILE: docs/docs/gpt-researcher/llms/supported-llms.md
================================================
# Supported LLMs

The following LLMs are supported by GPTR (though you'll need to install the relevant langchain package separately if you're not using OpenAI).

- openai
- anthropic
- azure_openai
- cohere
- google_vertexai
- google_genai
- fireworks
- gigachat
- ollama
- together
- mistralai
- huggingface
- groq
- bedrock
- dashscope
- xai
- deepseek
- litellm
- openrouter
- vllm

If you'd like to know the name of the langchain package for each LLM, you can check the [Langchain documentation](https://python.langchain.com/v0.2/docs/integrations/platforms/), or run GPTR as is and inspect the error message.

The GPTR LLM Module is built on top of the [Langchain LLM Module](https://python.langchain.com/v0.2/docs/integrations/llms/).

If you'd like to add a new LLM into GPTR, you can start with the [langchain documentation](https://python.langchain.com/v0.2/docs/integrations/platforms/) and then look into integrating it into the [GPTR LLM Module](https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/llm_provider/generic/base.py).


================================================
FILE: docs/docs/gpt-researcher/llms/testing-your-llm.md
================================================
# Testing your LLM

Here is a snippet of code to help you verify that your LLM-related environment variables are set up correctly.

```python
from gpt_researcher.config.config import Config
from gpt_researcher.utils.llm import create_chat_completion
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    cfg = Config()

    try:
        report = await create_chat_completion(
            model=cfg.smart_llm_model,
            messages = [{"role": "user", "content": "sup?"}],
            temperature=0.35,
            llm_provider=cfg.smart_llm_provider,
            stream=True,
            max_tokens=cfg.smart_token_limit,
            llm_kwargs=cfg.llm_kwargs
        )
    except Exception as e:
        print(f"Error in calling LLM: {e}")

# Run the async function
asyncio.run(main())
```


================================================
FILE: docs/docs/gpt-researcher/mcp-server/advanced-usage.md
================================================
---
sidebar_position: 2
---

# Advanced Usage

This guide covers advanced usage scenarios and configurations for the GPT Researcher MCP Server.

## Custom Configuration

You can customize the MCP server behavior by modifying various configuration parameters:

### Environment Variables

Create a `.env` file with additional configuration options:

```bash
# Required API keys
OPENAI_API_KEY=your_openai_api_key
TAVILY_API_KEY=your_tavily_api_key

# Optional configurations assuming using OpenAI
STRATEGIC_LLM=openai:gpt-4o-mini # Change default to faster reasoning model
MAX_ITERATIONS=2 # Make the research faster by reducing iterations
SCRAPER=tavily_extract # For production use, using hosted scraping methods (assuming you use tavily)
```

### Server Configuration File

You can create a `config.json` file to customize server behavior:

```json
{
  "host": "0.0.0.0",
  "port": 8000,
  "debug": false,
  "timeout": 300,
  "max_concurrent_requests": 10
}
```

## Integrating with Claude

To integrate with Claude effectively:

1. Make sure your Claude model has MCP capabilities enabled
2. Point Claude to the MCP server endpoint
3. Use the appropriate prompts to guide Claude in using the research tools

Example configuration for Claude:

```json
{
  "tools": [
    {
      "name": "gptr-researcher",
      "endpoint": "http://localhost:8000/mcp"
    }
  ]
}
```

## Advanced Tool Usage

### Conducting Deep Research

For deeper research capabilities:

```
Use the conduct_research tool with these advanced parameters:
{
  "query": "quantum computing advancements 2024",
  "depth": "deep",
  "focus_areas": ["hardware", "algorithms", "applications"],
  "timeline": "last 1 year"
}
```

### Customizing Report Generation

The write_report tool accepts several customization options:

```
Use the write_report tool with:
{
  "style": "academic",
  "format": "markdown",
  "include_images": true,
  "citation_style": "APA",
  "executive_summary": true
}
```

## Securing Your MCP Server

To secure your MCP server deployment:

1. Add API key authentication:
   ```python
   # Add to server.py
   @app.middleware("http")
   async def verify_api_key(request, call_next):
       api_key = request.headers.get("X-API-Key")
       if api_key != os.getenv("MCP_API_KEY"):
           return JSONResponse(status_code=401, content={"error": "Invalid API key"})
       return await call_next(request)
   ```

2. Enable HTTPS:
   ```bash
   # Run with HTTPS
   uvicorn server:app --host 0.0.0.0 --port 8000 --ssl-keyfile=./key.pem --ssl-certfile=./cert.pem
   ```

3. Set up rate limiting:
   ```python
   # Add rate limiting
   from fastapi import Depends, HTTPException
   from slowapi import Limiter, _rate_limit_exceeded_handler
   from slowapi.util import get_remote_address
   
   limiter = Limiter(key_func=get_remote_address)
   app.state.limiter = limiter
   app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
   
   @app.post("/mcp")
   @limiter.limit("10/minute")
   async def mcp_endpoint(request: Request, payload: dict):
       # Endpoint code
   ```

## Deploying with Docker

For easy deployment with Docker:

1. Create a Dockerfile:
```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "server.py"]
```

2. Build and run the Docker container:
```bash
docker build -t gpt-researcher-mcp .
docker run -p 8000:8000 -e OPENAI_API_KEY=your_key -e TAVILY_API_KEY=your_key gpt-researcher-mcp
```

## Monitoring and Logging

Enable detailed logging to monitor server activity:

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("mcp_server.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("mcp_server")
```

## Extending Functionality

You can extend the MCP server with additional capabilities:

1. Add new research tools
2. Implement custom report formats
3. Integrate with additional data sources
4. Add specialized research agents

For example, to add a new tool:

```python
@app.tool("analyze_sentiment")
async def analyze_sentiment(query: str):
    """Analyze the sentiment of research results."""
    # Implementation
    return {"sentiment": "positive", "confidence": 0.87}
```

## Troubleshooting Advanced Issues

### Handling Rate Limits

If you encounter rate limits with external APIs:

```python
import time
from tenacity import retry, wait_exponential, stop_after_attempt

@retry(wait=wait_exponential(multiplier=1, min=4, max=10), stop=stop_after_attempt(5))
def search_with_retry(query):
    try:
        return search_engine.search(query)
    except RateLimitError:
        time.sleep(5)
        raise
```

### Memory Management

For handling large research tasks:

```python
import gc

def clean_memory():
    """Force garbage collection to free memory"""
    gc.collect()
```

## Next Steps

- Explore [integrating with your own applications](../frontend/introduction)
- Learn about [creating custom agents](../multi_agents/langgraph) to enhance research capabilities
- Contribute to the [GPT Researcher project](../../contribute)

:-) 


================================================
FILE: docs/docs/gpt-researcher/mcp-server/claude-integration.md
================================================
---
sidebar_position: 3
---

# Claude Desktop Integration

This guide specifically focuses on how to integrate your locally running GPT Researcher MCP server with the Claude desktop application for Mac, providing a seamless research experience within the Claude interface.

Check out the official Anthropic MCP docs [here](https://modelcontextprotocol.io/quickstart/user)

## Prerequisites

Before integrating with Claude desktop client, you'll need:

1. GPT Researcher MCP server installed and running locally
2. Claude for Mac desktop application installed
3. Administrative access to your Mac to modify configuration files

## Setting Up Claude Desktop with MCP

To integrate your locally running MCP server with Claude for Mac, follow these steps:

### 1. Install and Run the GPT Researcher MCP Server

Make sure you have the GPT Researcher MCP server installed and running:

```bash
# Clone the repository (if you haven't already)
git clone https://github.com/assafelovic/gptr-mcp.git

# Install dependencies
pip install -r requirements.txt

# Set up your environment variables
cp .env.example .env
# Edit the .env file with your API keys

# Run the server
python server.py
```

Verify that the server is running properly by checking the console output. The server should be listening on port 8000 by default.

### 2. Configure Claude Desktop

1. **Locate Claude's Configuration File**:
   - Open Finder and press `Shift + Command + G` to open the "Go to Folder" dialog
   - Enter `~/Library/Application Support/Claude/` and click "Go"
   - Find the `claude_desktop_config.json` file in this directory. If it doesn't exist, create a new file with this name
   - Alternatively, you can open the Claude App -> Settings -> Developer -> Update Config.

2. **Edit the Configuration File**:
   - Open `claude_desktop_config.json` with a text editor
   - Add or update the `mcpServers` section to include your local GPT Researcher MCP server:

```json
{
  "mcpServers": {
    "gpt-researcher": {
      "command": "/path/to/python",
      "args": ["/path/to/gptr-mcp/server.py"]
    }
  }
}
```

Replace `/path/to/gptr-mcp/server.py` with the absolute path to your server.py file.

Alternatively, if you prefer to manually start the server and just have Claude connect to it:

```json
{
  "mcpServers": {},
  "externalMCPServers": {
    "gpt-researcher": "http://localhost:8000/mcp"
  }
}
```

### 3. Restart Claude for Desktop

Close and reopen the Claude application to apply the new configuration.

### 4. Verify the Integration

Upon restarting:
- Look for a hammer icon (ğŸ”¨) in the bottom right corner of the input box in Claude
- Clicking this icon should display the GPT Researcher tools provided by your MCP server
- If you don't see the hammer icon, check the Claude application logs for any errors

## Using GPT Researcher in Claude Desktop

Once integrated, you can use research capabilities by:

1. Clicking on the hammer icon (ğŸ”¨) in the message input area
2. Selecting the "conduct_research" tool
3. Entering your research query and other parameters
4. Submitting your query

You can also directly prompt Claude to use the tools:

```
I need to research the latest advancements in quantum computing. Please use the conduct_research tool to gather information, then create a comprehensive report.
```

## Troubleshooting

If you encounter issues with the integration:

1. **Server Connection Issues**:
   - Ensure the MCP server is running and listening on the expected port
   - Check firewall settings that might block the connection
   - Verify the path in the configuration file is correct

2. **Tool Availability Issues**:
   - If tools aren't showing up, restart both the MCP server and Claude
   - Check the server logs for any error messages
   - Make sure your API keys are properly configured in the .env file

3. **Permission Issues**:
   - Ensure Claude has permission to execute the server script
   - Check file permissions on the server.py file

4. **Configuration File Issues**:
   - Verify your JSON syntax is correct in the configuration file
   - Make sure the configuration directory exists and is accessible

## Next Steps

- Explore [advanced usage options](./advanced-usage) for customizing your research experience
- Learn about [additional configuration options](../gptr/config) for the GPT Researcher
- Check out [example prompts](./claude-integration#claude-specific-prompts) to effectively guide Claude in using the research tools



================================================
FILE: docs/docs/gpt-researcher/mcp-server/getting-started.md
================================================
---
sidebar_position: 1
---

# Getting Started

The GPT Researcher MCP Server provides Model Context Protocol (MCP) integration for GPT Researcher, allowing AI assistants to perform autonomous, comprehensive web research and generate reports via the MCP protocol.

## Why GPT Researcher MCP?

While many AI apps can access web search tools with MCP, GPT Researcher MCP delivers in-depth results. Standard search tools return raw results requiring manual filtering, often containing irrelevant sources and wasting context window space.

GPT Researcher performs autonomous, deep research - not just search. It intelligently explores and validates multiple sources, focusing only on relevant and up-to-date information. Though slightly slower (30-40 seconds) than standard search, it delivers higher quality information, optimized context, comprehensive results, and better reasoning for LLMs.

The MCP server exposes the following capabilities to AI assistants:

### Resources
- `research_resource`: Get web resources related to a given task via research.

### Primary Tools

- `deep_research`: Performs autonomous web research on a topic, finding the most reliable and relevant information
- `quick_search`: Performs a fast web search optimized for speed over quality, returning search results with snippets
- `write_report`: Generate a report based on research results
- `get_research_sources`: Get the sources used in the research
- `get_research_context`: Get the full context of the research

### Prompts

- `research_query`: Create a research query prompt

## Prerequisites

Before running the MCP server, make sure you have:

1. Python 3.10 or higher installed
2. API keys for the services you plan to use:
   - OpenAI API key
   - Tavily API key (or other search APIs you plan to use)

## Installation

1. Clone the GPT Researcher repository:
```bash
git clone https://github.com/assafelovic/gptr-mcp.git
```

2. Install the dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your environment variables:
   - Copy the `.env.example` file to create a new file named `.env`:
   ```bash
   cp .env.example .env
   ```
   - Edit the `.env` file and add your API keys and configure other settings:
   ```bash
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

## Running the MCP Server

You can start the MCP server in two ways:

### Method 1: Directly using Python

```bash
python server.py
```

### Method 2: Using the MCP CLI (if installed)

```bash
mcp run server.py
```

Once the server is running, you'll see output indicating that the server is ready to accept connections.

## Integrating with Claude

There are two primary ways to integrate your MCP server with Claude:

1. **[API Integration](./claude-integration)** - For programmatic access through Claude API
2. **[Desktop Integration](./claude-desktop-integration)** - For using with Claude desktop application on Mac

For detailed instructions on each method, follow the links above.

## Example Usage with Claude

```
User: I'm considering investing in NVIDIA. Can you research the current state of the company, recent stock performance, latest product announcements, and analyst opinions?

Claude: I'll help you research NVIDIA for investment purposes. This will take a moment as I conduct comprehensive research to find the most up-to-date information about their stock performance, recent developments, and expert analysis.

[Claude uses conduct_research tool - this takes 30-40 seconds]

Claude: Based on my research, here's a comprehensive analysis of NVIDIA's current state:

## NVIDIA (NVDA) Current Status

### Recent Stock Performance
- Current price: $942.89 (as of my research)
- YTD performance: +90.4% 
- 52-week range: $460.30 - $974.00
- Market cap: ~$2.32 trillion

### Recent Key Developments
1. **Blackwell AI Platform**: At GTC 2024, NVIDIA unveiled the Blackwell architecture, their next-generation AI chip platform promising 4x performance improvement over Hopper. Major cloud providers (AWS, Google, Microsoft) have already committed to integrating Blackwell.

2. **Q1 FY2025 Earnings (May 2024)**: 
   - Revenue: $26.0 billion (+262% YoY)
   - Data Center revenue: $22.6 billion (+427% YoY)
   - Gross margin: 78.4%
   - Announced 10-for-1 stock split effective June 7, 2024

3. **Supply Chain Expansion**: Increased investments in manufacturing capacity with TSMC to address ongoing chip shortages for their high-demand AI GPUs.

### Analyst Consensus
- Strong Buy consensus with 37 analysts rating it "Buy," 5 "Hold," and 0 "Sell"
- Average price target: $1,042 (post-split adjusted: $104.20)
- Key bull thesis: Continued AI infrastructure spending and NVIDIA's dominant position in AI chips
- Key bear concerns: Valuation, potential competition from AMD, Intel, and custom silicon from tech giants

### Industry Position
- Dominant in AI GPU market with estimated 80-90% market share
- Expanding ecosystem through CUDA software platform and industry partnerships
- Diversifying beyond chips with DGX Cloud, AI software platforms, and automotive solutions
```

## Troubleshooting

If you encounter issues while running the MCP server:

1. Make sure your API keys are correctly set in the `.env` file
2. Check that you're using Python 3.10 or higher
3. Ensure all dependencies are installed correctly
4. Check the server logs for error messages

## Next Steps

- Explore the [MCP protocol documentation](https://docs.anthropic.com/claude/docs/model-context-protocol) to better understand how to integrate with Claude
- Learn about [GPT Researcher's core features](../getting-started/introduction) to enhance your research capabilities
- Check out the [Advanced Usage](./advanced-usage) guide for more configuration options

:-) 


================================================
FILE: docs/docs/gpt-researcher/multi_agents/langgraph.md
================================================
# LangGraph

[LangGraph](https://python.langchain.com/docs/langgraph) is a library for building stateful, multi-actor applications with LLMs. 
This example uses Langgraph to automate the process of an in depth research on any given topic.

## Use case
By using Langgraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. 
Inspired by the recent [STORM](https://arxiv.org/abs/2402.14207) paper, this example showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.

An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.

Please note: This example uses the OpenAI API only for optimized performance.

## The Multi Agent Team
The research team is made up of 7 AI agents:
- **Human** - The human in the loop that oversees the process and provides feedback to the agents.
- **Chief Editor** - Oversees the research process and manages the team. This is the "master" agent that coordinates the other agents using Langgraph.
- **Researcher** (gpt-researcher) - A specialized autonomous agent that conducts in depth research on a given topic.
- **Editor** - Responsible for planning the research outline and structure.
- **Reviewer** - Validates the correctness of the research results given a set of criteria.
- **Revisor** - Revises the research results based on the feedback from the reviewer.
- **Writer** - Responsible for compiling and writing the final report.
- **Publisher** - Responsible for publishing the final report in various formats.

## How it works
Generally, the process is based on the following stages: 
1. Planning stage
2. Data collection and analysis
3. Review and revision
4. Writing and submission
5. Publication

### Architecture
<div align="center">
<img align="center" height="600" src="https://cowriter-images.s3.amazonaws.com/multi-agents-gptr.png"></img>
</div>
<br clear="all"/>

### Steps
More specifically (as seen in the architecture diagram) the process is as follows:
- Browser (gpt-researcher) - Browses the internet for initial research based on the given research task.
- Editor - Plans the report outline and structure based on the initial research.
- For each outline topic (in parallel):
  - Researcher (gpt-researcher) - Runs an in depth research on the subtopics and writes a draft.
  - Reviewer - Validates the correctness of the draft given a set of criteria and provides feedback.
  - Revisor - Revises the draft until it is satisfactory based on the reviewer feedback.
- Writer - Compiles and writes the final report including an introduction, conclusion and references section from the given research findings.
- Publisher - Publishes the final report to multi formats such as PDF, Docx, Markdown, etc.

## How to run
1. Install required packages:
    ```bash
    pip install -r requirements.txt
    ```
3. Update env variables
   ```bash
   export OPENAI_API_KEY={Your OpenAI API Key here}
   export TAVILY_API_KEY={Your Tavily API Key here}
   ```
2. Run the application:
    ```bash
    python main.py
    ```

## Usage
To change the research query and customize the report, edit the `task.json` file in the main directory.
#### Task.json contains the following fields:
- `query` - The research query or task.
- `model` - The OpenAI LLM to use for the agents.
- `max_sections` - The maximum number of sections in the report. Each section is a subtopic of the research query.
- `include_human_feedback` - If true, the user can provide feedback to the agents. If false, the agents will work autonomously.
- `publish_formats` - The formats to publish the report in. The reports will be written in the `output` directory.
- `source` - The location from which to conduct the research. Options: `web` or `local`. For local, please add `DOC_PATH` env var.
- `follow_guidelines` - If true, the research report will follow the guidelines below. It will take longer to complete. If false, the report will be generated faster but may not follow the guidelines.
- `guidelines` - A list of guidelines that the report must follow.
- `verbose` - If true, the application will print detailed logs to the console.

#### For example:
```json
{
  "query": "Is AI in a hype cycle?",
  "model": "gpt-4o",
  "max_sections": 3, 
  "publish_formats": { 
    "markdown": true,
    "pdf": true,
    "docx": true
  },
  "include_human_feedback": false,
  "source": "web",
  "follow_guidelines": true,
  "guidelines": [
    "The report MUST fully answer the original question",
    "The report MUST be written in apa format",
    "The report MUST be written in english"
  ],
  "verbose": true
}
```

## To Deploy

```shell
pip install langgraph-cli
langgraph up
```

From there, see documentation [here](https://github.com/langchain-ai/langgraph-example) on how to use the streaming and async endpoints, as well as the playground.

## NextJS Frontend App

The React app (located in `frontend` directory) is our Frontend 2.0 which we hope will enable us to display the robustness of the backend on the frontend, as well.

It comes with loads of added features, such as: 
 - a drag-n-drop user interface for uploading and deleting files to be used as local documents by GPTResearcher.
 - a GUI for setting your GPTR environment variables.
 - the ability to trigger the multi_agents flow via the Backend Module or Langgraph Cloud Host (currently in closed beta).
 - stability fixes
 - and more coming soon!

### Run the NextJS React App with Docker

> **Step 1** - [Install Docker](https://docs.gptr.dev/docs/gpt-researcher/getting-started/getting-started-with-docker)

> **Step 2** - Clone the '.env.example' file, add your API Keys to the cloned file and save the file as '.env'

> **Step 3** - Within the docker-compose file comment out services that you don't want to run with Docker.

```bash
$ docker-compose up --build
```

> **Step 4** - By default, if you haven't uncommented anything in your docker-compose file, this flow will start 2 processes:
 - the Python server running on localhost:8000
 - the React app running on localhost:3000

Visit localhost:3000 on any browser and enjoy researching!


### Run the NextJS React App with NPM

```bash
cd frontend/nextjs
nvm install 18.17.0
nvm use v18.17.0
npm install --legacy-peer-deps
npm run dev
```


================================================
FILE: docs/docs/gpt-researcher/retrievers/mcp-configs.mdx
================================================
# MCP Integration

The Model Context Protocol (MCP) enables GPT Researcher to connect with diverse data sources and tools through a standardized interface. GPT Researcher features an intelligent two-stage MCP approach that automatically selects the best tools and generates contextual research, powered by LangChain's [MCP adapters](https://github.com/langchain-ai/langchain-mcp-adapters) for seamless integration.

## How MCP Works in GPT Researcher

GPT Researcher uses a **two stage intelligent approach** for MCP integration:

1. **Stage 1: Smart Tool Selection** - LLM analyzes your query and available MCP servers to select the most relevant tools
2. **Stage 2: Contextual Research** - LLM uses selected tools with dynamically generated, query specific arguments

This happens automatically behind the scenes, optimized for the best balance of speed, cost, and research quality. The integration leverages the [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) library, ensuring compatibility with the growing ecosystem of MCP tool servers.

## MCP Research Flow

The following diagram illustrates the hybrid strategy using `RETRIEVER=tavily,mcp` as an example:

<img width="662" alt="Screenshot 2025-06-06 at 14 38 04" src="https://cowriter-images.s3.us-east-1.amazonaws.com/Screenshot+2025-06-06+at+14.38.04.png" />

### Flow Breakdown:

1. **Configuration**: Set `RETRIEVER` environment variable to enable MCP
2. **Strategy Selection**: Choose pure MCP or hybrid approach
3. **Initialization**: GPT Researcher loads your `mcp_configs`
4. **Stage 1**: LLM intelligently selects the most relevant tools from available MCP servers
5. **Stage 2**: LLM executes research using selected tools with query-specific arguments
6. **Hybrid Processing**: If using hybrid strategy, combines MCP results with web search
7. **Report Generation**: Synthesizes all findings into a comprehensive report

## Prerequisites

MCP support is included with GPT Researcher installation:

```bash
pip install gpt-researcher
# All MCP dependencies are included automatically
```

## Essential Configuration: Enabling MCP

**Important:** To use MCP with GPT Researcher, you must set the `RETRIEVER` environment variable:

### Pure MCP Research
```bash
export RETRIEVER=mcp
```

### Hybrid Strategy (Recommended)
```bash
# Combines web search with MCP for comprehensive research
export RETRIEVER=tavily,mcp

# Alternative hybrid combinations
export RETRIEVER=tavily,mcp
export RETRIEVER=google,mcp,arxiv
```

## Quick Start

```python
from gpt_researcher import GPTResearcher
import os

# Set retriever to enable MCP
os.environ["RETRIEVER"] = "tavily,mcp"  # Hybrid approach

# Simple MCP configuration - works automatically
researcher = GPTResearcher(
    query="How does React's useState hook work?",
    mcp_configs=[
        {
            "name": "github_api"
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")}
        }
    ]
)

context = await researcher.conduct_research()
report = await researcher.write_report()
```

## Configuration Structure

Each MCP configuration dictionary supports these keys:

| Key | Description | Example | Required |
|-----|-------------|---------|----------|
| `name` | Identifier for the MCP server | `"github"` | Yes |
| `command` | Command to start the server | `"python"` | Yes* |
| `args` | Arguments for the server command | `["-m", "my_server"]` | Yes* |
| `env` | Environment variables for the server | `{"API_KEY": "key"}` | No |
| `connection_url` | URL for remote connections | `"wss://api.example.com"` | Yes** |
| `connection_type` | Connection type (auto-detected) | `"websocket"` | No |
| `connection_token` | Authentication token | `"bearer_token"` | No |

**Local servers**: Require `name`, `command`, and `args`  
**Remote servers**: Require `name` and `connection_url`

## Examples

### News and Web Research with Tavily

Perfect for current events, market research, and general information gathering:

```python
from gpt_researcher import GPTResearcher
import os

# Enable hybrid research: web search + MCP
os.environ["RETRIEVER"] = "tavily,mcp"

researcher = GPTResearcher(
    query="What are the latest updates in the NBA playoffs?",
    mcp_configs=[
        {
            "name": "tavily",
            "command": "npx",
            "args": ["-y", "tavily-mcp@0.1.2"],
            "env": {
                "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")
            }
        }
    ]
)

context = await researcher.conduct_research()
report = await researcher.write_report()
```

### Code Research with GitHub

Ideal for technical documentation, code examples, and software development research:

```python
# Pure MCP research for technical queries
os.environ["RETRIEVER"] = "mcp"

researcher = GPTResearcher(
    query="What are the key features and implementation of React's useState hook? How has it evolved in recent versions?",
    mcp_configs=[
        {
            "name": "github",
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {
                "GITHUB_PERSONAL_ACCESS_TOKEN": os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
            }
        }
    ]
)
```

### Academic Research with Hybrid Strategy

Combining academic papers with MCP tools:

```python
# Academic + MCP hybrid approach
os.environ["RETRIEVER"] = "arxiv,semantic_scholar,mcp"

researcher = GPTResearcher(
    query="Analyze the latest developments in quantum error correction algorithms",
    mcp_configs=[
        {
            "name": "quantum_research",
            "command": "python",
            "args": ["quantum_mcp_server.py"],
            "env": {
                "ARXIV_API_KEY": os.getenv("ARXIV_API_KEY"),
                "RESEARCH_DB_PATH": "/path/to/quantum_papers.db"
            }
        }
    ]
)
```

## Multi-Server Research: Comprehensive Market Analysis

Here's a real-world example combining multiple MCP servers for comprehensive business intelligence:

```python
from gpt_researcher import GPTResearcher
import os

# Multi-retriever hybrid strategy for comprehensive coverage
os.environ["RETRIEVER"] = "tavily,google,mcp"

# Multi-domain research combining news, code, and financial data
researcher = GPTResearcher(
    query="Analyze Tesla's Q4 2024 performance, including stock trends, recent innovations, and market sentiment",
    mcp_configs=[
        # Financial data and stock analysis
        {
            "name": "financial_data",
            "command": "python",
            "args": ["financial_mcp_server.py"],
            "env": {
                "ALPHA_VANTAGE_KEY": os.getenv("ALPHA_VANTAGE_KEY"),
                "YAHOO_FINANCE_KEY": os.getenv("YAHOO_FINANCE_KEY")
            }
        },
        # News and market sentiment
        {
            "name": "news_research",
            "command": "npx",
            "args": ["-y", "tavily-mcp@0.1.2"],
            "env": {
                "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")
            }
        },
        # Technical innovations and patents
        {
            "name": "github_research",
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {
                "GITHUB_PERSONAL_ACCESS_TOKEN": os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
            }
        },
        # Academic research and papers
        {
            "name": "academic_papers",
            "command": "python",
            "args": ["arxiv_mcp_server.py"],
            "env": {
                "ARXIV_API_KEY": os.getenv("ARXIV_API_KEY")
            }
        }
    ]
)

# GPT Researcher automatically orchestrates all servers
context = await researcher.conduct_research()
report = await researcher.write_report()

print(f"Generated comprehensive report using {len(researcher.mcp_configs)} MCP servers")
print(f"Research cost: ${researcher.get_costs():.4f}")
```

This example demonstrates how GPT Researcher intelligently:
- **Selects relevant tools** from each server based on the query
- **Coordinates multi-domain research** across financial, news, technical, and academic sources
- **Synthesizes information** from different domains into a cohesive analysis
- **Optimizes performance** by using only the most relevant tools from each server

### E-commerce Competitive Analysis

Another practical multi-server scenario for business research:

```python
# Comprehensive hybrid strategy
os.environ["RETRIEVER"] = "tavily,bing,exa,mcp"

researcher = GPTResearcher(
    query="Comprehensive competitive analysis of sustainable fashion brands in 2024",
    mcp_configs=[
        # Web trends and consumer sentiment
        {
            "name": "web_trends",
            "command": "npx",
            "args": ["-y", "tavily-mcp@0.1.2"],
            "env": {"TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")}
        },
        # Social media analytics
        {
            "name": "social_analytics",
            "command": "python",
            "args": ["social_mcp_server.py"],
            "env": {
                "TWITTER_BEARER_TOKEN": os.getenv("TWITTER_BEARER_TOKEN"),
                "INSTAGRAM_ACCESS_TOKEN": os.getenv("INSTAGRAM_ACCESS_TOKEN")
            }
        },
        # Patent and innovation research
        {
            "name": "patent_research",
            "command": "python",
            "args": ["patent_mcp_server.py"],
            "env": {"USPTO_API_KEY": os.getenv("USPTO_API_KEY")}
        }
    ]
)
```

## Remote MCP Server

```python
# Enable MCP with web search fallback
os.environ["RETRIEVER"] = "tavily,mcp"

researcher = GPTResearcher(
    query="Latest AI research papers on transformer architectures",
    mcp_configs=[
        {
            "name": "arxiv_api",
            "connection_url": "wss://mcp.arxiv.org/ws",  # Auto-detects WebSocket
            "connection_token": os.getenv("ARXIV_TOKEN"),
        }
    ]
)
```

## Combining MCP with Web Search

MCP works seamlessly alongside traditional web search for comprehensive research:

```python
from gpt_researcher import GPTResearcher

# Hybrid strategy: combines web search with MCP automatically
os.environ["RETRIEVER"] = "tavily,mcp"

researcher = GPTResearcher(
    query="Impact of AI on software development practices",
    # MCP will be used alongside web search automatically
    mcp_configs=[
        {
            "name": "github",
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {"GITHUB_TOKEN": os.getenv("GITHUB_TOKEN")}
        }
    ]
)

# This uses both MCP (for code examples) and web search (for articles/news)
context = await researcher.conduct_research()
```

## Complete Working Example

Here's a production-ready example demonstrating MCP integration:

```python
import asyncio
import os
from gpt_researcher import GPTResearcher

async def main():
    # Set up environment
    os.environ["GITHUB_PERSONAL_ACCESS_TOKEN"] = "your_github_token"
    os.environ["OPENAI_API_KEY"] = "your_openai_key"
    os.environ["TAVILY_API_KEY"] = "your_tavily_key"
    
    # Enable hybrid research strategy
    os.environ["RETRIEVER"] = "tavily,mcp"
    
    # Create researcher with multi-server MCP configuration
    researcher = GPTResearcher(
        query="How are leading tech companies implementing AI safety measures in 2024?",
        mcp_configs=[
            # Code repositories and technical implementations
            {
                "name": "github",
                "command": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": {
                    "GITHUB_PERSONAL_ACCESS_TOKEN": os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
                }
            },
            # Current news and industry reports
            {
                "name": "tavily",
                "command": "npx",
                "args": ["-y", "tavily-mcp@0.1.2"],
                "env": {
                    "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY")
                }
            }
        ],
        verbose=True  # See the intelligent research process
    )
    
    print("ğŸ” Starting multi-source research...")
    
    # Intelligent tool selection and research happens automatically
    context = await researcher.conduct_research()
    
    print("ğŸ“ Generating comprehensive report...")
    report = await researcher.write_report()
    
    print("âœ… Research complete!")
    print(f"ğŸ“Š Report length: {len(report)} characters")
    print(f"ğŸ’° Total cost: ${researcher.get_costs():.4f}")
    
    # Save the report
    with open("ai_safety_research.md", "w") as f:
        f.write(report)

if __name__ == "__main__":
    asyncio.run(main())
```

## Retriever Strategy Comparison

| Strategy | Use Case | Performance | Coverage |
|----------|----------|------------|----------|
| `RETRIEVER=mcp` | Specialized domains, structured data | âš¡ Fast | ğŸ¯ Focused |
| `RETRIEVER=tavily,mcp` | General research with specialized tools | âš–ï¸ Balanced | ğŸŒ Comprehensive |
| `RETRIEVER=google,arxiv,tavily,mcp` | Maximum coverage, redundancy | ğŸŒ Slower | ğŸŒ Extensive |
| `RETRIEVER=arxiv,mcp` | Academic + specialized research | âš¡ Fast | ğŸ“ Academic-focused |

## Advanced Configuration

### Research Strategies

For advanced users who need more control over how MCP research is executed:

| Strategy | Description | Use Case | Performance |
|----------|-------------|----------|-------------|
| `"fast"` | Run MCP once with main query (default) | Most research needs | âš¡ Optimal |
| `"deep"` | Run MCP for all sub-queries | Comprehensive analysis | ğŸ” Thorough |
| `"disabled"` | Skip MCP entirely | Web-only research | âš¡ Fastest |

```python
# Default behavior (recommended for most use cases)
os.environ["RETRIEVER"] = "tavily,mcp"
researcher = GPTResearcher(
    query="Analyze Tesla's performance",
    mcp_configs=[...]
)

# For comprehensive analysis (advanced)
os.environ["MCP_STRATEGY"] = "deep"
researcher = GPTResearcher(
    query="Comprehensive renewable energy analysis",
    mcp_configs=[...]
)

# For web-only research (advanced)
os.environ["RETRIEVER"] = "tavily"  # Excludes MCP entirely
```

### Environment Variable Configuration

Set global defaults using environment variables:

```bash
# Essential: Enable MCP
export RETRIEVER=tavily,mcp

# Advanced: Set MCP strategy
export MCP_STRATEGY=deep

# Or in .env file
RETRIEVER=tavily,mcp
MCP_STRATEGY=fast
MCP_AUTO_TOOL_SELECTION=true
```

### Custom Tool Selection

Enable automatic tool selection for servers with multiple tools:

```python
# Environment variable approach
os.environ["MCP_AUTO_TOOL_SELECTION"] = "true"
os.environ["RETRIEVER"] = "mcp"

researcher = GPTResearcher(
    query="your query",
    mcp_configs=[
        {
            "command": "python",
            "args": ["multi_tool_server.py"]
            # AI will choose the best tool automatically
        }
    ]
)
```

### Connection Type Detection

GPT Researcher automatically detects connection types:

```python
# WebSocket (detected from wss:// prefix)
{"connection_url": "wss://api.example.com/mcp"}

# HTTP (detected from https:// prefix)  
{"connection_url": "https://api.example.com/mcp"}

# Stdio (default when no URL provided)
{"command": "python", "args": ["server.py"]}
```

## Troubleshooting

### Common Issues

**"No retriever specified" or "MCP not working"**
- **Solution:** Set `RETRIEVER=mcp` or `RETRIEVER=tavily,mcp`
- Verify environment variable is set: `echo $RETRIEVER`

**"Invalid retriever(s) found"**
- Check available retrievers: `tavily`, `mcp`, `google`, `bing`, `arxiv`, etc.
- Ensure no typos in retriever names

**"No MCP server configurations found"**
- Ensure `mcp_configs` is a list of dictionaries
- Verify at least one configuration is provided
- Check configuration format matches examples

**"MCP server connection failed"**
- Verify server command and arguments
- Check environment variables are set correctly
- Test the MCP server independently
- Ensure required dependencies are installed

**"No tools available from MCP server"**
- Verify the server exposes tools correctly
- Check server startup logs for errors
- Try enabling `MCP_AUTO_TOOL_SELECTION=true`

**"Tool execution failed"**
- Check authentication tokens and API keys
- Verify tool arguments are valid
- Review server logs for detailed errors
- Enable debug logging for more information

### Debug Mode

Enable detailed logging to diagnose issues:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Your research code here - will show detailed MCP operations
```

### Testing Your Setup

Quick test to verify MCP configuration:

```python
import os
from gpt_researcher import GPTResearcher

# Test retriever configuration
os.environ["RETRIEVER"] = "mcp"

# Test basic configuration
researcher = GPTResearcher(
    query="test query",
    mcp_configs=[
        {
            "name": "test",
            "command": "echo",
            "args": ["hello world"]
        }
    ]
)

print(f"âœ… RETRIEVER set to: {os.environ.get('RETRIEVER')}")
print(f"âœ… MCP configs loaded: {len(researcher.mcp_configs)}")
```

## Best Practices

1. **Always set the RETRIEVER environment variable** - This is required for MCP functionality
2. **Use hybrid strategies** (`tavily,mcp`) for comprehensive research
3. **Use descriptive server names** for easier debugging
4. **Store sensitive data in environment variables**
5. **Test MCP servers independently** before integration
6. **Enable verbose mode** during development
7. **Choose appropriate retriever combinations** based on your research domain
8. **Let the default settings handle optimization** for most use cases

---

*For more examples and advanced use cases, check out the [GPT Researcher examples repository](https://github.com/assafelovic/gpt-researcher/tree/master/examples).* :-) 


================================================
FILE: docs/docs/gpt-researcher/search-engines/search-engines.md
================================================
# Search Engines

Search Engines are used to find the most relevant web sources and content for a given research task.
You can specify your preferred web search or use any custom retriever of your choice.

## Web Search Engines

GPT Researcher defaults to using the [Tavily](https://app.tavily.com) search engine for retrieving search results.
But you can also use other search engines by specifying the `RETRIEVER` env var. Please note that each search engine has its own API Key requirements and usage limits.

For example:

```bash
RETRIEVER=bing
```

You can also specify multiple retrievers by separating them with commas. The system will use each specified retriever in sequence.
For example:

```bash
RETRIEVER=tavily, arxiv
```

Thanks to our community, we have integrated the following web search engines:

- [Tavily](https://app.tavily.com) - Default
- [Bing](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api) - Env: `RETRIEVER=bing`
- [Google](https://developers.google.com/custom-search/v1/overview) - Env: `RETRIEVER=google`
- [SearchApi](https://www.searchapi.io/) - Env: `RETRIEVER=searchapi`
- [Serp API](https://serpapi.com/) - Env: `RETRIEVER=serpapi`
- [Serper](https://serper.dev/) - Env: `RETRIEVER=serper` - [Setup Guide](#serper)
- [Searx](https://searx.github.io/searx/) - Env: `RETRIEVER=searx`
- [Duckduckgo](https://pypi.org/project/duckduckgo-search/) - Env: `RETRIEVER=duckduckgo`
- [Arxiv](https://info.arxiv.org/help/api/index.html) - Env: `RETRIEVER=arxiv`
- [Exa](https://docs.exa.ai/reference/getting-started) - Env: `RETRIEVER=exa`
- [PubMedCentral](https://www.ncbi.nlm.nih.gov/home/develop/api/) - Env: `RETRIEVER=pubmed_central`

## Custom Retrievers

You can also use any custom retriever of your choice by specifying the `RETRIEVER=custom` env var.
Custom retrievers allow you to use any search engine that provides an API to retrieve documents and is widely used for enterprise research tasks.

In addition to setting the `RETRIEVER` env, you also need to set the following env vars:

- `RETRIEVER_ENDPOINT`: The endpoint URL of the custom retriever.
- Additional arguments required by the retriever should be prefixed with `RETRIEVER_ARG_` (e.g., RETRIEVER_ARG_API_KEY).

### Example

```bash
RETRIEVER=custom
RETRIEVER_ENDPOINT=https://api.myretriever.com
RETRIEVER_ARG_API_KEY=YOUR_API_KEY
```

### Response Format

For the custom retriever to work correctly, the response from the endpoint should be in the following format:

```json
[
  {
    "url": "http://example.com/page1",
    "raw_content": "Content of page 1"
  },
  {
    "url": "http://example.com/page2",
    "raw_content": "Content of page 2"
  }
]
```

The system assumes this response format and processes the list of sources accordingly.

## Search Engine Configuration

### Serper

To use [Serper](https://serper.dev/) as your search engine:

1. Get your API key from [serper.dev](https://serper.dev/)
2. Set the required environment variables:

```bash
RETRIEVER=serper
SERPER_API_KEY=your_api_key_here
```

**Optional Configuration:**

```bash
SERPER_REGION=us                    # Country code (us, kr, jp, etc.)
SERPER_LANGUAGE=en                  # Language code (en, ko, ja, etc.)
SERPER_TIME_RANGE=qdr:w            # Time filter (qdr:h, qdr:d, qdr:w, qdr:m, qdr:y)
SERPER_EXCLUDE_SITES=youtube.com   # Exclude sites (comma-separated)
```

Missing a retriever? Feel free to contribute to this project by submitting issues or pull requests on our [GitHub](https://github.com/assafelovic/gpt-researcher) page.



================================================
FILE: docs/docs/gpt-researcher/search-engines/test-your-retriever.md
================================================
# Testing your Retriever

To test your retriever, you can use the following code snippet. The script will search for a sub-query and display the search results.

```python
import asyncio
from dotenv import load_dotenv
from gpt_researcher.config.config import Config
from gpt_researcher.actions.retriever import get_retrievers
from gpt_researcher.skills.researcher import ResearchConductor
import pprint
# Load environment variables from .env file
load_dotenv()

async def test_scrape_data_by_query():
    # Initialize the Config object
    config = Config()

    # Retrieve the retrievers based on the current configuration
    retrievers = get_retrievers({}, config)
    print("Retrievers:", retrievers)

    # Create a mock researcher object with necessary attributes
    class MockResearcher:
        def init(self):
            self.retrievers = retrievers
            self.cfg = config
            self.verbose = True
            self.websocket = None
            self.scraper_manager = None  # Mock or implement scraper manager
            self.vector_store = None  # Mock or implement vector store

    researcher = MockResearcher()
    research_conductor = ResearchConductor(researcher)
    # print('research_conductor',dir(research_conductor))
    # print('MockResearcher',dir(researcher))
    # Define a sub-query to test
    sub_query = "design patterns for autonomous ai agents"

    # Iterate through all retrievers
    for retriever_class in retrievers:
        # Instantiate the retriever with the sub-query
        retriever = retriever_class(sub_query)

        # Perform the search using the current retriever
        search_results = await asyncio.to_thread(
            retriever.search, max_results=10
        )

        print("\033[35mSearch results:\033[0m")
        pprint.pprint(search_results, indent=4, width=80)

if __name__ == "__main__":
    asyncio.run(test_scrape_data_by_query())
```

The output of the search results will include the title, body, and href of each search result. For example:
    
```json
[{   
    "body": "Jun 5, 2024 ... Three AI Design Patterns of Autonomous "
                "Agents. Overview of the Three Patterns. Three notable AI "
                "design patterns for autonomous agents include:.",
    "href": "https://accredianpublication.medium.com/building-smarter-systems-the-role-of-agentic-design-patterns-in-genai-13617492f5df",
    "title": "Building Smarter Systems: The Role of Agentic Design "
                "Patterns in ..."},
    ...]
```


================================================
FILE: docs/docs/reference/sidebar.json
================================================
{
  "items": [],
  "label": "Reference",
  "type": "category"
}


================================================
FILE: docs/docs/reference/config/config.md
================================================
---
sidebar_label: config
title: config.config
---

Configuration class to store the state of bools for different scripts access.

## Config Objects

```python
class Config(metaclass=Singleton)
```

Configuration class to store the state of bools for different scripts access.

#### \_\_init\_\_

```python
def __init__() -> None
```

Initialize the Config class

#### set\_fast\_llm\_model

```python
def set_fast_llm_model(value: str) -> None
```

Set the fast LLM model value.

#### set\_smart\_llm\_model

```python
def set_smart_llm_model(value: str) -> None
```

Set the smart LLM model value.

#### set\_fast\_token\_limit

```python
def set_fast_token_limit(value: int) -> None
```

Set the fast token limit value.

#### set\_smart\_token\_limit

```python
def set_smart_token_limit(value: int) -> None
```

Set the smart token limit value.

#### set\_browse\_chunk\_max\_length

```python
def set_browse_chunk_max_length(value: int) -> None
```

Set the browse_website command chunk max length value.

#### set\_openai\_api\_key

```python
def set_openai_api_key(value: str) -> None
```

Set the OpenAI API key value.

#### set\_debug\_mode

```python
def set_debug_mode(value: bool) -> None
```

Set the debug mode value.

## APIKeyError Objects

```python
class APIKeyError(Exception)
```

Exception raised when an API key is not set in config.py or as an environment variable.

#### check\_openai\_api\_key

```python
def check_openai_api_key(cfg) -> None
```

Check if the OpenAI API key is set in config.py or as an environment variable.

#### check\_tavily\_api\_key

```python
def check_tavily_api_key(cfg) -> None
```

Check if the Tavily Search API key is set in config.py or as an environment variable.

#### check\_google\_api\_key

```python
def check_google_api_key(cfg) -> None
```

Check if the Google API key is set in config.py or as an environment variable.

#### check\_serp\_api\_key

```python
def check_serp_api_key(cfg) -> None
```

Check if the SERP API key is set in config.py or as an environment variable.

#### check\_searx\_url

```python
def check_searx_url(cfg) -> None
```

Check if the Searx URL is set in config.py or as an environment variable.




================================================
FILE: docs/docs/reference/config/singleton.md
================================================
---
sidebar_label: singleton
title: config.singleton
---

The singleton metaclass for ensuring only one instance of a class.

## Singleton Objects

```python
class Singleton(abc.ABCMeta, type)
```

Singleton metaclass for ensuring only one instance of a class.

#### \_\_call\_\_

```python
def __call__(cls, *args, **kwargs)
```

Call method for the singleton metaclass.

## AbstractSingleton Objects

```python
class AbstractSingleton(abc.ABC, metaclass=Singleton)
```

Abstract singleton class for ensuring only one instance of a class.




================================================
FILE: docs/docs/reference/processing/html.md
================================================
---
sidebar_label: html
title: processing.html
---

HTML processing functions

#### extract\_hyperlinks

```python
def extract_hyperlinks(soup: BeautifulSoup,
                       base_url: str) -> list[tuple[str, str]]
```

Extract hyperlinks from a BeautifulSoup object

**Arguments**:

- `soup` _BeautifulSoup_ - The BeautifulSoup object
- `base_url` _str_ - The base URL
  

**Returns**:

  List[Tuple[str, str]]: The extracted hyperlinks

#### format\_hyperlinks

```python
def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]
```

Format hyperlinks to be displayed to the user

**Arguments**:

- `hyperlinks` _List[Tuple[str, str]]_ - The hyperlinks to format
  

**Returns**:

- `List[str]` - The formatted hyperlinks




================================================
FILE: docs/docs/reference/processing/text.md
================================================
---
sidebar_label: text
title: processing.text
---

Text processing functions

#### split\_text

```python
def split_text(text: str,
               max_length: int = 8192) -> Generator[str, None, None]
```

Split text into chunks of a maximum length

**Arguments**:

- `text` _str_ - The text to split
- `max_length` _int, optional_ - The maximum length of each chunk. Defaults to 8192.
  

**Yields**:

- `str` - The next chunk of text
  

**Raises**:

- `ValueError` - If the text is longer than the maximum length

#### summarize\_text

```python
def summarize_text(url: str,
                   text: str,
                   question: str,
                   driver: Optional[WebDriver] = None) -> str
```

Summarize text using the OpenAI API

**Arguments**:

- `url` _str_ - The url of the text
- `text` _str_ - The text to summarize
- `question` _str_ - The question to ask the model
- `driver` _WebDriver_ - The webdriver to use to scroll the page
  

**Returns**:

- `str` - The summary of the text

#### scroll\_to\_percentage

```python
def scroll_to_percentage(driver: WebDriver, ratio: float) -> None
```

Scroll to a percentage of the page

**Arguments**:

- `driver` _WebDriver_ - The webdriver to use
- `ratio` _float_ - The percentage to scroll to
  

**Raises**:

- `ValueError` - If the ratio is not between 0 and 1

#### create\_message

```python
def create_message(chunk: str, question: str) -> Dict[str, str]
```

Create a message for the chat completion

**Arguments**:

- `chunk` _str_ - The chunk of text to summarize
- `question` _str_ - The question to answer
  

**Returns**:

  Dict[str, str]: The message to send to the chat completion

#### write\_to\_file

```python
def write_to_file(filename: str, text: str) -> None
```

Write text to a file

**Arguments**:

- `text` _str_ - The text to write
- `filename` _str_ - The filename to write to




================================================
FILE: docs/npm/Readme.md
================================================
# GPT Researcher

The gpt-researcher npm package is a WebSocket client for interacting with GPT Researcher.

<div align="center" id="top">

<img src="https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3" alt="Logo" width="80">

####

[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)

[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)
![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)
[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)
[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)

[English](README.md) | [ä¸­æ–‡](README-zh_CN.md) | [æ—¥æœ¬èª](README-ja_JP.md) | [í•œêµ­ì–´](README-ko_KR.md)

</div>

# ğŸ” GPT Researcher

**GPT Researcher is an open deep research agent designed for both web and local research on any given task.** 

The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.

**Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.**

## Installation

```bash
npm install gpt-researcher
```

## Usage

### Basic Usage

```javascript
const GPTResearcher = require('gpt-researcher');

const researcher = new GPTResearcher({
  host: 'http://localhost:8000',
  logListener: (data) => console.log('logListener logging data: ',data)
});

researcher.sendMessage({
  query: 'Does providing better context reduce LLM hallucinations?'
});
```


### Log Data Structure

The `logListener` function receives log data with this structure:

```javascript
{
  type: 'logs',
  content: string,    // e.g., 'added_source_url', 'researching', 'scraping_content'
  output: string,     // Human-readable output message
  metadata: any       // Additional data (URLs, counts, etc.)
}
```

Common log content types:

```javascript
'added_source_url': New source URL added
'researching': Research status updates
'scraping_urls': Starting URL scraping
'scraping_content': Content scraping progress
'scraping_images': Image processing updates
'scraping_complete': Scraping completion
'fetching_query_content': Query processing
```

### Parameters

- `task` (required): The research question or task to investigate
- `reportType` (optional): Type of report to generate (default: 'research_report')
- `reportSource` (optional): Source of the report data (default: 'web')
- `tone` (optional): Tone of the report
- `queryDomains` (optional): Array of domain names to filter search results


### Advanced usage

```javascript
const researcher = new GPTResearcher({
  host: 'http://localhost:8000',
  logListener: (data) => console.log('Log:', data)
});

// Advanced usage with all parameters
researcher.sendMessage({
  task: "What are the latest developments in AI?",
  reportType: "research_report",
  reportSource: "web",
  queryDomains: ["techcrunch.com", "wired.com"]
});


================================================
FILE: docs/npm/index.js
================================================
// index.js
const WebSocket = require('ws');

class GPTResearcher {
  constructor(options = {}) {
    this.host = options.host || 'http://localhost:8000';
    this.socket = null;
    this.responseCallbacks = new Map();
    this.logListener = options.logListener;
    this.tone = options.tone || 'Reflective';
  }

  async initializeWebSocket() {
    if (!this.socket) {      
      const protocol = this.host.includes('https') ? 'wss:' : 'ws:';
      const cleanHost = this.host.replace('http://', '').replace('https://', '');
      const ws_uri = `${protocol}//${cleanHost}/ws`;

      this.socket = new WebSocket(ws_uri);

      this.socket.onopen = () => {
        console.log('WebSocket connection established');
      };

      this.socket.onmessage = (event) => {
        const data = JSON.parse(event.data);
        
        // Handle logs with custom listener if provided
        if (this.logListener) {
          this.logListener(data);
        } else {
          console.log('WebSocket data received:', data);
        }

        const callback = this.responseCallbacks.get('current');
        
      };

      this.socket.onclose = () => {
        console.log('WebSocket connection closed');
        this.socket = null;
      };

      this.socket.onerror = (error) => {
        console.error('WebSocket error:', error);
      };
    }
  }

  async sendMessage({
    task,
    useHTTP = false,
    reportType = 'research_report',
    reportSource = 'web', 
    queryDomains = [],
    tone = 'Reflective',
    query,
    moreContext
  }) {
    const data = {
      task: query ? `${query}. Additional context: ${moreContext}` : task,
      report_type: reportType,
      report_source: reportSource,
      headers: {},
      tone: tone,
      query_domains: queryDomains
    };

    if (useHTTP) {
      return this.sendHttpRequest(data);
    }

    return new Promise((resolve, reject) => {
      if (!this.socket || this.socket.readyState !== WebSocket.OPEN) {
        this.initializeWebSocket();
      }


      const payload = "start " + JSON.stringify(data);

      this.responseCallbacks.set('current', {
        onProgress: (progressData) => {
          resolve({ type: 'progress', data: progressData });
        },
        onComplete: (finalData) => {
          resolve({ type: 'complete', data: finalData });
        }
      });

      if (this.socket.readyState === WebSocket.OPEN) {
        this.socket.send(payload);
        console.log('Message sent:', payload);
      } else {
        this.socket.onopen = () => {
          this.socket.send(payload);
          console.log('Message sent after connection:', payload);
        };
      }
    });
  }

  async sendHttpRequest(data) {
    try {
      const response = await axios.post(`${this.host}/report/`, data);
      return { message: 'success', data: response.data };
    } catch (error) {
      console.error('HTTP request error:', error);
      return { message: 'error', error: error.message };
    }
  }

  async getReport(reportId) {
    try {
      const response = await axios.get(`${this.host}/report/${reportId}`);
      return response;
    } catch (error) {
      console.error('HTTP request error:', error);
      return { message: 'error', error: error.message };
    }
  }
}

module.exports = GPTResearcher;


================================================
FILE: docs/npm/package.json
================================================
{
  "name": "gpt-researcher",
  "version": "1.0.27",
  "description": "WebSocket client for GPT Researcher",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [
    "gpt-researcher",
    "websocket",
    "ai",
    "research"
  ],
  "dependencies": {
    "ws": "^8.18.0"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/assafelovic/gpt-researcher.git"
  },
  "author": "GPT Researcher Team",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/assafelovic/gpt-researcher/issues"
  },
  "homepage": "https://github.com/assafelovic/gpt-researcher#readme"
}


================================================
FILE: docs/src/components/HomepageFeatures.js
================================================
import React from 'react';
import clsx from 'clsx';
import { Link } from 'react-router-dom';
import styles from './HomepageFeatures.module.css';

const FeatureList = [
  {
    title: 'GPT Researcher',
    Svg: require('../../static/img/gptr-logo.png').default,
    docLink: './docs/gpt-researcher/getting-started/getting-started',
    description: (
      <>
        GPT Researcher is an open source autonomous agent designed for comprehensive online research on a variety of tasks.
      </>
    ),
  },
  /*{
    title: 'Tavily Search API',
    Svg: require('../../static/img/tavily.png').default,
    docLink: './docs/tavily-api/introduction',
    description: (
      <>
        Tavily Search API is a search engine optimized for LLMs, optimized for a factual, efficient, and persistent search experience
      </>
    ),
  },*/
  {
    title: 'Multi-Agent Assistant',
    Svg: require('../../static/img/multi-agent.png').default,
    docLink: './docs/gpt-researcher/multi_agents/langgraph',
    description: (
      <>
        Learn how a team of AI agents can work together to conduct research on a given topic, from planning to publication.
      </>
    ),
  },
  {
    title: 'Examples and Demos',
    Svg: require('../../static/img/examples.png').default,
    docLink: './docs/examples/examples',
    description: (
      <>
          Check out GPT Researcher in action across multiple frameworks and use cases such as hybrid research and long detailed reports.
      </>
    ),
  },
];

function Feature({Svg, title, description, docLink}) {
  return (
    <div className={clsx('col col--4')}>
      <div className="text--center">
        {/*<Svg className={styles.featureSvg} alt={title} />*/}
        <img src={Svg} alt={title} height="60"/>
      </div>
      <div className="text--center padding-horiz--md">
        <Link to={docLink}>
            <h3>{title}</h3>
        </Link>
        <p>{description}</p>
      </div>
    </div>
  );
}

export default function HomepageFeatures() {
  return (
    <section className={styles.features}>
      <div className="container" style={{marginTop: 30}}>
        <div className="row" style={{justifyContent: 'center'}}>
          {FeatureList.map((props, idx) => (
            <Feature key={idx} {...props} />
          ))}
        </div>
      </div>
    </section>
  );
}



================================================
FILE: docs/src/components/HomepageFeatures.module.css
================================================
/* stylelint-disable docusaurus/copyright-header */

.features {
  display: flex;
  align-items: center;
  padding: 2rem 0;
  width: 100%;
}

.featureSvg {
  height: 120px;
  width: 200px;
}



================================================
FILE: docs/src/css/custom.css
================================================
:root {
  --ifm-font-size-base: 16px;
  --ifm-code-font-size: 90%;

  --ifm-color-primary: #0c4da2;
  --ifm-color-primary-dark: rgb(11, 69, 146);
  --ifm-color-primary-darker: #0a418a;
  --ifm-color-primary-darkest: #083671;
  --ifm-color-primary-light: #0d55b2;
  --ifm-color-primary-lighter: #0e59ba;
  --ifm-color-primary-lightest: #1064d3;

  --ifm-color-emphasis-300: #1064d3;
  --ifm-link-color: #1064d3;
  --ifm-menu-color-active: #1064d3;
}

.docusaurus-highlight-code-line {
background-color: rgba(0, 0, 0, 0.1);
display: block;
margin: 0 calc(-1 * var(--ifm-pre-padding));
padding: 0 var(--ifm-pre-padding);
}
html[data-theme='dark'] .docusaurus-highlight-code-line {
background-color: rgb(0, 0, 0, 0.3);
}

.admonition-content a {
text-decoration: underline;
font-weight: 600;
color: inherit;
}

a {
font-weight: 600;
}

.markdown > p {
    font-size: 16px;
}

.navbar {
    font-size: 16px;
}

li {
font-size: 16px;
}

blockquote {
  /* samsung blue with lots of transparency */
  background-color: #0c4da224;
}
@media (prefers-color-scheme: dark) {
:root {
  --ifm-hero-text-color: white;
}
}
@media (prefers-color-scheme: dark) {
.hero.hero--primary { --ifm-hero-text-color: white;}
}

@media (prefers-color-scheme: dark) {
blockquote {
  --ifm-color-emphasis-300: var(--ifm-color-primary);
  /* border-left: 6px solid var(--ifm-color-emphasis-300); */
}
}
@media (prefers-color-scheme: dark) {
code {
  /* background-color: rgb(41, 45, 62); */
}
}


/* Docusaurus still defaults to their green! */
@media (prefers-color-scheme: dark) {
.react-toggle-thumb {
  border-color: var(--ifm-color-primary) !important;
}
}


.header-github-link:hover {
opacity: 0.6;
}

.header-github-link:before {
content: '';
width: 24px;
height: 24px;
display: flex;
background: url("data:image/svg+xml,%3Csvg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12'/%3E%3C/svg%3E")
  no-repeat;
}

html[data-theme='dark'] .header-github-link:before {
background: url("data:image/svg+xml,%3Csvg viewBox='0 0 24 24' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath fill='white' d='M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12'/%3E%3C/svg%3E")
  no-repeat;
}



================================================
FILE: docs/src/pages/index.js
================================================
import React from 'react';
import clsx from 'clsx';
import Layout from '@theme/Layout';
import Link from '@docusaurus/Link';
import useDocusaurusContext from '@docusaurus/useDocusaurusContext';
import styles from './index.module.css';
import HomepageFeatures from '../components/HomepageFeatures';

function HomepageHeader() {
  const {siteConfig} = useDocusaurusContext();
  return (
    <header className={clsx('hero hero--primary', styles.heroBanner)} style={{backgroundImage: "linear-gradient(to right, #f472b6, #a78bfa, #22d3ee)"}}>
      <div className="container">
        <h1 className="hero__title">{siteConfig.title}</h1>
        <p className="hero__subtitle">{siteConfig.tagline}</p>
        <div className={styles.buttons}>
          <Link
            className="button button--secondary button--lg"
            to="/docs/gpt-researcher/getting-started">
            Getting Started - 5 min â±ï¸
          </Link>
        </div>
      </div>
    </header>
  );
}

export default function Home() {
  const {siteConfig} = useDocusaurusContext();
  return (
    <Layout
      title={`Documentation`}
      description="GPT Researcher is the leading autonomous agent designed for comprehensive online research on a variety of tasks.">
      <HomepageHeader />
      <main>
        <HomepageFeatures />
      </main>
    </Layout>
  );
}



================================================
FILE: docs/src/pages/index.module.css
================================================
/* stylelint-disable docusaurus/copyright-header */

/**
 * CSS files with the .module.css suffix will be treated as CSS modules
 * and scoped locally.
 */

.heroBanner {
  padding: 5rem 0;
  text-align: center;
  position: relative;
  overflow: hidden;
}

@media screen and (max-width: 966px) {
  .heroBanner {
    padding: 2rem;
  }
}

.buttons {
  display: flex;
  align-items: center;
  justify-content: center;
}



================================================
FILE: docs/static/CNAME
================================================
docs.gptr.dev


================================================
FILE: docs/static/.nojekyll
================================================



================================================
FILE: frontend/README.md
================================================
# Frontend Application

This frontend project aims to enhance the user experience of GPT-Researcher, providing an intuitive and efficient interface for automated research. It offers two deployment options to suit different needs and environments.

## Option 1: Static Frontend (FastAPI)

A lightweight solution using FastAPI to serve static files.

#### Prerequisites
- Python 3.11+
- pip

#### Setup and Running

1. Install required packages:
   ```
   pip install -r requirements.txt
   ```

2. Start the server:
   ```
   python -m uvicorn main:app
   ```

3. Access at `http://localhost:8000`

#### Demo
https://github.com/assafelovic/gpt-researcher/assets/13554167/dd6cf08f-b31e-40c6-9907-1915f52a7110

## Option 2: NextJS Frontend

A more robust solution with enhanced features and performance.

#### Prerequisites
- Node.js (v18.17.0 recommended)
- npm

#### Setup and Running

1. Navigate to NextJS directory:
   ```
   cd nextjs
   ```

2. Set up Node.js:
   ```
   nvm install 18.17.0
   nvm use v18.17.0
   ```

3. Install dependencies:
   ```
   npm install --legacy-peer-deps
   ```

4. Start development server:
   ```
   npm run dev
   ```

5. Access at `http://localhost:3000`

Note: Requires backend server on `localhost:8000` as detailed in option 1.

#### Demo
https://github.com/user-attachments/assets/092e9e71-7e27-475d-8c4f-9dddd28934a3

## Choosing an Option

- Static Frontend: Quick setup, lightweight deployment.
- NextJS Frontend: Feature-rich, scalable, better performance and SEO.

For production, NextJS is recommended.

## Frontend Features

Our frontend enhances GPT-Researcher by providing:

1. Intuitive Research Interface: Streamlined input for research queries.
2. Real-time Progress Tracking: Visual feedback on ongoing research tasks.
3. Interactive Results Display: Easy-to-navigate presentation of findings.
4. Customizable Settings: Adjust research parameters to suit specific needs.
5. Responsive Design: Optimal experience across various devices.

These features aim to make the research process more efficient and user-friendly, complementing GPT-Researcher's powerful agent capabilities.


================================================
FILE: frontend/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
    <title>GPT Researcher</title>
    <meta name="description" content="A research assistant powered by GPT-4">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="./static/favicon.ico">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="/site/styles.css" />
    <style>
        .avatar {
            width: 80px;
            height: 80px;
            border-radius: 50%;
        }

        .agent-name {
            text-align: center;
        }

        .agent-item {
            display: flex;
            flex-direction: column;
            align-items: center;
        }

        .agent-choices {
            display: none;
        }

        .btn-show {
            display: none;
        }

        /* Icon button style for inline buttons */
        .icon-button {
            background: none;
            border: none;
            cursor: pointer;
            padding: 5px;
            margin-left: 5px;
            border-radius: 4px;
            transition: background-color 0.2s, color 0.2s;
        }

        .icon-button:hover {
            background-color: rgba(123, 104, 238, 0.1); /* Placeholder, adjust in styles.css */
        }

        .icon-button:active {
        }

        /* Ensure buttons are properly aligned in headings */
        h2 .icon-button, h2 .expand-button {
            vertical-align: middle;
            font-size: 0.8em;
        }
        
        /* New navigation buttons in the top-right corner */
        .nav-buttons {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            gap: 10px;
            z-index: 100;
        }
    </style>
</head>

<body>
    <!-- Navigation Buttons (moved from top bar) -->
    <div class="nav-buttons">
        <div id="websocketPanelOpenBtn" class="top-websocket-button">
            <i class="fas fa-network-wired"></i> Status
        </div>
        <div id="historyPanelOpenBtn" class="top-history-button">
            <i class="fas fa-history"></i> History
        </div>
    </div>

    <!-- WebSocket Status Panel -->
    <div class="websocket-panel" id="websocketPanel">
        <div class="websocket-panel-header">
            <h3><i class="fas fa-plug"></i> Connection Status</h3>
            <div class="websocket-panel-actions">
                <button id="websocketPanelToggle" class="websocket-action-btn" title="Close panel">
                    <i class="fas fa-chevron-left"></i>
                </button>
            </div>
        </div>
        <div class="websocket-status">
            <div class="status-item">
                <span class="status-label">Connection:</span>
                <span class="status-value" id="connectionStatus">Disconnected</span>
                <span class="status-indicator" id="connectionIndicator"></span>
            </div>
            <div class="status-item">
                <span class="status-label">Research:</span>
                <span class="status-value" id="researchStatus">Inactive</span>
            </div>
            <div class="status-item">
                <span class="status-label">Connected for:</span>
                <span class="status-value" id="connectionDuration">-</span>
            </div>
            <div class="status-item">
                <span class="status-label">Last activity:</span>
                <span class="status-value" id="lastActivity">-</span>
            </div>
            <div class="status-item">
                <span class="status-label">ReadyState:</span>
                <span class="status-value" id="readyState">-</span>
            </div>
            <div class="status-divider"></div>
            <div class="status-item">
                <span class="status-label">Connection attempts:</span>
                <span class="status-value" id="connectionAttempts">0</span>
            </div>
            <div class="status-item">
                <span class="status-label">Messages received:</span>
                <span class="status-value" id="messagesReceived">0</span>
            </div>
            <div class="status-divider"></div>
            <div class="status-item">
                <span class="status-label">Current task:</span>
                <span class="status-value" id="currentTask">-</span>
            </div>
        </div>
    </div>

    <section class="landing">
        <div class="max-w-5xl mx-auto text-center">
            <h1 class="text-4xl font-extrabold mx-auto lg:text-7xl">
                Say Goodbye to <br>
                <span
                    style="background-image:linear-gradient(to right, #9867F0, #ED4E50); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">Hours
                    of Research</span>
            </h1>
            <p class="max-w-6xl mx-auto text-gray-600 mt-8" style="font-size:20px">
                Say Hello to <b>GPT Researcher</b>, your AI mate for rapid insights and comprehensive research. <br>
                GPT Researcher takes care of everything from accurate source gathering and organization of research results to generation of customized reports with citations.
            </p>
            <a href="#form" class="btn btn-primary">Start Researching</a>
        </div>
    </section>

    <main class="container" id="form">
        <div class="agent-item"><img src="/static/gptr-logo.png" class="avatar" alt="Auto Agent"></div>
        <form method="POST" class="mt-3" id="researchForm">
            <div class="form-group">
                <label for="task" class="agent-question">What would you like me to research next?</label>
                <textarea id="task" name="task" class="form-control highlight-connection" placeholder="Enter any topic, question, or idea..." required autocomplete="on"></textarea>
                <input type="radio" name="agent" id="autoAgent" value="Auto Agent" checked hidden>
            </div>
            <div class="form-group">
                <div class="row">


                </div>
                <button type="button" id="btnShowAuto" class="btn btn-secondary mt-3 btn-show">Auto Agent</button>
            </div>
            <div class="form-group">
                <label for="report_type" class="agent-question">What type of report would you like me to
                    generate?</label>
                <select name="report_type" id="report_type" class="form-control highlight-connection" required>
                    <option value="research_report">Summary - Short and fast (~2 min)</option>
                    <option value="detailed_report">Detailed - In depth and longer (~5 min)</option>
                    <option value="resource_report">Resource Report</option>
                    <option value="deep">Deep Research</option>
                </select>
            </div>
            <div class="form-group">
                <label for="tone" class="agent-question">In which tone would you like the report to be
                    generated?</label>
                <select name="tone" id="tone" class="form-control highlight-connection" required>
                    <option value="Objective">Objective - Impartial and unbiased presentation of facts and findings
                    </option>
                    <option value="Formal">Formal - Adheres to academic standards with sophisticated language and
                        structure</option>
                    <option value="Analytical">Analytical - Critical evaluation and detailed examination of data and
                        theories</option>
                    <option value="Persuasive">Persuasive - Convincing the audience of a particular viewpoint or
                        argument</option>
                    <option value="Informative">Informative - Providing clear and comprehensive information on a topic
                    </option>
                    <option value="Explanatory">Explanatory - Clarifying complex concepts and processes</option>
                    <option value="Descriptive">Descriptive - Detailed depiction of phenomena, experiments, or case
                        studies</option>
                    <option value="Critical">Critical - Judging the validity and relevance of the research and its
                        conclusions</option>
                    <option value="Comparative">Comparative - Juxtaposing different theories, data, or methods to
                        highlight differences and similarities</option>
                    <option value="Speculative">Speculative - Exploring hypotheses and potential implications or future
                        research directions</option>
                    <option value="Reflective">Reflective - Considering the research process and personal insights or
                        experiences</option>
                    <option value="Narrative">Narrative - Telling a story to illustrate research findings or
                        methodologies</option>
                    <option value="Humorous">Humorous - Light-hearted and engaging, usually to make the content more
                        relatable</option>
                    <option value="Optimistic">Optimistic - Highlighting positive findings and potential benefits
                    </option>
                    <option value="Pessimistic">Pessimistic - Focusing on limitations, challenges, or negative outcomes
                    </option>
                </select>
            </div>
            <div class="form-group">
                <label for="report_source" class="agent-question">What sources would you like me to research
                    from?</label>
                <p class="text-left mt-0 pt-0" style="font-size: 0.7rem;">You can now do research on local documents as
                    well. Please make sure to add the DOC_PATH env variable pointing to your documents folder.</p>
                <select name="report_source" id="report_source" class="form-control highlight-connection" required>
                    <option value="web">The Web</option>
                    <option value="local">My Documents</option>
                    <option value="hybrid">Hybrid</option>
                    <option value="azure">Azure storage</option>
                </select>
            </div>
            <div class="form-group">
                <label for="queryDomains" class="form-label">Query Domains (Optional)</label>
                <input type="text" class="form-control highlight-connection" id="queryDomains" name="query_domains" placeholder="Enter domains separated by commas" autocomplete="on">
                <small class="text-muted">Example: techcrunch.com, forbes.com</small>
            </div>

            <!-- MCP Configuration Section -->
            <div class="form-group">
                <div class="mcp-section">
                    <div class="mcp-header">
                        <label for="mcpEnabled" class="form-label">
                            <input type="checkbox" id="mcpEnabled" name="mcp_enabled" class="mcp-toggle">
                            Enable MCP (Model Context Protocol)
                        </label>
                        <button type="button" id="mcpInfoBtn" class="mcp-info-btn" title="Learn about MCP">
                            <i class="fas fa-info-circle"></i>
                        </button>
                    </div>
                    <small class="text-muted">Connect to external tools and data sources through MCP servers</small>
                    
                    <div id="mcpConfigSection" class="mcp-config-section" style="display: none;">
                        <div class="mcp-presets">
                            <label class="form-label">Quick Presets</label>
                            <div class="preset-buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm preset-btn" data-preset="github">
                                    <i class="fab fa-github"></i> GitHub
                                </button>
                                <button type="button" class="btn btn-outline-secondary btn-sm preset-btn" data-preset="tavily">
                                    <i class="fas fa-search"></i> Tavily Web Search
                                </button>
                                <button type="button" class="btn btn-outline-secondary btn-sm preset-btn" data-preset="filesystem">
                                    <i class="fas fa-folder"></i> Local Files
                                </button>
                            </div>
                            <small class="text-muted">Click a preset to add pre-configured MCP servers to the JSON below</small>
                        </div>

                        <div class="mcp-config-group">
                            <label for="mcpConfig" class="form-label">MCP Servers Configuration</label>
                            <textarea id="mcpConfig" name="mcp_config" class="form-control mcp-config-textarea" rows="8" placeholder="Paste your MCP servers configuration as JSON array...">[]</textarea>
                            <div class="mcp-config-status">
                                <span id="mcpConfigStatus" class="mcp-status-text">Valid JSON âœ“</span>
                                <button type="button" id="mcpFormatBtn" class="btn btn-sm btn-secondary">
                                    <i class="fas fa-code"></i> Format JSON
                                </button>
                            </div>
                            <small class="text-muted">
                                Paste your MCP servers configuration as a JSON array. Each server should have properties like 
                                <code>name</code>, <code>command</code>, <code>args</code>, and optional <code>env</code> variables.
                                <a href="#" id="mcpExampleLink">See example â†’</a>
                            </small>
                        </div>
                    </div>
                </div>
            </div>

            <input type="submit" value="Begin Research" class="btn btn-primary button-padding" id="submitButton">
        </form>

        <!-- Add JSON button above Research Progress section -->
        <div class="margin-div" id="jsonButtonContainer" style="display: none; text-align: right; margin-bottom: 10px;">
            <a id="downloadLinkJsonTop" href="#" class="report-action-btn disabled" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-code"></i> JSON
            </a>
        </div>

        <div class="margin-div research-output-container">
            <!-- Move spinner to the left side of the text -->
            <h2><div id="modernSpinner" class="modern-spinner"></div> Research Progress <button id="expandOutputBtn" class="expand-button" title="Expand"><i class="fas fa-expand-alt"></i></button></h2>
            <p class="mt-2 text-left" style="font-size: 0.8rem;">
                Watch as the AI works to gather information and analyze your topic in real-time.</p>
            <div id="output"></div>
        </div>
        <div class="images_div">
            <div id="selectedImagesContainer" style="display: none;"></div>
        </div>
        <div class="margin-div report-container">
            <h2>Research Report
                <button id="copyToClipboardTop" class="icon-button" title="Copy" style="display: none;">
                    <i class="fas fa-copy"></i>
                </button>
                <button id="expandReportBtn" class="expand-button" title="Expand">
                    <i class="fas fa-expand-alt"></i>
                </button>
            </h2>
            <!-- Add download buttons above the report container -->
            <div class="report-actions" style="display: none;">
                <a id="downloadLinkTop" href="#" class="report-action-btn disabled" target="_blank" rel="noopener noreferrer">
                    <i class="fas fa-file-pdf"></i> PDF
                </a>
                <a id="downloadLinkWordTop" href="#" class="report-action-btn disabled" target="_blank" rel="noopener noreferrer">
                    <i class="fas fa-file-word"></i> Word
                </a>
                <a id="downloadLinkMdTop" href="#" class="report-action-btn disabled" target="_blank" rel="noopener noreferrer">
                    <i class="fas fa-file-lines"></i> Markdown
                </a>
            </div>
            <div id="reportContainer"></div>
            <div id="reportActions" style="display: none;">
                <div class="alert alert-info" role="alert" id="status"></div>
            </div>
        </div>

        <!-- Chat Container -->
        <div class="margin-div chat-container" id="chatContainer" style="display: none;">
            <h2><i class="fas fa-comments"></i> Chat with AI about this research <button id="expandChatBtn" class="expand-button" title="Expand"><i class="fas fa-expand-alt"></i></button></h2>
            <p class="text-muted">Ask questions about the research report to get more insights</p>
            <div id="chatMessages" class="chat-messages"></div>
            <div class="chat-input-container">
                <textarea id="chatInput" class="form-control chat-input" placeholder="Ask a question about this research..." rows="2"></textarea>
                <button id="voiceInputBtn" class="btn btn-secondary" title="Use voice input">
                    <i class="fas fa-microphone"></i>
                </button>
                <button id="sendChatBtn" class="btn btn-primary">
                    <i class="fas fa-paper-plane"></i> Send
                </button>
            </div>
        </div>

        <!-- Fixed bottom bar styled like the top credits bar -->
    </main>

    <!-- Conversation History Panel -->
    <div class="history-panel" id="historyPanel">
        <div class="history-panel-header">
            <h3><i class="fas fa-history"></i> Research History</h3>
            <div class="history-panel-actions">
                <button id="historyPanelToggle" class="history-action-btn" title="Close panel">
                    <i class="fas fa-times"></i>
                </button>
            </div>
        </div>
        <div class="history-panel-search">
            <input type="text" id="historySearch" placeholder="Search research history...">
            <button id="historySearchBtn" class="history-action-btn" title="Search">
                <i class="fas fa-search"></i>
            </button>
        </div>
        <div class="history-panel-filters">
            <select id="historySortOrder">
                <option value="newest">Newest First</option>
                <option value="oldest">Oldest First</option>
            </select>
            <!-- JS will add Import/Export/Debug buttons here -->
            <button id="historyClearBtn" class="history-action-btn" title="Clear all history">
                <i class="fas fa-trash-alt"></i>
            </button>
        </div>
        <div class="history-panel-entries" id="historyEntries">
            <!-- Entries will be populated dynamically -->
        </div>
    </div>
    
    <!-- Sticky Downloads Bar -->
    <div class="sticky-downloads-bar" id="stickyDownloadsBar" style="display: none;"> <!-- Initially hidden -->
        <div class="download-buttons-container">
            <a id="copyToClipboard" class="download-option-btn disabled">
                <i class="fas fa-copy"></i> Copy (Markdown)
            </a>
            <a id="downloadLinkMd" href="#" class="download-option-btn disabled" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-lines"></i> Markdown
            </a>
            <a id="downloadLink" href="#" class="download-option-btn disabled" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-pdf"></i> PDF
            </a>
            <a id="downloadLinkWord" href="#" class="download-option-btn disabled" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-word"></i> Word
            </a>
            <a id="downloadLinkJson" href="#" class="download-option-btn disabled" target="_blank" rel="noopener noreferrer">
                <i class="fas fa-file-code"></i> Log (JSON)
            </a>
        </div>
    </div>

    <footer>
        <p>
            <a target="_blank" href="https://gptr.dev">Homepage</a> |
            <a target="_blank" href="https://github.com/assafelovic/gpt-researcher">GitHub</a> |
            <a target="_blank" href="https://discord.gg/spBgZmm3Xe">Discord</a>
        </p>
        <p>GPT Researcher &copy; 2024</p>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.1/showdown.min.js"></script>
    <script src="/site/scripts.js"></script>
    <script>
        // Auto-resize textarea as content grows
        const taskTextarea = document.getElementById('task');
        if (taskTextarea) {
            // Set initial height
            taskTextarea.setAttribute('style', 'height: 38px; overflow-y: hidden;');

            // Function to resize textarea based on content
            const resizeTextarea = () => {
                taskTextarea.style.height = 'auto';
                taskTextarea.style.height = taskTextarea.scrollHeight + 'px';
            };

            // Add event listeners for input and focus
            taskTextarea.addEventListener('input', resizeTextarea);
            taskTextarea.addEventListener('focus', resizeTextarea);
        }

        // Ensure feature panels are positioned correctly on window resize
        window.addEventListener('resize', function () {
            // Adjust the feature panel width based on screen size
            const viewportWidth = window.innerWidth;
            const featurePanel = document.querySelector('.feature-panel');

            if (featurePanel) {
                if (viewportWidth < 1400) {
                    featurePanel.style.display = 'none';
                } else {
                    featurePanel.style.display = 'block';
                    // Adjust width based on screen size
                    const panelWidth = Math.min(280, Math.max(200, viewportWidth * 0.15));
                    featurePanel.style.width = `${panelWidth}px`;
                }
            }
        });
    </script>
</body>

</html>



================================================
FILE: frontend/pdf_styles.css
================================================
body {
    font-family: 'Libre Baskerville', serif;
    font-size: 12pt; /* standard size for academic papers */
    line-height: 1.6; /* for readability */
    color: #333; /* softer on the eyes than black */
    background-color: #fff; /* white background */
    margin: 0;
    padding: 0;
}

h1, h2, h3, h4, h5, h6 {
    font-family: 'Libre Baskerville', serif;
    color: #000; /* darker than the body text */
    margin-top: 1em; /* space above headers */
}

h1 {
    font-size: 2em; /* make h1 twice the size of the body text */
}

h2 {
    font-size: 1.5em;
}

/* Add some space between paragraphs */
p {
    margin-bottom: 1em;
}

/* Style for blockquotes, often used in academic papers */
blockquote {
    font-style: italic;
    margin: 1em 0;
    padding: 1em;
    background-color: #f9f9f9; /* a light grey background */
}

/* You might want to style tables, figures, etc. too */
table {
    border-collapse: collapse;
    width: 100%;
}

table, th, td {
    border: 1px solid #ddd;
    text-align: left;
    padding: 8px;
}

th {
    background-color: #f2f2f2;
    color: black;
}


================================================
FILE: frontend/scripts.js
================================================
const GPTResearcher = (() => {
  let isResearchActive = false;
  let connectionTimeout = null;
  let conversationHistory = [];
  let isInitialLoad = true; // Flag to track initial page load
  let cookiesEnabled = true; // Flag to track if cookies are enabled
  let allReports = ''; // Store all reports cumulatively
  let currentReport = ''; // Store the current report (will be overwritten)
  let isFirstReport = true; // Flag to track if this is the first report
  let chatContainer = null; // Global reference to chat container
  let lastRequestData = null; // Store the last request data for reconnection

  // Add WebSocket monitoring variables
  let socket = null;
  let connectionStartTime = null;
  let lastActivityTime = null;
  let connectionAttempts = 0;
  let messagesReceived = 0;
  let websocketMonitorInterval = null;
  let dispose_socket = null; // Re-add dispose_socket
  let reconnectAttempts = 0;
  let maxReconnectAttempts = 5;
  let reconnectInterval = 2000; // Start with 2 seconds

  const init = () => {
    // Check if cookies are enabled
    checkCookiesEnabled();

    // Load history immediately on page load
    loadConversationHistory();

    // After a short delay, mark initial load as complete
    setTimeout(() => {
      isInitialLoad = false;
    }, 1000);

    // Setup form submission
    document.getElementById('researchForm').addEventListener('submit', (e) => {
      e.preventDefault();
      startResearch();
      return false;
    });

    document
      .getElementById('copyToClipboard')
      .addEventListener('click', copyToClipboard)

    // Add event listener for the top copy button
    const topCopyButton = document.getElementById('copyToClipboardTop');
    if (topCopyButton) {
      topCopyButton.addEventListener('click', copyToClipboard);
    }

    // Initialize expand buttons
    initExpandButtons();

    // Initialize history panel functionality
    initHistoryPanel();

    // Initialize WebSocket monitoring panel
    initWebSocketPanel();

    // Initialize MCP functionality
    initMCPSection();

    // The download bar is now fixed in place with CSS
    // No need to set display property here

    updateState('initial');

    // Initialize research icon to not spinning
    updateResearchIcon(false);

    // Hide loading overlay if it exists
    const loadingOverlay = document.getElementById('loadingOverlay');
    if (loadingOverlay) {
      loadingOverlay.classList.add('loading-hidden');
    }
  }

  // Check if cookies are enabled
  const checkCookiesEnabled = () => {
    try {
      // Try to set a test cookie
      document.cookie = "testcookie=1; path=/";
      const cookieEnabled = document.cookie.indexOf("testcookie") !== -1;

      if (!cookieEnabled) {
        console.warn("Cookies are disabled in this browser");
        cookiesEnabled = false;
        showToast("Cookies are disabled. History will use localStorage instead.", 5000);
      } else {
        // Clean up test cookie
        document.cookie = "testcookie=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;";
        cookiesEnabled = true;
      }

      return cookieEnabled;
    } catch (e) {
      console.error("Error checking cookies:", e);
      cookiesEnabled = false;
      return false;
    }
  }

  // Initialize conversation history panel functionality
  const initHistoryPanel = () => {
    // Load history from cookie
    loadConversationHistory();

    // Setup history panel toggle button
    const historyPanelOpenBtn = document.getElementById('historyPanelOpenBtn');
    const historyPanel = document.getElementById('historyPanel');
    const historyPanelToggle = document.getElementById('historyPanelToggle');

    if (historyPanelOpenBtn) {
      historyPanelOpenBtn.addEventListener('click', () => {
        loadConversationHistory(); // Reload history when opening panel
        historyPanel.classList.add('open');
      });
    }

    if (historyPanelToggle) {
      historyPanelToggle.addEventListener('click', () => {
        historyPanel.classList.remove('open');
      });
    }

    // Close panel when clicking outside
    document.addEventListener('click', (e) => {
      // If the panel is open and the click is outside the panel and not on the toggle button
      if (historyPanel.classList.contains('open') &&
        !historyPanel.contains(e.target) &&
        e.target !== historyPanelOpenBtn &&
        !historyPanelOpenBtn.contains(e.target)) {
        historyPanel.classList.remove('open');
      }
    });

    // Setup search functionality
    const historySearch = document.getElementById('historySearch');
    const historySearchBtn = document.getElementById('historySearchBtn');

    if (historySearch && historySearchBtn) {
      historySearch.addEventListener('input', filterHistoryEntries);
      historySearchBtn.addEventListener('click', () => filterHistoryEntries());
    }

    // Setup sort functionality
    const historySortOrder = document.getElementById('historySortOrder');
    if (historySortOrder) {
      historySortOrder.addEventListener('change', () => {
        sortHistoryEntries(historySortOrder.value);
        renderHistoryEntries();
      });
    }

    // Setup clear history button
    const historyClearBtn = document.getElementById('historyClearBtn');
    if (historyClearBtn) {
      historyClearBtn.addEventListener('click', clearConversationHistory);
    }

    // Add action buttons to history panel
    const historyFilters = document.querySelector('.history-panel-filters');
    if (historyFilters) {
      // Create a container for the buttons
      const actionsContainer = document.createElement('div');
      actionsContainer.className = 'history-actions-container';

      // Add export history button with enhanced styling and tooltip
      const exportBtn = document.createElement('button');
      exportBtn.className = 'history-action-btn';
      exportBtn.title = 'Export research history to file';
      exportBtn.innerHTML = '<i class="fas fa-file-export"></i>';
      exportBtn.addEventListener('click', exportHistory);

      // Add import history button with enhanced styling and tooltip
      const importBtn = document.createElement('button');
      importBtn.className = 'history-action-btn';
      importBtn.title = 'Import research history from file';
      importBtn.innerHTML = '<i class="fas fa-file-import"></i>';
      importBtn.addEventListener('click', triggerImportHistory);

      // Add cookie debug button with enhanced styling and tooltip
      const debugBtn = document.createElement('button');
      debugBtn.className = 'history-action-btn';
      debugBtn.title = 'Check storage status';
      debugBtn.innerHTML = '<i class="fas fa-database"></i>';
      debugBtn.addEventListener('click', checkCookieStatus);

      // Add buttons to container in a logical order
      actionsContainer.appendChild(importBtn);
      actionsContainer.appendChild(exportBtn);
      actionsContainer.appendChild(debugBtn);

      // Create a hidden file input for importing
      const fileInput = document.createElement('input');
      fileInput.type = 'file';
      fileInput.id = 'historyFileInput';
      fileInput.accept = '.json';
      fileInput.style.display = 'none';
      fileInput.addEventListener('change', handleFileImport);

      // Add container and file input to filters
      historyFilters.appendChild(actionsContainer);
      historyFilters.appendChild(fileInput);
    }

    // Initial render of history entries
    renderHistoryEntries();
  }

  // Initialize WebSocket monitoring panel
  const initWebSocketPanel = () => {
    const websocketPanel = document.getElementById('websocketPanel');
    const websocketPanelOpenBtn = document.getElementById('websocketPanelOpenBtn');
    const websocketPanelToggle = document.getElementById('websocketPanelToggle');

    if (!websocketPanel || !websocketPanelOpenBtn || !websocketPanelToggle) {
      console.error("WebSocket panel elements not found");
      return;
    }

    console.log("Initializing WebSocket panel");

    // Ensure it starts hidden
    websocketPanel.classList.remove('open');

    websocketPanelOpenBtn.addEventListener('click', (e) => {
      e.preventDefault();
      e.stopPropagation();
      console.log("Opening WebSocket panel");
      websocketPanel.classList.add('open');
    });

    websocketPanelToggle.addEventListener('click', (e) => {
      e.preventDefault();
      e.stopPropagation();
      console.log("Closing WebSocket panel");
      websocketPanel.classList.remove('open');
    });

    // Close panel when clicking outside
    document.addEventListener('click', (e) => {
      // If the panel is open and the click is outside the panel and not on the toggle button
      if (websocketPanel.classList.contains('open') &&
        !websocketPanel.contains(e.target) &&
        e.target !== websocketPanelOpenBtn &&
        !websocketPanelOpenBtn.contains(e.target)) {
        websocketPanel.classList.remove('open');
      }
    });

    // Start periodic WebSocket status updates
    startWebSocketMonitoring();
  }

  // Start WebSocket monitoring
  const startWebSocketMonitoring = () => {
    console.log("Starting WebSocket monitoring");

    // Update status immediately
    updateWebSocketStatus();

    // Clear any existing interval
    if (websocketMonitorInterval) {
      clearInterval(websocketMonitorInterval);
    }

    // Update status every 2 seconds
    websocketMonitorInterval = setInterval(updateWebSocketStatus, 2000);
  }

  // Update WebSocket status in the panel
  const updateWebSocketStatus = () => {
    // Only proceed if the necessary elements exist
    const connectionStatusEl = document.getElementById('connectionStatus');
    const connectionIndicatorEl = document.getElementById('connectionIndicator');
    const researchStatusEl = document.getElementById('researchStatus');
    const connectionDurationEl = document.getElementById('connectionDuration');
    const lastActivityEl = document.getElementById('lastActivity');
    const readyStateEl = document.getElementById('readyState');
    const connectionAttemptsEl = document.getElementById('connectionAttempts');
    const messagesReceivedEl = document.getElementById('messagesReceived');
    const currentTaskEl = document.getElementById('currentTask');

    if (!connectionStatusEl || !connectionIndicatorEl) return;

    // Update connection status
    const socketStatus = getSocketStatus();
    connectionStatusEl.textContent = socketStatus.statusText;

    // Update indicator class
    connectionIndicatorEl.className = 'status-indicator';
    connectionIndicatorEl.classList.add(socketStatus.indicatorClass);

    // Update research status
    if (researchStatusEl) {
      researchStatusEl.textContent = isResearchActive ? 'Active' : 'Inactive';
    }

    // Update connection duration
    if (connectionDurationEl && connectionStartTime) {
      const duration = Math.floor((Date.now() - connectionStartTime) / 1000);
      connectionDurationEl.textContent = formatDuration(duration);
    } else if (connectionDurationEl) {
      connectionDurationEl.textContent = '-';
    }

    // Update last activity
    if (lastActivityEl && lastActivityTime) {
      const elapsed = Math.floor((Date.now() - lastActivityTime) / 1000);
      lastActivityEl.textContent = elapsed < 60 ? `${elapsed} sec ago` : formatDuration(elapsed) + ' ago';
    } else if (lastActivityEl) {
      lastActivityEl.textContent = '-';
    }

    // Update ReadyState
    if (readyStateEl && socket) {
      readyStateEl.textContent = getReadyStateText(socket.readyState);
    } else if (readyStateEl) {
      readyStateEl.textContent = '-';
    }

    // Update connection attempts
    if (connectionAttemptsEl) {
      connectionAttemptsEl.textContent = connectionAttempts.toString();
    }

    // Update messages received
    if (messagesReceivedEl) {
      messagesReceivedEl.textContent = messagesReceived.toString();
    }

    // Update current task
    if (currentTaskEl) {
      const taskInput = document.getElementById('task');
      currentTaskEl.textContent = isResearchActive && taskInput && taskInput.value ?
        (taskInput.value.length > 30 ? taskInput.value.substring(0, 27) + '...' : taskInput.value) :
        '-';
    }
  }

  // Get socket status object
  const getSocketStatus = () => {
    if (!socket) {
      return {
        statusText: 'Disconnected',
        indicatorClass: 'disconnected'
      };
    }

    switch (socket.readyState) {
      case WebSocket.CONNECTING:
        return {
          statusText: 'Connecting',
          indicatorClass: 'connecting'
        };
      case WebSocket.OPEN:
        return {
          statusText: 'Connected',
          indicatorClass: 'connected'
        };
      case WebSocket.CLOSING:
        return {
          statusText: 'Closing',
          indicatorClass: 'connecting'
        };
      case WebSocket.CLOSED:
      default:
        return {
          statusText: 'Disconnected',
          indicatorClass: 'disconnected'
        };
    }
  }

  // Get readable text for WebSocket readyState
  const getReadyStateText = (readyState) => {
    switch (readyState) {
      case WebSocket.CONNECTING:
        return '0 (Connecting)';
      case WebSocket.OPEN:
        return '1 (Open)';
      case WebSocket.CLOSING:
        return '2 (Closing)';
      case WebSocket.CLOSED:
        return '3 (Closed)';
      default:
        return `${readyState} (Unknown)`;
    }
  }

  // Format duration in seconds to human-readable string
  const formatDuration = (seconds) => {
    if (seconds < 60) {
      return `${seconds} sec`;
    } else if (seconds < 3600) {
      return `${Math.floor(seconds / 60)} min ${seconds % 60} sec`;
    } else {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      return `${hours} hr ${minutes} min`;
    }
  }

  // Load conversation history from cookie
  const loadConversationHistory = () => {
    try {
      const storedHistory = getCookie('conversationHistory');
      if (storedHistory && storedHistory.trim() !== '') {
        try {
          const parsedHistory = JSON.parse(storedHistory);
          if (Array.isArray(parsedHistory)) {
            conversationHistory = parsedHistory;
            console.debug('Loaded research history from storage:', conversationHistory);
            console.log('Loaded research history:', conversationHistory.length, 'items');
          } else {
            console.warn('History storage does not contain an array');
            conversationHistory = [];
            deleteCookie('conversationHistory');
          }
        } catch (jsonError) {
          console.error('Invalid JSON in history storage:', jsonError);
          conversationHistory = [];
          deleteCookie('conversationHistory');
        }
      } else {
        console.log('No research history found in storage');
        conversationHistory = [];
      }
    } catch (error) {
      console.error('Error loading research history from storage:', error);
      conversationHistory = [];
      // If JSON parsing fails, delete the corrupt cookie
      deleteCookie('conversationHistory');
    }

    // Force render after loading
    renderHistoryEntries();
  }

  // Save conversation history to cookie
  const saveConversationHistory = () => {
    try {
      if (conversationHistory.length === 0) {
        deleteCookie('conversationHistory');
        console.debug('No history to save, deleted storage');
        return;
      }

      // Only keep the last 20 entries
      let storageHistory = [...conversationHistory];
      if (storageHistory.length > 20) {
        storageHistory = storageHistory.slice(0, 20);
        console.debug('Trimmed history to last 20 entries');
      }

      // Only keep minimal fields: prompt, links and timestamp
      storageHistory = storageHistory.map(entry => ({
        prompt: entry.prompt || '',
        links: entry.links || {},
        timestamp: entry.timestamp || new Date().toISOString()
      }));

      const jsonString = JSON.stringify(storageHistory);
      console.debug('History JSON size:', jsonString.length, 'characters');

      setCookie('conversationHistory', jsonString, 30);

      if (storageHistory.length > 0 && !isInitialLoad) {
        showToast('Research history saved!');
      }
    } catch (error) {
      console.error('Error saving research history:', error);
      showToast('Error saving history. Some entries may not be saved.');
    }
  }

  // Delete a history entry
  const deleteHistoryEntry = (index) => {
    if (confirm('Are you sure you want to delete this research entry?')) {
      conversationHistory.splice(index, 1);
      saveConversationHistory();
      renderHistoryEntries();
      showToast('Entry deleted successfully');
    }
  }

  // Clear all conversation history
  const clearConversationHistory = () => {
    if (confirm('Are you sure you want to clear all research history? This cannot be undone.')) {
      conversationHistory = [];
      saveConversationHistory();
      renderHistoryEntries();
      showToast('Research history cleared successfully');
    }
  }

  // Filter history entries based on search term
  const filterHistoryEntries = () => {
    const searchTerm = document.getElementById('historySearch').value.toLowerCase();
    const historyEntries = document.getElementById('historyEntries');

    if (!historyEntries) return;

    const entries = historyEntries.querySelectorAll('.history-entry');

    entries.forEach(entry => {
      const title = entry.querySelector('.history-entry-title').textContent.toLowerCase();
      // Search only in the title since we no longer have preview text
      if (title.includes(searchTerm)) {
        entry.style.display = 'block';
      } else {
        entry.style.display = 'none';
      }
    });
  }

  // Sort history entries by timestamp
  const sortHistoryEntries = (order) => {
    conversationHistory.sort((a, b) => {
      // Default to newest first if timestamps don't exist
      if (!a.timestamp || !b.timestamp) return 0;

      if (order === 'newest') {
        return new Date(b.timestamp) - new Date(a.timestamp);
      } else {
        return new Date(a.timestamp) - new Date(b.timestamp);
      }
    });
  }

  // Render history entries in the panel
  const renderHistoryEntries = () => {
    const historyEntries = document.getElementById('historyEntries');
    if (!historyEntries) return;

    historyEntries.innerHTML = '';

    if (!conversationHistory || conversationHistory.length === 0) {
      historyEntries.innerHTML = '<p class="text-center mt-4 text-muted">No research history yet.</p>';
      return;
    }

    // Sort by the current selection
    const sortOrder = document.getElementById('historySortOrder')?.value || 'newest';
    sortHistoryEntries(sortOrder);
    console.debug('Sorted history entries:', sortOrder);

    conversationHistory.forEach((entry, index) => {
      const entryElement = document.createElement('div');
      entryElement.className = 'history-entry';
      entryElement.setAttribute('data-id', index);

      // Make the entire entry clickable to load it
      entryElement.addEventListener('click', () => {
        loadResearchEntry(index);
      });

      // Format timestamp if available
      let timestampHTML = '';
      if (entry.timestamp) {
        try {
          const timestamp = new Date(entry.timestamp);
          const formattedDate = timestamp.toLocaleDateString();
          const formattedTime = timestamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
          timestampHTML = `<span class="history-entry-timestamp">${formattedDate} ${formattedTime}</span>`;
        } catch (e) {
          console.error('Error formatting timestamp:', e);
        }
      }

      // Make sure links object exists
      const links = entry.links || {};

      // Build the HTML for the entry with enhanced formatting
      entryElement.innerHTML = `
        <div class="history-entry-header">
          <h4 class="history-entry-title">${entry.prompt || 'Unnamed Research'}</h4>
          ${timestampHTML}
        </div>
        <div class="history-entry-format">
          ${links.pdf ? `<a href="${links.pdf}" class="history-entry-action" target="_blank" title="Open PDF Report"><i class="fas fa-file-pdf"></i> PDF</a>` : ''}
          ${links.docx ? `<a href="${links.docx}" class="history-entry-action" target="_blank" title="Open Word Document"><i class="fas fa-file-word"></i> Word</a>` : ''}
          ${links.md ? `<a href="${links.md}" class="history-entry-action" target="_blank" title="Open Markdown File"><i class="fas fa-file-lines"></i> MD</a>` : ''}
          ${links.json ? `<a href="${links.json}" class="history-entry-action" target="_blank" title="Open JSON Data"><i class="fas fa-file-code"></i> JSON</a>` : ''}
        </div>
        <div class="history-entry-actions">
          <button class="history-entry-action delete-entry" title="Delete this research entry"><i class="fas fa-trash-alt"></i></button>
        </div>
      `;

      // Add action button handlers
      const deleteBtn = entryElement.querySelector('.delete-entry');
      if (deleteBtn) {
        deleteBtn.addEventListener('click', (e) => {
          e.stopPropagation();
          deleteHistoryEntry(index);
        });
      }

      historyEntries.appendChild(entryElement);
      setTimeout(() => {
        entryElement.style.animationDelay = `${index * 50}ms`;
      }, 0);
    });
  }

  // Load a research entry from history
  const loadResearchEntry = (index) => {
    const entry = conversationHistory[index];
    if (!entry) return;

    // Fill form with the entry data
    document.getElementById('task').value = entry.prompt; // Changed from entry.task for consistency
    
    // Check if report_type, report_source, and tone are in entry, otherwise use defaults or skip
    const reportTypeSelect = document.querySelector('select[name="report_type"]');
    if (reportTypeSelect && entry.reportType) {
        reportTypeSelect.value = entry.reportType;
    } else if (reportTypeSelect) {
        reportTypeSelect.value = reportTypeSelect.options[0].value; // Default to first option
    }

    const reportSourceSelect = document.querySelector('select[name="report_source"]');
    if (reportSourceSelect && entry.reportSource) {
        reportSourceSelect.value = entry.reportSource;
    } else if (reportSourceSelect) {
        reportSourceSelect.value = reportSourceSelect.options[0].value; // Default to first option
    }

    const toneSelect = document.querySelector('select[name="tone"]');
    if (toneSelect && entry.tone) {
        toneSelect.value = entry.tone;
    } else if (toneSelect) {
        toneSelect.value = toneSelect.options[0].value; // Default to first option
    }

    const queryDomainsInput = document.querySelector('input[name="query_domains"]');
    if (queryDomainsInput) {
        if (entry.queryDomains && Array.isArray(entry.queryDomains) && entry.queryDomains.length > 0) {
            queryDomainsInput.value = entry.queryDomains.join(', ');
        } else {
            queryDomainsInput.value = ''; // Clear if not present
        }
    }

    // Clear current research/report areas
    document.getElementById('output').innerHTML = '';
    document.getElementById('reportContainer').innerHTML = '';
    document.getElementById('selectedImagesContainer').innerHTML = '';
    document.getElementById('selectedImagesContainer').style.display = 'none';

    // Hide download bar and chat
    const stickyDownloadsBar = document.getElementById('stickyDownloadsBar');
    if (stickyDownloadsBar) {
        stickyDownloadsBar.classList.remove('visible');
    }
    const chatContainer = document.getElementById('chatContainer');
    if (chatContainer) {
        chatContainer.style.display = 'none';
    }

    // Reset UI state and report-specific buttons
    updateState('initial'); // This will hide copy buttons etc.

    // Close the history panel
    const historyPanel = document.getElementById('historyPanel');
    if (historyPanel) {
        historyPanel.classList.remove('open');
    }

    // Scroll to the form
    const formElement = document.getElementById('form');
    if (formElement) {
        formElement.scrollIntoView({ behavior: 'smooth' });
    }

    // Inform user
    showToast('Research parameters loaded. You can start the research again.');
  }

  // Copy entry content to clipboard
  const copyEntryToClipboard = (index) => {
    const entry = conversationHistory[index];
    if (!entry || !entry.content) return;

    const textarea = document.createElement('textarea');
    textarea.value = entry.content;
    document.body.appendChild(textarea);
    textarea.select();
    document.execCommand('copy');
    document.body.removeChild(textarea);

    // Show a toast notification
    showToast('Research content copied to clipboard!');
  }

  // Show a toast notification
  const showToast = (message, duration = 3000) => {
    // Create toast element if it doesn't exist
    let toast = document.getElementById('toast-notification');
    if (!toast) {
      toast = document.createElement('div');
      toast.id = 'toast-notification';
      toast.className = 'toast-notification';
      document.body.appendChild(toast);
    }

    // Set message and show
    toast.textContent = message;
    toast.classList.add('show');

    // Hide after specified duration
    setTimeout(() => {
      toast.classList.remove('show');
    }, duration);
  }

  // Save current research to history (minimal: prompt and links only)
  const saveToHistory = (report, downloadLinks) => {
    if (!downloadLinks) {
      console.error('No download links provided');
      showToast('Error: Could not save research to history');
      return;
    }

    const prompt = document.getElementById('task').value;

    // Create links object with proper structure
    const links = {
      pdf: downloadLinks.pdf || '',
      docx: downloadLinks.docx || '',
      md: downloadLinks.md || '',
      json: downloadLinks.json || ''
    };

    console.debug('Saving history with links:', links);

    // Create history entry with timestamp
    const historyEntry = {
      prompt,
      links,
      timestamp: new Date().toISOString()
    };

    // Add to beginning of array if it's not empty
    if (!conversationHistory) {
      conversationHistory = [];
    }

    conversationHistory.unshift(historyEntry);
    saveConversationHistory();
    renderHistoryEntries();
    document.getElementById('historyPanel').classList.add('open');

    // Prompt user about storage method
    if (cookiesEnabled) {
      showToast('Research saved! Your history is stored in a browser cookie.');
    } else {
      showToast('Research saved! Your history is stored using localStorage.');
    }
  }

  // Function to update the research icon spinning state
  const updateResearchIcon = (isSpinning) => {
    const modernSpinner = document.getElementById('modernSpinner');
    if (modernSpinner) {
      if (isSpinning) {
        modernSpinner.classList.add('spinning');
      } else {
        modernSpinner.classList.remove('spinning');
      }
    }
  };

  const startResearch = () => {
    document.getElementById('output').innerHTML = ''
    document.getElementById('reportContainer').innerHTML = ''
    dispose_socket?.() // Call previous dispose function if it exists

    // Reset report variables
    allReports = '';
    currentReport = '';
    isFirstReport = true;

    // Hide the download bar
    const stickyDownloadsBar = document.getElementById('stickyDownloadsBar');
    if (stickyDownloadsBar) {
      stickyDownloadsBar.classList.remove('visible');
    }

    // Hide the chat container
    chatContainer = document.getElementById('chatContainer');
    if (chatContainer) {
      chatContainer.style.display = 'none';
    }

    const imageContainer = document.getElementById('selectedImagesContainer')
    imageContainer.innerHTML = ''
    imageContainer.style.display = 'none'

    updateState('in_progress')

    addAgentResponse({
      output: 'ğŸ§™â€â™‚ï¸ Gathering information and analyzing your research topic...',
    })

    // Scroll to the "Research Progress" section
    const researchOutputContainer = document.querySelector('.research-output-container');
    if (researchOutputContainer) {
        researchOutputContainer.scrollIntoView({
            behavior: 'smooth',
            block: 'start'
        });
    }

    dispose_socket = listenToSockEvents() // Assign the new dispose function
  }

  const listenToSockEvents = () => {
    const { protocol, host, pathname } = window.location
    const ws_uri = `${protocol === 'https:' ? 'wss:' : 'ws:'
      }//${host}${pathname}ws`

    // Set a timeout for connection - if it takes too long, stop the spinner
    connectionTimeout = setTimeout(() => {
      updateResearchIcon(false);
      console.log("WebSocket connection timed out");
    }, 10000); // 10 seconds timeout

    // Configure Showdown converter to properly handle code blocks
    const converter = new showdown.Converter({
      ghCodeBlocks: true,         // GitHub style code blocks
      tables: true,               // Enable tables
      tasklists: true,            // Enable task lists
      smartIndentationFix: true,  // Fix weird indentation
      simpleLineBreaks: true,     // Treat newlines as <br>
      openLinksInNewWindow: true, // Open links in new tab
      parseImgDimensions: true    // Parse image dimensions from markdown
    });

    // Fix issues with code block formatting
    converter.setOption('literalMidWordUnderscores', true);

    // Increment connection attempts counter
    connectionAttempts++;

    // Update WebSocket status
    updateWebSocketStatus();

    socket = new WebSocket(ws_uri)
    let reportContent = ''; // Store the report content for history
    let downloadLinkData = null; // Store download links

    socket.onmessage = (event) => {
      // Reset reconnect attempts on successful message
      reconnectAttempts = 0;

      const data = JSON.parse(event.data)
      console.log("Received message:", data);  // Debug log

      // Update WebSocket metrics
      messagesReceived++;
      lastActivityTime = Date.now();
      updateWebSocketStatus();

      if (data.type === 'logs') {
        addAgentResponse(data)
      } else if (data.type === 'images') {
        console.log("Received images:", data);  // Debug log
        displaySelectedImages(data)
      } else if (data.type === 'report') {
        // Add to reportContent for history
        reportContent += data.output;

        // Get the current report_type
        const report_type = document.querySelector('select[name="report_type"]').value;

        // Determine if we're using detailed_report
        const isDetailedReport = report_type === 'detailed_report';

        if (isDetailedReport) {
          allReports += data.output; // Accumulate raw markdown
          // Always render the HTML of *all accumulated markdown* for detailed reports during streaming.
          // writeReport will replace the container's content.
          writeReport({ output: allReports, type: 'report' }, converter, false, false);
        } else {
          // For all other report types, append HTML of current chunk to the container.
          writeReport({ output: data.output, type: 'report' }, converter, false, true); // append = true
        }
      } else if (data.type === 'path') {
        updateState('finished')
        downloadLinkData = updateDownloadLink(data)
        isResearchActive = false;

        // Get the current report_type
        const report_type = document.querySelector('select[name="report_type"]').value;

        // Only for detailed_report, show the complete accumulated report at the end
        if (report_type === 'detailed_report' && allReports) {
          const finalData = { output: allReports, type: 'report' };
          writeReport(finalData, converter, true, false); // isFinal=true, append=false
        }

        // Save to history now that research is complete
        if (reportContent && downloadLinkData) {
          saveToHistory(reportContent, downloadLinkData);

          // Reset variables for next research session
          reportContent = '';
          allReports = '';
          currentReport = '';
          isFirstReport = true;
        }

        // Update WebSocket status
        updateWebSocketStatus();
      } else if (data.type === 'chat') {
        // Handle chat messages from the AI
        // Remove loading indicator and add AI's response
        const loadingElements = document.querySelectorAll('.chat-loading');
        if (loadingElements.length > 0) {
          loadingElements[loadingElements.length - 1].remove();
        }

        // Add AI message to chat
        if (data.content) {
          addChatMessage(data.content, false);
        }
      }
    }

    socket.onopen = (event) => {
      // Clear the connection timeout
      clearTimeout(connectionTimeout);

      // Update WebSocket metrics
      connectionStartTime = Date.now();
      lastActivityTime = Date.now();
      updateWebSocketStatus();

      // Reset reconnect attempts on successful connection
      reconnectAttempts = 0;

      // Ensure the research icon is spinning when connection is established
      updateResearchIcon(true);

      // If this is a reconnection and we're in research mode, don't send a new start command
      if (isResearchActive && lastRequestData) {
        console.log("Reconnected during active research, not sending new start command");
        return;
      }

      const task = document.getElementById('task').value
      const report_type = document.querySelector(
        'select[name="report_type"]'
      ).value
      const report_source = document.querySelector(
        'select[name="report_source"]'
      ).value
      const tone = document.querySelector('select[name="tone"]').value
      const agent = document.querySelector('input[name="agent"]:checked').value
      let source_urls = tags

      if (report_source !== 'sources' && source_urls.length > 0) {
        source_urls = source_urls.slice(0, source_urls.length - 1)
      }

      const query_domains_str = document.querySelector('input[name="query_domains"]').value
      let query_domains = []
      if (query_domains_str) {
        query_domains = query_domains_str.split(',')
          .map((domain) => domain.trim())
          .filter((domain) => domain.length > 0);
      }

      const requestData = {
        task: task,
        report_type: report_type,
        report_source: report_source,
        source_urls: source_urls,
        tone: tone,
        agent: agent,
        query_domains: query_domains,
      }

      // Add MCP configuration if enabled
      const mcpData = collectMCPData();
      if (mcpData) {
        Object.assign(requestData, mcpData);
        console.log('Including MCP configuration:', mcpData);
      }

      // Store the request data for potential reconnection
      lastRequestData = requestData;

      socket.send(`start ${JSON.stringify(requestData)}`)
    }

    socket.onclose = (event) => {
      // Update metrics and status when connection closes
      connectionStartTime = null;
      updateWebSocketStatus();

      console.log("WebSocket connection closed", event);

      // If research is active, try to automatically reconnect
      if (isResearchActive) {
        reconnectWebSocket();
      }
    }

    socket.onerror = (error) => {
      console.error("WebSocket error:", error);
      updateWebSocketStatus();
    }

    // return dispose function
    return () => {
      try {
        isResearchActive = false; // Mark research as inactive
        if (socket && socket.readyState !== WebSocket.CLOSED && socket.readyState !== WebSocket.CLOSING) {
          socket.close();
        }

        // Update metrics on socket disposal
        connectionStartTime = null;
        updateWebSocketStatus();
      } catch (e) {
        console.error('Error closing socket:', e)
      }
    };
  }

  const addAgentResponse = (data) => {
    const output = document.getElementById('output');
    const responseDiv = document.createElement('div');
    responseDiv.className = 'agent_response';
    responseDiv.innerHTML = data.output; // Assuming data.output is safe HTML or simple text from agent
    output.appendChild(responseDiv);
    output.scrollTop = output.scrollHeight;
    output.style.display = 'block';
  }

  const writeReport = (data, converter, isFinal = false, append = false) => {
    const reportContainer = document.getElementById('reportContainer');

    // Convert markdown to HTML
    const markdownOutput = converter.makeHtml(data.output);

    // If this is the final report or we should append
    if (isFinal) {
      // For final reports, always replace content
      reportContainer.innerHTML = markdownOutput;
    } else if (append) {
      // Append mode - add to existing content
      reportContainer.innerHTML += markdownOutput;
    } else {
      // Replace mode - overwrite existing content
      reportContainer.innerHTML = markdownOutput;
    }

    // Auto-scroll to the bottom of the container
    reportContainer.scrollTop = reportContainer.scrollHeight;
  }

  const updateDownloadLink = (data) => {
    if (!data.output) {
      console.error('No output data received');
      return;
    }

    const { pdf, docx, md, json } = data.output;
    console.log('Received paths:', { pdf, docx, md, json });

    // Store these links for history
    const currentLinks = { pdf, docx, md, json };

    // Helper function to safely update link
    const updateLink = (id, path) => {
      const element = document.getElementById(id);
      if (element && path) {
        console.log(`Setting ${id} href to:`, path);
        element.setAttribute('href', path);
        element.classList.remove('disabled');
      } else {
        console.warn(`Either element ${id} not found or path not provided`);
      }
    };

    // Update links in sticky download bar
    updateLink('downloadLink', pdf);
    updateLink('downloadLinkWord', docx);
    updateLink('downloadLinkMd', md);
    updateLink('downloadLinkJson', json);

    // Update duplicate buttons above the report
    updateLink('downloadLinkTop', pdf);
    updateLink('downloadLinkWordTop', docx);
    updateLink('downloadLinkMdTop', md);
    updateLink('downloadLinkJsonTop', json);

    // Make sure download buttons are visible when download links are ready
    showDownloadPanels();

    // Return links for history saving
    return currentLinks;
  }

  const copyToClipboard = () => {
    const textarea = document.createElement('textarea')
    textarea.id = 'temp_element'
    textarea.style.height = 0
    document.body.appendChild(textarea)
    textarea.value = document.getElementById('reportContainer').innerText
    const selector = document.querySelector('#temp_element')
    selector.select()
    document.execCommand('copy')
    document.body.removeChild(textarea)

    // Show a temporary success message with icon change and toast notification
    const copyBtn = document.getElementById('copyToClipboard');
    const copyBtnTop = document.getElementById('copyToClipboardTop');

    // Function to reset the icon for both buttons
    const resetIcons = () => {
      if (copyBtn) {
        copyBtn.innerHTML = '<i class="fas fa-copy"></i> Copy';
      }
      if (copyBtnTop) {
        copyBtnTop.innerHTML = '<i class="fas fa-copy"></i>';
      }
    };

    // Change to green check mark
    if (copyBtn) {
      copyBtn.innerHTML = '<i class="fas fa-check" style="color: green;"></i> Copied!';
    }
    if (copyBtnTop) {
      copyBtnTop.innerHTML = '<i class="fas fa-check" style="color: green;"></i>';
    }

    // Show toast notification
    showToast('Copied to clipboard!');

    // Reset the button after 3 seconds
    setTimeout(resetIcons, 3000);
  }

  const updateState = (state) => {
    var status = ''
    switch (state) {
      case 'in_progress':
        status = 'Research in progress...'
        setReportActionsStatus('disabled')
        isResearchActive = true;
        // Make the research icon spin
        updateResearchIcon(true);
        // Hide chat container during research
        chatContainer = document.getElementById('chatContainer');
        if (chatContainer) {
          chatContainer.style.display = 'none';
        }
        // Hide the copy button in the header
        const copyBtnTop = document.getElementById('copyToClipboardTop');
        if (copyBtnTop) {
          copyBtnTop.style.display = 'none';
        }
        // Hide the JSON button container
        const jsonContainer = document.getElementById('jsonButtonContainer');
        if (jsonContainer) {
          jsonContainer.style.display = 'none';
        }
        break
      case 'finished':
        status = 'Research finished!'
        setReportActionsStatus('enabled')
        isResearchActive = false;
        // Stop the research icon spinning
        updateResearchIcon(false);

        // Show download panels and hide feature panels when research is finished
        showDownloadPanels();

        // Enable the copy button
        const copyButton = document.getElementById('copyToClipboard');
        if (copyButton) {
          copyButton.classList.remove('disabled');
        }

        // Show copy button in the header
        const topCopyButton = document.getElementById('copyToClipboardTop');
        if (topCopyButton) {
          topCopyButton.style.display = 'inline-block';
          topCopyButton.addEventListener('click', copyToClipboard);
        }

        // Show JSON button container
        const jsonButtonContainer = document.getElementById('jsonButtonContainer');
        if (jsonButtonContainer) {
          jsonButtonContainer.style.display = 'block';
        }

        // Show chat container when research is finished
        chatContainer = document.getElementById('chatContainer');
        if (chatContainer) {
          chatContainer.style.display = 'block';
          // Initialize chat if not already initialized
          initChat();
        }
        break
      case 'error':
        status = 'Research failed!'
        setReportActionsStatus('disabled')
        isResearchActive = false;
        // Stop the research icon spinning
        updateResearchIcon(false);
        break
      case 'initial':
        status = ''
        setReportActionsStatus('hidden')
        isResearchActive = false;
        // Make sure the research icon is not spinning initially
        updateResearchIcon(false);
        // Hide the copy button in the header
        const initialCopyBtnTop = document.getElementById('copyToClipboardTop');
        if (initialCopyBtnTop) {
          initialCopyBtnTop.style.display = 'none';
        }
        // Hide the JSON button container
        const initialJsonContainer = document.getElementById('jsonButtonContainer');
        if (initialJsonContainer) {
          initialJsonContainer.style.display = 'none';
        }
        break
      default:
        setReportActionsStatus('disabled')
    }
    document.getElementById('status').innerHTML = status
    if (document.getElementById('status').innerHTML == '') {
      document.getElementById('status').style.display = 'none'
    } else {
      document.getElementById('status').style.display = 'block'
    }
  }

  /**
   * Shows or hides the download and copy buttons
   * @param {str} status Kind of hacky. Takes "enabled", "disabled", or "hidden". "Hidden is same as disabled but also hides the div"
   */
  const setReportActionsStatus = (status) => {
    const reportActions = document.getElementById('reportActions')
    // Disable everything in reportActions until research is finished

    if (status == 'enabled') {
      reportActions.querySelectorAll('a').forEach((link) => {
        link.classList.remove('disabled')
        link.removeAttribute('onclick')
        reportActions.style.display = 'block'
      })
    } else {
      reportActions.querySelectorAll('a').forEach((link) => {
        link.classList.add('disabled')
        link.setAttribute('onclick', 'return false;')
      })
      if (status == 'hidden') {
        reportActions.style.display = 'none'
      }
    }
  }

  const tagsInput = document.getElementById('tags-input');
  const input = document.getElementById('custom_source');

  const tags = [];

  const addTag = (url) => {
    if (tags.includes(url)) return;
    tags.push(url);

    const tagElement = document.createElement('span');
    tagElement.className = 'tag';
    tagElement.textContent = url;

    const removeButton = document.createElement('span');
    removeButton.className = 'remove-tag';
    removeButton.textContent = 'x';
    removeButton.onclick = function () {
      tagsInput.removeChild(tagElement);
      tags.splice(tags.indexOf(url), 1);
    };

    tagElement.appendChild(removeButton);
    tagsInput.insertBefore(tagElement, input);
  }

  const displaySelectedImages = (data) => {
    const imageContainer = document.getElementById('selectedImagesContainer')
    //imageContainer.innerHTML = '<h3>Selected Images</h3>'
    const images = JSON.parse(data.output)
    console.log("Received images:", images);  // Debug log
    if (images && images.length > 0) {
      images.forEach(imageUrl => {
        const imgElement = document.createElement('img')
        imgElement.src = imageUrl
        imgElement.alt = 'Research Image'
        imgElement.style.maxWidth = '200px'
        imgElement.style.margin = '5px'
        imgElement.style.cursor = 'pointer'
        imgElement.onclick = () => showImageDialog(imageUrl)
        imageContainer.appendChild(imgElement)
      })
      imageContainer.style.display = 'block'
    } else {
      imageContainer.innerHTML += '<p>No images found for this research.</p>'
    }
  }

  const showImageDialog = (imageUrl) => {
    let dialog = document.querySelector('.image-dialog');
    if (!dialog) {
        dialog = document.createElement('div');
        dialog.className = 'image-dialog';

        const img = document.createElement('img');
        img.alt = 'Full-size Research Image';

        const closeBtn = document.createElement('button');
        closeBtn.textContent = 'Close';
        closeBtn.className = 'close-btn'; // Added class for styling

        dialog.appendChild(img);
        dialog.appendChild(closeBtn);
        document.body.appendChild(dialog);

        closeBtn.onclick = () => {
            dialog.classList.remove('visible');
        };
        // Close on clicking backdrop
        dialog.addEventListener('click', (e) => {
            if (e.target === dialog) {
                dialog.classList.remove('visible');
            }
        });
    }

    const imgElement = dialog.querySelector('img');
    imgElement.src = imageUrl;
    dialog.classList.add('visible');

    // Close with Escape key
    const escapeKeyListener = (e) => {
        if (e.key === 'Escape') {
            dialog.classList.remove('visible');
            document.removeEventListener('keydown', escapeKeyListener);
        }
    };
    document.addEventListener('keydown', escapeKeyListener);
}

  // Function to show download bar and enable buttons
  const showDownloadPanels = () => {
    // Show the bar by adding the visible class
    const stickyDownloadsBar = document.getElementById('stickyDownloadsBar');
    if (stickyDownloadsBar) {
      stickyDownloadsBar.classList.add('visible');
    }

    // Enable all download buttons
    const downloadButtons = document.querySelectorAll('.download-option-btn, .report-action-btn');
    downloadButtons.forEach(button => {
      button.classList.remove('disabled');
    });

    // Make top buttons report-actions section visible
    const reportActions = document.querySelector('.report-actions');
    if (reportActions) {
      reportActions.style.display = 'flex';
    }
  }

  // --- Storage Helpers (Cookies or LocalStorage) ---
  function setCookie(name, value, days) {
    // Maximum cookie size is around 4KB (4096 bytes)
    const MAX_COOKIE_SIZE = 4000;

    // If cookies are disabled, use localStorage instead
    if (!cookiesEnabled) {
      try {
        localStorage.setItem(name, value);
        console.debug(`Data saved to localStorage: ${name}`);
        return true;
      } catch (e) {
        console.error("Error saving to localStorage:", e);
        return false;
      }
    }

    let expires = '';
    if (days) {
      const date = new Date();
      date.setTime(date.getTime() + (days * 24 * 60 * 60 * 1000));
      expires = '; expires=' + date.toUTCString();
    }

    // Encode the value
    const encodedValue = encodeURIComponent(value);

    // Calculate cookie size
    const cookieSize = (name + '=' + encodedValue + expires + '; path=/').length;
    console.debug(`Setting cookie: ${name}, size: ${cookieSize} bytes`);

    // If cookie is too large, display warning and truncate history
    if (cookieSize > MAX_COOKIE_SIZE) {
      console.warn(`Cookie size (${cookieSize} bytes) exceeds the ${MAX_COOKIE_SIZE} bytes limit!`);
      showToast('Warning: History too large for cookie storage! Oldest entries will be removed.');

      if (name === 'conversationHistory') {
        try {
          // Parse, reduce entries, and try again
          const historyData = JSON.parse(value);
          if (Array.isArray(historyData) && historyData.length > 1) {
            // Remove the last entry and try again recursively
            const reducedHistory = historyData.slice(0, -1);
            console.debug(`Reducing history from ${historyData.length} to ${reducedHistory.length} entries`);
            setCookie(name, JSON.stringify(reducedHistory), days);
            return; // Exit after recursive call
          }
        } catch (e) {
          console.error('Could not parse history to reduce size:', e);
        }
      }

      return false; // Indicate failure
    }

    // Set the cookie
    document.cookie = name + '=' + encodedValue + expires + '; path=/';
    console.debug(`Cookie set: ${name}`);
    return true; // Indicate success
  }

  function getCookie(name) {
    console.debug(`Getting data: ${name}`);

    // If cookies are disabled, use localStorage instead
    if (!cookiesEnabled) {
      try {
        const value = localStorage.getItem(name);
        if (value) {
          console.debug(`Data found in localStorage: ${name}, length: ${value.length} chars`);
          return value;
        }
        console.debug(`Data not found in localStorage: ${name}`);
        return null;
      } catch (e) {
        console.error("Error retrieving from localStorage:", e);
        return null;
      }
    }

    const nameEQ = name + '=';
    const ca = document.cookie.split(';');
    for (let i = 0; i < ca.length; i++) {
      let c = ca[i];
      while (c.charAt(0) == ' ') c = c.substring(1, c.length);
      if (c.indexOf(nameEQ) == 0) {
        const value = decodeURIComponent(c.substring(nameEQ.length, c.length));
        console.debug(`Found cookie: ${name}, length: ${value.length} chars`);
        return value;
      }
    }
    console.debug(`Cookie not found: ${name}`);
    return null;
  }

  function deleteCookie(name) {
    console.debug(`Deleting storage: ${name}`);

    // If cookies are disabled, use localStorage instead
    if (!cookiesEnabled) {
      try {
        localStorage.removeItem(name);
        return;
      } catch (e) {
        console.error("Error removing from localStorage:", e);
        return;
      }
    }

    document.cookie = name + '=; expires=Thu, 01 Jan 1970 00:00:00 UTC; path=/;';
  }
  // --- End Storage Helpers ---

  // Debug Helper - check cookie status
  const checkCookieStatus = () => {
    if (!cookiesEnabled) {
      const storageData = localStorage.getItem('conversationHistory');
      if (storageData) {
        const byteSize = new Blob([storageData]).size;
        const kilobyteSize = (byteSize / 1024).toFixed(2);

        try {
          const parsed = JSON.parse(storageData);
          const entryCount = Array.isArray(parsed) ? parsed.length : 0;

          showToast(`Using localStorage: ${kilobyteSize}KB, ${entryCount} entries`);
          console.debug(`LocalStorage size: ${byteSize} bytes, ${kilobyteSize}KB`);
          console.debug(`LocalStorage entries: ${entryCount}`);
        } catch (e) {
          showToast(`LocalStorage contains invalid data: ${kilobyteSize}KB`);
          console.error('LocalStorage parse error:', e);
        }
      } else {
        showToast('No research history found in localStorage');
      }
      return;
    }

    const allCookies = document.cookie;
    console.debug('All cookies:', allCookies);

    const conversationCookie = getCookie('conversationHistory');
    if (conversationCookie) {
      const byteSize = new Blob([conversationCookie]).size;
      const kilobyteSize = (byteSize / 1024).toFixed(2);

      try {
        const parsed = JSON.parse(conversationCookie);
        const entryCount = Array.isArray(parsed) ? parsed.length : 0;

        showToast(`Cookie found: ${kilobyteSize}KB, ${entryCount} research entries`);
        console.debug(`Cookie size: ${byteSize} bytes, ${kilobyteSize}KB`);
        console.debug(`Cookie entries: ${entryCount}`);
      } catch (e) {
        showToast(`Cookie found but invalid: ${kilobyteSize}KB`);
        console.error('Cookie parse error:', e);
      }
    } else {
      showToast('No research history cookie found');
    }
  }

  // Export history to a downloadable JSON file
  const exportHistory = () => {
    try {
      if (!conversationHistory || conversationHistory.length === 0) {
        showToast('No research history to export');
        return;
      }

      // Create a formatted JSON string with pretty-printing
      const historyJson = JSON.stringify(conversationHistory, null, 2);

      // Create a Blob containing the data
      const blob = new Blob([historyJson], { type: 'application/json' });

      // Create an object URL for the blob
      const url = URL.createObjectURL(blob);

      // Create a temporary link element
      const link = document.createElement('a');
      link.href = url;

      // Set download attribute with filename
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      link.download = `research-history-${timestamp}.json`;

      // Append to the document
      document.body.appendChild(link);

      // Programmatically click the link to trigger the download
      link.click();

      // Clean up
      document.body.removeChild(link);
      URL.revokeObjectURL(url);

      showToast('Research history exported to JSON file');
      console.debug('History exported, entries:', conversationHistory.length);
    } catch (error) {
      console.error('Error exporting history:', error);
      showToast('Error exporting research history');
    }
  }

  // Trigger the file input for importing history
  const triggerImportHistory = () => {
    const fileInput = document.getElementById('historyFileInput');
    if (fileInput) {
      fileInput.click();
    } else {
      showToast('Import functionality not available');
    }
  }

  // Handle the file import for history
  const handleFileImport = (event) => {
    const file = event.target.files[0];
    if (!file) {
      return;
    }

    const reader = new FileReader();

    reader.onload = (e) => {
      try {
        const content = e.target.result;
        const importedData = JSON.parse(content);

        // Validate the imported data
        if (!Array.isArray(importedData)) {
          throw new Error('Imported data is not an array');
        }

        // Check if each entry has the required fields
        const validEntries = importedData.filter(entry => {
          return entry &&
            typeof entry === 'object' &&
            (entry.prompt || entry.task) && // Allow both prompt and legacy task field
            (entry.links || entry.downloadLinks); // Allow both links and legacy downloadLinks
        });

        if (validEntries.length === 0) {
          showToast('No valid research entries found in the imported file');
          return;
        }

        // Map the entries to the current structure if needed
        const mappedEntries = validEntries.map(entry => {
          return {
            prompt: entry.prompt || entry.task || '',
            links: entry.links || entry.downloadLinks || {},
            timestamp: entry.timestamp || new Date().toISOString()
          };
        });

        // Confirm before overwriting existing history
        if (conversationHistory && conversationHistory.length > 0) {
          if (confirm(`You have ${conversationHistory.length} existing research entries. Do you want to:
- Click OK to MERGE imported history with existing history
- Click Cancel to REPLACE all existing history with imported data`)) {
            // Merge with existing history
            conversationHistory = [...mappedEntries, ...conversationHistory];
          } else {
            // Replace existing history
            conversationHistory = mappedEntries;
          }
        } else {
          // No existing history, just set the imported data
          conversationHistory = mappedEntries;
        }

        // Save the new history and update the UI
        saveConversationHistory();
        renderHistoryEntries();

        showToast(`Successfully imported ${validEntries.length} research entries`);
        console.debug('Research history imported, valid entries:', validEntries.length);

      } catch (error) {
        console.error('Error importing history:', error);
        showToast('Error importing research history: Invalid file format');
      }

      // Reset the file input so the same file can be selected again
      event.target.value = '';
    };

    reader.onerror = () => {
      console.error('Error reading file');
      showToast('Error reading the imported file');
      event.target.value = '';
    };

    reader.readAsText(file);
  }

  // Initialize chat functionality
  const initChat = () => {
    const chatInput = document.getElementById('chatInput');
    const sendChatBtn = document.getElementById('sendChatBtn');
    const voiceInputBtn = document.getElementById('voiceInputBtn');

    if (!chatInput || !sendChatBtn) return;

    // Clear previous messages
    const chatMessages = document.getElementById('chatMessages');
    if (chatMessages) {
      chatMessages.innerHTML = '';
    }

    // Add event listeners for chat input
    chatInput.addEventListener('keydown', (e) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        sendChatMessage();
      }
    });

    sendChatBtn.addEventListener('click', sendChatMessage);

    // Initialize speech recognition if supported
    if (voiceInputBtn) {
      initSpeechRecognition(voiceInputBtn, chatInput);
    }

    // Auto-resize textarea as content grows
    chatInput.addEventListener('input', () => {
      chatInput.style.height = 'auto';
      chatInput.style.height = (chatInput.scrollHeight) + 'px';
    });

    // Add welcome message
    addChatMessage('I can answer questions about the research report. What would you like to know?', false);
  }

  // Initialize speech recognition
  const initSpeechRecognition = (button, inputElement) => {
    // Check if browser supports speech recognition
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.warn('Speech recognition not supported in this browser');
      button.style.display = 'none';
      return;
    }

    const recognition = new SpeechRecognition();

    // Configure speech recognition
    recognition.continuous = false;
    recognition.lang = 'en-US';
    recognition.interimResults = true;
    recognition.maxAlternatives = 1;

    let isListening = false;
    let finalTranscript = '';

    // Add event listeners for speech recognition
    recognition.onstart = () => {
      isListening = true;
      finalTranscript = '';
      button.classList.add('listening');
      button.innerHTML = '<i class="fas fa-microphone-slash"></i>';
      button.title = 'Stop listening';

      // Show visual feedback
      showToast('Listening...', 1000);
    };

    recognition.onresult = (event) => {
      let interimTranscript = '';

      // Loop through the results
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;

        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      // Update the input element with the transcription
      inputElement.value = finalTranscript + interimTranscript;

      // Trigger input event to resize textarea
      const inputEvent = new Event('input', { bubbles: true });
      inputElement.dispatchEvent(inputEvent);
    };

    recognition.onerror = (event) => {
      console.error('Speech recognition error', event.error);
      resetRecognition();

      if (event.error === 'not-allowed') {
        showToast('Microphone access denied. Please allow microphone access in your browser settings.', 3000);
      } else {
        showToast('Speech recognition error: ' + event.error, 3000);
      }
    };

    recognition.onend = () => {
      resetRecognition();
    };

    // Reset the recognition state
    const resetRecognition = () => {
      isListening = false;
      button.classList.remove('listening');
      button.innerHTML = '<i class="fas fa-microphone"></i>';
      button.title = 'Use voice input';
    };

    // Toggle speech recognition on button click
    button.addEventListener('click', () => {
      if (isListening) {
        recognition.stop();
      } else {
        recognition.start();
      }
    });
  };

  // Create a new function to handle WebSocket reconnection
  const reconnectWebSocket = (message = null) => {
    // Don't attempt too many reconnections
    if (reconnectAttempts >= maxReconnectAttempts) {
      console.error(`Failed to reconnect after ${maxReconnectAttempts} attempts`);
      addChatMessage(`Unable to reconnect after ${maxReconnectAttempts} attempts. Please refresh the page.`, false);
      return false;
    }

    reconnectAttempts++;

    // Calculate backoff time (exponential backoff)
    const backoff = reconnectInterval * Math.pow(1.5, reconnectAttempts - 1);
    console.log(`Attempting to reconnect (${reconnectAttempts}/${maxReconnectAttempts}) in ${backoff}ms...`);

    // Show reconnection status to user
    addChatMessage(`Connection lost. Attempting to reconnect (${reconnectAttempts}/${maxReconnectAttempts})...`, false);

    // Try to reconnect after delay
    setTimeout(() => {
      try {
        // Setup new WebSocket connection
        dispose_socket = listenToSockEvents();

        // Set up a one-time handler to send the message after reconnection
        if (message) {
          const messageToSend = message;
          const checkConnectionAndSend = () => {
            if (socket && socket.readyState === WebSocket.OPEN) {
              console.log("Reconnected successfully, sending queued message");
              socket.send(messageToSend);
              return true;
            } else if (reconnectAttempts < maxReconnectAttempts) {
              console.log("Socket not ready yet, retrying...");
              setTimeout(checkConnectionAndSend, 1000);
              return false;
            }
            return false;
          };

          setTimeout(checkConnectionAndSend, 1000);
        }

        return true;
      } catch (e) {
        console.error("Error during reconnection:", e);
        return false;
      }
    }, backoff);

    return true;
  };

  // Send a chat message
  const sendChatMessage = () => {
    const chatInput = document.getElementById('chatInput');
    if (!chatInput || !chatInput.value.trim()) return;

    const message = chatInput.value.trim();

    // Add user message to chat
    addChatMessage(message, true);

    // Clear input
    chatInput.value = '';
    chatInput.style.height = 'auto';

    // Add loading indicator
    const loadingId = addLoadingIndicator();

    // Prepare the message to send
    const messageToSend = `chat ${JSON.stringify({ message: message })}`;

    // Send message through WebSocket
    if (socket && socket.readyState === WebSocket.OPEN) {
      socket.send(messageToSend);
    } else {
      // If socket is closed, try to reconnect
      removeLoadingIndicator(loadingId);

      // Reset reconnect attempts if this is a new chat session
      if (reconnectAttempts >= maxReconnectAttempts) {
        reconnectAttempts = 0;
      }

      // Attempt to reconnect and queue the message to be sent after reconnection
      if (!reconnectWebSocket(messageToSend)) {
        // If reconnection fails or max attempts reached
        addChatMessage('Unable to send message. Connection is unavailable.', false);
      }
    }
  }

  // Add a chat message to the UI
  const addChatMessage = (message, isUser = false) => {
    const chatMessages = document.getElementById('chatMessages');
    if (!chatMessages) return;

    const messageEl = document.createElement('div');
    messageEl.className = `chat-message ${isUser ? 'user-message' : 'ai-message'}`;

    // Process message for AI responses (convert markdown to HTML for AI messages)
    let processedMessage = message;
    if (!isUser) {
      // Use showdown for markdown conversion
      const converter = new showdown.Converter({
        ghCodeBlocks: true,
        tables: true,
        tasklists: true,
        openLinksInNewWindow: true
      });
      processedMessage = converter.makeHtml(message);
    }

    // Set message content
    messageEl.innerHTML = isUser ? escapeHtml(processedMessage) : processedMessage;

    // Add timestamp
    const timestampEl = document.createElement('div');
    timestampEl.className = 'chat-timestamp';
    const now = new Date();
    timestampEl.textContent = now.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
    messageEl.appendChild(timestampEl);

    // Add to chat container
    chatMessages.appendChild(messageEl);

    // Scroll to bottom
    chatMessages.scrollTop = chatMessages.scrollHeight;
  }

  // Add a loading indicator
  const addLoadingIndicator = () => {
    const chatMessages = document.getElementById('chatMessages');
    if (!chatMessages) return null;

    const loadingId = 'loading-' + Date.now();
    const loadingEl = document.createElement('div');
    loadingEl.className = 'chat-message ai-message chat-loading';
    loadingEl.id = loadingId;

    // Create the dots
    for (let i = 0; i < 3; i++) {
      const dot = document.createElement('div');
      dot.className = 'chat-dot';
      loadingEl.appendChild(dot);
    }

    chatMessages.appendChild(loadingEl);
    chatMessages.scrollTop = chatMessages.scrollHeight;

    return loadingId;
  }

  // Remove loading indicator
  const removeLoadingIndicator = (loadingId) => {
    if (!loadingId) return;

    const loadingEl = document.getElementById(loadingId);
    if (loadingEl) {
      loadingEl.remove();
    }
  }

  // Escape HTML to prevent XSS in user messages
  const escapeHtml = (text) => {
    const div = document.createElement('div');
    div.textContent = text;
    return div.innerHTML;
  }

  // Initialize expand buttons
  const initExpandButtons = () => {
    // Report container expand button
    const expandReportBtn = document.getElementById('expandReportBtn');
    if (expandReportBtn) {
      expandReportBtn.addEventListener('click', () => {
        const reportContainer = document.querySelector('.report-container');
        toggleExpand(reportContainer);
      });
    }

    // Chat container expand button
    const expandChatBtn = document.getElementById('expandChatBtn');
    if (expandChatBtn) {
      expandChatBtn.addEventListener('click', () => {
        const chatContainer = document.getElementById('chatContainer');
        toggleExpand(chatContainer);
      });
    }

    // Output container expand button
    const expandOutputBtn = document.getElementById('expandOutputBtn');
    if (expandOutputBtn) {
      expandOutputBtn.addEventListener('click', () => {
        const outputContainer = document.querySelector('.research-output-container');
        toggleExpand(outputContainer);
      });
    }

    // Close expanded view when ESC key is pressed
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape') {
        const expandedElements = document.querySelectorAll('.expanded-view');
        expandedElements.forEach(el => {
          // Reset the button icon
          const button = el.querySelector('.expand-button i');
          if (button) {
            button.classList.remove('fa-compress-alt');
            button.classList.add('fa-expand-alt');
          }

          // Reset content container styles
          const contentContainers = el.querySelectorAll('#reportContainer, #output, #chatMessages');
          contentContainers.forEach(container => {
            if (container) {
              container.style.maxHeight = '';
            }
          });

          // Remove expanded-view class
          el.classList.remove('expanded-view');
        });
      }
    });
  }

  // Toggle expand mode for an element
  const toggleExpand = (element) => {
    if (!element) return;

    // Toggle expanded-view class
    element.classList.toggle('expanded-view');

    // Change button icon and title based on state
    const buttonIcon = element.querySelector('.expand-button i');
    const button = element.querySelector('.expand-button');

    if (buttonIcon && button) {
      if (element.classList.contains('expanded-view')) {
        buttonIcon.classList.remove('fa-compress-alt');
        buttonIcon.classList.add('fa-compress-alt');
        button.title = 'Collapse'; // Update title to Collapse

        // Find content containers and expand their height
        const contentContainers = element.querySelectorAll('#reportContainer, #output, #chatMessages');
        contentContainers.forEach(container => {
          if (container) {
            // Set expanded heights - no positioning changes
            if (container.id === 'reportContainer') {
              container.style.maxHeight = '800px'; // Fixed expanded height for report
            } else {
              container.style.maxHeight = '600px'; // Fixed expanded height for other content
            }
          }
        });
      } else {
        buttonIcon.classList.remove('fa-compress-alt');
        buttonIcon.classList.add('fa-expand-alt');
        button.title = 'Expand'; // Update title to Expand

        // Reset heights back to original when collapsed
        const contentContainers = element.querySelectorAll('#reportContainer, #output, #chatMessages');
        contentContainers.forEach(container => {
          if (container) {
            container.style.maxHeight = '';
          }
        });
      }
    }
  }

  // MCP Configuration Management
  
  // Initialize MCP functionality
  const initMCPSection = () => {
    const mcpEnabled = document.getElementById('mcpEnabled');
    const mcpConfigSection = document.getElementById('mcpConfigSection');
    const mcpInfoBtn = document.getElementById('mcpInfoBtn');
    const mcpConfig = document.getElementById('mcpConfig');
    const mcpFormatBtn = document.getElementById('mcpFormatBtn');
    const mcpExampleLink = document.getElementById('mcpExampleLink');

    if (!mcpEnabled || !mcpConfigSection) {
      console.warn('MCP elements not found');
      return;
    }

    // Toggle MCP config section
    mcpEnabled.addEventListener('change', () => {
      if (mcpEnabled.checked) {
        mcpConfigSection.style.display = 'block';
        updateRetrieverForMCP(true);
      } else {
        mcpConfigSection.style.display = 'none';
        updateRetrieverForMCP(false);
      }
    });

    // MCP info modal
    if (mcpInfoBtn) {
      mcpInfoBtn.addEventListener('click', showMCPInfo);
    }

    // JSON validation and formatting
    if (mcpConfig) {
      mcpConfig.addEventListener('input', validateMCPConfig);
      mcpConfig.addEventListener('blur', validateMCPConfig);
    }

    if (mcpFormatBtn) {
      mcpFormatBtn.addEventListener('click', formatMCPConfig);
    }

    if (mcpExampleLink) {
      mcpExampleLink.addEventListener('click', (e) => {
        e.preventDefault();
        showMCPExample();
      });
    }

    // Preset buttons
    const presetButtons = document.querySelectorAll('.preset-btn');
    presetButtons.forEach(btn => {
      btn.addEventListener('click', (e) => {
        const preset = e.currentTarget.dataset.preset;
        addMCPPreset(preset);
      });
    });

    // Create MCP info modal
    createMCPInfoModal();

    // Initial validation
    validateMCPConfig();
  };

  // Validate MCP JSON configuration
  const validateMCPConfig = () => {
    const mcpConfig = document.getElementById('mcpConfig');
    const mcpConfigStatus = document.getElementById('mcpConfigStatus');
    
    if (!mcpConfig || !mcpConfigStatus) return;

    const configText = mcpConfig.value.trim();
    
    if (!configText || configText === '[]') {
      mcpConfig.className = 'form-control mcp-config-textarea';
      mcpConfigStatus.textContent = 'Empty configuration';
      mcpConfigStatus.className = 'mcp-status-text';
      return true;
    }

    try {
      const parsed = JSON.parse(configText);
      
      if (!Array.isArray(parsed)) {
        throw new Error('Configuration must be an array');
      }

      // Validate each server config
      const errors = [];
      parsed.forEach((server, index) => {
        if (!server.name) {
          errors.push(`Server ${index + 1}: missing name`);
        }
        if (!server.command && !server.connection_url) {
          errors.push(`Server ${index + 1}: missing command or connection_url`);
        }
      });

      if (errors.length > 0) {
        throw new Error(errors.join('; '));
      }

      mcpConfig.className = 'form-control mcp-config-textarea valid';
      mcpConfigStatus.textContent = `Valid JSON âœ“ (${parsed.length} server${parsed.length !== 1 ? 's' : ''})`;
      mcpConfigStatus.className = 'mcp-status-text valid';
      return true;

    } catch (error) {
      mcpConfig.className = 'form-control mcp-config-textarea invalid';
      mcpConfigStatus.textContent = `Invalid JSON: ${error.message}`;
      mcpConfigStatus.className = 'mcp-status-text invalid';
      return false;
    }
  };

  // Format MCP JSON configuration
  const formatMCPConfig = () => {
    const mcpConfig = document.getElementById('mcpConfig');
    if (!mcpConfig) return;

    try {
      const parsed = JSON.parse(mcpConfig.value.trim() || '[]');
      mcpConfig.value = JSON.stringify(parsed, null, 2);
      validateMCPConfig();
      showToast('JSON formatted successfully!');
    } catch (error) {
      showToast('Cannot format invalid JSON');
    }
  };

  // Show MCP configuration example
  const showMCPExample = () => {
    const exampleConfig = [
      {
        "name": "github",
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-github"],
        "env": {
          "GITHUB_PERSONAL_ACCESS_TOKEN": "your_github_token_here"
        }
      },
      {
        "name": "filesystem",
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"],
        "env": {}
      }
    ];

    const mcpConfig = document.getElementById('mcpConfig');
    if (mcpConfig) {
      mcpConfig.value = JSON.stringify(exampleConfig, null, 2);
      validateMCPConfig();
      showToast('Example configuration loaded!');
    }
  };

  // Update retriever configuration for MCP
  const updateRetrieverForMCP = (enableMCP) => {
    if (enableMCP) {
      showToast('MCP enabled - will be included in research process');
    } else {
      showToast('MCP disabled - using web search only');
    }
  };

  // Show MCP information modal
  const showMCPInfo = () => {
    const modal = document.getElementById('mcpInfoModal');
    if (modal) {
      modal.classList.add('visible');
    }
  };

  // Create MCP info modal
  const createMCPInfoModal = () => {
    if (document.getElementById('mcpInfoModal')) return;

    const modal = document.createElement('div');
    modal.id = 'mcpInfoModal';
    modal.className = 'mcp-info-modal';
    
    modal.innerHTML = `
      <div class="mcp-info-content">
        <button class="mcp-info-close" onclick="closeMCPInfo()">
          <i class="fas fa-times"></i>
        </button>
        <h3>Model Context Protocol (MCP)</h3>
        <p>MCP enables GPT Researcher to connect with external tools and data sources through a standardized protocol.</p>
        
        <h4 class="highlight">Benefits:</h4>
        <ul>
          <li><span class="highlight">Access Local Data:</span> Connect to databases, file systems, and APIs</li>
          <li><span class="highlight">Use External Tools:</span> Integrate with web services and third-party tools</li>
          <li><span class="highlight">Extend Capabilities:</span> Add custom functionality through MCP servers</li>
          <li><span class="highlight">Maintain Security:</span> Controlled access with proper authentication</li>
        </ul>

        <h4 class="highlight">Quick Start:</h4>
        <ul>
          <li>Enable MCP using the checkbox above</li>
          <li>Click a preset to add pre-configured servers to the JSON</li>
          <li>Or paste your own MCP configuration as a JSON array</li>
          <li>Start your research - MCP will run with optimal settings</li>
        </ul>

        <h4 class="highlight">Configuration Format:</h4>
        <p>Each MCP server should be a JSON object with these properties:</p>
        <ul>
          <li><span class="highlight">name:</span> Unique identifier (e.g., "github", "filesystem")</li>
          <li><span class="highlight">command:</span> Command to run the server (e.g., "npx", "python")</li>
          <li><span class="highlight">args:</span> Array of arguments (e.g., ["-y", "@modelcontextprotocol/server-github"])</li>
          <li><span class="highlight">env:</span> Object with environment variables (e.g., {"API_KEY": "your_key"})</li>
        </ul>
      </div>
    `;

    document.body.appendChild(modal);

    // Close modal when clicking outside
    modal.addEventListener('click', (e) => {
      if (e.target === modal) {
        modal.classList.remove('visible');
      }
    });

    // Close with Escape key
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Escape' && modal.classList.contains('visible')) {
        modal.classList.remove('visible');
      }
    });
  };

  // Close MCP info modal
  window.closeMCPInfo = () => {
    const modal = document.getElementById('mcpInfoModal');
    if (modal) {
      modal.classList.remove('visible');
    }
  };

  // Add MCP preset configurations
  const addMCPPreset = (preset) => {
    const presets = {
      github: {
        "name": "github",
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-github"],
        "env": {
          "GITHUB_PERSONAL_ACCESS_TOKEN": "your_github_token_here"
        }
      },
      tavily: {
        "name": "tavily",
        "command": "npx",
        "args": ["-y", "tavily-mcp@0.1.2"],
        "env": {
          "TAVILY_API_KEY": "your_tavily_api_key_here"
        }
      },
      filesystem: {
        "name": "filesystem",
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/directory"],
        "env": {}
      }
    };

    const config = presets[preset];
    if (!config) return;

    const mcpConfig = document.getElementById('mcpConfig');
    if (!mcpConfig) return;

    try {
      let currentConfig = [];
      const currentText = mcpConfig.value.trim();
      
      if (currentText && currentText !== '[]') {
        currentConfig = JSON.parse(currentText);
      }

      // Check if server already exists
      const existingIndex = currentConfig.findIndex(server => server.name === config.name);
      
      if (existingIndex !== -1) {
        // Replace existing server
        currentConfig[existingIndex] = config;
        showToast(`Updated ${preset} MCP server configuration`);
      } else {
        // Add new server
        currentConfig.push(config);
        showToast(`Added ${preset} MCP server configuration`);
      }

      mcpConfig.value = JSON.stringify(currentConfig, null, 2);
      validateMCPConfig();

    } catch (error) {
      console.error('Error adding preset:', error);
      showToast('Error adding preset configuration');
    }
  };

  // Collect MCP configuration data
  const collectMCPData = () => {
    const mcpEnabled = document.getElementById('mcpEnabled');
    if (!mcpEnabled || !mcpEnabled.checked) {
      return null;
    }

    const mcpConfig = document.getElementById('mcpConfig');
    
    if (!mcpConfig) {
      console.warn('MCP config element not found for data collection');
      return null;
    }

    // Validate configuration before collecting
    if (!validateMCPConfig()) {
      showToast('Invalid MCP configuration - please fix errors before submitting');
      return null;
    }

    try {
      const configText = mcpConfig.value.trim();
      const mcpConfigs = configText && configText !== '[]' ? JSON.parse(configText) : [];

      return {
        mcp_enabled: true,
        mcp_strategy: "fast", // Always use "fast" strategy as default
        mcp_configs: mcpConfigs
      };
    } catch (error) {
      console.error('Error collecting MCP data:', error);
      showToast('Error processing MCP configuration');
      return null;
    }
  };

  return {
    init,
    startResearch,
    addTag,
    copyToClipboard,
    displaySelectedImages,
    showImageDialog,
    checkCookieStatus,
    exportHistory,
    importHistory: triggerImportHistory,  // Add import function to return object
    initChat,
    sendChatMessage,
    addChatMessage
  }
})()

window.addEventListener('DOMContentLoaded', GPTResearcher.init)



================================================
FILE: frontend/styles.css
================================================
@keyframes gradientBG {
    0% {
        background-position: 0 50%;
    }
    50% {
        background-position: 100% 50%;
    }
    100% {
        background-position: 0 50%;
    }
}

/* Animate the global background with subtle gradient */
html, body {
    /* Keep smooth scrolling */
    scroll-behavior: smooth;
    /* Replace radial dark gray with animated multi-stop gradient */
    background: linear-gradient(45deg, #1a1a2e, #08192f, #0f2351, #1a1a2e);
    background-size: 400% 400%;
    animation: gradientBG 20s ease infinite;
}

body {
    font-family: 'Montserrat', sans-serif;
    color: #fff;
    line-height: 1.6;
    background-color: #1e272e;
    background-image: radial-gradient(circle at 30% 20%, #151520 0%, #080808 80%);
    background-attachment: fixed;
    position: relative !important;
    background-color: #121212 !important;
    padding-top: 40px;
}

body::before {
    content: "";
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    /* Use an existing asset to avoid 404s */
    background-image: url('./static/gptr-logo.png');
    background-position: center;
    background-size: contain;
    background-repeat: no-repeat;
    background-attachment: fixed;
    opacity: 0.4;
    z-index: -1;
}

.landing {
    display: flex;
    justify-content: center;
    align-items: center;
    height: calc(100vh - 40px);
    text-align: center;
    padding: 0 20px;
    margin-top: 0; /* Removed top margin that compensated for the top bar */
}

.landing h1 {
    font-size: 3.5rem;
    font-weight: 700;
    margin-bottom: 2rem;
}

.gradient-text {
    background-image: linear-gradient(to right, #2dd4bf, #0d9488);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    color: transparent;
}

.landing p {
    font-size: 1.5rem;
    font-weight: 400;
    max-width: 1000px;
    padding: 0 25px 0 25px;
    margin: auto auto 2rem auto;
}

.landing-description {
    font-size: 20px;
}

.container {
    padding: 20px;
    background-color: rgba(255, 255, 255, 0.1);
    border-radius: 12px;
    box-shadow: 0 10px 25px rgba(0, 0, 0, 0.1);
    transition: all .3s ease-in-out;
    margin: auto auto 180px auto;
}

.container:hover {
    /* Uncomment this to enable hover zoom effect (transform scale effect).
        This is not recommended because it can be obnoxious on desktop, and straight up unusable on mobile. */
    /* transform: scale(1.01); */
    box-shadow: 0 15px 30px rgba(0, 0, 0, 0.4);
}

/* Glass-style inputs, selects, output areas */
input, select, #output, #reportContainer {
    background-color: rgba(255, 255, 255, 0.1);
    border: none;
    color: #fff;
    transition: all .3s ease-in-out;
}

input:hover, input:focus, select:hover, select:focus {
    background-color: #dfe4ea;
    border: 1px solid rgba(255, 255, 255, 0.5);
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    transition: all 0.3s ease-in-out;
}

.btn-primary {
    background: linear-gradient(to right, #0d9488, #14b8a6);
    border: none;
    transition: all .3s ease-in-out;
    font-size: 1.1rem;
    padding: 12px 30px;
    border-radius: 30px;
    min-width: 180px;
    text-align: center;
    box-shadow: 0 4px 15px rgba(20, 184, 166, 0.3);
    font-weight: 600;
}

.btn-secondary {
    background: linear-gradient(to right, #6c757d, #6c757d);
    border: none;
    transition: all .3s ease-in-out;
    font-size: 1.1rem;
    padding: 12px 30px;
    border-radius: 30px;
    min-width: 180px;
    text-align: center;
}

.btn-action {
    background: rgba(40, 40, 40, 0.8);
    border: 1px solid rgba(20, 184, 166, 0.5);
}

.btn:hover {
    opacity: 0.9;
    transform: translateY(-3px);
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
}

.agent-question {
    font-size: 1.4rem;
    font-weight: 500;
    margin-bottom: 0.2rem;
}

footer {
    position: relative;
    left: 0;
    bottom: 0;
    width: 100%;
    color: white;
    text-align: center;
    padding: 20px 0;
    margin-top: 40px;
    background: rgba(18, 18, 18, 0.8);
    backdrop-filter: blur(10px);
    border-top: 1px solid rgba(20, 184, 166, 0.3);
}

footer p {
    margin: 5px 0;
    font-size: 0.9rem;
}

footer a {
    color: #2dd4bf;
    font-weight: 500;
    text-decoration: none;
    transition: all 0.2s ease;
}

footer a:hover {
    color: #14b8a6;
    text-decoration: underline;
}

.margin-div {
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 25px;
    background-color: rgba(20, 20, 28, 0.8);
    border-radius: 16px;
    border: 1px solid rgba(100, 100, 130, 0.2);
    transition: all 0.3s ease;
}

.research-output-container, .report-container {
    position: relative;
}

.research-output-container h2, .report-container h2 {
    color: #FFFFFF;
    font-weight: 600;
    font-size: 1.8rem;
    margin-bottom: 1.2rem;
    border-bottom: 2px solid rgba(20, 184, 166, 0.3);
    padding-bottom: 0.8rem;
    display: flex;
    align-items: center;
}

/* Explicitly override any icon that might be added before the h2 */
.research-output-container h2::before {
    content: none !important;
    display: none !important;
}

.report-container h2::before {
    content: '\f15c';
    font-family: 'Font Awesome 6 Free';
    font-weight: 900;
    margin-right: 12px;
    color: #14b8a6;
    font-size: 1.4rem;
}

.images_div {
    padding: 0 25px 0 25px;
}

.agent_response {
    background-color: #747d8c;
    margin: 10px;
    padding: 10px;
    border-radius: 12px;
    animation: fadeIn 0.5s ease;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

#output {
    height: 300px;
    overflow: auto;
    padding: 10px;
    margin-bottom: 10px;
    margin-top: 10px;
    border-radius: 12px;
}

#output::-webkit-scrollbar {
    width: 8px;
}

#output::-webkit-scrollbar-track {
    background: rgba(60, 60, 60, 0.7);
    border-radius: 10px;
}

#output::-webkit-scrollbar-thumb {
    background-color: #14b8a6;
    border-radius: 10px;
}

#reportContainer {
    font-family: Georgia, 'Times New Roman', serif;
    font-size: 18px !important;
    background-color: transparent;
    border: none;
    color: #fff;
    transition: all .3s ease-in-out;
    padding: 25px;
    border-radius: 12px;
}

#reportContainer h1,
#reportContainer h2,
#reportContainer h3 {
    color: #FFFFFF;
}

#reportContainer a {
    color: #2dd4bf;
    text-decoration: none;
}

#reportContainer a:hover {
    text-decoration: underline;
}

#reportContainer blockquote {
    border-left: 3px solid #14b8a6;
    padding-left: 15px;
    color: #B8B8B8;
    font-style: italic;
}

.tags-input {
    display: flex;
    flex-wrap: wrap;
    gap: 5px;
    border: 1px solid #ccc;
    padding: 5px;
    border-radius: 5px;
}

.tag {
    background-color: #14b8a6;
    color: white;
    padding: 5px 10px;
    border-radius: 3px;
    display: flex;
    align-items: center;
}

.tag .remove-tag {
    margin-left: 10px;
    cursor: pointer;
    font-weight: bold;
}

.tag-input {
    border: none;
    outline: none;
    flex-grow: 1;
}

.credits-bar {
    background: rgba(18, 18, 18, 0.8);
    backdrop-filter: blur(10px);
    padding: 8px 0;
    width: 100%;
    border-bottom: 1px solid rgba(20, 184, 166, 0.3);
    z-index: 100;
}

.top-credits {
    position: fixed;
    top: 0;
    left: 0;
}

/* Bottom credits bar (download bar) */
.sticky-downloads-bar.credits-bar {
    position: fixed;
    bottom: 0;
    left: 0;
}

.credits-content {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0 20px;
    max-width: 1400px;
    margin: 0 auto;
}

.credits-bar p {
    margin: 0;
    font-size: 0.9rem;
    color: #B8B8B8;
    text-align: center;
    flex: 1;
}

/* Top history button styling */
.top-history-button {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 6px 14px;
    border-radius: 20px;
    background: rgba(20, 184, 166, 0.2);
    color: #2dd4bf;
    font-weight: 500;
    font-size: 0.9rem;
    cursor: pointer;
    transition: all 0.2s ease;
    min-width: 100px;
    justify-content: center;
}

.top-history-button:hover {
    background: rgba(20, 184, 166, 0.3);
    transform: translateY(-2px);
}

.top-history-button i {
    font-size: 1rem;
}

/* Top WebSocket button styling */
.top-websocket-button {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 6px 14px;
    border-radius: 20px;
    background: rgba(20, 184, 166, 0.2);
    color: #2dd4bf;
    font-weight: 500;
    font-size: 0.9rem;
    cursor: pointer;
    transition: all 0.2s ease;
    min-width: 100px;
    justify-content: center;
    margin-left: 10px;
}

.top-websocket-button:hover {
    background: rgba(20, 184, 166, 0.3);
    transform: translateY(-2px);
}

.top-websocket-button i {
    font-size: 1rem;
}

/* Side feature panels */
.feature-panel {
    position: fixed;
    top: 80px;
    width: auto;
    max-width: 320px;
    z-index: 90;
    display: none;
    max-height: calc(100vh - 150px);
    overflow-y: visible;
    background: transparent;
    /* Add transition for smooth fading effect when panels appear */
    transition: opacity 0.5s ease;
}

.left-panel {
    left: 20px;
    right: auto;
}

.right-panel {
    right: 20px;
    left: auto;
}

.feature-panel::-webkit-scrollbar {
    width: 6px;
}

.feature-panel::-webkit-scrollbar-track {
    background: rgba(40, 40, 50, 0.7);
    border-radius: 10px;
}

.feature-panel::-webkit-scrollbar-thumb {
    background-color: #14b8a6;
    border-radius: 10px;
}

.feature-card {
    background: rgba(20, 20, 30, 0.4);
    backdrop-filter: blur(5px);
    border-radius: 16px;
    padding: 25px;
    margin-bottom: 20px;
    border: 1px solid rgba(20, 184, 166, 0.1);
    border-left: 3px solid;
    transition: all 0.3s ease;
    cursor: pointer;
    min-height: 160px;
    width: 100%;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.08);
    position: relative;
    overflow: hidden;
}

.feature-card::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: radial-gradient(circle at top right, rgba(25, 25, 35, 0.3), transparent 70%);
    z-index: -1;
    opacity: 0.7;
    border-radius: inherit;
    pointer-events: none;
}

.feature-card:hover {
    transform: translateY(-5px);
    background: rgba(30, 30, 40, 0.6);
    box-shadow: 0 12px 25px rgba(0, 0, 0, 0.15);
    border-color: rgba(20, 184, 166, 0.25);
}

/* Adjust card colors to match background better */
.feature-card.primary {
    border-left-color: rgba(20, 184, 166, 0.7);
}

.feature-card.success {
    border-left-color: rgba(78, 221, 152, 0.7);
}

.feature-card.info {
    border-left-color: rgba(20, 184, 166, 0.7);
}

.feature-card.warning {
    border-left-color: rgba(237, 189, 78, 0.7);
}

.feature-card.danger {
    border-left-color: rgba(237, 78, 78, 0.7);
}

.feature-icon {
    display: inline-flex;
    align-items: center;
    justify-content: center;
    width: 60px;
    height: 60px;
    border-radius: 15px;
    margin-bottom: 18px;
    font-size: 26px;
    color: #FFF;
    position: relative;
}

.feature-icon::after {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(255, 255, 255, 0.1);
    border-radius: 15px;
    transform: scale(0);
    transition: transform 0.3s ease;
}

.feature-card:hover .feature-icon::after {
    transform: scale(1);
}

.feature-card.primary .feature-icon {
    background: linear-gradient(135deg, #14b8a6, #0d9488);
}

.feature-card.success .feature-icon {
    background: linear-gradient(135deg, #4EDD98, #2CBF7B);
}

.feature-card.info .feature-icon {
    background: linear-gradient(135deg, #14b8a6, #0d9488);
}

.feature-card.warning .feature-icon {
    background: linear-gradient(135deg, #EDBD4E, #BF962C);
}

.feature-card.danger .feature-icon {
    background: linear-gradient(135deg, #ED4E4E, #BF2C2C);
}

.feature-title {
    font-weight: 600;
    font-size: 20px;
    margin-bottom: 12px;
    color: #FFF;
}

.feature-text {
    color: #CCC;
    font-size: 15px;
    line-height: 1.6;
}

.feature-card::after {
    content: '';
    position: absolute;
    top: 0;
    right: 0;
    width: 100px;
    height: 100px;
    background: linear-gradient(135deg, transparent, rgba(255, 255, 255, 0.05));
    border-radius: 50%;
    transform: translate(50%, -50%);
    z-index: -1;
}

/* Animation for feature cards */
@keyframes fadeInUp {
    from {
        opacity: 0;
        transform: translateY(20px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@keyframes fadeInLeft {
    from {
        opacity: 0;
        transform: translateX(-30px);
    }
    to {
        opacity: 1;
        transform: translateX(0);
    }
}

@keyframes fadeInRight {
    from {
        opacity: 0;
        transform: translateX(30px);
    }
    to {
        opacity: 1;
        transform: translateX(0);
    }
}

.left-panel .feature-card {
    animation: fadeInLeft 0.6s ease forwards;
    opacity: 0;
}

.right-panel .feature-card {
    animation: fadeInRight 0.6s ease forwards;
    opacity: 0;
}

.feature-card:nth-child(1) {
    animation-delay: 0.1s;
}

.feature-card:nth-child(2) {
    animation-delay: 0.25s;
}

.feature-card:nth-child(3) {
    animation-delay: 0.4s;
}

/* Highlight connection between side panels and form elements */
.highlight-connection {
    transition: box-shadow 0.5s ease;
    position: relative;
    z-index: 1;
}

/* Button glow effect for feature cards interaction */
.highlight-glow-button {
    box-shadow: 0 0 25px 10px rgba(20, 184, 166, 0.7) !important;
    border-color: rgba(20, 184, 166, 0.9) !important;
    transform: translateY(-3px) !important;
    transition: all 0.3s ease !important;
}

/* Intensify highlight glows for feature-panel interactions */
.highlight-glow-container {
    box-shadow: 0 0 30px 15px rgba(20, 184, 166, 0.6) !important;
    border: 1px solid rgba(20, 184, 166, 0.8) !important;
}
.highlight-glow-report_source,
.highlight-glow-query_domains {
    box-shadow: 0 0 20px 10px rgba(78, 221, 152, 0.7) !important;
}
.highlight-glow-tone {
    box-shadow: 0 0 20px 10px rgba(20, 184, 166, 0.7) !important;
    border: 1px solid rgba(20, 184, 166, 0.8) !important;
}

/* Ensure subtle transitions for all glows */
.highlight-connection,
.highlight-glow-container,
.highlight-glow-report_source,
.highlight-glow-query_domains,
.highlight-glow-tone {
    transition: box-shadow 0.5s ease;
}

/* Scroll to bottom button */
.scroll-to-bottom {
    display: none !important;
}

/* Responsive adjustments */
@media (min-width: 1400px) {
    .feature-panel {
        display: block;
    }

    .container {
        max-width: 900px !important;
    }
}

/* Medium devices (tablets, less than 1400px) */
@media (max-width: 1399px) {
    .feature-panel {
        display: none !important;
    }
}

@media (max-width: 991px) {
    .landing h1 {
        font-size: 2.5rem;
    }

    .landing p {
        font-size: 1.2rem;
    }

    .landing {
        margin-top: 50px;
    }

    .container {
        padding: 20px;
    }

    .btn-primary, .btn-secondary, .btn-action {
        padding: 10px 20px;
        margin-bottom: 10px;
    }

    #selectedImagesContainer img {
        width: 140px;
        height: 140px;
    }

    .scroll-to-bottom {
        width: 60px;
        height: 60px;
        bottom: 70px;
    }

    .credits-content {
        flex-wrap: wrap;
        justify-content: center;
    }

    .credits-bar p {
        width: 100%;
        text-align: center;
        margin-bottom: 5px;
    }

    .top-history-button,
    .top-websocket-button {
        margin: 5px;
    }

    .sticky-downloads-bar {
        padding: 8px 10px;
        bottom: 50px;
    }

    .download-option-btn {
        padding: 6px 10px;
        font-size: 0.85rem;
    }

    .history-panel {
        width: 100%;
    }

    .history-panel-toggle {
        right: 20px;
        bottom: 120px;
        top: auto;
    }

    .download-panel {
        position: fixed;
        top: auto;
        left: 0;
        right: 0;
        bottom: 60px;
        width: 100%;
        transform: none;
        padding: 0 15px;
        z-index: 99;
    }

    .download-panel-left, .download-panel-right {
        transform: none;
        left: 0;
        right: 0;
        margin: 0 auto;
        max-width: 300px;
    }

    .download-panel-right {
        bottom: 180px;
    }

    .download-card {
        margin-bottom: 10px;
    }

    .download-options {
        flex-direction: column;
        align-items: stretch;
        padding: 0 20px;
    }

    footer {
        padding: 10px 0;
    }

    .feature-panel {
        width: 100%;
        max-width: none;
        left: 0;
        right: 0;
        padding: 0 20px;
    }

    .feature-card {
        padding: 18px;
        min-height: auto;
    }
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

/* Modern spinner styles */
.modern-spinner {
    display: inline-block;
    width: 24px;
    height: 24px;
    border-radius: 50%;
    border: 3px solid rgba(20, 184, 166, 0.3);
    border-top-color: #14b8a6;
    vertical-align: middle;
    margin-right: 12px;
}

/* Only apply spinning animation when the spinning class is added */
.modern-spinner.spinning {
    animation: spin 2s linear infinite;
}

/* Spinner in Research Progress title */
.research-output-container h2::before {
    content: '\f110';
    font-family: 'Font Awesome 6 Free';
    font-weight: 900;
    margin-right: 12px;
    color: #14b8a6;
    font-size: 1.4rem;
}

/* Add the styles from index.html */
.avatar {
    width: 100px;
    height: 100px;
    border-radius: 10px;
    border: 2px solid #14b8a6;
    box-shadow: 0 0 15px rgba(20, 184, 166, 0.5);
}

.agent-name {
    text-align: center;
    font-weight: 600;
}

.agent-item {
    display: flex;
    flex-direction: column;
    align-items: center;
    margin-bottom: 20px;
}

/* Hover effect to make panels reappear */
.feature-panel:hover {
    --panel-shift: 0px !important;
    --panel-opacity: 1 !important;
    --panel-rotate: 0deg !important;
    --panel-scale: 1 !important;
    box-shadow: none;
    transition: transform 0.5s cubic-bezier(0.34, 1.56, 0.64, 1), opacity 0.5s ease, box-shadow 0.5s ease;
}

/* Add 3D shading effect to the panels */
.feature-panel::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: none;
    z-index: -1;
    opacity: var(--panel-depth, 0.5);
    transition: opacity 0.4s ease;
    pointer-events: none;
}

.feature-panel:hover::before {
    opacity: 0;
}

/* Add a subtle corner glow on hover */
.feature-card::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: radial-gradient(circle at top right, rgba(25, 25, 35, 0.15), transparent 70%);
    opacity: 0.5;
    transition: opacity 0.3s ease;
    pointer-events: none;
    z-index: -1;
}

.feature-card::after {
    content: '';
    position: absolute;
    top: 0;
    right: 0;
    width: 100%;
    height: 100%;
    background: radial-gradient(circle at top right, transparent, rgba(10, 10, 15, 0.1) 70%);
    opacity: 1;
    z-index: -2;
}

/* Conversation History Panel */
.history-panel {
    position: fixed;
    top: 40px;
    right: 0;
    width: 320px;
    height: calc(100vh - 40px);
    background: rgba(18, 18, 24, 0.95);
    backdrop-filter: blur(10px);
    border-left: 1px solid rgba(20, 184, 166, 0.3);
    z-index: 1000;
    display: flex;
    flex-direction: column;
    transform: translateX(100%);
    transition: transform 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);
    box-shadow: -5px 0 25px rgba(0, 0, 0, 0.4);
}

.history-panel.open {
    transform: translateX(0);
}

.history-panel-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 20px;
    border-bottom: 1px solid rgba(20, 184, 166, 0.3);
}

.history-panel-header h3 {
    margin: 0;
    font-size: 1.2rem;
    color: #FFFFFF;
    display: flex;
    align-items: center;
}

.history-panel-header h3 i {
    margin-right: 10px;
    color: #14b8a6;
}

.history-panel-actions {
    display: flex;
}

.history-action-btn {
    background: transparent;
    border: none;
    color: #E4E4E4;
    cursor: pointer;
    width: 32px;
    height: 32px;
    border-radius: 50%;
    display: flex;
    justify-content: center;
    align-items: center;
    transition: all 0.2s ease;
}

.history-action-btn:hover {
    background: rgba(20, 184, 166, 0.2);
    color: #FFFFFF;
}

.history-panel-search, .history-panel-filters {
    padding: 15px;
    border-bottom: 1px solid rgba(20, 184, 166, 0.2);
    display: flex;
    align-items: center;
}

.history-panel-search input {
    flex: 1;
    background: rgba(30, 30, 40, 0.8);
    border: 1px solid rgba(20, 184, 166, 0.3);
    border-radius: 20px;
    padding: 8px 15px;
    color: #E4E4E4;
    font-size: 0.9rem;
}

.history-panel-search button {
    margin-left: 8px;
}

.history-panel-filters select {
    flex: 1;
    background: rgba(30, 30, 40, 0.8);
    border: 1px solid rgba(20, 184, 166, 0.3);
    border-radius: 20px;
    padding: 8px 15px;
    color: #E4E4E4;
    font-size: 0.9rem;
    appearance: none;
    background-image: url("data:image/svg+xml;charset=UTF-8,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='none' stroke='%2314b8a6' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3e%3cpolyline points='6 9 12 15 18 9'%3e%3c/polyline%3e%3c/svg%3e");
    background-repeat: no-repeat;
    background-position: right 10px center;
    background-size: 15px;
}

.history-panel-entries {
    flex: 1;
    overflow-y: auto;
    padding: 15px;
    display: flex;
    flex-direction: column;
    gap: 12px;
    scrollbar-width: thin;
    scrollbar-color: #14b8a6 rgba(40, 40, 50, 0.7);
}

.history-panel-entries::-webkit-scrollbar {
    width: 6px;
}

.history-panel-entries::-webkit-scrollbar-track {
    background: rgba(40, 40, 50, 0.7);
    border-radius: 10px;
}

.history-panel-entries::-webkit-scrollbar-thumb {
    background-color: #14b8a6;
    border-radius: 10px;
}

.history-entry {
    background: rgba(30, 30, 40, 0.6);
    border-radius: 10px;
    padding: 15px;
    border-left: 3px solid #14b8a6;
    transition: all 0.2s ease;
    cursor: pointer;
    animation: fadeInRight 0.3s ease forwards;
    opacity: 0;
    transform: translateX(20px);
}

@keyframes fadeInRight {
    to {
        opacity: 1;
        transform: translateX(0);
    }
}

.history-entry:hover {
    background: rgba(40, 40, 50, 0.8);
    transform: translateY(-2px);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.2);
}

.history-entry-header {
    display: flex;
    justify-content: space-between;
    align-items: flex-start;
    margin-bottom: 10px;
}

.history-entry-title {
    font-weight: 600;
    color: #FFFFFF;
    font-size: 0.95rem;
    margin: 0;
    max-width: 80%;
    white-space: nowrap;
    overflow: hidden;
    text-overflow: ellipsis;
}

.history-entry-timestamp {
    font-size: 0.75rem;
    color: #B8B8B8;
}

.history-entry-preview {
    font-size: 0.85rem;
    color: #CCCCCC;
    margin: 0 0 10px 0;
    display: -webkit-box;
    -webkit-line-clamp: 2;
    -webkit-box-orient: vertical;
    overflow: hidden;
    text-overflow: ellipsis;
}

.history-entry-actions {
    display: flex;
    gap: 8px;
    opacity: 0;
    height: 0;
    overflow: hidden;
    transition: all 0.2s ease;
}

.history-entry:hover .history-entry-actions {
    opacity: 1;
    height: 32px;
    margin-top: 10px;
}

.history-entry-action {
    background: rgba(40, 40, 50, 0.8);
    border: 1px solid rgba(20, 184, 166, 0.3);
    border-radius: 6px;
    padding: 5px 10px;
    font-size: 0.75rem;
    color: #E4E4E4;
    display: flex;
    align-items: center;
    gap: 5px;
    transition: all 0.2s ease;
    cursor: pointer;
}

.history-entry-action:hover {
    background: rgba(20, 184, 166, 0.2);
    border-color: #14b8a6;
    color: #FFFFFF;
}

.history-entry-action i {
    font-size: 0.8rem;
}

.history-entry-format {
    display: flex;
    gap: 5px;
    margin-top: 5px;
}

.history-entry-format span {
    display: flex;
    align-items: center;
    gap: 3px;
    font-size: 0.7rem;
    color: #B8B8B8;
    background: rgba(30, 30, 40, 0.7);
    padding: 3px 6px;
    border-radius: 4px;
}

.history-entry-format span i {
    color: #14b8a6;
}

.history-entry-details {
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.3s ease;
}

.history-entry.expanded .history-entry-details {
    max-height: 200px;
    margin-top: 10px;
}

.history-entry-detail {
    font-size: 0.8rem;
    color: #B8B8B8;
    margin: 3px 0;
}

.history-panel-toggle {
    position: fixed;
    top: 100px;
    right: 30px;
    left: auto;
    width: 70px;
    height: 70px;
    background: linear-gradient(135deg, #14b8a6, #0d9488);
    border-radius: 15px;
    display: flex;
    flex-direction: column;
    align-items: center;
    justify-content: center;
    color: #FFF;
    box-shadow: 0 8px 20px rgba(0, 0, 0, 0.3);
    cursor: pointer;
    transition: all 0.3s ease;
    z-index: 999;
}

.history-panel-toggle:hover {
    transform: translateY(-3px);
    box-shadow: 0 12px 25px rgba(0, 0, 0, 0.4);
}

.history-panel-toggle i {
    font-size: 26px;
    margin-bottom: 5px;
}

.history-panel-toggle::after {
    content: 'History';
    font-size: 12px;
    font-weight: 500;
}

@media (max-width: 768px) {
    .history-panel {
        width: 100%;
    }

    .history-panel-toggle {
        right: 20px;
        bottom: 120px;
        top: auto;
    }
}

@media (min-width: 769px) and (max-width: 1200px) {
    .history-panel {
        width: 280px;
    }
}

.sticky-downloads-bar {
    position: fixed !important;
    bottom: 0 !important;
    left: 0 !important;
    width: 100% !important;
    height: auto !important;
    background: rgba(18, 18, 18, 0.8) !important;
    backdrop-filter: blur(10px) !important;
    z-index: 9999 !important;
    padding: 8px 0 !important;
    display: none !important;
    justify-content: center !important;
    border-top: 1px solid rgba(20, 184, 166, 0.3) !important;
    box-shadow: 0 -5px 15px rgba(0, 0, 0, 0.3) !important;
    transform: none !important;
    margin: 0 !important;
    max-height: none !important;
    min-height: 40px !important;
    opacity: 1 !important;
    pointer-events: auto !important;
    visibility: visible !important;
}

.sticky-downloads-bar.visible {
    display: flex !important;
}

.download-options {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0 20px;
    max-width: 1400px;
    margin: 0 auto;
    gap: 10px;
}

.download-buttons-container {
    display: flex;
    align-items: center;
    gap: 10px;
}

.download-option-btn {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 6px 14px;
    border-radius: 20px;
    background: rgba(20, 184, 166, 0.2);
    color: #2dd4bf;
    font-weight: 500;
    font-size: 0.9rem;
    cursor: pointer;
    transition: all 0.2s ease;
    text-decoration: none;
}

.download-option-btn:hover {
    background: rgba(20, 184, 166, 0.3);
    transform: translateY(-2px);
    text-decoration: none;
    color: #FFFFFF;
}

.download-option-btn i {
    font-size: 1rem;
}

.download-option-btn.disabled {
    opacity: 0.5;
    cursor: not-allowed;
    pointer-events: none;
}

@media (min-width: 769px) and (max-width: 1200px) {
    .feature-panel {
        width: 280px;
    }

    .feature-card {
        padding: 20px;
        min-height: 150px;
    }

    .feature-icon {
        width: 55px;
        height: 55px;
        font-size: 24px;
    }
}

.credits-bar a {
    color: #2dd4bf;
    font-weight: 500;
    text-decoration: none;
}

.credits-bar a:hover {
    text-decoration: underline;
}

.websocket-panel {
    position: fixed;
    top: 40px;
    left: 0;
    width: 300px;
    height: auto;
    max-height: calc(100vh - 80px);
    background: rgba(18, 18, 24, 0.95);
    backdrop-filter: blur(10px);
    border-right: 1px solid rgba(20, 184, 166, 0.3);
    z-index: 1000;
    display: flex;
    flex-direction: column;
    transform: translateX(-100%);
    transition: transform 0.3s cubic-bezier(0.175, 0.885, 0.32, 1.275);
    box-shadow: 5px 0 25px rgba(0, 0, 0, 0.4);
    overflow-y: auto;
}

.websocket-panel.open {
    transform: translateX(0);
}

.websocket-panel-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 15px;
    border-bottom: 1px solid rgba(20, 184, 166, 0.3);
}

.websocket-panel-header h3 {
    margin: 0;
    font-size: 1.2rem;
    color: #FFFFFF;
    display: flex;
    align-items: center;
}

.websocket-panel-header h3 i {
    margin-right: 10px;
    color: #14b8a6;
}

.websocket-panel-actions {
    display: flex;
}

.websocket-action-btn {
    background: transparent;
    border: none;
    color: #E4E4E4;
    cursor: pointer;
    width: 32px;
    height: 32px;
    border-radius: 50%;
    display: flex;
    justify-content: center;
    align-items: center;
    transition: all 0.2s ease;
}

.websocket-action-btn:hover {
    background: rgba(20, 184, 166, 0.2);
    color: #FFFFFF;
}

.websocket-status {
    padding: 15px;
    overflow-y: auto;
}

.status-item {
    display: flex;
    align-items: center;
    margin-bottom: 15px;
    font-size: 0.9rem;
    position: relative;
}

.status-label {
    color: #B8B8B8;
    margin-right: 10px;
    width: 130px;
    flex-shrink: 0;
}

.status-value {
    color: #FFFFFF;
    font-weight: 500;
}

.status-indicator {
    width: 10px;
    height: 10px;
    border-radius: 50%;
    margin-left: 10px;
    background-color: #777;
}

.status-indicator.connected {
    background-color: #4EDD98;
    box-shadow: 0 0 10px rgba(78, 221, 152, 0.7);
}

.status-indicator.connecting {
    background-color: #EDBD4E;
    box-shadow: 0 0 10px rgba(237, 189, 78, 0.7);
}

.status-indicator.disconnected {
    background-color: #ED4E4E;
    box-shadow: 0 0 10px rgba(237, 78, 78, 0.7);
}

.status-divider {
    height: 1px;
    background: rgba(20, 184, 166, 0.2);
    margin: 15px 0 20px;
}

@media (max-width: 768px) {
    .websocket-panel {
        width: 100%;
    }

    .websocket-panel-toggle-btn {
        left: 20px;
        top: 80px;
        width: 50px;
        height: 50px;
    }

    .websocket-panel-toggle-btn i {
        font-size: 22px;
    }
}

@media (max-width: 768px) {
    .sticky-downloads-bar .credits-content {
        flex-direction: column;
        padding: 5px 10px;
    }

    .sticky-downloads-bar p {
        margin-bottom: 5px;
        text-align: center;
    }

    .download-buttons-container {
        flex-wrap: wrap;
        justify-content: center;
    }

    .download-option-btn {
        margin: 2px;
        font-size: 0.8rem;
        padding: 4px 10px;
    }
}

.chat-container {
    margin-top: 40px;
    padding: 20px;
    border-radius: 10px;
    background-color: rgba(20, 20, 28, 0.7);
    border: 1px solid rgba(255, 255, 255, 0.1);
}

.chat-messages {
    max-height: 400px;
    overflow-y: auto;
    margin-bottom: 20px;
    padding: 15px;
    border-radius: 8px;
    background-color: transparent;
    scrollbar-width: thin;
    scrollbar-color: #14b8a6 rgba(40, 40, 50, 0.7);
}

.chat-messages::-webkit-scrollbar {
    width: 6px;
}

.chat-messages::-webkit-scrollbar-track {
    background: rgba(40, 40, 50, 0.7);
    border-radius: 10px;
}

.chat-messages::-webkit-scrollbar-thumb {
    background-color: #14b8a6;
    border-radius: 10px;
}

.chat-message {
    margin-bottom: 15px;
    padding: 12px 18px;
    border-radius: 10px;
    max-width: 80%;
    position: relative;
}

.user-message {
    background-color: #4a4a5e;
    color: white;
    margin-left: auto;
    border-bottom-right-radius: 0;
}

.ai-message {
    background-color: #14b8a6;
    color: white;
    margin-right: auto;
    border-bottom-left-radius: 0;
}

.chat-input-container {
    display: flex;
    gap: 10px;
}

.chat-input {
    flex-grow: 1;
    border-radius: 20px;
    padding: 10px 15px;
    background-color: #2f3542;
    color: white;
    border: 1px solid rgba(255, 255, 255, 0.2);
    font-weight: 400;
}

.chat-input::placeholder {
    color: rgba(255, 255, 255, 0.7);
}

.chat-input:focus {
    background-color: #3b4252;
    border-color: #14b8a6;
    box-shadow: 0 0 0 0.2rem rgba(20, 184, 166, 0.25);
    color: white;
    font-weight: 500;
}

.chat-timestamp {
    font-size: 0.7rem;
    color: rgba(255, 255, 255, 0.5);
    margin-top: 5px;
    text-align: right;
}

#sendChatBtn {
    border-radius: 20px;
    padding: 10px 20px;
}

.chat-loading {
    display: flex;
    align-items: center;
    gap: 5px;
}

.chat-dot {
    width: 8px;
    height: 8px;
    border-radius: 50%;
    background-color: white;
    opacity: 0.7;
    animation: dot-pulse 1.5s infinite;
}

.chat-dot:nth-child(2) {
    animation-delay: 0.2s;
}

.chat-dot:nth-child(3) {
    animation-delay: 0.4s;
}

@keyframes dot-pulse {
    0%, 100% { transform: scale(0.7); opacity: 0.5; }
    50% { transform: scale(1); opacity: 1; }
}

.highlight-glow-download-button {
    box-shadow: 0 0 15px #14b8a6, 0 0 25px rgba(20, 184, 166, 0.5) !important;
    transform: translateY(-2px);
    transition: all 0.3s ease;
    border-color: #14b8a6 !important;
}

.report-actions {
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
    margin-bottom: 15px;
    padding: 15px;
    background: rgba(20, 20, 28, 0.4);
    border-radius: 8px;
    border: 1px solid rgba(100, 100, 130, 0.15);
}

.report-action-btn {
    display: flex;
    align-items: center;
    gap: 8px;
    padding: 6px 15px;
    border-radius: 6px;
    background: rgba(35, 35, 45, 0.8);
    color: #E4E4E4;
    font-weight: 500;
    font-size: 0.9rem;
    cursor: pointer;
    transition: all 0.2s ease;
    text-decoration: none;
    border: 1px solid rgba(20, 184, 166, 0.2);
}

.report-action-btn:hover {
    background: rgba(20, 184, 166, 0.2);
    transform: translateY(-2px);
    text-decoration: none;
    color: #FFFFFF;
    border-color: rgba(20, 184, 166, 0.5);
}

.report-action-btn i {
    font-size: 1rem;
    color: #14b8a6;
}

.report-action-btn.disabled {
    opacity: 0.5;
    cursor: not-allowed;
    pointer-events: none;
}

.expand-button {
    background: transparent;
    border: none;
    color: #14b8a6;
    font-size: 0.85rem;
    cursor: pointer;
    padding: 5px;
    margin-right: 0;
    margin-left: auto;
    border-radius: 4px;
    transition: all 0.2s ease;
    position: absolute;
    right: 10px;
    top: 50%;
    transform: translateY(-50%);
    z-index: 100;
}

.expand-button:hover {
    background: rgba(20, 184, 166, 0.15);
    color: #2dd4bf;
}

.expand-button:focus {
    outline: none;
}

.expanded-view {
    --modal-top: 50%;
    --modal-height: 85vh;

    position: fixed !important;
    top: var(--modal-top) !important;
    left: 50% !important;
    height: var(--modal-height) !important;
    width: 90vw !important;
    max-width: 1200px !important;
    transform: translate(-50%, -50%) !important;
    z-index: 9000 !important;
    margin: 0 !important;
    border-radius: 12px !important;
    padding: 20px !important;
    overflow-y: auto !important;
    background-color: rgba(18, 18, 24, 0.98) !important;
    box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5) !important;
    border: 1px solid rgba(20, 184, 166, 0.3) !important;
    transition: all 0.3s cubic-bezier(0.25, 0.8, 0.25, 1) !important;
}

@keyframes modalAppear {
    from {
        opacity: 0.8;
        transform: translateX(-50%) scale(0.95);
    }
    to {
        opacity: 1;
        transform: translateX(-50%) scale(1);
    }
}

.research-output-container h2,
.report-container h2,
#chatContainer h2 {
    position: relative;
    padding-right: 50px;
    display: flex;
    align-items: center;
}

#reportContainer,
#output,
#chatMessages {
    transition: all 0.3s ease;
}

.report-container,
.research-output-container,
#chatContainer {
    transition: all 0.3s ease;
    position: relative;
}

.expand-button {
    z-index: 100;
    top: 0;
    right: 0;
    transform: none;
    margin: 10px;
    position: absolute;
}

@media (max-width: 768px) {
    .expanded-view {
        top: 50% !important;
        left: 50% !important;
        width: 95vw !important;
        height: 90vh !important;
        transform: translate(-50%, -50%) !important;
        border-radius: 8px !important;
        padding: 15px !important;
    }

    .expanded-view #output,
    .expanded-view #reportContainer,
    .expanded-view #chatMessages {
        max-height: 70vh !important;
        width: 100% !important;
        padding: 15px !important;
    }
}

#voiceInputBtn {
    margin-right: 8px;
    border-radius: 20px;
    display: flex;
    justify-content: center;
    align-items: center;
    background: #EDBF66 !important;
    border: 1px solid #D8A854 !important;
    padding: 10px;
    font-size: 1.1rem;
    line-height: 1;
    transition: all 0.2s ease;
}

#voiceInputBtn i {
    font-size: 1.2rem;
    color: #FFFFFF;
    transition: all 0.2s ease;
}

#voiceInputBtn:hover {
    background: #D8A854 !important;
    border-color: #C09040 !important;
}

#voiceInputBtn.listening {
    background: rgba(220, 53, 69, 0.2) !important;
    border-color: rgba(220, 53, 69, 0.5) !important;
    animation: pulse 1.5s infinite;
}

@keyframes pulse {
    0% {
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.4);
    }
    70% {
        box-shadow: 0 0 0 10px rgba(220, 53, 69, 0);
    }
    100% {
        box-shadow: 0 0 0 0 rgba(220, 53, 69, 0);
    }
}

.toast-notification {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background: rgba(40, 40, 50, 0.95);
    color: #FFF;
    padding: 12px 25px;
    border-radius: 8px;
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
    z-index: 10000;
    font-size: 14px;
    opacity: 0;
    transition: opacity 0.3s, transform 0.3s;
    pointer-events: none;
    border-left: 3px solid #14b8a6;
}

.toast-notification.show {
    opacity: 1;
    transform: translate(-50%, -10px);
}

.history-actions-container {
    display: flex;
    gap: 8px;
    margin-left: 10px;
}

.history-action-btn {
    width: 32px;
    height: 32px;
    display: flex;
    align-items: center;
    justify-content: center;
    border-radius: 50%;
    background: rgba(40, 40, 50, 0.8);
    border: 1px solid rgba(20, 184, 166, 0.3);
    color: #E4E4E4;
    cursor: pointer;
    transition: all 0.2s ease;
    padding: 0;
    font-size: 14px;
}

.history-action-btn:hover {
    background: rgba(20, 184, 166, 0.2);
    border-color: #14b8a6;
    transform: translateY(-2px);
}

#historyFileInput {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    border: 0;
}

#task {
    min-height: 38px;
    overflow-y: hidden;
    resize: none;
}

.expanded-view h2 {
    position: relative;
    padding-right: 40px;
    margin-bottom: 20px !important;
}

.expanded-view .expand-button i {
    /* transform: rotate(180deg); */ /* Removed to prevent icon confusion */
}

.expanded-view #reportContainer {
    padding: 20px !important;
    font-size: 1.1rem !important;
    max-height: 75vh !important;
    overflow-y: auto !important;
    width: 100% !important;
    border: none !important;
}

.expanded-view #chatMessages {
    max-height: 60vh !important;
    overflow-y: auto !important;
    width: 100% !important;
}

.expanded-view #output {
    height: 60vh !important;
    max-height: 60vh !important;
    overflow-y: auto !important;
    width: 100% !important;
    border: none !important;
    background-color: transparent !important;
}

.expanded-view {
    animation: modalAppear 0.3s cubic-bezier(0.25, 0.8, 0.25, 1) forwards;
}

@media (max-width: 768px) {
    .expanded-view {
        top: var(--modal-top) !important;
        left: 50% !important;
        height: var(--modal-height) !important;
        width: 95vw !important;
        transform: translateX(-50%) !important;
        border-radius: 8px !important;
        padding: 15px !important;
    }

    .expanded-view #output,
    .expanded-view #reportContainer,
    .expanded-view #chatMessages {
        width: 100% !important;
        padding: 15px !important;
    }
}

.landing .btn {
    display: block;
    margin: 2rem auto;
    max-width: 220px;
}

#researchForm input[type="submit"] {
    display: block;
    margin: 2rem auto;
    font-size: 1.2rem;
    padding: 15px 40px;
    min-width: 220px;
    box-shadow: 0 6px 18px rgba(20, 184, 166, 0.4);
}

#researchForm input[type="submit"]:hover {
    transform: translateY(-4px);
    box-shadow: 0 12px 25px rgba(20, 184, 166, 0.5);
}

@media (max-width: 768px) {
    .btn-primary, .btn-secondary {
        font-size: 1rem;
        padding: 10px 25px;
        min-width: 160px;
    }
    
    #researchForm input[type="submit"] {
        padding: 12px 30px;
        font-size: 1.1rem;
        min-width: 180px;
    }
}

/* Image Dialog Styles */
.image-dialog {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.75); /* Darker backdrop for better focus */
    display: flex;
    flex-direction: column; /* Align button below image */
    justify-content: center;
    align-items: center;
    z-index: 10000; /* Ensure it's on top of everything */
    padding: 20px; /* Add some padding around */
    box-sizing: border-box; /* Include padding in width/height */
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s ease, visibility 0.3s ease;
}

.image-dialog.visible {
    opacity: 1;
    visibility: visible;
}

.image-dialog img {
    max-width: 90%; /* Responsive image width */
    max-height: 80%; /* Responsive image height */
    object-fit: contain; /* Ensure image aspect ratio is maintained */
    border-radius: 8px; /* Slightly rounded corners for the image */
    box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3); /* Subtle shadow for depth */
}

.image-dialog .close-btn {
    background: linear-gradient(to right, #0d9488, #14b8a6);
    color: white;
    border: none;
    padding: 10px 20px;
    border-radius: 20px;
    cursor: pointer;
    font-size: 1rem;
    font-weight: 600;
    margin-top: 20px; /* Space between image and button */
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(20, 184, 166, 0.3);
}

.image-dialog .close-btn:hover {
    opacity: 0.9;
    transform: translateY(-2px);
    box-shadow: 0 8px 20px rgba(20, 184, 166, 0.4);
}

/* Ensure the dialog is not selectable when hidden */
.image-dialog:not(.visible) {
    pointer-events: none;
}

/* MCP Configuration Styles */
.mcp-section {
    margin-bottom: 1rem;
}

.mcp-header {
    display: flex;
    align-items: center;
    gap: 10px;
    margin-bottom: 5px;
}

.mcp-header label {
    display: flex;
    align-items: center;
    gap: 8px;
    font-weight: 600;
    margin: 0;
}

.mcp-toggle {
    margin: 0;
}

.mcp-info-btn {
    background: none;
    border: none;
    color: #6c757d;
    cursor: pointer;
    padding: 4px;
    border-radius: 4px;
}

.mcp-info-btn:hover {
    color: #007bff;
    background-color: rgba(0, 123, 255, 0.1);
}

.mcp-config-section {
    margin-top: 15px;
    padding: 15px;
    border: 1px solid #e0e0e0;
    border-radius: 8px;
    background-color: #f8f9fa;
}

.mcp-presets {
    margin-bottom: 10px;
}

.preset-buttons {
    display: flex;
    gap: 10px;
    flex-wrap: wrap;
    margin-bottom: 5px;
}

.preset-btn {
    display: flex;
    align-items: center;
    gap: 5px;
    border: 1px solid #6c757d;
    color: #6c757d;
    background: white;
}

.preset-btn:hover {
    border-color: #007bff;
    color: #007bff;
    background: rgba(0, 123, 255, 0.05);
}

.preset-btn i {
    font-size: 0.9em;
}

.mcp-config-group {
    margin-bottom: 10px;
}

.mcp-config-textarea {
    font-family: 'Monaco', 'Consolas', 'Courier New', monospace;
    font-size: 13px;
    line-height: 1.4;
    resize: vertical;
    border: 1px solid #ccc;
    border-radius: 4px;
    padding: 10px;
    background-color: #f8f8f8;
}

.mcp-config-textarea:focus {
    border-color: #007bff;
    background-color: white;
    box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);
}

.mcp-config-textarea.invalid {
    border-color: #dc3545;
    background-color: #fff5f5;
}

.mcp-config-textarea.valid {
    border-color: #28a745;
    background-color: #f0fff4;
}

.mcp-config-status {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-top: 8px;
    margin-bottom: 5px;
}

.mcp-status-text {
    font-size: 0.875rem;
    font-weight: 500;
}

.mcp-status-text.valid {
    color: #28a745;
}

.mcp-status-text.invalid {
    color: #dc3545;
}

#mcpFormatBtn {
    font-size: 0.8rem;
    padding: 4px 8px;
}

#mcpExampleLink {
    color: #007bff;
    text-decoration: none;
}

#mcpExampleLink:hover {
    text-decoration: underline;
}

/* MCP Info Modal */
.mcp-info-modal {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.5);
    display: flex;
    justify-content: center;
    align-items: center;
    z-index: 1000;
    opacity: 0;
    visibility: hidden;
    transition: all 0.3s ease;
}

.mcp-info-modal.visible {
    opacity: 1;
    visibility: visible;
}

.mcp-info-content {
    background: rgba(0, 0, 0, 0.5);
    padding: 25px;
    border-radius: 12px;
    max-width: 600px;
    max-height: 80vh;
    overflow-y: auto;
    position: relative;
    margin: 20px;
}

.mcp-info-close {
    position: absolute;
    top: 15px;
    right: 15px;
    background: none;
    border: none;
    font-size: 1.5rem;
    cursor: pointer;
    color: #6c757d;
    width: 30px;
    height: 30px;
    display: flex;
    align-items: center;
    justify-content: center;
    border-radius: 50%;
}

.mcp-info-close:hover {
    background-color: #f8f9fa;
    color: #dc3545;
}

.mcp-info-content h3 {
    margin-top: 0;
    color: #333;
    border-bottom: 2px solid #007bff;
    padding-bottom: 10px;
}

.mcp-info-content p {
    line-height: 1.6;
    color: #555;
}

.mcp-info-content ul {
    padding-left: 20px;
    line-height: 1.6;
}

.mcp-info-content li {
    margin-bottom: 8px;
}

.mcp-info-content .highlight {
    color: #007bff;
    font-weight: 600;
}

@media (max-width: 768px) {
    .preset-buttons {
        flex-direction: column;
    }
    
    .preset-btn {
        justify-content: center;
    }
    
    .mcp-info-content {
        margin: 10px;
        padding: 20px;
    }
    
    .mcp-config-status {
        flex-direction: column;
        align-items: flex-start;
        gap: 8px;
    }
}



================================================
FILE: frontend/nextjs/README.md
================================================
# GPT Researcher UI

A React component library for integrating the GPT Researcher interface into your React applications. Take it for a test ride with the [GPTR React Starter Template](https://github.com/elishakay/gpt-researcher-react), or simply:

<div align="center" id="top">

<img src="https://github.com/assafelovic/gpt-researcher/assets/13554167/20af8286-b386-44a5-9a83-3be1365139c3" alt="Logo" width="80">

####

[![Website](https://img.shields.io/badge/Official%20Website-gptr.dev-teal?style=for-the-badge&logo=world&logoColor=white&color=0891b2)](https://gptr.dev)
[![Documentation](https://img.shields.io/badge/Documentation-DOCS-f472b6?logo=googledocs&logoColor=white&style=for-the-badge)](https://docs.gptr.dev)
[![Discord Follow](https://dcbadge.vercel.app/api/server/QgZXvJAccX?style=for-the-badge&theme=clean-inverted&?compact=true)](https://discord.gg/QgZXvJAccX)

[![PyPI version](https://img.shields.io/pypi/v/gpt-researcher?logo=pypi&logoColor=white&style=flat)](https://badge.fury.io/py/gpt-researcher)
![GitHub Release](https://img.shields.io/github/v/release/assafelovic/gpt-researcher?style=flat&logo=github)
[![Open In Colab](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=grey&color=yellow&label=%20&style=flat&logoSize=40)](https://colab.research.google.com/github/assafelovic/gpt-researcher/blob/master/docs/docs/examples/pip-run.ipynb)
[![Docker Image Version](https://img.shields.io/docker/v/elestio/gpt-researcher/latest?arch=amd64&style=flat&logo=docker&logoColor=white&color=1D63ED)](https://hub.docker.com/r/gptresearcher/gpt-researcher)

[English](README.md) | [ä¸­æ–‡](README-zh_CN.md) | [æ—¥æœ¬èª](README-ja_JP.md) | [í•œêµ­ì–´](README-ko_KR.md)

</div>

# ğŸ” GPT Researcher

**GPT Researcher is an open deep research agent designed for both web and local research on any given task.** 

The agent produces detailed, factual, and unbiased research reports with citations. GPT Researcher provides a full suite of customization options to create tailor made and domain specific research agents. Inspired by the recent [Plan-and-Solve](https://arxiv.org/abs/2305.04091) and [RAG](https://arxiv.org/abs/2005.11401) papers, GPT Researcher addresses misinformation, speed, determinism, and reliability by offering stable performance and increased speed through parallelized agent work.

**Our mission is to empower individuals and organizations with accurate, unbiased, and factual information through AI.**


## Installation

```bash
npm install gpt-researcher-ui
```

## Usage

```javascript
import React from 'react';
import { GPTResearcher } from 'gpt-researcher-ui';

function App() {
  return (
    <div className="App">
      <GPTResearcher 
        apiUrl="http://localhost:8000"
        defaultPrompt="What is quantum computing?"
        onResultsChange={(results) => console.log('Research results:', results)}
      />
    </div>
  );
}

export default App;
```

## Advanced Usage

```javascript
import React, { useState } from 'react';
import { GPTResearcher } from 'gpt-researcher-ui';

function App() {
  const [results, setResults] = useState([]);

  const handleResultsChange = (newResults) => {
    setResults(newResults);
    console.log('Research progress:', newResults);
  };

  return (
    <div className="App">
      <h1>My Research Application</h1>
      
      <GPTResearcher 
        apiUrl="http://localhost:8000"
        apiKey="your-api-key-if-needed"
        defaultPrompt="Explain the impact of quantum computing on cryptography"
        onResultsChange={handleResultsChange}
      />
      
      {/* You can use the results state elsewhere in your app */}
      <div className="results-summary">
        {results.length > 0 && (
          <p>Research in progress: {results.length} items processed</p>
        )}
      </div>
    </div>
  );
}

export default App;
```


================================================
FILE: frontend/nextjs/Dockerfile
================================================
###############################################
# 1) Dependencies layer
###############################################
FROM node:18.17.0-alpine AS deps
WORKDIR /app

# Copy only package manifest first for better layer caching
COPY package.json ./

# Install dependencies (no lock file present â€“ recommend adding one for reproducibility)
RUN npm install --legacy-peer-deps

###############################################
# 2) Builder layer â€“ builds Next.js (.next)
###############################################
FROM node:18.17.0-alpine AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .
# Build Next.js application (produces .next)
RUN npm run build \
	&& npm prune --production

###############################################
# 3) Runner layer â€“ production image serving Next.js
###############################################
FROM node:18.17.0-alpine AS runner
WORKDIR /app
ENV NODE_ENV=production

# Copy only what is required at runtime
COPY --from=builder /app/package.json ./
COPY --from=builder /app/next.config.mjs ./
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules

# Expose port (Next.js default)
EXPOSE 3000

# Start the Next.js production server (serves API routes too)
CMD ["npm", "run", "start"]



================================================
FILE: frontend/nextjs/Dockerfile.dev
================================================
FROM node:18.17.0-alpine
WORKDIR /app
COPY ./package.json ./
RUN npm install --legacy-peer-deps
COPY . .
CMD ["npm", "run", "dev"]


================================================
FILE: frontend/nextjs/next.config.mjs
================================================
import withPWAInit from "@ducanh2912/next-pwa";

/** @type {import('next').NextConfig} */
const nextConfig = {
  images: {
    remotePatterns: [
      {
        hostname: 'www.google.com',
      },
      {
        hostname: 'www.google-analytics.com',
      }
    ],
  },
};

const withPWA = withPWAInit({
  dest: "public",
  register: true,
  skipWaiting: true,
  disable: process.env.NODE_ENV === "development",
});

export default withPWA(nextConfig);



================================================
FILE: frontend/nextjs/package.json
================================================
{
  "name": "gpt-researcher-ui",
  "description": "GPT Researcher frontend as a React component",
  "version": "0.1.74",
  "main": "dist/index.js",
  "module": "dist/index.esm.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist",
    "styles/*",
    "app/globals.css",
    "components/Settings/App.css"
  ],
  "private": false,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "build:lib": "rollup -c",
    "build:types": "cp src/index.d.ts dist/",
    "dev:lib": "rollup -c -w"
  },
  "dependencies": {
    "@emotion/react": "^11.10.5",
    "@emotion/styled": "^11.10.5",
    "@langchain/langgraph-sdk": "^0.0.1-rc.12",
    "@mozilla/readability": "^0.5.0",
    "@next/third-parties": "^15.1.6",
    "axios": "^1.3.2",
    "date-fns": "^4.1.0",
    "eventsource-parser": "^1.1.2",
    "framer-motion": "^9.0.2",
    "geist": "^1.3.1",
    "next": "^14.2.28",
    "next-plausible": "^3.12.0",
    "react": "^18.0.0",
    "react-dom": "^18.0.0",
    "react-dropzone": "^14.2.3",
    "react-ga4": "^2.1.0",
    "react-hot-toast": "^2.4.1",
    "rehype-prism-plus": "^2.0.0",
    "remark": "^15.0.1",
    "remark-gfm": "^4.0.1",
    "remark-html": "^16.0.1",
    "remark-parse": "^11.0.0",
    "zod": "^3.0.0",
    "zod-to-json-schema": "^3.23.0"
  },
  "devDependencies": {
    "@babel/core": "^7.26.9",
    "@babel/plugin-syntax-flow": "^7.26.0",
    "@babel/plugin-transform-typescript": "^7.26.8",
    "@babel/preset-env": "^7.26.9",
    "@babel/preset-react": "^7.26.3",
    "@babel/preset-typescript": "^7.26.0",
    "@rollup/plugin-alias": "^5.1.1",
    "@rollup/plugin-babel": "^6.0.4",
    "@rollup/plugin-commonjs": "^28.0.2",
    "@rollup/plugin-json": "^6.1.0",
    "@rollup/plugin-node-resolve": "^16.0.0",
    "@rollup/plugin-replace": "^6.0.2",
    "@rollup/plugin-typescript": "^12.1.2",
    "@tailwindcss/typography": "^0.5.16",
    "@types/jsdom": "^21.1.6",
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "@types/uuid": "^10.0.0",
    "autoprefixer": "^10.4.20",
    "eslint": "^8",
    "eslint-config-next": "14.2.3",
    "postcss": "^8",
    "prettier": "^3.2.5",
    "prettier-plugin-tailwindcss": "^0.6.0",
    "react-ga4": "^2.1.0",
    "rollup": "^2.79.2",
    "rollup-plugin-peer-deps-external": "^2.2.4",
    "rollup-plugin-postcss": "^4.0.2",
    "rollup-plugin-terser": "^7.0.2",
    "rollup-plugin-typescript2": "^0.31.2",
    "tailwindcss": "^3.4.1",
    "typescript": "^5",
    "@ducanh2912/next-pwa": "^10.0.1"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/assafelovic/gpt-researcher.git"
  },
  "keywords": [
    "gpt",
    "researcher",
    "ai",
    "react",
    "nextjs"
  ],
  "author": "GPT Researcher Team",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/assafelovic/gpt-researcher/issues"
  },
  "homepage": "https://github.com/assafelovic/gpt-researcher#readme"
}



================================================
FILE: frontend/nextjs/package.lib.json
================================================
{
  "name": "gpt-researcher-ui",
  "description": "GPT Researcher frontend as a React component",
  "version": "0.1.74",
  "main": "dist/index.js",
  "module": "dist/index.esm.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist",
    "styles/*",
    "app/globals.css",
    "components/Settings/App.css"
  ],
  "private": false,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint",
    "prepare": "npm run build:lib",
    "build:lib": "rollup -c",
    "build:types": "cp src/index.d.ts dist/",
    "dev:lib": "rollup -c -w"
  },
  "peerDependencies": {
    "next": "^14.2.0",
    "react": "^18.0.0",
    "react-dom": "^18.0.0"
  },
  "dependencies": {
    "@emotion/react": "^11.10.5",
    "@emotion/styled": "^11.10.5",
    "@mozilla/readability": "^0.5.0",
    "axios": "^1.3.2",
    "framer-motion": "^9.0.2",
    "react-dropzone": "^14.2.3",
    "react-hot-toast": "^2.4.1",
    "remark": "^15.0.1",
    "remark-html": "^16.0.1",
    "remark-parse": "^11.0.0",
    "zod": "^3.0.0",
    "zod-to-json-schema": "^3.23.0"
  },
  "devDependencies": {
    "@babel/core": "^7.26.9",
    "@babel/plugin-syntax-flow": "^7.26.0",
    "@babel/plugin-transform-typescript": "^7.26.8",
    "@babel/preset-env": "^7.26.9",
    "@babel/preset-react": "^7.26.3",
    "@babel/preset-typescript": "^7.26.0",
    "@rollup/plugin-alias": "^5.1.1",
    "@rollup/plugin-babel": "^6.0.4",
    "@rollup/plugin-commonjs": "^28.0.2",
    "@rollup/plugin-json": "^6.1.0",
    "@rollup/plugin-node-resolve": "^16.0.0",
    "@rollup/plugin-replace": "^6.0.2",
    "@rollup/plugin-typescript": "^12.1.2",
    "@types/jsdom": "^21.1.6",
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "autoprefixer": "^10.4.20",
    "eslint": "^8",
    "eslint-config-next": "14.2.3",
    "postcss": "^8",
    "prettier": "^3.2.5",
    "prettier-plugin-tailwindcss": "^0.6.0",
    "react-ga4": "^2.1.0",
    "rollup": "^2.79.2",
    "rollup-plugin-peer-deps-external": "^2.2.4",
    "rollup-plugin-postcss": "^4.0.2",
    "rollup-plugin-terser": "^7.0.2",
    "rollup-plugin-typescript2": "^0.31.2",
    "tailwindcss": "^3.4.1",
    "typescript": "^5"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/assafelovic/gpt-researcher.git"
  },
  "keywords": [
    "gpt",
    "researcher",
    "ai",
    "react",
    "nextjs"
  ],
  "author": "GPT Researcher Team",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/assafelovic/gpt-researcher/issues"
  },
  "homepage": "https://github.com/assafelovic/gpt-researcher#readme"
}



================================================
FILE: frontend/nextjs/postcss.config.mjs
================================================
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;



================================================
FILE: frontend/nextjs/rollup.config.js
================================================
import resolve from '@rollup/plugin-node-resolve';
import commonjs from '@rollup/plugin-commonjs';
import typescript from 'rollup-plugin-typescript2';
import babel from '@rollup/plugin-babel';
import { terser } from 'rollup-plugin-terser';
import json from '@rollup/plugin-json';
import postcss from 'rollup-plugin-postcss';
import tailwindcss from 'tailwindcss';
import autoprefixer from 'autoprefixer';
import imageTransformPlugin from './src/utils/imageTransformPlugin';

const removeUseClientPlugin = {
  name: 'remove-use-client',
  transform(code) {
    return code.replace(/"use client";?/, '');
  }
};

export default {
  input: 'src/index.ts',
  output: [
    {
      file: 'dist/index.js',
      format: 'cjs',
      sourcemap: true
    },
    {
      file: 'dist/index.esm.js',
      format: 'esm',
      sourcemap: true
    }
  ],
  plugins: [
    postcss({
      plugins: [
        tailwindcss('./tailwind.config.ts'),
        autoprefixer,
      ],
      inject: true, // This will automatically inject CSS
      minimize: true,
      extract: false // Keep CSS in JS for easier consumption
    }),
    json(), // Add this plugin to handle JSON files   
    removeUseClientPlugin,
    imageTransformPlugin(),
    resolve({
      extensions: ['.js', '.jsx', '.ts', '.tsx'],
      browser: true, // Ensures it only includes browser-compatible modules
      preferBuiltins: false // Prevents bundling Node.js modules
    }),
    commonjs(),
    typescript({
      tsconfig: './tsconfig.lib.json',
      useTsconfigDeclarationDir: true,
      clean: true
    }),
    babel({
      babelHelpers: 'bundled',
      configFile: './.babelrc.build.json',
      extensions: ['.js', '.jsx', '.ts', '.tsx'],
      presets: [
        '@babel/preset-env',
        '@babel/preset-react',
        ['@babel/preset-typescript', { allowNamespaces: true, onlyRemoveTypeImports: true }]
      ],
      plugins: [
        ['@babel/plugin-transform-typescript', { allowNamespaces: true }]
      ],
      exclude: 'node_modules/**'
    }),
    terser()
  ],
  external: [
    'next',
    'react', 
    'react-dom',
    'react-hot-toast',
    'remark',
    'remark-html',
    '@langchain/langgraph-sdk',
     // Ensure all Node.js built-in modules are excluded 
     'fs', 'path', 'crypto', 'util', 'http', 'https', 'zlib', 'stream', 'url', 'assert', 'tty'
  ]
};


================================================
FILE: frontend/nextjs/tailwind.config.ts
================================================
import type { Config } from 'tailwindcss';

const config: Config = {
  content: [
    './pages/**/*.{js,ts,jsx,tsx,mdx}',
    './components/**/*.{js,ts,jsx,tsx,mdx}',
    './app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    screens: {
      sm: '640px',
      md: '768px',
      lg: '898px',
      // xl:"1024px"
    },
    container: {
      center: true,
    },
    extend: {
      animation: {
        'gradient-x': 'gradient-x 10s ease infinite',
        'shimmer': 'shimmer 2s linear infinite',
        'pulse-slow': 'pulse 8s cubic-bezier(0.4, 0, 0.6, 1) infinite',
      },
      keyframes: {
        'gradient-x': {
          '0%, 100%': { backgroundPosition: '0% 50%' },
          '50%': { backgroundPosition: '100% 50%' },
        },
        'shimmer': {
          '100%': { transform: 'translateX(100%)' },
        },
      },
      backgroundImage: {
        'gradient-radial': 'radial-gradient(var(--tw-gradient-stops))',
        'custom-gradient':
          'linear-gradient(150deg, #1B1B16 1.28%, #565646 90.75%)',
        'gradient-conic':
          'conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))',
        'hero-gradient': 'linear-gradient(135deg, #9867F0, #ED4E50)',
        'teal-gradient': 'linear-gradient(135deg, #0d9488, #0891b2, #2563eb)',
      },
      boxShadow: {
        'glow': '0 0 40px rgba(152, 103, 240, 0.5)',
        'teal-glow': '0 0 40px rgba(13, 148, 136, 0.5)',
      },
      colors: {
        'primary': {
          '50': '#f0fdfa',
          '100': '#ccfbf1',
          '200': '#99f6e4',
          '300': '#5eead4',
          '400': '#2dd4bf',
          '500': '#14b8a6',
          '600': '#0d9488',
          '700': '#0f766e',
          '800': '#115e59',
          '900': '#134e4a',
          '950': '#042f2e',
        },
      },
    },
  },
  plugins: [],
};
export default config;



================================================
FILE: frontend/nextjs/tsconfig.json
================================================
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts", "components/Task/ImagesCarousel.jsx"],
  "exclude": ["node_modules"]
}


================================================
FILE: frontend/nextjs/tsconfig.lib.json
================================================
{
  "extends": "./tsconfig.json",
  "compilerOptions": {
    "outDir": "./dist",
    "declaration": true,
    "declarationDir": "./dist",
    "emitDeclarationOnly": false,
    "sourceMap": true,
    "noEmit": false,
    "jsx": "react-jsx",
    "moduleResolution": "node",
    "allowSyntheticDefaultImports": true,
    "esModuleInterop": true
  },
  "include": [
    "src/**/*.ts",
    "src/**/*.tsx",
    "components/**/*.jsx",
    "components/**/*.tsx"
  ],
  "exclude": ["node_modules", "**/*.test.ts", "**/*.test.tsx"]
}


================================================
FILE: frontend/nextjs/.babelrc.build.json
================================================
{
  "env": {
    "production": {
      "presets": [
        "@babel/preset-env",
        "@babel/preset-react",
        ["@babel/preset-typescript", { "allowNamespaces": true, "onlyRemoveTypeImports": true }]
      ],
      "plugins": [
        ["@babel/plugin-transform-typescript", { "allowNamespaces": true }]
      ]
    }
  }
}


================================================
FILE: frontend/nextjs/.dockerignore
================================================
.git

# Ignore env containing secrets
.env
.venv
.envrc

# Ignore Virtual Env
env/
venv/
.venv/

# Other Environments
ENV/
env.bak/
venv.bak/

# Ignore generated outputs
outputs/

# Ignore my local docs
my-docs/

# Ignore pycache
**/__pycache__/

# Ignore mypy cache
.mypy_cache/

# Node modules
node_modules

# Ignore IDE config
.idea

# macOS specific files
.DS_Store

# Docusaurus build artifacts
.docusaurus

# Build directories
build
docs/build

# Language graph data
.langgraph-data/

# Next.js build artifacts
.next/

# Package lock file
package-lock.json

# Docker-specific exclusions (if any)
Dockerfile
docker-compose.yml



================================================
FILE: frontend/nextjs/.eslintrc.json
================================================
{
  "extends": "next/core-web-vitals",
  "rules": {
    "no-unused-vars": "off",
    "no-undef": "off",
    "no-console": "off",
    "@next/next/no-img-element": "off",
    "@typescript-eslint/no-explicit-any": "off",
    "@typescript-eslint/no-unused-vars": "off",
    "react/no-unescaped-entities": "off" // Disabled to allow natural apostrophes in JSX text
  },
  "ignorePatterns": ["build/**/*"]
}



================================================
FILE: frontend/nextjs/.example.env
================================================
TOGETHER_API_KEY=
BING_API_KEY=
HELICONE_API_KEY=



================================================
FILE: frontend/nextjs/.prettierrc
================================================
{ "plugins": ["prettier-plugin-tailwindcss"] }



================================================
FILE: frontend/nextjs/.python-version
================================================
3.11.13



================================================
FILE: frontend/nextjs/actions/apiActions.ts
================================================
import { createParser, ParsedEvent, ReconnectInterval } from "eventsource-parser";

export async function handleSourcesAndAnswer(question: string) {
  let sourcesResponse = await fetch("/api/getSources", {
    method: "POST",
    body: JSON.stringify({ question }),
  });
  let sources = await sourcesResponse.json();

  const response = await fetch("/api/getAnswer", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ question, sources }),
  });

  if (!response.ok) {
    throw new Error(response.statusText);
  }

  if (response.status === 202) {
    const fullAnswer = await response.text();
    return fullAnswer;
  }

  // This data is a ReadableStream
  const data = response.body;
  if (!data) {
    return;
  }

  const onParse = (event: ParsedEvent | ReconnectInterval) => {
    if (event.type === "event") {
      const data = event.data;
      try {
        const text = JSON.parse(data).text ?? "";
        return text;
      } catch (e) {
        console.error(e);
      }
    }
  };

  // https://web.dev/streams/#the-getreader-and-read-methods
  const reader = data.getReader();
  const decoder = new TextDecoder();
  const parser = createParser(onParse);
  let done = false;
  while (!done) {
    const { value, done: doneReading } = await reader.read();
    done = doneReading;
    const chunkValue = decoder.decode(value);
    parser.feed(chunkValue);
  }
}

export async function handleSimilarQuestions(question: string) {
  let res = await fetch("/api/getSimilarQuestions", {
    method: "POST",
    body: JSON.stringify({ question }),
  });
  let questions = await res.json();
  return questions;
}

export async function handleLanggraphAnswer(question: string) {
  const response = await fetch("/api/generateLanggraph", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ question }),
  });

  if (!response.ok) {
    throw new Error(response.statusText);
  }

  // This data is a ReadableStream
  const data = response.body;
  if (!data) {
    return;
  }

  const onParse = (event: ParsedEvent | ReconnectInterval) => {
    if (event.type === "event") {
      const data = event.data;
      try {
        const text = JSON.parse(data).text ?? "";
        return text;
      } catch (e) {
        console.error(e);
      }
    }
  };

  const reader = data.getReader();
  const decoder = new TextDecoder();
  const parser = createParser(onParse);
  let done = false;
  while (!done) {
    const { value, done: doneReading } = await reader.read();
    done = doneReading;
    const chunkValue = decoder.decode(value);
    parser.feed(chunkValue);
  }
}


================================================
FILE: frontend/nextjs/app/globals.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

@keyframes gradientBG {
  0% {background-position: 0% 50%;}
  50% {background-position: 100% 50%;}
  100% {background-position: 0% 50%;}
}

@keyframes float {
  0%, 100% {
    transform: translateY(0) translateX(0);
  }
  25% {
    transform: translateY(-20px) translateX(10px);
  }
  50% {
    transform: translateY(-10px) translateX(-15px);
  }
  75% {
    transform: translateY(-25px) translateX(5px);
  }
}

html {
  scroll-behavior: smooth;
  height: 100%;
}

textarea {
  max-height: 300px; /* Set an appropriate max height */
  overflow-y: auto;  /* Enable internal scrolling */
  /* transition: height 0.2s ease-in-out; */
}

.log-message {
  word-wrap: break-word; /* For handling long URLs or text */
  overflow-wrap: break-word; /* For handling overflow in modern browsers */
  overflow-x: hidden; /* Hide horizontal overflow */
  word-break: break-word; /* Break long words if needed */
}

body {
  font-family: 'GeistSans', sans-serif;
  /* font-family: 'Inter', sans-serif; */
  /* font-family: 'Montserrat', sans-serif; */
  line-height: 1.6;
  margin: 0px !important;
  min-height: 100%;
  position: relative;
  background: #0C111F;
  overflow-x: hidden;
}

/* Background gradient orbs - static version */
body::before {
  content: "";
  position: fixed;
  top: -25%;
  left: -10%;
  width: 60%;
  height: 60%;
  border-radius: 9999px;
  background-color: rgba(13, 148, 136, 0.12);
  filter: blur(120px);
  z-index: -10;
}

body::after {
  content: "";
  position: fixed;
  bottom: -25%;
  right: -10%;
  width: 60%;
  height: 60%;
  border-radius: 9999px;
  background-color: rgba(8, 145, 178, 0.12);
  filter: blur(120px);
  z-index: -10;
}

/* Additional orb */
.app-container::before {
  content: "";
  position: fixed;
  top: 40%;
  right: 20%;
  width: 35%;
  height: 35%;
  border-radius: 9999px;
  background-color: rgba(37, 99, 235, 0.06);
  filter: blur(80px);
  z-index: -10;
}

.landing {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 30vh;
  text-align: center;
  color: white;
}

.landing h1 {
  font-size: 3.5rem;
  font-weight: 700;
  margin-bottom: 2rem;
}

@layer utilities {
  .text-balance {
    text-wrap: balance;
  }
  /* Hide scrollbar for Chrome, Safari and Opera */
  .no-scrollbar::-webkit-scrollbar {
    display: none;
  }
  /* Hide scrollbar for IE, Edge and Firefox */
  .no-scrollbar {
    -ms-overflow-style: none; /* IE and Edge */
    scrollbar-width: none; /* Firefox */
  }
  .loader {
    text-align: left;
    display: flex;
    gap: 3px;
  }

  .loader span {
    display: inline-block;
    vertical-align: middle;
    width: 7px;
    height: 7px;
    /* background: #4b4b4b; */
    background: white;
    border-radius: 50%;
    animation: loader 0.6s infinite alternate;
  }

  .loader span:nth-of-type(2) {
    animation-delay: 0.2s;
  }

  .loader span:nth-of-type(3) {
    animation-delay: 0.6s;
  }

  @keyframes loader {
    0% {
      opacity: 1;
      transform: scale(0.6);
    }

    100% {
      opacity: 0.3;
      transform: scale(1);
    }
  }
}

/* Add these styles for the scrollbar */
.scrollbar-thin {
  scrollbar-width: thin;
}

.scrollbar-thumb-gray-600::-webkit-scrollbar-thumb {
  background-color: #4B5563;
  border-radius: 6px;
}

.scrollbar-track-gray-300::-webkit-scrollbar-track {
  background-color: #D1D5DB;
}

.scrollbar-thin::-webkit-scrollbar {
  width: 6px;
}

/* Sidebar styles */
.sidebar-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.3);
  z-index: 40;
  transition: opacity 0.3s ease;
}

/* Sidebar backdrop blur */
.sidebar-backdrop {
  backdrop-filter: blur(8px);
  -webkit-backdrop-filter: blur(8px);
}

/* Scrollbar styling for the sidebar */
.sidebar-scrollbar::-webkit-scrollbar {
  width: 6px;
}

.sidebar-scrollbar::-webkit-scrollbar-track {
  background: #1f2937;
}

.sidebar-scrollbar::-webkit-scrollbar-thumb {
  background-color: #4b5563;
  border-radius: 3px;
}

.sidebar-scrollbar::-webkit-scrollbar-thumb:hover {
  background-color: #6b7280;
}

/* Ensure sidebar is above other content */
.sidebar-z-index {
  z-index: 50;
}




================================================
FILE: frontend/nextjs/app/layout.tsx
================================================
import type { Metadata } from "next";
import { Lexend } from "next/font/google";
import PlausibleProvider from "next-plausible";
import { GoogleAnalytics } from '@next/third-parties/google'
import { ResearchHistoryProvider } from "@/hooks/ResearchHistoryContext";
import "./globals.css";
import Script from 'next/script';

const inter = Lexend({ subsets: ["latin"] });

let title = "GPT Researcher";
let description =
  "LLM based autonomous agent that conducts local and web research on any topic and generates a comprehensive report with citations.";
let url = "https://github.com/assafelovic/gpt-researcher";
let ogimage = "/favicon.ico";
let sitename = "GPT Researcher";

export const metadata: Metadata = {
  metadataBase: new URL(url),
  title,
  description,
  manifest: '/manifest.json',
  icons: {
    icon: "/img/gptr-black-logo.png",
    apple: '/img/gptr-black-logo.png',
  },
  appleWebApp: {
    capable: true,
    statusBarStyle: 'default',
    title: title,
  },
  openGraph: {
    images: [ogimage],
    title,
    description,
    url: url,
    siteName: sitename,
    locale: "en_US",
    type: "website",
  },
  twitter: {
    card: "summary_large_image",
    images: [ogimage],
    title,
    description,
  },
  viewport: {
    width: 'device-width',
    initialScale: 1,
    maximumScale: 1,
    userScalable: false,
  },
  themeColor: '#111827',
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {

  return (
    <html className="gptr-root" lang="en" suppressHydrationWarning>
      <head>
        <PlausibleProvider domain="localhost:3000" />
        <GoogleAnalytics gaId={process.env.NEXT_PUBLIC_GA_MEASUREMENT_ID!} />
        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="default" />
        <link rel="apple-touch-icon" href="/img/gptr-black-logo.png" />
      </head>
      <body
        className={`app-container ${inter.className} flex min-h-screen flex-col justify-between`}
        suppressHydrationWarning
      >
        <ResearchHistoryProvider>
          {children}
        </ResearchHistoryProvider>
      </body>
    </html>
  );
}



================================================
FILE: frontend/nextjs/app/page.tsx
================================================
"use client";

import { useRef, useState, useEffect } from "react";
import { useRouter } from "next/navigation";
import { useWebSocket } from '@/hooks/useWebSocket';
import { useResearchHistoryContext } from '@/hooks/ResearchHistoryContext';
import { useScrollHandler } from '@/hooks/useScrollHandler';
import { startLanggraphResearch } from '../components/Langgraph/Langgraph';
import findDifferences from '../helpers/findDifferences';
import { Data, ChatBoxSettings, QuestionData, ChatMessage, ChatData } from '../types/data';
import { preprocessOrderedData } from '../utils/dataProcessing';
import { toast } from "react-hot-toast";
import { v4 as uuidv4 } from 'uuid';

import Hero from "@/components/Hero";
import ResearchPageLayout from "@/components/layouts/ResearchPageLayout";
import CopilotLayout from "@/components/layouts/CopilotLayout";
import ResearchContent from "@/components/research/ResearchContent";
import CopilotResearchContent from "@/components/research/CopilotResearchContent";
import HumanFeedback from "@/components/HumanFeedback";
import ResearchSidebar from "@/components/ResearchSidebar";
import { getAppropriateLayout } from "@/utils/getLayout";

// Import the mobile components
import MobileHomeScreen from "@/components/mobile/MobileHomeScreen";
import MobileResearchContent from "@/components/mobile/MobileResearchContent";

export default function Home() {
  const router = useRouter();
  const [promptValue, setPromptValue] = useState("");
  const [chatPromptValue, setChatPromptValue] = useState("");
  const [showResult, setShowResult] = useState(false);
  const [answer, setAnswer] = useState("");
  const [loading, setLoading] = useState(false);
  const [isInChatMode, setIsInChatMode] = useState(false);
  const [chatBoxSettings, setChatBoxSettings] = useState<ChatBoxSettings>(() => {
    // Default settings
    const defaultSettings = {
      report_type: "research_report",
      report_source: "web",
      tone: "Objective",
      domains: [],
      defaultReportType: "research_report",
      layoutType: 'copilot',
      mcp_enabled: false,
      mcp_configs: [],
      mcp_strategy: "fast",
    };

    // Try to load all settings from localStorage
    if (typeof window !== 'undefined') {
      const savedSettings = localStorage.getItem('chatBoxSettings');
      if (savedSettings) {
        try {
          const parsedSettings = JSON.parse(savedSettings);
          return {
            ...defaultSettings,
            ...parsedSettings, // Override defaults with saved settings
          };
        } catch (e) {
          console.error('Error parsing saved settings:', e);
        }
      }
    }
    return defaultSettings;
  });
  const [question, setQuestion] = useState("");
  const [orderedData, setOrderedData] = useState<Data[]>([]);
  const [showHumanFeedback, setShowHumanFeedback] = useState(false);
  const [questionForHuman, setQuestionForHuman] = useState<true | false>(false);
  const [allLogs, setAllLogs] = useState<any[]>([]);
  const [isStopped, setIsStopped] = useState(false);
  const mainContentRef = useRef<HTMLDivElement>(null);
  const [sidebarOpen, setSidebarOpen] = useState(false);
  const [currentResearchId, setCurrentResearchId] = useState<string | null>(null);
  const [isMobile, setIsMobile] = useState(false);
  const [isProcessingChat, setIsProcessingChat] = useState(false);

  // Use our custom scroll handler
  const { showScrollButton, scrollToBottom } = useScrollHandler(mainContentRef);

  // Check if we're on mobile
  useEffect(() => {
    const checkIfMobile = () => {
      setIsMobile(window.innerWidth < 768);
    };
    
    // Initial check
    checkIfMobile();
    
    // Add event listener for window resize
    window.addEventListener('resize', checkIfMobile);
    
    // Cleanup
    return () => window.removeEventListener('resize', checkIfMobile);
  }, []);

  const { 
    history, 
    saveResearch, 
    updateResearch,
    getResearchById, 
    deleteResearch,
    addChatMessage,
    getChatMessages
  } = useResearchHistoryContext();

  // Only initialize the WebSocket hook reference, don't connect automatically
  const websocketRef = useRef(useWebSocket(
    setOrderedData,
    setAnswer,
    setLoading,
    setShowHumanFeedback,
    setQuestionForHuman
  ));
  
  // Use the reference to access websocket functions
  const { socket, initializeWebSocket } = websocketRef.current;

  const handleFeedbackSubmit = (feedback: string | null) => {
    if (socket) {
      socket.send(JSON.stringify({ type: 'human_feedback', content: feedback }));
    }
    setShowHumanFeedback(false);
  };

  const handleChat = async (message: string) => {
    if (!currentResearchId && !answer) {
      // On mobile, if there's no research yet, treat this as a new research request
      if (isMobile) {
        // Show immediate feedback for better UX
        setShowResult(true);
        setPromptValue(message); // Keep the message visible
        
        // Start the research with the chat message
        handleDisplayResult(message);
        return;
      }
    }
    
    setShowResult(true);
    setIsProcessingChat(true);
    setChatPromptValue("");
    
    // Create a user message
    const userMessage: ChatMessage = {
      role: 'user',
      content: message,
      timestamp: Date.now()
    };
    
    // Add question to display in research results immediately
    const questionData: QuestionData = { type: 'question', content: message };
    setOrderedData(prevOrder => [...prevOrder, questionData]);
    
    // Add user message to history asynchronously
    if (currentResearchId) {
      addChatMessage(currentResearchId, userMessage).catch(error => {
        console.error('Error adding chat message to history:', error);
      });
    }
    
    // Mobile implementation - simplified for chat only
    if (isMobile) {
      try {
        // Direct API call instead of websockets
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            messages: [{ role: 'user', content: message }],
            report: answer || '',
          }),
        });
        
        if (!response.ok) {
          throw new Error(`API error: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data.response && data.response.content) {
          // Add AI response to chat history asynchronously
          if (currentResearchId) {
            addChatMessage(currentResearchId, data.response).catch(error => {
              console.error('Error adding AI response to history:', error);
            });
            
            // Also update the research with the new messages
            const chatData: ChatData = { 
              type: 'chat', 
              content: data.response.content,
              metadata: data.response.metadata 
            };
            
            setOrderedData(prevOrder => [...prevOrder, chatData]);
            
            // Get current ordered data and add new messages
            const updatedOrderedData = [...orderedData, questionData, chatData];
            
            // Update research in history
            updateResearch(
              currentResearchId, 
              answer, 
              updatedOrderedData
            ).catch(error => {
              console.error('Error updating research:', error);
            });
          } else {
            // If no research ID, just update the UI
            setOrderedData(prevOrder => [...prevOrder, { 
              type: 'chat', 
              content: data.response.content,
              metadata: data.response.metadata
            } as ChatData]);
          }
        } else {
          // Show error message
          setOrderedData(prevOrder => [...prevOrder, { 
            type: 'chat', 
            content: 'Sorry, something went wrong. Please try again.' 
          } as ChatData]);
        }
      } catch (error) {
        console.error('Error during chat:', error);
        
        // Add error message
        setOrderedData(prevOrder => [...prevOrder, { 
          type: 'chat', 
          content: 'Sorry, there was an error processing your request. Please try again.' 
        } as ChatData]);
      } finally {
        setIsProcessingChat(false);
      }
      return;
    }
    
    // Desktop implementation (unchanged)
    try {
      // Fetch all chat messages for this research
      let chatMessages: { role: string; content: string }[] = [];
      
      if (currentResearchId) {
        // If we have a research ID, get all messages from history
        chatMessages = getChatMessages(currentResearchId);
      }
      
      // Format messages to ensure they only contain role and content properties
      const formattedMessages = [...chatMessages, userMessage].map(msg => ({
        role: msg.role,
        content: msg.content
      }));
      
      // Call the chat API
      const response = await fetch(`/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          report: answer || "",
          messages: formattedMessages
        }),
      });
      
      if (!response.ok) {
        throw new Error(`Failed to get chat response: ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.response) {
        // Check if response contains valid content
        if (!data.response.content) {
          console.error('Response content is null or empty');
          // Show error message in results
          setOrderedData(prevOrder => [...prevOrder, { 
            type: 'chat', 
            content: 'I apologize, but I couldn\'t generate a proper response. Please try asking your question again.' 
          }]);
        } else {
          // Add AI response to chat history asynchronously
          if (currentResearchId) {
            addChatMessage(currentResearchId, data.response).catch(error => {
              console.error('Error adding AI response to history:', error);
            });
          }
          
          // Add response to display in research results
          setOrderedData(prevOrder => {
            return [...prevOrder, { 
              type: 'chat', 
              content: data.response.content,
              metadata: data.response.metadata
            }];
          });
        }
        
        // Explicitly enable chat mode after getting a response
        if (!isInChatMode) {
          setIsInChatMode(true);
        }
      } else {
        // Show error message
        setOrderedData(prevOrder => [...prevOrder, { 
          type: 'chat', 
          content: 'Sorry, something went wrong. Please try again.' 
        }]);
      }
    } catch (error) {
      console.error('Error during chat:', error);
      
      // Add error message to display
      setOrderedData(prevOrder => [...prevOrder, { 
        type: 'chat', 
        content: 'Sorry, there was an error processing your request. Please try again.' 
      }]);
    } finally {
      setLoading(false);
      setIsProcessingChat(false);
    }
  };

  const handleDisplayResult = async (newQuestion: string) => {
    // Exit chat mode when starting a new research
    setIsInChatMode(false);
    setShowResult(true);
    setLoading(true);
    setQuestion(newQuestion);
    setPromptValue("");
    setAnswer("");
    setCurrentResearchId(null); // Reset current research ID for new research
    setOrderedData((prevOrder) => [...prevOrder, { type: 'question', content: newQuestion }]);

    // For mobile, use a simplified approach without websockets
    if (isMobile) {
      try {
        // Create a new unique ID for this research
        const newResearchId = `${Date.now()}-${Math.random().toString(36).substring(2, 9)}`;
        
        // First save the initial question to history - with proper parameters
        const initialOrderedData: Data[] = [{ type: 'question', content: newQuestion } as QuestionData];
        await saveResearch(
          newQuestion,  // question
          '',           // empty answer initially
          initialOrderedData  // ordered data
        );
        
        // Make direct API call to get response
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            messages: [{ role: 'user', content: newQuestion }],
            // No report since this is a new research
          }),
        });
        
        if (!response.ok) {
          throw new Error(`API error: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data.response && data.response.content) {
          // Add the AI response to the ordered data
          const chatData: ChatData = { 
            type: 'chat', 
            content: data.response.content,
            metadata: data.response.metadata 
          };
          
          // Set the answer
          const chatAnswer = data.response.content;
          setAnswer(chatAnswer);
          setOrderedData(prevOrder => [...prevOrder, chatData]);
          
          // Update the research with the answer
          const updatedOrderedData: Data[] = [
            { type: 'question', content: newQuestion } as QuestionData,
            chatData
          ];
          
          // Save the completed research with proper parameters
          await updateResearch(
            newResearchId,    // id
            chatAnswer,       // answer
            updatedOrderedData // ordered data
          );
          
          // Set current research ID so we can continue the conversation
          setCurrentResearchId(newResearchId);
        } else {
          // Handle error
          setOrderedData(prevOrder => [...prevOrder, { 
            type: 'chat', 
            content: 'Sorry, I couldn\'t generate a research response. Please try again.' 
          } as ChatData]);
        }
      } catch (error) {
        console.error('Error in mobile research:', error);
        // Show error message
        setOrderedData(prevOrder => [...prevOrder, { 
          type: 'chat', 
          content: 'Sorry, there was an error processing your request. Please try again.' 
        } as ChatData]);
      } finally {
        setLoading(false);
      }
      return;
    }

    const storedConfig = localStorage.getItem('apiVariables');
    const apiVariables = storedConfig ? JSON.parse(storedConfig) : {};
    const langgraphHostUrl = apiVariables.LANGGRAPH_HOST_URL;

    // Starting new research - tracking for redirection once complete
    const newResearchStarted = Date.now().toString();
    // We'll use this as a temporary ID to keep track of this research
    const tempResearchId = `temp-${newResearchStarted}`;

    if (chatBoxSettings.report_type === 'multi_agents' && langgraphHostUrl) {
      let { streamResponse, host, thread_id } = await startLanggraphResearch(newQuestion, chatBoxSettings.report_source, langgraphHostUrl);
      const langsmithGuiLink = `https://smith.langchain.com/studio/thread/${thread_id}?baseUrl=${host}`;
      setOrderedData((prevOrder) => [...prevOrder, { type: 'langgraphButton', link: langsmithGuiLink }]);

      let previousChunk = null;
      for await (const chunk of streamResponse) {
        if (chunk.data.report != null && chunk.data.report != "Full report content here") {
          setOrderedData((prevOrder) => [...prevOrder, { ...chunk.data, output: chunk.data.report, type: 'report' }]);
          setLoading(false);
        
          // Save research and navigate to its unique URL once it's complete
          setAnswer(chunk.data.report);
        } else if (previousChunk) {
          const differences = findDifferences(previousChunk, chunk);
          setOrderedData((prevOrder) => [...prevOrder, { type: 'differences', content: 'differences', output: JSON.stringify(differences) }]);
        }
        previousChunk = chunk;
      }
    } else {
      initializeWebSocket(newQuestion, chatBoxSettings);
    }
  };

  // Mobile-specific implementation for research
  const handleMobileDisplayResult = async (newQuestion: string) => {
    // Update UI state
    setIsInChatMode(false);
    setShowResult(true);
    setLoading(true);
    setQuestion(newQuestion);
    setPromptValue("");
    setAnswer("");
    setCurrentResearchId(null);
    
    // Start with just the question
    setOrderedData([{ type: 'question', content: newQuestion } as QuestionData]);
    
    try {
      // Generate unique ID for this research
      const mobileResearchId = `mobile-${Date.now()}-${Math.random().toString(36).substring(2, 7)}`;
      
      // Save initial research with just the question
      const initialOrderedData: Data[] = [{ type: 'question', content: newQuestion } as QuestionData];
      
      // Save to research history
      await saveResearch(
        newQuestion,  // question
        '',           // empty answer initially
        initialOrderedData  // ordered data
      );
      
      // Make direct API call instead of using websockets
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: [{ role: 'user', content: newQuestion }],
          // Include the required parameters
          report: '',  // No report since this is a new research
          report_source: chatBoxSettings.report_source || 'web',
          tone: chatBoxSettings.tone || 'Objective'
        }),
        // Set reasonable timeout
        signal: AbortSignal.timeout(30000) // 30-second timeout
      });
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.response && data.response.content) {
        // Extract the response
        const responseContent = data.response.content;
        
        // Update UI with the answer
        setAnswer(responseContent);
        
        // Create chat data object
        const chatData: ChatData = { 
          type: 'chat', 
          content: responseContent,
          metadata: data.response.metadata 
        };
        
        // Update ordered data to include the response
        setOrderedData(prevData => [...prevData, chatData]);
        
        // Update the complete research
        const updatedOrderedData: Data[] = [
          { type: 'question', content: newQuestion } as QuestionData,
          chatData
        ];
        
        // Update research history with the answer
        await updateResearch(
          mobileResearchId,
          responseContent,
          updatedOrderedData
        );
        
        // Set current research ID for future interactions
        setCurrentResearchId(mobileResearchId);
      } else {
        // Handle error in response
        setOrderedData(prevData => [
          ...prevData, 
          { 
            type: 'chat', 
            content: "I'm sorry, I couldn't generate a complete response. Please try rephrasing your question." 
          } as ChatData
        ]);
      }
    } catch (error) {
      console.error('Mobile research error:', error);
      
      // Show error in UI
      setOrderedData(prevData => [
        ...prevData, 
        { 
          type: 'chat', 
          content: "Sorry, there was an error processing your request. Please try again." 
        } as ChatData
      ]);
    } finally {
      // Always finish loading state
      setLoading(false);
    }
  };

  // Mobile-specific chat handler
  const handleMobileChat = async (message: string) => {
    // Set states for UI feedback
    setIsProcessingChat(true);
    
    // Format user message
    const userMessage = {
      role: 'user',
      content: message
    };
    
    // Add question to UI immediately
    const questionData: QuestionData = { 
      type: 'question', 
      content: message 
    };
    
    setOrderedData(prevOrder => [...prevOrder, questionData]);
    
    try {
      // Direct API call instead of websockets
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: [userMessage],
          report: answer || '',
          report_source: chatBoxSettings.report_source || 'web',
          tone: chatBoxSettings.tone || 'Objective'
        }),
        // Set reasonable timeout
        signal: AbortSignal.timeout(20000) // 20-second timeout
      });
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.response && data.response.content) {
        // Add AI response to chat history asynchronously
        if (currentResearchId) {
          addChatMessage(currentResearchId, data.response).catch(error => {
            console.error('Error adding AI response to history:', error);
          });
          
          // Also update the research with the new messages
          const chatData: ChatData = { 
            type: 'chat', 
            content: data.response.content,
            metadata: data.response.metadata 
          };
          
          setOrderedData(prevOrder => [...prevOrder, chatData]);
          
          // Get current ordered data and add new messages
          const updatedOrderedData = [...orderedData, questionData, chatData];
          
          // Update research in history
          updateResearch(
            currentResearchId, 
            answer, 
            updatedOrderedData
          ).catch(error => {
            console.error('Error updating research:', error);
          });
        } else {
          // If no research ID, just update the UI
          setOrderedData(prevOrder => [...prevOrder, { 
            type: 'chat', 
            content: data.response.content,
            metadata: data.response.metadata
          } as ChatData]);
        }
      } else {
        // Show error message
        setOrderedData(prevOrder => [...prevOrder, { 
          type: 'chat', 
          content: 'Sorry, something went wrong. Please try again.' 
        } as ChatData]);
      }
    } catch (error) {
      console.error('Error during mobile chat:', error);
      
      // Add error message
      setOrderedData(prevOrder => [...prevOrder, { 
        type: 'chat', 
        content: 'Sorry, there was an error processing your request. Please try again.' 
      } as ChatData]);
    } finally {
      setIsProcessingChat(false);
      setChatPromptValue('');
    }
  };

  const reset = () => {
    // Reset UI states
    setShowResult(false);
    setPromptValue("");
    setIsStopped(false);
    setIsInChatMode(false);
    setCurrentResearchId(null); // Reset research ID
    setIsProcessingChat(false);
    
    // Clear previous research data
    setQuestion("");
    setAnswer("");
    setOrderedData([]);
    setAllLogs([]);

    // Reset feedback states
    setShowHumanFeedback(false);
    setQuestionForHuman(false);
    
    // Clean up connections
    if (socket) {
      socket.close();
    }
    setLoading(false);
  };

  const handleClickSuggestion = (value: string) => {
    setPromptValue(value);
    const element = document.getElementById('input-area');
    if (element) {
      element.scrollIntoView({ behavior: 'smooth' });
    }
  };

  /**
   * Handles stopping the current research
   * - Closes WebSocket connection
   * - Stops loading state
   * - Marks research as stopped
   * - Preserves current results
   * - Reloads the page to fully reset the connection
   */
  const handleStopResearch = () => {
    if (socket) {
      socket.close();
    }
    setLoading(false);
    setIsStopped(true);
    
    // Reload the page to completely reset the socket connection
    window.location.reload();
  };

  /**
   * Handles starting a new research
   * - Clears all previous research data and states
   * - Resets UI to initial state
   * - Closes any existing WebSocket connections
   */
  const handleStartNewResearch = () => {
    reset();
    setSidebarOpen(false);
  };

  const handleCopyUrl = () => {
    if (!currentResearchId) return;
    
    const url = `${window.location.origin}/research/${currentResearchId}`;
    navigator.clipboard.writeText(url)
      .then(() => {
        toast.success("URL copied to clipboard!");
      })
      .catch(() => {
        toast.error("Failed to copy URL");
      });
  };

  // Add a ref to track if an update is in progress to prevent infinite loops
  const isUpdatingRef = useRef(false);

  // Save or update research in history based on mode
  useEffect(() => {
    // Define an async function inside the effect
    const saveOrUpdateResearch = async () => {
      // Prevent infinite loops by checking if we're already updating
      if (isUpdatingRef.current) return;
      
      if (showResult && !loading && answer && question && orderedData.length > 0) {
        if (isInChatMode && currentResearchId) {
          // Prevent redundant updates by checking if data has changed
          try {
            const currentResearch = await getResearchById(currentResearchId);
            if (currentResearch && (currentResearch.answer !== answer || JSON.stringify(currentResearch.orderedData) !== JSON.stringify(orderedData))) {
              isUpdatingRef.current = true;
              await updateResearch(currentResearchId, answer, orderedData);
              // Reset the flag after a short delay to allow state updates to complete
              setTimeout(() => {
                isUpdatingRef.current = false;
              }, 100);
            }
          } catch (error) {
            console.error('Error updating research:', error);
            isUpdatingRef.current = false;
          }
        } else if (!isInChatMode) {
          // Check if this is a new research (not loaded from history)
          const isNewResearch = !history.some(item => 
            item.question === question && item.answer === answer
          );
          
          if (isNewResearch) {
            isUpdatingRef.current = true;
            try {
              const newId = await saveResearch(question, answer, orderedData);
              setCurrentResearchId(newId);
              
              // Don't navigate to the research page URL anymore
              // Just save the ID for sharing purposes
              
            } catch (error) {
              console.error('Error saving research:', error);
            } finally {
              // Reset the flag after a short delay to allow state updates to complete
              setTimeout(() => {
                isUpdatingRef.current = false;
              }, 100);
            }
          }
        }
      }
    };
    
    // Call the async function
    saveOrUpdateResearch();
  }, [showResult, loading, answer, question, orderedData, history, saveResearch, updateResearch, isInChatMode, currentResearchId, getResearchById]);

  // Handle selecting a research from history
  const handleSelectResearch = async (id: string) => {
    try {
      const research = await getResearchById(id);
      if (research) {
        // Navigate to the research page instead of loading it here
        router.push(`/research/${id}`);
      }
    } catch (error) {
      console.error('Error selecting research:', error);
      toast.error('Could not load the selected research');
    }
  };

  // Toggle sidebar
  const toggleSidebar = () => {
    setSidebarOpen(!sidebarOpen);
  };

  /**
   * Processes ordered data into logs for display
   * Updates whenever orderedData changes
   */
  useEffect(() => {
    const groupedData = preprocessOrderedData(orderedData);
    const statusReports = ["agent_generated", "starting_research", "planning_research", "error"];
    
    const newLogs = groupedData.reduce((acc: any[], data) => {
      // Process accordion blocks (grouped data)
      if (data.type === 'accordionBlock') {
        const logs = data.items.map((item: any, subIndex: any) => ({
          header: item.content,
          text: item.output,
          metadata: item.metadata,
          key: `${item.type}-${item.content}-${subIndex}`,
        }));
        return [...acc, ...logs];
      } 
      // Process status reports
      else if (statusReports.includes(data.content)) {
        return [...acc, {
          header: data.content,
          text: data.output,
          metadata: data.metadata,
          key: `${data.type}-${data.content}`,
        }];
      }
      return acc;
    }, []);
    
    setAllLogs(newLogs);
  }, [orderedData]);

  // Save chatBoxSettings to localStorage when they change
  useEffect(() => {
    localStorage.setItem('chatBoxSettings', JSON.stringify(chatBoxSettings));
  }, [chatBoxSettings]);

  // Set chat mode when a report is complete
  useEffect(() => {
    if (showResult && !loading && answer && !isInChatMode) {
      setIsInChatMode(true);
    }
  }, [showResult, loading, answer, isInChatMode]);

  // Update the renderMobileContent function to use both mobile-specific functions
  const renderMobileContent = () => {
    if (!showResult) {
      return (
        <MobileHomeScreen
          promptValue={promptValue}
          setPromptValue={setPromptValue}
          handleDisplayResult={handleMobileDisplayResult}
          isLoading={loading}
        />
      );
    } else {
      return (
        <MobileResearchContent
          orderedData={orderedData}
          answer={answer}
          loading={loading}
          isStopped={isStopped}
          chatPromptValue={chatPromptValue}
          setChatPromptValue={setChatPromptValue}
          handleChat={handleMobileChat} // Use mobile-specific chat handler
          isProcessingChat={isProcessingChat}
          onNewResearch={handleStartNewResearch}
          currentResearchId={currentResearchId || undefined}
          onShareClick={currentResearchId ? handleCopyUrl : undefined}
        />
      );
    }
  };

  return (
    <>
      {isMobile ? (
        // Mobile view - simplified layout with focus on chat
        getAppropriateLayout({
          loading,
          isStopped,
          showResult,
          onStop: handleStopResearch,
          onNewResearch: handleStartNewResearch,
          chatBoxSettings,
          setChatBoxSettings,
          mainContentRef,
          toggleSidebar,
          isProcessingChat,
          children: renderMobileContent()
        })
      ) : !showResult ? (
        // Desktop view - home page
        getAppropriateLayout({
          loading,
          isStopped,
          showResult,
          onStop: handleStopResearch,
          onNewResearch: handleStartNewResearch,
          chatBoxSettings,
          setChatBoxSettings,
          mainContentRef,
          showScrollButton,
          onScrollToBottom: scrollToBottom,
          children: (
            <>
              <ResearchSidebar
                history={history}
                onSelectResearch={handleSelectResearch}
                onNewResearch={handleStartNewResearch}
                onDeleteResearch={deleteResearch}
                isOpen={sidebarOpen}
                toggleSidebar={toggleSidebar}
              />
              
              <Hero
                promptValue={promptValue}
                setPromptValue={setPromptValue}
                handleDisplayResult={handleDisplayResult}
              />
            </>
          )
        })
      ) : (
        // Desktop view - research results
        getAppropriateLayout({
          loading,
          isStopped,
          showResult,
          onStop: handleStopResearch,
          onNewResearch: handleStartNewResearch,
          chatBoxSettings,
          setChatBoxSettings,
          mainContentRef,
          children: (
            <div className="relative">
              <ResearchSidebar
                history={history}
                onSelectResearch={handleSelectResearch}
                onNewResearch={handleStartNewResearch}
                onDeleteResearch={deleteResearch}
                isOpen={sidebarOpen}
                toggleSidebar={toggleSidebar}
              />
              
              {chatBoxSettings.layoutType === 'copilot' ? (
                <CopilotResearchContent
                  orderedData={orderedData}
                  answer={answer}
                  allLogs={allLogs}
                  chatBoxSettings={chatBoxSettings}
                  loading={loading}
                  isStopped={isStopped}
                  promptValue={promptValue}
                  chatPromptValue={chatPromptValue}
                  setPromptValue={setPromptValue}
                  setChatPromptValue={setChatPromptValue}
                  handleDisplayResult={handleDisplayResult}
                  handleChat={handleChat}
                  handleClickSuggestion={handleClickSuggestion}
                  currentResearchId={currentResearchId || undefined}
                  onShareClick={currentResearchId ? handleCopyUrl : undefined}
                  reset={reset}
                  isProcessingChat={isProcessingChat}
                  onNewResearch={handleStartNewResearch}
                  toggleSidebar={toggleSidebar}
                />
              ) : (
                <ResearchContent
                  showResult={showResult}
                  orderedData={orderedData}
                  answer={answer}
                  allLogs={allLogs}
                  chatBoxSettings={chatBoxSettings}
                  loading={loading}
                  isInChatMode={isInChatMode}
                  isStopped={isStopped}
                  promptValue={promptValue}
                  chatPromptValue={chatPromptValue}
                  setPromptValue={setPromptValue}
                  setChatPromptValue={setChatPromptValue}
                  handleDisplayResult={handleDisplayResult}
                  handleChat={handleChat}
                  handleClickSuggestion={handleClickSuggestion}
                  currentResearchId={currentResearchId || undefined}
                  onShareClick={currentResearchId ? handleCopyUrl : undefined}
                  reset={reset}
                  isProcessingChat={isProcessingChat}
                />
              )}
              
              {showHumanFeedback && false && (
                <HumanFeedback
                  questionForHuman={questionForHuman}
                  websocket={socket}
                  onFeedbackSubmit={handleFeedbackSubmit}
                />
              )}
            </div>
          )
        })
      )}
    </>
  );
}


================================================
FILE: frontend/nextjs/app/api/chat/route.ts
================================================
import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    // Parse the request body
    let body;
    try {
      body = await request.json();
    } catch (parseError) {
      console.error('Error parsing request body:', parseError);
      return NextResponse.json(
        { error: 'Invalid JSON in request body' },
        { status: 400 }
      );
    }
    
    console.log(`POST /api/chat - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });
    
    const data = await response.json();
    return NextResponse.json(data, { status: response.status });
  } catch (error: any) {
    console.error('POST /api/chat - Error proxying to backend:', error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
} 


================================================
FILE: frontend/nextjs/app/api/reports/route.ts
================================================
import { NextResponse } from 'next/server';

export async function GET(request: Request) {
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    const { searchParams, pathname } = new URL(request.url);
    
    // Check if we're requesting a specific report by ID
    const pathParts = pathname.split('/');
    const reportId = pathParts[pathParts.length - 1];
    
    if (reportId && reportId !== 'reports') {
      // Request for a specific report by ID - this should be handled by [id]/route.ts
      console.error(`GET /api/reports - Unexpected path format with ID: ${reportId}`);
      return NextResponse.json(
        { error: 'Invalid request path' },
        { status: 400 }
      );
    }
    
    // Normal list reports request
    const params = new URLSearchParams();
    
    // Forward any query parameters received
    Array.from(searchParams.entries()).forEach(([key, value]) => {
      params.append(key, value);
    });
    
    const queryString = params.toString();
    const endpoint = queryString ? `/api/reports?${queryString}` : '/api/reports';
    
    console.log(`GET ${endpoint} - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}${endpoint}`);
    
    if (!response.ok) {
      // Handle backend errors
      const errorData = await response.json().catch(() => ({ detail: `Error ${response.status}` }));
      console.error(`GET /api/reports - Backend error: ${JSON.stringify(errorData)}`);
      return NextResponse.json(
        { error: errorData.detail || 'Failed to fetch reports' },
        { status: response.status }
      );
    }
    
    const data = await response.json();
    
    // Ensure data has the expected structure
    if (!data.reports) {
      console.warn('Backend response missing reports array, adding empty array');
      data.reports = [];
    }
    
    console.log(`GET /api/reports - Successfully retrieved ${data.reports.length} reports`);
    return NextResponse.json(data, { status: 200 });
  } catch (error) {
    console.error('GET /api/reports - Error proxying to backend:', error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
}

export async function POST(request: Request) {
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    // Parse the request body
    let body;
    try {
      body = await request.json();
    } catch (parseError) {
      console.error('Error parsing request body:', parseError);
      return NextResponse.json(
        { error: 'Invalid JSON in request body' },
        { status: 400 }
      );
    }
    
    console.log(`POST /api/reports - Proxying request to backend for ID: ${body.id || 'unknown'}`);
    
    const response = await fetch(`${backendUrl}/api/reports`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });
    
    if (!response.ok) {
      // Handle backend errors
      const errorData = await response.json().catch(() => ({ detail: `Error ${response.status}` }));
      console.error(`POST /api/reports - Backend error: ${JSON.stringify(errorData)}`);
      return NextResponse.json(
        { error: errorData.detail || 'Failed to create/update report' },
        { status: response.status }
      );
    }
    
    const data = await response.json();
    console.log(`POST /api/reports - Successfully created/updated report with ID: ${data.id || body.id || 'unknown'}`);
    return NextResponse.json(data, { status: 200 });
  } catch (error) {
    console.error('POST /api/reports - Error proxying to backend:', error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
} 


================================================
FILE: frontend/nextjs/app/api/reports/[id]/route.ts
================================================
import { NextResponse } from 'next/server';

export async function GET(
  request: Request,
  { params }: { params: { id: string } }
) {
  const { id } = params;
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    console.log(`GET /api/reports/${id} - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/reports/${id}`);
    
    if (!response.ok) {
      // Handle backend errors
      const errorData = await response.json().catch(() => ({ detail: `Error ${response.status}` }));
      return NextResponse.json(
        { error: errorData.detail || 'Failed to fetch report' },
        { status: response.status }
      );
    }
    
    const data = await response.json();
    return NextResponse.json(data, { status: 200 });
  } catch (error) {
    console.error(`GET /api/reports/${id} - Error proxying to backend:`, error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
}

export async function DELETE(
  request: Request,
  { params }: { params: { id: string } }
) {
  const { id } = params;
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    console.log(`DELETE /api/reports/${id} - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/reports/${id}`, {
      method: 'DELETE',
    });
    
    if (!response.ok && response.status !== 404) {
      // Handle backend errors
      const errorData = await response.json().catch(() => ({ detail: `Error ${response.status}` }));
      return NextResponse.json(
        { error: errorData.detail || 'Failed to delete report' },
        { status: response.status }
      );
    }
    
    return NextResponse.json({ success: true }, { status: 200 });
  } catch (error) {
    console.error(`DELETE /api/reports/${id} - Error proxying to backend:`, error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
}

export async function PUT(
  request: Request,
  { params }: { params: { id: string } }
) {
  const { id } = params;
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    // Parse the request body
    let body;
    try {
      body = await request.json();
    } catch (parseError) {
      console.error('Error parsing request body:', parseError);
      return NextResponse.json(
        { error: 'Invalid JSON in request body' },
        { status: 400 }
      );
    }
    
    console.log(`PUT /api/reports/${id} - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/reports/${id}`, {
      method: 'PUT',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });
    
    if (!response.ok) {
      // Handle backend errors
      const errorData = await response.json().catch(() => ({ detail: `Error ${response.status}` }));
      return NextResponse.json(
        { error: errorData.detail || 'Failed to update report' },
        { status: response.status }
      );
    }
    
    const data = await response.json();
    return NextResponse.json(data, { status: 200 });
  } catch (error) {
    console.error(`PUT /api/reports/${id} - Error proxying to backend:`, error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
} 


================================================
FILE: frontend/nextjs/app/api/reports/[id]/chat/route.ts
================================================
import { NextResponse } from 'next/server';

export async function GET(
  request: Request,
  { params }: { params: { id: string } }
) {
  const { id } = params;
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    if (!id) {
      return NextResponse.json(
        { error: 'Missing report ID parameter' },
        { status: 400 }
      );
    }
    
    console.log(`GET /api/reports/${id}/chat - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/reports/${id}/chat`);
    const data = await response.json();
    
    return NextResponse.json(data, { status: response.status });
  } catch (error: any) {
    console.error(`GET /api/reports/${id}/chat - Error proxying to backend:`, error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
}

export async function POST(
  request: Request,
  { params }: { params: { id: string } }
) {
  const { id } = params;
  const backendUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || 'http://localhost:8000';
  
  try {
    if (!id) {
      return NextResponse.json(
        { error: 'Missing report ID parameter' },
        { status: 400 }
      );
    }
    
    // Parse the request body
    let body;
    try {
      body = await request.json();
    } catch (parseError) {
      console.error('Error parsing request body:', parseError);
      return NextResponse.json(
        { error: 'Invalid JSON in request body' },
        { status: 400 }
      );
    }
    
    console.log(`POST /api/reports/${id}/chat - Proxying request to backend`);
    
    const response = await fetch(`${backendUrl}/api/reports/${id}/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(body),
    });
    
    const data = await response.json();
    return NextResponse.json(data, { status: response.status });
  } catch (error: any) {
    console.error(`POST /api/reports/${id}/chat - Error proxying to backend:`, error);
    return NextResponse.json(
      { error: 'Failed to connect to backend service' },
      { status: 500 }
    );
  }
} 


================================================
FILE: frontend/nextjs/app/research/[id]/page.tsx
================================================
"use client";

import React, { useEffect, useState, useRef } from "react";
import { useRouter } from "next/navigation";
import { useResearchHistoryContext } from "@/hooks/ResearchHistoryContext";
import { preprocessOrderedData } from "@/utils/dataProcessing";
import { ChatBoxSettings, Data, ChatData, ChatMessage, QuestionData } from "@/types/data";
import { toast } from "react-hot-toast";
import { getAppropriateLayout } from "@/utils/getLayout";

import ResearchPageLayout from "@/components/layouts/ResearchPageLayout";
import CopilotLayout from "@/components/layouts/CopilotLayout";
import ResearchContent from "@/components/research/ResearchContent";
import CopilotResearchContent from "@/components/research/CopilotResearchContent";
import NotFoundContent from "@/components/research/NotFoundContent";
import LoadingDots from "@/components/LoadingDots";
import ResearchSidebar from "@/components/ResearchSidebar";

// Import mobile components
import MobileResearchContent from "@/components/mobile/MobileResearchContent";

export default function ResearchPage({ params }: { params: { id: string } }) {
  const router = useRouter();
  const { id } = params;
  const [loading, setLoading] = useState(true);
  const [question, setQuestion] = useState("");
  const [answer, setAnswer] = useState("");
  const [chatPromptValue, setChatPromptValue] = useState("");
  const [orderedData, setOrderedData] = useState<Data[]>([]);
  const [allLogs, setAllLogs] = useState<any[]>([]);
  const [isStopped, setIsStopped] = useState(false);
  const [currentResearchId, setCurrentResearchId] = useState<string | null>(null);
  const [isProcessingChat, setIsProcessingChat] = useState(false);
  const [sidebarOpen, setSidebarOpen] = useState(false);
  const [isMobile, setIsMobile] = useState(false);
  const [chatBoxSettings, setChatBoxSettings] = useState<ChatBoxSettings>(() => {
    // Default settings
    const defaultSettings = {
      report_source: "web",
      report_type: "research_report",
      tone: "Objective",
      domains: [],
      defaultReportType: "research_report",
      layoutType: 'copilot',
      mcp_enabled: false,
      mcp_configs: [],
      mcp_strategy: "fast",
    };

    // Try to load all settings from localStorage
    if (typeof window !== 'undefined') {
      const savedSettings = localStorage.getItem('chatBoxSettings');
      if (savedSettings) {
        try {
          const parsedSettings = JSON.parse(savedSettings);
          return {
            ...defaultSettings,
            ...parsedSettings, // Override defaults with saved settings
          };
        } catch (e) {
          console.error('Error parsing saved settings:', e);
        }
      }
    }
    return defaultSettings;
  });
  const [notFound, setNotFound] = useState(false);
  const [fetchAttempted, setFetchAttempted] = useState(false);
  const bottomRef = useRef<HTMLDivElement>(null);
  const toastShownRef = useRef(false);

  const { 
    history,
    getResearchById, 
    addChatMessage,
    getChatMessages,
    updateResearch,
    deleteResearch
  } = useResearchHistoryContext();

  // Toggle sidebar
  const toggleSidebar = () => {
    setSidebarOpen(!sidebarOpen);
  };

  // Handle selecting a research from the sidebar
  const handleSelectResearch = async (researchId: string) => {
    if (researchId !== id) {
      router.push(`/research/${researchId}`);
    }
    setSidebarOpen(false);
  };

  // Save chatBoxSettings to localStorage when they change
  useEffect(() => {
    localStorage.setItem('chatBoxSettings', JSON.stringify(chatBoxSettings));
  }, [chatBoxSettings]);

  // Load research data on mount
  useEffect(() => {
    // Prevent multiple fetch attempts for the same ID
    if (fetchAttempted) {
      console.log(`Skipping duplicate fetch for research ${id} (already attempted)`);
      return;
    }
    
    const fetchResearch = async () => {
      setLoading(true);
      setFetchAttempted(true); // Mark that we've attempted a fetch
      console.log(`Attempting to load research ${id}...`);
      
      // Reset toast tracking on each fetch attempt
      toastShownRef.current = false;
      
      // Step 1: Try to find it in localStorage first
      const storedHistory = localStorage.getItem('researchHistory');
      let localItem = null;
      
      if (storedHistory) {
        try {
          const localHistory = JSON.parse(storedHistory);
          localItem = localHistory.find((item: any) => item.id === id);
          
          if (localItem) {
            console.log(`Found research ${id} in localStorage!`);
            console.log(`- Question length: ${localItem.question.length}`);
            console.log(`- Answer length: ${localItem.answer?.length || 0}`);
          }
        } catch (error) {
          console.error('Error parsing localStorage:', error);
        }
      }
      
      // Step 2: Try to find it in the backend
      let foundInBackend = false;
      try {
        console.log(`Checking backend for research ${id}...`);
        const response = await fetch(`/api/reports/${id}`);
        
        if (response.ok) {
          console.log(`Found research ${id} in backend!`);
          foundInBackend = true;
          const data = await response.json();
          
          // Validate backend data
          if (!data.report) {
            console.error(`Backend response missing report object for ${id}`);
          } else {
            console.log(`- Question length: ${data.report.question.length}`);
            console.log(`- Answer length: ${data.report.answer?.length || 0}`);
            
            // Use the backend data, ensuring orderedData and chatMessages are arrays
            setQuestion(data.report.question);
            setAnswer(data.report.answer || '');
            setOrderedData(Array.isArray(data.report.orderedData) ? data.report.orderedData : []);
            setCurrentResearchId(id);
            setLoading(false);
          }
        } else if (response.status === 500) {
          // Handle server error
          console.error(`Backend server error when fetching research ${id}`);
          
          // Only show error toast if we haven't shown a toast yet in this component instance
          if (!toastShownRef.current) {
            console.log('Showing backend error toast');
            toast.error("Server connection error. Using local data if available.", {
              id: `server-error-${id}`, // Unique ID per research
            });
            toastShownRef.current = true;
          }
          
          // If we have local data, use it even if backend fails
          if (localItem) {
            setQuestion(localItem.question);
            setAnswer(localItem.answer || '');
            setOrderedData(Array.isArray(localItem.orderedData) ? localItem.orderedData : []);
            setCurrentResearchId(id);
            setLoading(false);
            return;
          }
          
          // If no local data, show not found
          setNotFound(true);
          setLoading(false);
        } else {
          console.log(`Research ${id} not found in backend (status: ${response.status})`);
        }
      } catch (error) {
        console.error('Error fetching from backend:', error);
        
        // Only show error toast if we haven't shown a toast yet in this component instance
        if (!toastShownRef.current) {
          console.log('Showing fetch error toast');
          toast.error("Failed to connect to server. Using local data if available.", {
            id: `fetch-error-${id}`, // Unique ID per research
          });
          toastShownRef.current = true;
        }
        
        // If we have local data, use it as fallback
        if (localItem) {
          setQuestion(localItem.question);
          setAnswer(localItem.answer || '');
          setOrderedData(Array.isArray(localItem.orderedData) ? localItem.orderedData : []);
          setCurrentResearchId(id);
          setLoading(false);
          return;
        }
      }
      
      // Step 3: If found in localStorage but not in backend, save it
      if (localItem && !foundInBackend) {
        console.log(`Saving research ${id} from localStorage to backend...`);
        try {
          // Ensure data is clean and serializable
          const cleanItem = {
            id: localItem.id,
            question: localItem.question,
            answer: localItem.answer || '',
            orderedData: Array.isArray(localItem.orderedData) ? JSON.parse(JSON.stringify(localItem.orderedData)) : [],
            chatMessages: Array.isArray(localItem.chatMessages) ? JSON.parse(JSON.stringify(localItem.chatMessages)) : [],
          };
          
          const saveResponse = await fetch('/api/reports', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify(cleanItem),
          });
          
          if (saveResponse.ok) {
            console.log(`Successfully saved research ${id} to backend!`);
          } else {
            console.warn(`Failed to save research to backend: ${await saveResponse.text()}`);
          }
          
          // Use the localStorage data
          setQuestion(localItem.question);
          setAnswer(localItem.answer || '');
          setOrderedData(Array.isArray(localItem.orderedData) ? localItem.orderedData : []);
          setCurrentResearchId(id);
          setLoading(false);
        } catch (error) {
          console.error('Error saving to backend:', error);
          
          // Still use the localStorage data even if save fails
          setQuestion(localItem.question);
          setAnswer(localItem.answer || '');
          setOrderedData(Array.isArray(localItem.orderedData) ? localItem.orderedData : []);
          setCurrentResearchId(id);
          setLoading(false);
        }
      }
      
      // Step 4: If not found anywhere, show not found message
      if (!localItem && !foundInBackend) {
        console.log(`Research ${id} not found anywhere`);
        setNotFound(true);
        setLoading(false);
      }
    };
    
    fetchResearch();
  }, [id, fetchAttempted]);

  // Process ordered data into logs for display
  useEffect(() => {
    const groupedData = preprocessOrderedData(orderedData);
    const statusReports = ["agent_generated", "starting_research", "planning_research", "error"];
    
    const newLogs = groupedData.reduce((acc: any[], data) => {
      // Process accordion blocks (grouped data)
      if (data.type === 'accordionBlock') {
        const logs = data.items.map((item: any, subIndex: any) => ({
          header: item.content,
          text: item.output,
          metadata: item.metadata,
          key: `${item.type}-${item.content}-${subIndex}`,
        }));
        return [...acc, ...logs];
      } 
      // Process status reports
      else if (statusReports.includes(data.content)) {
        return [...acc, {
          header: data.content,
          text: data.output,
          metadata: data.metadata,
          key: `${data.type}-${data.content}`,
        }];
      }
      return acc;
    }, []);
    
    setAllLogs(newLogs);
  }, [orderedData]);

  // Scroll to bottom when chat updates
  const scrollToBottom = () => {
    bottomRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    // Scroll to bottom when orderedData changes
    if (isProcessingChat === false && orderedData.length > 0) {
      setTimeout(scrollToBottom, 100); // Small delay to ensure content is rendered
    }
  }, [orderedData, isProcessingChat]);

  // Check if on mobile
  useEffect(() => {
    const checkIfMobile = () => {
      setIsMobile(window.innerWidth < 768);
    };
    
    // Initial check
    checkIfMobile();
    
    // Add event listener for window resize
    window.addEventListener('resize', checkIfMobile);
    
    // Cleanup
    return () => window.removeEventListener('resize', checkIfMobile);
  }, []);

  const handleChat = async (message: string) => {
    if (!currentResearchId || !answer) return;
    
    setIsProcessingChat(true);
    setChatPromptValue("");
    
    // Create a user message
    const userMessage: ChatMessage = {
      role: 'user',
      content: message,
      timestamp: Date.now()
    };
    
    // Create question data object to be shown immediately
    const questionData: QuestionData = { type: 'question', content: message };
    
    // IMPORTANT CHANGE: Add user question to UI immediately for better responsiveness
    setOrderedData(prevOrder => [...prevOrder, questionData]);
    
    // Then add to history asynchronously
    addChatMessage(currentResearchId, userMessage).catch(error => {
      console.error('Error adding chat message to history:', error);
    });
    
    try {
      // Get all chat messages for this research
      const chatMessages = getChatMessages(currentResearchId);
      
      // Format messages to ensure they only contain role and content properties
      const formattedMessages = [...chatMessages, userMessage].map(msg => ({
        role: msg.role,
        content: msg.content
      }));
      
      // Call the chat API
      const response = await fetch(`/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          report: answer,
          messages: formattedMessages
        }),
      });
      
      if (!response.ok) {
        throw new Error(`Failed to get chat response: ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.response) {
        // Add AI response to chat history asynchronously
        addChatMessage(currentResearchId, data.response).catch(error => {
          console.error('Error adding AI response to history:', error);
        });
        
        // Add response to the UI with any metadata
        const chatData: ChatData = { 
          type: 'chat', 
          content: data.response.content,
          metadata: data.response.metadata // Include metadata from the response
        };
        setOrderedData(prevOrder => [...prevOrder, chatData]);
        
        // Update research in history with both question and response asynchronously
        // Create a copy of the current orderedData plus the new items
        const updatedOrderedData = [...orderedData, questionData, chatData];
        updateResearch(currentResearchId, answer, updatedOrderedData).catch(error => {
          console.error('Error updating research:', error);
        });
      } else {
        // Show error message
        const errorChatData: ChatData = { 
          type: 'chat', 
          content: 'Sorry, something went wrong. Please try again.' 
        };
        setOrderedData(prevOrder => [...prevOrder, errorChatData]);
      }
    } catch (error) {
      console.error('Error during chat:', error);
      
      // Add error message
      const errorChatData: ChatData = { 
        type: 'chat', 
        content: 'Sorry, there was an error processing your request. Please try again.' 
      };
      setOrderedData(prevOrder => [...prevOrder, errorChatData]);
    } finally {
      setIsProcessingChat(false);
    }
  };

  const handleNewResearch = () => {
    router.push('/');
  };

  const handleCopyUrl = () => {
    const url = window.location.href;
    navigator.clipboard.writeText(url)
      .then(() => {
        toast.success("URL copied to clipboard!", {
          id: `copy-success-${id}`, // Unique ID per research
        });
      })
      .catch(() => {
        toast.error("Failed to copy URL", {
          id: `copy-error-${id}`, // Unique ID per research
        });
      });
  };

  // Custom toast options for this page
  const toastOptions = {
    duration: 4000,
    id: 'research-page-toast',
    style: {
      background: '#363636',
      color: '#fff',
    }
  };

  // Render mobile content
  const renderMobileContent = () => {
    if (notFound) {
      return <NotFoundContent onNewResearch={handleNewResearch} />;
    }
    
    if (loading) {
      return (
        <div className="min-h-[100vh] flex items-center justify-center">
          <LoadingDots />
        </div>
      );
    }
    
    // Make sure we're loading chat messages for the current research
    const chatMessages = currentResearchId ? getChatMessages(currentResearchId) : [];
    
    return (
      <MobileResearchContent
        orderedData={orderedData}
        answer={answer}
        loading={false}
        isStopped={isStopped}
        chatPromptValue={chatPromptValue}
        setChatPromptValue={setChatPromptValue}
        handleChat={handleChat}
        isProcessingChat={isProcessingChat}
        onNewResearch={handleNewResearch}
        currentResearchId={currentResearchId || undefined}
        onShareClick={handleCopyUrl}
      />
    );
  };

  // Loading state
  if (loading && !isMobile) {
    return getAppropriateLayout({
      loading,
      isStopped,
      showResult: true,
      onNewResearch: handleNewResearch,
      chatBoxSettings,
      setChatBoxSettings,
      toastOptions,
      children: (
        <div className="min-h-[100vh] pt-[120px] flex items-center justify-center">
          <LoadingDots />
        </div>
      )
    });
  }

  // Not found state for desktop
  if (notFound && !isMobile) {
    return getAppropriateLayout({
      loading: false,
      isStopped: false,
      showResult: false,
      onNewResearch: handleNewResearch,
      chatBoxSettings,
      setChatBoxSettings,
      toastOptions,
      children: <NotFoundContent onNewResearch={handleNewResearch} />
    });
  }

  // Mobile layout
  if (isMobile) {
    return getAppropriateLayout({
      loading,
      isStopped,
      showResult: true,
      onNewResearch: handleNewResearch,
      chatBoxSettings,
      setChatBoxSettings,
      toastOptions,
      toggleSidebar,
      isProcessingChat,
      children: renderMobileContent()
    });
  }

  // Normal state - research found on desktop
  return getAppropriateLayout({
    loading: false,
    isStopped,
    showResult: true,
    onNewResearch: handleNewResearch,
    chatBoxSettings,
    setChatBoxSettings,
    toastOptions,
    children: (
      <div className="relative">
        <ResearchSidebar
          history={history}
          onSelectResearch={handleSelectResearch}
          onNewResearch={handleNewResearch}
          onDeleteResearch={deleteResearch}
          isOpen={sidebarOpen}
          toggleSidebar={toggleSidebar}
        />
        
        {chatBoxSettings.layoutType === 'copilot' ? (
          <CopilotResearchContent
            orderedData={orderedData}
            answer={answer}
            allLogs={allLogs}
            chatBoxSettings={chatBoxSettings}
            loading={false}
            isStopped={isStopped}
            promptValue=""
            chatPromptValue={chatPromptValue}
            setPromptValue={() => {}}
            setChatPromptValue={setChatPromptValue}
            handleDisplayResult={() => {}}
            handleChat={handleChat}
            handleClickSuggestion={() => {}}
            currentResearchId={currentResearchId || undefined}
            onShareClick={handleCopyUrl}
            isProcessingChat={isProcessingChat}
            onNewResearch={handleNewResearch}
          />
        ) : (
          <ResearchContent
            showResult={true}
            orderedData={orderedData}
            answer={answer}
            allLogs={allLogs}
            chatBoxSettings={chatBoxSettings}
            loading={false}
            isInChatMode={true}
            isStopped={isStopped}
            promptValue=""
            chatPromptValue={chatPromptValue}
            setPromptValue={() => {}}
            setChatPromptValue={setChatPromptValue}
            handleDisplayResult={() => {}}
            handleChat={handleChat}
            handleClickSuggestion={() => {}}
            currentResearchId={currentResearchId || undefined}
            onShareClick={handleCopyUrl}
            isProcessingChat={isProcessingChat}
          />
        )}
      </div>
    )
  });
} 


================================================
FILE: frontend/nextjs/components/Footer.tsx
================================================
import React from 'react';
import Image from "next/image";
import Link from "next/link";
import Modal from './Settings/Modal';
import { ChatBoxSettings } from '@/types/data';

interface FooterProps {
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
}

const Footer: React.FC<FooterProps> = ({ chatBoxSettings, setChatBoxSettings }) => {
  // Add domain filtering from URL parameters
  if (typeof window !== 'undefined') {
    const urlParams = new URLSearchParams(window.location.search);
    const urlDomains = urlParams.get("domains");
    if (urlDomains) {
      // Split domains by comma if multiple domains are provided
      const domainArray = urlDomains.split(',').map(domain => ({
        value: domain.trim()
      }));
      localStorage.setItem('domainFilters', JSON.stringify(domainArray));
    }
  }

  return (
    <>
      <div className="container flex flex-col sm:flex-row min-h-[60px] sm:min-h-[72px] mt-2 items-center justify-center sm:justify-between border-t border-gray-200 px-4 pb-3 pt-4 sm:py-5 lg:px-0 bg-transparent backdrop-blur-sm gap-3 sm:gap-0">
        <Modal setChatBoxSettings={setChatBoxSettings} chatBoxSettings={chatBoxSettings} />
        <div className="text-xs sm:text-sm text-gray-700 text-center sm:text-left order-2 sm:order-1">
            Â© {new Date().getFullYear()} GPT Researcher. All rights reserved.
        </div>
        <div className="flex items-center gap-4 order-1 sm:order-2 mb-2 sm:mb-0">
          <Link href={"https://gptr.dev"} target="_blank" className="p-1">
              <svg 
                xmlns="http://www.w3.org/2000/svg" 
                viewBox="0 0 24 24" 
                fill="none" 
                stroke="currentColor" 
                strokeWidth="2" 
                strokeLinecap="round" 
                strokeLinejoin="round" 
                className="w-6 h-6 sm:w-7 sm:h-7 text-gray-800	800	 hover:text-blue-600 transition-colors duration-300"
              >
                <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z" />
                <polyline points="9 22 9 12 15 12 15 22" />
              </svg>
          </Link>
          <Link href={"https://github.com/assafelovic/gpt-researcher"} target="_blank" className="p-1">
            <img
              src={"/img/github.svg"}
              alt="github"
              width={24}
              height={24}
              className="w-6 h-6 sm:w-7 sm:h-7"
            />{" "}
          </Link>
          <Link href={"https://discord.gg/QgZXvJAccX"} target="_blank" className="p-1">
              <img
                src={"/img/discord.svg"}
                alt="discord"
                width={24}
                height={24}
                className="w-6 h-6 sm:w-7 sm:h-7"
              />{" "}
          </Link>
          <Link href={"https://hub.docker.com/r/gptresearcher/gpt-researcher"} target="_blank" className="p-1">
              <img
                src={"/img/docker.svg"}
                alt="docker"
                width={24}
                height={24}
                className="w-6 h-6 sm:w-7 sm:h-7"
              />{" "}
          </Link>
        </div>
      </div>
    </>
  );
};

export default Footer;


================================================
FILE: frontend/nextjs/components/Header.tsx
================================================
import React from 'react';
import Image from "next/image";

interface HeaderProps {
  loading?: boolean;      // Indicates if research is currently in progress
  isStopped?: boolean;    // Indicates if research was manually stopped
  showResult?: boolean;   // Controls if research results are being displayed
  onStop?: () => void;    // Handler for stopping ongoing research
  onNewResearch?: () => void;  // Handler for starting fresh research
  isCopilotMode?: boolean; // Indicates if we are in copilot mode
}

const Header = ({ loading, isStopped, showResult, onStop, onNewResearch, isCopilotMode }: HeaderProps) => {
  return (
    <div className="fixed top-0 left-0 right-0 z-50">
      {/* Pure transparent blur background */}
      <div className="absolute inset-0 backdrop-blur-sm bg-transparent"></div>
      
      {/* Header container */}
      <div className="container relative h-[60px] px-4 lg:h-[80px] lg:px-0 pt-4 pb-4">
        <div className="flex flex-col items-center">
          {/* Logo/Home link */}
          <a href="/">
            <img
              src="/img/gptr-logo.png"
              alt="logo"
              width={60}
              height={60}
              className="lg:h-16 lg:w-16"
            />
          </a>
          
          {/* Action buttons container */}
          <div className="flex gap-2 mt-2 transition-all duration-300 ease-in-out">
            {/* Stop button - shown only during active research */}
            {loading && !isStopped && (
              <button
                onClick={onStop}
                className="flex items-center justify-center px-4 sm:px-6 h-9 sm:h-10 text-sm text-gray-800	800	 bg-red-500 rounded-full hover:bg-red-600 transform hover:scale-105 transition-all duration-200 shadow-lg whitespace-nowrap min-w-[80px]"
              >
                Stop
              </button>
            )}
            {/* New Research button - shown after stopping or completing research - but not in copilot mode */}
            {(isStopped || !loading) && showResult && !isCopilotMode && (
              <button
                onClick={onNewResearch}
                className="flex items-center justify-center px-4 sm:px-6 h-9 sm:h-10 text-sm text-gray-800	800	 bg-blue-500 rounded-full hover:bg-blue-500 transform hover:scale-105 transition-all duration-200 shadow-lg whitespace-nowrap min-w-[120px]"
              >
                New Research
              </button>
            )}
          </div>
        </div>
      </div>
    </div>
  );
};

export default Header;



================================================
FILE: frontend/nextjs/components/Hero.tsx
================================================
import Image from "next/image";
import React, { FC, useEffect, useState, useRef } from "react";
import InputArea from "./ResearchBlocks/elements/InputArea";
import { motion, AnimatePresence } from "framer-motion";

type THeroProps = {
  promptValue: string;
  setPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleDisplayResult: (query : string) => void;
};

const Hero: FC<THeroProps> = ({
  promptValue,
  setPromptValue,
  handleDisplayResult,
}) => {
  const [isVisible, setIsVisible] = useState(false);
  const [showGradient, setShowGradient] = useState(true);
  const particlesContainerRef = useRef<HTMLDivElement>(null);
  
  useEffect(() => {
    setIsVisible(true);
    
    // Create particles for the background effect
    if (particlesContainerRef.current) {
      const container = particlesContainerRef.current;
      const particleCount = window.innerWidth < 768 ? 15 : 30; // Reduce particles on mobile
      
      // Clear any existing particles
      container.innerHTML = '';
      
      for (let i = 0; i < particleCount; i++) {
        const particle = document.createElement('div');
        
        // Random particle attributes
        const size = Math.random() * 4 + 1;
        const posX = Math.random() * 100;
        const posY = Math.random() * 100;
        const duration = Math.random() * 50 + 20;
        const delay = Math.random() * 5;
        const opacity = Math.random() * 0.3 + 0.1;
        
        // Apply styles
        particle.className = 'absolute rounded-full bg-white';
        Object.assign(particle.style, {
          width: `${size}px`,
          height: `${size}px`,
          left: `${posX}%`,
          top: `${posY}%`,
          opacity: opacity.toString(),
          animation: `float ${duration}s ease-in-out ${delay}s infinite`,
        });
        
        container.appendChild(particle);
      }
    }
    
    // Add scroll event listener to show/hide gradient
    let lastScrollY = window.scrollY;
    const threshold = 50; // Amount of scroll before hiding gradient (reduced for quicker response)
    
    const handleScroll = () => {
      const currentScrollY = window.scrollY;
      
      if (currentScrollY <= threshold) {
        // At or near the top, show gradient
        setShowGradient(true);
      } else if (currentScrollY > lastScrollY) {
        // Scrolling down, hide gradient
        setShowGradient(false);
      } else if (currentScrollY < lastScrollY) {
        // Scrolling up, show gradient
        setShowGradient(true);
      }
      
      lastScrollY = currentScrollY;
    };
    
    window.addEventListener('scroll', handleScroll);
    
    const container = particlesContainerRef.current;
    // Clean up function
    return () => {
      if (container) {
        container.innerHTML = '';
      }
      window.removeEventListener('scroll', handleScroll);
    };
  }, []);

  const handleClickSuggestion = (value: string) => {
    setPromptValue(value);
  };

  // Animation variants for consistent animations
  const fadeInUp = {
    hidden: { opacity: 0, y: 20 },
    visible: { opacity: 1, y: 0 }
  };

  return (
    <div className="relative overflow-visible min-h-[100vh] flex items-center pt-[60px] sm:pt-[80px] mt-[-60px] sm:mt-[-130px]">
      {/* Particle background */}
      <div ref={particlesContainerRef} className="absolute inset-0 -z-20"></div>
      
      <motion.div 
        initial="hidden"
        animate={isVisible ? "visible" : "hidden"}
        variants={fadeInUp}
        transition={{ duration: 0.8 }}
        className="flex flex-col items-center justify-center w-full py-6 sm:py-8 md:py-16 lg:pt-10 lg:pb-20"
      >
        {/* Header text */}
        <motion.h1 
          variants={fadeInUp}
          transition={{ duration: 0.8, delay: 0.1 }}
          className="text-2xl sm:text-3xl md:text-4xl font-medium text-center text-gray-800	800	 mb-8 sm:mb-10 md:mb-12 px-4"
        >
          What would you like to research next?
        </motion.h1>

        {/* Input section with enhanced styling */}
        <motion.div 
          variants={fadeInUp}
          transition={{ duration: 0.8, delay: 0.2 }}
          className="w-full max-w-[800px] pb-6 sm:pb-8 md:pb-10 px-4"
        >
          <div className="relative group">
            <div className="absolute -inset-1 bg-gradient-to-r from-teal-600/70 via-cyan-500/60 to-blue-600/70 rounded-xl blur-md opacity-60 group-hover:opacity-85 transition duration-1000 group-hover:duration-200 animate-gradient-x"></div>
            <div className="relative bg-black bg-opacity-20 backdrop-blur-sm rounded-xl ring-1 ring-gray-200
              <InputArea
                promptValue={promptValue}
                setPromptValue={setPromptValue}
                handleSubmit={handleDisplayResult}
              />
            </div>
          </div>
          
          {/* Disclaimer text */}
          <motion.div
            variants={fadeInUp}
            transition={{ duration: 0.6, delay: 0.3 }}
            className="mt-6 text-center px-4"
          >
            <p className="text-gray-500 text-sm font-light">
              GPT Researcher may make mistakes. Verify important information and check sources.
            </p>
          </motion.div>
        </motion.div>

        {/* Suggestions section with enhanced styling */}
        <motion.div 
          variants={fadeInUp}
          transition={{ duration: 0.8, delay: 0.4 }}
          className="flex flex-wrap items-center justify-center gap-2 xs:gap-3 md:gap-4 pb-6 sm:pb-8 md:pb-10 px-4 lg:flex-nowrap lg:justify-normal"
        >
          <AnimatePresence>
            {suggestions.map((item, index) => (
              <motion.div
                key={item.id}
                variants={fadeInUp}
                initial="hidden"
                animate="visible"
                transition={{ duration: 0.4, delay: 0.6 + (index * 0.1) }}
                className="flex h-[38px] sm:h-[42px] cursor-pointer items-center justify-center gap-[6px] rounded-lg 
                         border border-solid border-teal-500/30 bg-gradient-to-r from-teal-900/30 to-cyan-900/30 
                         backdrop-blur-sm px-2 sm:px-3 py-1 sm:py-2 hover:border-teal-500/60 hover:from-teal-900/40 
                         hover:to-cyan-900/40 transition-all duration-300 hover:shadow-lg hover:shadow-teal-900/20 min-w-[100px]"
                onClick={() => handleClickSuggestion(item?.name)}
                whileHover={{ scale: 1.05 }}
                whileTap={{ scale: 0.98 }}
              >
                <img
                  src={item.icon}
                  alt={item.name}
                  width={18}
                  height={18}
                  className="w-[18px] sm:w-[20px] opacity-80 filter invert brightness-100"
                />
                <span className="text-xs sm:text-sm font-medium leading-[normal] text-gray-700">
                  {item.name}
                </span>
              </motion.div>
            ))}
          </AnimatePresence>
        </motion.div>
      </motion.div>

      {/* Magical premium gradient glow at the bottom */}
      <motion.div 
        initial={{ opacity: 0 }}
        animate={{ opacity: showGradient ? 1 : 0 }}
        transition={{ duration: 1.2 }}
        className="fixed bottom-0 left-0 right-0 h-[12px] z-50 overflow-hidden pointer-events-none"
      >
        <div className="relative w-full h-full">
          {/* Main perfect center glow with smooth fade at edges */}
          <div 
            className="absolute inset-0"
            style={{
              opacity: 0.85,
              background: 'radial-gradient(ellipse at center, rgba(12, 219, 182, 1) 0%, rgba(6, 219, 238, 0.7) 25%, rgba(6, 219, 238, 0.2) 50%, rgba(0, 0, 0, 0) 75%)',
              boxShadow: '0 0 30px 6px rgba(12, 219, 182, 0.5), 0 0 60px 10px rgba(6, 219, 238, 0.25)'
            }}
          />
          
          {/* Subtle shimmer overlay with perfect center focus */}
          <div 
            className="absolute inset-0"
            style={{
              animation: 'shimmer 8s ease-in-out infinite alternate',
              opacity: 0.5,
              background: 'radial-gradient(ellipse at center, rgba(255, 255, 255, 0.8) 0%, rgba(255, 255, 255, 0.2) 30%, rgba(255, 255, 255, 0) 60%)'
            }}
          />
          
          {/* Gentle breathing effect */}
          <div 
            className="absolute inset-0"
            style={{
              opacity: 0.4,
              animation: 'breathe 7s cubic-bezier(0.4, 0.0, 0.2, 1) infinite',
              background: 'radial-gradient(circle at center, rgba(255, 255, 255, 0.6) 0%, rgba(255, 255, 255, 0) 50%)'
            }}
          />
        </div>
      </motion.div>
      
      {/* Custom keyframes for magical animations */}
      <style jsx global>{`
        @keyframes shimmer {
          0% {
            opacity: 0.4;
            transform: scale(0.98);
          }
          50% {
            opacity: 0.6;
          }
          100% {
            opacity: 0.4;
            transform: scale(1.02);
          }
        }
        
        @keyframes breathe {
          0%, 100% {
            opacity: 0.3;
            transform: scale(0.96);
          }
          50% {
            opacity: 0.5;
            transform: scale(1.04);
          }
        }
      `}</style>
    </div>
  );
};

type suggestionType = {
  id: number;
  name: string;
  icon: string;
};

const suggestions: suggestionType[] = [
  {
    id: 1,
    name: "Stock analysis on ",
    icon: "/img/stock2.svg",
  },
  {
    id: 2,
    name: "Help me plan an adventure to ",
    icon: "/img/hiker.svg",
  },
  {
    id: 3,
    name: "What are the latest news on ",
    icon: "/img/news.svg",
  },
];

export default Hero;



================================================
FILE: frontend/nextjs/components/HumanFeedback.tsx
================================================
// /multi_agents/frontend/components/HumanFeedback.tsx

import React, { useState, useEffect } from 'react';

interface HumanFeedbackProps {
  websocket: WebSocket | null;
  onFeedbackSubmit: (feedback: string | null) => void;
  questionForHuman: boolean;
}

const HumanFeedback: React.FC<HumanFeedbackProps> = ({ questionForHuman, websocket, onFeedbackSubmit }) => {
  const [feedbackRequest, setFeedbackRequest] = useState<string | null>(null);
  const [userFeedback, setUserFeedback] = useState<string>('');

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    onFeedbackSubmit(userFeedback === '' ? null : userFeedback);
    setFeedbackRequest(null);
    setUserFeedback('');
  };

  return (
    <div className="bg-gray-100 p-4 rounded-lg shadow-md">
      <h3 className="text-lg font-semibold mb-2">Human Feedback Required</h3>
      <p className="mb-4">{questionForHuman}</p>
      <form onSubmit={handleSubmit}>
        <textarea
          className="w-full p-2 border rounded-md"
          value={userFeedback}
          onChange={(e) => setUserFeedback(e.target.value)}
          placeholder="Enter your feedback here (or leave blank for 'no')"
        />
        <button
          type="submit"
          className="mt-2 px-4 py-2 bg-blue-500 text-gray-800	800	 rounded-md hover:bg-blue-600"
        >
          Submit Feedback
        </button>
      </form>
    </div>
  );
};

export default HumanFeedback;


================================================
FILE: frontend/nextjs/components/LoadingDots.tsx
================================================
import React from 'react';

const LoadingDots = () => {
  return (
    <div className="flex justify-center py-4">
      <div className="animate-pulse flex space-x-2">
        <div className="w-2 h-2 bg-gray-300 rounded-full"></div>
        <div className="w-2 h-2 bg-gray-300 rounded-full"></div>
        <div className="w-2 h-2 bg-gray-300 rounded-full"></div>
      </div>
    </div>
  );
};

export default LoadingDots; 


================================================
FILE: frontend/nextjs/components/ResearchResults.tsx
================================================
import React from 'react';
import Question from './ResearchBlocks/Question';
import Report from './ResearchBlocks/Report';
import Sources from './ResearchBlocks/Sources';
import ImageSection from './ResearchBlocks/ImageSection';
import SubQuestions from './ResearchBlocks/elements/SubQuestions';
import LogsSection from './ResearchBlocks/LogsSection';
import AccessReport from './ResearchBlocks/AccessReport';
import { preprocessOrderedData } from '../utils/dataProcessing';
import { Data } from '../types/data';

interface ResearchResultsProps {
  orderedData: Data[];
  answer: string;
  allLogs: any[];
  chatBoxSettings: any;
  handleClickSuggestion: (value: string) => void;
  currentResearchId?: string;
  isProcessingChat?: boolean;
  onShareClick?: () => void;
}

export const ResearchResults: React.FC<ResearchResultsProps> = ({
  orderedData,
  answer,
  allLogs,
  chatBoxSettings,
  handleClickSuggestion,
  currentResearchId,
  isProcessingChat = false,
  onShareClick
}) => {
  const groupedData = preprocessOrderedData(orderedData);
  const pathData = groupedData.find(data => data.type === 'path');
  const initialQuestion = groupedData.find(data => data.type === 'question');

  const chatComponents = groupedData
    .filter(data => {
      if (data.type === 'question' && data === initialQuestion) {
        return false;
      }
      return (data.type === 'question' || data.type === 'chat');
    })
    .map((data, index) => {
      if (data.type === 'question') {
        return <Question key={`question-${index}`} question={data.content} />;
      } else {
        return <Report key={`chat-${index}`} answer={data.content} />;
      }
    });

  const sourceComponents = groupedData
    .filter(data => data.type === 'sourceBlock')
    .map((data, index) => (
      <Sources key={`sourceBlock-${index}`} sources={data.items}/>
    ));

  const imageComponents = groupedData
    .filter(data => data.type === 'imagesBlock')
    .map((data, index) => (
      <ImageSection key={`images-${index}-${data.metadata?.length || 0}`} metadata={data.metadata} />
    ));

  const initialReport = groupedData.find(data => data.type === 'reportBlock');
  const finalReport = groupedData
    .filter(data => data.type === 'reportBlock')
    .pop();
  const subqueriesComponent = groupedData.find(data => data.content === 'subqueries');

  return (
    <>
      {initialQuestion && <Question question={initialQuestion.content} />}
      {orderedData.length > 0 && <LogsSection logs={allLogs} />}
      {subqueriesComponent && (
        <SubQuestions
          metadata={subqueriesComponent.metadata}
          handleClickSuggestion={handleClickSuggestion}
        />
      )}
      {sourceComponents}
      {imageComponents}
      {finalReport && <Report answer={finalReport.content} researchId={currentResearchId} />}
      {pathData && <AccessReport accessData={pathData.output} report={answer} chatBoxSettings={chatBoxSettings} onShareClick={onShareClick} />}
      {chatComponents}
    </>
  );
}; 


================================================
FILE: frontend/nextjs/components/ResearchSidebar.tsx
================================================
import React, { useState, useRef, useEffect } from 'react';
import Link from 'next/link';
import { ResearchHistoryItem } from '../types/data';
import { formatDistanceToNow } from 'date-fns';
import { motion, AnimatePresence } from 'framer-motion';

interface ResearchSidebarProps {
  history: ResearchHistoryItem[];
  onSelectResearch: (id: string) => void;
  onNewResearch: () => void;
  onDeleteResearch: (id: string) => void;
  isOpen: boolean;
  toggleSidebar: () => void;
}

const ResearchSidebar: React.FC<ResearchSidebarProps> = ({
  history,
  onSelectResearch,
  onNewResearch,
  onDeleteResearch,
  isOpen,
  toggleSidebar,
}) => {
  const [hoveredItem, setHoveredItem] = useState<string | null>(null);
  const sidebarRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    const handleClickOutside = (event: MouseEvent) => {
      if (isOpen && 
          sidebarRef.current && 
          !sidebarRef.current.contains(event.target as Node)) {
        toggleSidebar();
      }
    };

    document.addEventListener('mousedown', handleClickOutside);
    
    return () => {
      document.removeEventListener('mousedown', handleClickOutside);
    };
  }, [isOpen, toggleSidebar]);

  // Format timestamp for display
  const formatTimestamp = (timestamp: number | string | Date | undefined) => {
    if (!timestamp) return 'Unknown time';
    
    try {
      const date = new Date(timestamp);
      if (isNaN(date.getTime())) return 'Unknown time';
      return formatDistanceToNow(date, { addSuffix: true });
    } catch {
      return 'Unknown time';
    }
  };

  // Animation variants
  const sidebarVariants = {
    open: { 
      width: 'var(--sidebar-width)', 
      transition: { type: 'spring', stiffness: 250, damping: 25 } 
    },
    closed: { 
      width: 'var(--sidebar-min-width)', 
      transition: { type: 'spring', stiffness: 250, damping: 25, delay: 0.1 } 
    }
  };
  
  const fadeInVariants = {
    hidden: { opacity: 0, transition: { duration: 0.2 } },
    visible: { opacity: 1, transition: { duration: 0.3 } }
  };

  return (
    <>
      {/* Overlay for mobile */}
      <AnimatePresence>
        {isOpen && (
          <motion.div 
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            transition={{ duration: 0.3 }}
            className="sidebar-overlay md:hidden fixed inset-0 bg-black bg-opacity-50 backdrop-blur-sm z-40" 
            onClick={toggleSidebar}
            aria-hidden="true"
          />
        )}
      </AnimatePresence>
      
      <motion.div 
        ref={sidebarRef} 
        className="fixed top-0 left-0 h-full sidebar-z-index"
        variants={sidebarVariants}
        initial={false}
        animate={isOpen ? 'open' : 'closed'}
        style={{
          '--sidebar-width': 'min(300px, 85vw)',
          '--sidebar-min-width': '12px'
        } as React.CSSProperties}
      >
        {/* Sidebar content */}
        <div 
          className={`h-full transition-all duration-300 text-gray-800	800	 overflow-hidden 
            ${isOpen 
              ? 'bg-gray-50/80 backdrop-blur-md shadow-2xl shadow-black/30 p-3 sm:p-4' 
              : 'bg-transparent p-0'
            }`}
        >
          {/* Toggle button - only shown when sidebar is closed */}
          <AnimatePresence mode="wait">
            {!isOpen ? (
              <motion.div
                key="toggle-button"
                initial={{ opacity: 0 }}
                animate={{ opacity: 1 }}
                exit={{ opacity: 0 }}
                transition={{ duration: 0.2 }}
                className="absolute left-4 sm:left-6 mx-auto top-1.5 sm:top-3.5 w-8 sm:w-10 h-8 sm:h-10 flex items-center justify-center rounded-full shadow-sm z-10 overflow-hidden cursor-pointer group"
                onClick={toggleSidebar}
                aria-label="Open sidebar"
              >
                {/* Subtle glowing background */}
                <div className="absolute inset-0 bg-gradient-to-br from-teal-500/15 via-cyan-400/12 to-blue-500/10 group-hover:from-teal-500/25 group-hover:via-cyan-400/20 group-hover:to-blue-500/15 transition-all duration-300 group-hover:shadow-[0_0_15px_rgba(20,184,166,0.3)]"></div>
                
                {/* Icon with subtle glow effect */}
                <svg 
                  xmlns="http://www.w3.org/2000/svg" 
                  className="h-5 sm:h-6 w-5 sm:w-6 relative text-teal-100/90 filter drop-shadow-[0_0_1px_rgba(45,212,191,0.5)]" 
                  fill="none" 
                  viewBox="0 0 24 24" 
                  stroke="currentColor"
                >
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
                </svg>
              </motion.div>
            ) : (
              <motion.div
                key="sidebar-content"
                initial="hidden"
                animate="visible"
                exit="hidden"
                variants={fadeInVariants}
              >
                <div className="flex justify-between items-center mb-5 sm:mb-6">
                  <h2 className="text-lg sm:text-xl font-semibold bg-gradient-to-r from-teal-400 to-cyan-400 bg-clip-text text-transparent">Research History</h2>
                  <button
                    onClick={toggleSidebar}
                    className="w-8 h-8 sm:w-10 sm:h-10 flex items-center justify-center bg-gray-100/60 text-gray-800	800	 rounded-full shadow-lg hover:bg-gray-100 transition-all duration-300 group"
                    aria-label="Close sidebar"
                  >
                    <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4 transition-transform duration-300 group-hover:scale-110" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M15 19l-7-7 7-7" />
                    </svg>
                  </button>
                </div>

                {/* New Research button */}
                <button
                  onClick={onNewResearch}
                  className="relative w-full py-2.5 sm:py-3 px-3 sm:px-4 mb-5 sm:mb-6 bg-blue-500 text-gray-800	800	 rounded-md font-bold text-sm transition-all duration-300 overflow-hidden group"
                >
                  {/* Gradient background on hover */}
                  <div className="absolute inset-0 opacity-0 group-hover:opacity-100 bg-gradient-to-br from-[#0cdbb6] via-[#1fd0f0] to-[#06dbee] transition-opacity duration-500"></div>
                  
                  {/* Magical glow effect */}
                  <div className="absolute inset-0 opacity-0 group-hover:opacity-100 transition-opacity duration-500" 
                      style={{
                        boxShadow: 'inset 0 0 20px 5px rgba(255, 255, 255, 0.2)',
                        background: 'radial-gradient(circle at center, rgba(255, 255, 255, 0.15) 0%, rgba(255, 255, 255, 0) 70%)'
                      }}>
                  </div>
                  
                  <div className="relative z-10 flex items-center justify-center">
                    <svg xmlns="http://www.w3.org/2000/svg" className="h-4 sm:h-5 w-4 sm:w-5 mr-2 transition-transform duration-300 group-hover:scale-110" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 4v16m8-8H4" />
                    </svg>
                    New Research
                  </div>
                </button>

                {/* History list with improved scrollbar */}
                <div className="overflow-y-auto h-[calc(100vh-150px)] sm:h-[calc(100vh-190px)] pr-1 custom-scrollbar">
                  {history.length === 0 ? (
                    <div className="text-center py-8 sm:py-10 px-4">
                      <div className="w-16 h-16 sm:w-20 sm:h-20 mx-auto mb-4 rounded-full bg-gradient-to-br from-gray-800/60 to-gray-700/40 flex items-center justify-center">
                        <svg xmlns="http://www.w3.org/2000/svg" className="h-8 sm:h-10 w-8 sm:w-10 text-gray-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
                        </svg>
                      </div>
                      <h3 className="text-lg font-medium text-gray-700 mb-2">No research history yet</h3>
                      <p className="text-sm text-gray-500">Start your first research journey to build your knowledge library</p>
                    </div>
                  ) : (
                    <ul className="space-y-2 sm:space-y-3">
                      {history.map((item) => (
                        <motion.li 
                          key={item.id}
                          className="relative rounded-xl transition-all duration-300 overflow-hidden group bg-gray-50/40 hover:bg-gray-100/60 border border-gray-200 hover:border-gray-600/50 backdrop-blur-sm"
                          onMouseEnter={() => setHoveredItem(item.id)}
                          onMouseLeave={() => setHoveredItem(null)}
                        >
                          
                          <Link
                            href={`/research/${item.id}`}
                            className="block w-full text-left p-3 sm:p-4 pr-10 min-h-[56px] relative"
                            onClick={(e) => {
                              // Only prevent default if we're just closing the sidebar
                              if (!isOpen) {
                                e.preventDefault();
                              }
                              // Call onSelectResearch only if we're actually navigating
                              if (isOpen) {
                                onSelectResearch(item.id);
                              }
                              // Always close the sidebar
                              toggleSidebar();
                            }}
                          >
                            <h3 className="font-medium truncate text-gray-700 text-sm sm:text-base transition-colors duration-200 group-hover:text-blue-600">{item.question}</h3>
                            <p className="text-xs text-gray-500 mt-1.5 flex items-center">
                              <svg xmlns="http://www.w3.org/2000/svg" className="h-3.5 w-3.5 mr-1 text-gray-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
                              </svg>
                              {formatTimestamp(item.timestamp || (item as any).updated_at || (item as any).created_at)}
                            </p>
                          </Link>
                          
                          <button
                            onClick={(e) => {
                              e.stopPropagation();
                              onDeleteResearch(item.id);
                            }}
                            className="absolute top-2 right-2 p-1.5 rounded-full opacity-0 group-hover:opacity-100 transition-opacity text-gray-500 hover:text-gray-800	800	 hover:bg-gray-700"
                            aria-label="Delete research"
                          >
                            <svg xmlns="http://www.w3.org/2000/svg" className="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 7l-.867 12.142A2 2 0 0116.138 21H7.862a2 2 0 01-1.995-1.858L5 7m5 4v6m4-6v6m1-10V4a1 1 0 00-1-1h-4a1 1 0 00-1 1v3M4 7h16" />
                            </svg>
                          </button>
                        </motion.li>
                      ))}
                    </ul>
                  )}
                </div>
              </motion.div>
            )}
          </AnimatePresence>
        </div>
      </motion.div>
      
      {/* Custom scrollbar styles */}
      <style jsx global>{`
        .custom-scrollbar::-webkit-scrollbar {
          width: 5px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(15, 23, 42, 0.3);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(45, 212, 191, 0.3);
          border-radius: 20px;
          transition: all 0.3s;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(45, 212, 191, 0.6);
        }
      `}</style>
    </>
  );
};

export default ResearchSidebar;


================================================
FILE: frontend/nextjs/components/SimilarTopics.tsx
================================================
import Image from "next/image";

const SimilarTopics = ({
  similarQuestions,
  handleDisplayResult,
  reset,
}: {
  similarQuestions: string[];
  handleDisplayResult: (item: string) => void;
  reset: () => void;
}) => {
  return (
    <div className="container flex h-auto w-full shrink-0 gap-4 rounded-lg border border-solid border-[#C2C2C2] bg-white p-5 lg:p-10">
      <div className="hidden lg:block">
        <img
          src="/img/similarTopics.svg"
          alt="footer"
          width={24}
          height={24}
        />
      </div>
      <div className="flex-1 divide-y divide-[#E5E5E5]">
        <div className="flex gap-4 pb-3">
          <img
            src="/img/similarTopics.svg"
            alt="footer"
            width={24}
            height={24}
            className="block lg:hidden"
          />
          <h3 className="text-base font-bold uppercase text-black">
            Similar topics:{" "}
          </h3>
        </div>

        <div className="max-w-[890px] space-y-[15px] divide-y divide-[#E5E5E5]">
          {similarQuestions.length > 0 ? (
            similarQuestions.map((item) => (
              <button
                className="flex cursor-pointer items-center gap-4 pt-3.5"
                key={item}
                onClick={() => {
                  reset();
                  handleDisplayResult(item);
                }}
              >
                <div className="flex items-center">
                  <img
                    src="/img/arrow-circle-up-right.svg"
                    alt="footer"
                    width={24}
                    height={24}
                  />
                </div>
                <p className="text-sm font-light leading-[normal] text-[#1B1B16] [leading-trim:both] [text-edge:cap]">
                  {item}
                </p>
              </button>
            ))
          ) : (
            <>
              <div className="h-10 w-full animate-pulse rounded-md bg-gray-300" />
              <div className="h-10 w-full animate-pulse rounded-md bg-gray-300" />
              <div className="h-10 w-full animate-pulse rounded-md bg-gray-300" />
            </>
          )}
        </div>
      </div>
    </div>
  );
};

export default SimilarTopics;



================================================
FILE: frontend/nextjs/components/TypeAnimation.tsx
================================================
const TypeAnimation = () => {
  return (
    <div className="loader pb-1">
      <span></span>
      <span></span>
      <span></span>
    </div>
  );
};

export default TypeAnimation;



================================================
FILE: frontend/nextjs/components/Images/ImageModal.tsx
================================================
import React, { useEffect } from 'react';
import { TouchEventHandler } from 'react';

interface ImageModalProps {
    imageSrc: any;
    isOpen: boolean;
    onClose: () => void;
    onNext?: () => void;
    onPrev?: () => void;
  }


export default function ImageModal({ imageSrc, isOpen, onClose, onNext, onPrev }: ImageModalProps) {
    useEffect(() => {
        if (!isOpen) return;
        
        const handleKeyDown = (e: KeyboardEvent) => {
            if (e.key === 'ArrowLeft') {
                onPrev?.();
            } else if (e.key === 'ArrowRight') {
                onNext?.();
            } else if (e.key === 'Escape') {
                onClose();
            }
        };

        document.addEventListener('keydown', handleKeyDown);
        return () => document.removeEventListener('keydown', handleKeyDown);
    }, [isOpen, onClose, onNext, onPrev]);

    if (!isOpen) return null;

    // Swipe detection for mobile
    let touchStartX = 0;
    let touchEndX = 0;

    const handleTouchStart = (e: TouchEvent) => {
        touchStartX = e.changedTouches[0].screenX;
    };

    const handleTouchEnd = (e: TouchEvent) => {
        touchEndX = e.changedTouches[0].screenX;
        handleSwipeGesture();
    };

    const handleSwipeGesture = () => {
        if (touchEndX < touchStartX - 50) {
            onNext?.();
        } else if (touchEndX > touchStartX + 50) {
            onPrev?.();
        }
    };

    const handleClose = (e: React.MouseEvent<HTMLDivElement>) => {
        if (e.target === e.currentTarget) {
            onClose();
        }
    };

    return (
        <div
            className="fixed inset-0 bg-black bg-opacity-75 z-50 flex items-center justify-center p-4 overflow-auto"
            onClick={handleClose}
            onTouchStart={handleTouchStart as unknown as TouchEventHandler<HTMLDivElement>}
            onTouchEnd={handleTouchEnd as unknown as TouchEventHandler<HTMLDivElement>}
        >
            <div className="relative max-w-[90vw] max-h-[90vh] flex items-center justify-center">
                <button
                        onClick={onPrev}
                    className="absolute left-4 z-10 bg-black bg-opacity-50 text-gray-800	800	 p-2 rounded-full hover:bg-opacity-75"
                >
                    â†
                </button>
                <img
                    src={imageSrc}
                    alt="Modal view"
                    className="max-h-[90vh] max-w-[90vw] object-contain"
                />
                <button
                    onClick={onNext}
                    className="absolute right-4 z-10 bg-black bg-opacity-50 text-gray-800	800	 p-2 rounded-full hover:bg-opacity-75"
                >
                    â†’
                </button>
                <button
                    onClick={onClose}
                    className="absolute top-4 right-4 z-10 bg-black bg-opacity-50 text-gray-800	800	 p-2 rounded-full hover:bg-opacity-75"
                >
                    Ã—
                </button>
            </div>
        </div>
    );
}



================================================
FILE: frontend/nextjs/components/Images/ImagesAlbum.tsx
================================================
import React, { useState, useEffect } from 'react';
import ImageModal from './ImageModal';

type ImageType = any; // Simple type definition to avoid errors

interface ImagesAlbumProps {
  images: ImageType[];
}

export default function ImagesAlbum({ images }: ImagesAlbumProps) {
    const [isModalOpen, setIsModalOpen] = useState(false);
    const [selectedImage, setSelectedImage] = useState(null);
    const [selectedIndex, setSelectedIndex] = useState(0);
    const [validImages, setValidImages] = useState(images);

    const openModal = (image: ImageType, index: number) => {
        setSelectedImage(image);
        setSelectedIndex(index);
        setIsModalOpen(true);
    };

    const closeModal = () => {
        setIsModalOpen(false);
        setSelectedImage(null);
    };

    // Handle navigation in modal
    const nextImage = () => {
        setSelectedIndex((prevIndex) => (prevIndex + 1) % validImages.length);
        setSelectedImage(validImages[(selectedIndex + 1) % validImages.length]);
    };

    const prevImage = () => {
        setSelectedIndex((prevIndex) => (prevIndex - 1 + validImages.length) % validImages.length);
        setSelectedImage(validImages[(selectedIndex - 1 + validImages.length) % validImages.length]);
    };

    // Handle broken images by filtering them out
    const handleImageError = (brokenImage: ImageType) => {
        setValidImages((prevImages) => prevImages.filter((img) => img !== brokenImage));
    };

    useEffect(() => {
        const imagesToHide: ImageType[] = []
        const filteredImages = images.filter((img) => !imagesToHide.includes(img));
        setValidImages(filteredImages);
    }, [images]);

    if (validImages.length === 0) return null;

    return (
        <div className="w-full h-full min-h-[200px] max-h-[400px]">
            <div className="grid grid-cols-2 md:grid-cols-4 lg:grid-cols-4 gap-4 pb-4">
                {validImages.map((image: ImageType, index: number) => (
                    <div 
                        key={index} 
                        className="relative aspect-square bg-gray-700 rounded-lg overflow-hidden hover:shadow-lg transition-shadow duration-300"
                    >
                        <img
                            src={image}
                            alt={`Image ${index + 1}`}
                            className="absolute inset-0 w-full h-full object-cover cursor-pointer hover:opacity-90 transition-opacity duration-300"
                            onClick={() => openModal(image, index)}
                            onError={() => handleImageError(image)}
                            loading="lazy"
                        />
                    </div>
                ))}
            </div>

            {selectedImage && (
                <ImageModal
                    imageSrc={selectedImage}
                    isOpen={isModalOpen}
                    onClose={closeModal}
                    onNext={nextImage}
                    onPrev={prevImage}
                />
            )}
        </div>
    );
}


================================================
FILE: frontend/nextjs/components/Langgraph/Langgraph.js
================================================
import { Client } from "@langchain/langgraph-sdk";
import { task } from '../../config/task';

export async function startLanggraphResearch(newQuestion, report_source, langgraphHostUrl) {
    // Update the task query with the new question
    task.task.query = newQuestion;
    task.task.source = report_source;
    const host = langgraphHostUrl;
    
    // Add your Langgraph Cloud Authentication token here
    const authToken = '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';

    const client = new Client({
        apiUrl: host,
        defaultHeaders: {
            'Content-Type': 'application/json',
            'X-Api-Key': authToken
        }
    });
  
    // List all assistants
    const assistants = await client.assistants.search({
      metadata: null,
      offset: 0,
      limit: 10,
    });
  
    console.log('assistants: ', assistants);
  
    // We auto-create an assistant for each graph you register in config.
    const agent = assistants[0];
  
    // Start a new thread
    const thread = await client.threads.create();
  
    // Start a streaming run
    const input = task;
  
    const streamResponse = client.runs.stream(
      thread["thread_id"],
      agent["assistant_id"],
      {
        input,
      },
    );

    return {streamResponse, host, thread_id: thread["thread_id"]};
}


================================================
FILE: frontend/nextjs/components/layouts/CopilotLayout.tsx
================================================
import React, { useRef, useEffect, useState } from "react";
import { Toaster } from "react-hot-toast";
import Header from "@/components/Header";
import Footer from "@/components/Footer";
import { ChatBoxSettings } from "@/types/data";
import Image from "next/image";

interface CopilotLayoutProps {
  children: React.ReactNode;
  loading: boolean;
  isStopped: boolean;
  showResult: boolean;
  onStop?: () => void;
  onNewResearch?: () => void;
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
  mainContentRef?: React.RefObject<HTMLDivElement>;
  toastOptions?: Record<string, any>;
  toggleSidebar?: () => void;
}

export default function CopilotLayout({
  children,
  loading,
  isStopped,
  showResult,
  onStop,
  onNewResearch,
  chatBoxSettings,
  setChatBoxSettings,
  mainContentRef,
  toastOptions = {},
  toggleSidebar
}: CopilotLayoutProps) {
  const defaultRef = useRef<HTMLDivElement>(null);
  const contentRef = mainContentRef || defaultRef;
  
  return (
    <main className="flex flex-col min-h-screen">
      <Toaster 
        position="bottom-center" 
        toastOptions={toastOptions}
      />
      
      {/* Show Header only when not in research mode */}
      {!showResult && (
        <Header 
          loading={loading}
          isStopped={isStopped}
          showResult={showResult}
          onStop={onStop || (() => {})}
          onNewResearch={onNewResearch}
          isCopilotMode={true}
        />
      )}
      
      <div 
        ref={contentRef}
        className={`flex-1 flex flex-col ${!showResult ? 'pt-[120px]' : ''}`}
      >
        {children}
      </div>
      
      <div className="relative z-10">
        <Footer setChatBoxSettings={setChatBoxSettings} chatBoxSettings={chatBoxSettings} />
      </div>
    </main>
  );
} 


================================================
FILE: frontend/nextjs/components/layouts/MobileLayout.tsx
================================================
import React, { useRef, useState } from "react";
import { Toaster } from "react-hot-toast";
import Image from "next/image";
import { ChatBoxSettings } from "@/types/data";
import { useResearchHistoryContext } from "@/hooks/ResearchHistoryContext";
import { formatDistanceToNow } from "date-fns";

interface MobileLayoutProps {
  children: React.ReactNode;
  loading: boolean;
  isStopped: boolean;
  showResult: boolean;
  onStop?: () => void;
  onNewResearch?: () => void;
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
  mainContentRef?: React.RefObject<HTMLDivElement>;
  toastOptions?: Record<string, any>;
  toggleSidebar?: () => void;
}

export default function MobileLayout({
  children,
  loading,
  isStopped,
  showResult,
  onStop,
  onNewResearch,
  chatBoxSettings,
  setChatBoxSettings,
  mainContentRef,
  toastOptions = {},
  toggleSidebar
}: MobileLayoutProps) {
  const defaultRef = useRef<HTMLDivElement>(null);
  const contentRef = mainContentRef || defaultRef;
  const [showSettings, setShowSettings] = useState(false);
  const [showHistory, setShowHistory] = useState(false);
  
  // Get research history from context
  const { history } = useResearchHistoryContext();
  
  // Format timestamp for display
  const formatTimestamp = (timestamp: number | string | Date | undefined) => {
    if (!timestamp) return 'Unknown time';
    
    try {
      const date = new Date(timestamp);
      if (isNaN(date.getTime())) return 'Unknown time';
      return formatDistanceToNow(date, { addSuffix: true });
    } catch {
      return 'Unknown time';
    }
  };
  
  // Handle history item selection
  const handleHistoryItemClick = (id: string) => {
    setShowHistory(false);
    window.location.href = `/research/${id}`;
  };
  
  return (
    <main className="flex flex-col min-h-screen bg-gray-50">
      <Toaster 
        position="bottom-center" 
        toastOptions={toastOptions}
      />
      
      {/* Mobile Header - simplified and compact */}
      <header className="fixed top-0 left-0 right-0 z-50 bg-gray-50/95 backdrop-blur-md border-b border-gray-300/50 shadow-md">
        <div className="flex items-center justify-between px-4 h-14">
          {/* Logo */}
          <div className="flex items-center">
            <a href="/" className="flex items-center">
              <img
                src="/img/gptr-logo.png"
                alt="GPT Researcher"
                width={30}
                height={30}
                className="rounded-md mr-2"
              />
              <span className="font-medium text-gray-700 text-sm">GPT Researcher</span>
            </a>
          </div>
          
          {/* Actions */}
          <div className="flex items-center space-x-2">
            {loading && (
              <button
                onClick={onStop}
                className="p-2 rounded-full bg-red-500/20 text-red-300 hover:bg-red-500/30"
                aria-label="Stop research"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <rect x="6" y="6" width="12" height="12" rx="2" ry="2"></rect>
                </svg>
              </button>
            )}
            
            {showResult && onNewResearch && (
              <button
                onClick={onNewResearch}
                className="p-2 rounded-full bg-sky-500/20 text-sky-300 hover:bg-sky-500/30"
                aria-label="New research"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <line x1="12" y1="5" x2="12" y2="19"></line>
                  <line x1="5" y1="12" x2="19" y2="12"></line>
                </svg>
              </button>
            )}
            
            <button
              onClick={() => {
                setShowHistory(!showHistory);
                setShowSettings(false);
                if (toggleSidebar) toggleSidebar();
              }}
              className="p-2 rounded-full bg-gray-100/50 text-gray-700 hover:bg-gray-700/50"
              aria-label="View history"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
                <polyline points="14 2 14 8 20 8"></polyline>
                <line x1="16" y1="13" x2="8" y2="13"></line>
                <line x1="16" y1="17" x2="8" y2="17"></line>
                <polyline points="10 9 9 9 8 9"></polyline>
              </svg>
            </button>
            
            <button
              onClick={() => {
                setShowSettings(!showSettings);
                setShowHistory(false);
              }}
              className="p-2 rounded-full bg-gray-100/50 text-gray-700 hover:bg-gray-700/50"
              aria-label="Settings"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                <circle cx="12" cy="12" r="3"></circle>
                <path d="M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1 0 2.83 2 2 0 0 1-2.83 0l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-2 2 2 2 0 0 1-2-2v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83 0 2 2 0 0 1 0-2.83l.06-.06a1.65 1.65 0 0 0 .33-1.82 1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1-2-2 2 2 0 0 1 2-2h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 0-2.83 2 2 0 0 1 2.83 0l.06.06a1.65 1.65 0 0 0 1.82.33H9a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 2-2 2 2 0 0 1 2 2v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 0 2 2 0 0 1 0 2.83l-.06.06a1.65 1.65 0 0 0-.33 1.82V9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 2 2 2 2 0 0 1-2 2h-.09a1.65 1.65 0 0 0-1.51 1z"></path>
              </svg>
            </button>
          </div>
        </div>
        
        {/* History panel - slides down when active */}
        {showHistory && (
          <div className="px-4 py-3 bg-gray-100/90 border-t border-gray-700/50 animate-slide-down shadow-lg max-h-[70vh] overflow-y-auto custom-scrollbar">
            <div className="mb-3 flex justify-between items-center">
              <h3 className="text-sm font-medium text-gray-700">Research History</h3>
              <button 
                onClick={() => setShowHistory(false)}
                className="text-gray-500 hover:text-gray-700"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <line x1="18" y1="6" x2="6" y2="18"></line>
                  <line x1="6" y1="6" x2="18" y2="18"></line>
                </svg>
              </button>
            </div>
            
            {history.length > 0 ? (
              <div className="space-y-2">
                {history.map((item) => (
                  <button
                    key={item.id}
                    onClick={() => handleHistoryItemClick(item.id)}
                    className="w-full bg-gray-50/60 hover:bg-gray-100 rounded-lg p-3 text-left transition-colors focus:outline-none focus:ring-1 focus:ring-teal-500/50 border border-gray-200"
                  >
                    <h3 className="text-sm font-medium text-gray-700 line-clamp-1">{item.question}</h3>
                    <p className="text-xs text-gray-500 mt-1.5 flex items-center">
                      <svg xmlns="http://www.w3.org/2000/svg" className="h-3 w-3 mr-1 text-gray-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z" />
                      </svg>
                      {formatTimestamp(item.timestamp || (item as any).updated_at || (item as any).created_at)}
                    </p>
                  </button>
                ))}
              </div>
            ) : (
              <div className="text-center py-6">
                <div className="w-12 h-12 mx-auto mb-3 rounded-full bg-gray-700/50 flex items-center justify-center">
                  <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6 text-gray-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <circle cx="12" cy="12" r="10"></circle>
                    <polyline points="12 6 12 12 16 14"></polyline>
                  </svg>
                </div>
                <p className="text-sm text-gray-500">No research history yet</p>
                <button 
                  onClick={onNewResearch} 
                  className="mt-3 px-4 py-2 text-xs text-blue-600 bg-teal-900/30 hover:bg-teal-800/40 rounded-md transition-colors"
                >
                  Start New Research
                </button>
              </div>
            )}
            
            {history.length > 0 && (
              <div className="mt-3 pt-2 border-t border-gray-200 flex justify-center">
                <a 
                  href="/history" 
                  className="text-xs text-blue-600 hover:text-blue-600 transition-colors"
                >
                  View All Research History
                </a>
              </div>
            )}
          </div>
        )}
        
        {/* Settings panel - slides down when active */}
        {showSettings && (
          <div className="px-4 py-3 bg-gray-100/90 border-t border-gray-700/50 animate-slide-down shadow-lg">
            <div className="mb-2 flex justify-between items-center">
              <h3 className="text-sm font-medium text-gray-700">Settings</h3>
              <button 
                onClick={() => setShowSettings(false)}
                className="text-gray-500 hover:text-gray-700"
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <line x1="18" y1="6" x2="6" y2="18"></line>
                  <line x1="6" y1="6" x2="18" y2="18"></line>
                </svg>
              </button>
            </div>
            
            <div className="space-y-3">
              <div>
                <label className="block text-xs text-gray-500 mb-1">Report Type</label>
                <select 
                  className="w-full bg-gray-50 border border-gray-700 rounded-md py-1.5 px-2 text-sm text-gray-700 focus:outline-none focus:ring-1 focus:ring-teal-500 focus:border-teal-500"
                  value={chatBoxSettings.report_type}
                  onChange={(e) => setChatBoxSettings({...chatBoxSettings, report_type: e.target.value})}
                >
                  <option value="research_report">Summary - Short and fast (~2 min)</option>
                  <option value="deep">Deep Research Report</option>
                  <option value="multi_agents">Multi Agents Report</option>
                  <option value="detailed_report">Detailed - In depth and longer (~5 min)</option>
                </select>
              </div>
              
              <div>
                <label className="block text-xs text-gray-500 mb-1">Research Source</label>
                <select 
                  className="w-full bg-gray-50 border border-gray-700 rounded-md py-1.5 px-2 text-sm text-gray-700 focus:outline-none focus:ring-1 focus:ring-teal-500 focus:border-teal-500"
                  value={chatBoxSettings.report_source}
                  onChange={(e) => setChatBoxSettings({...chatBoxSettings, report_source: e.target.value})}
                >
                  <option value="web">Web</option>
                  <option value="scholar">Scholar</option>
                </select>
              </div>
              
              <div>
                <label className="block text-xs text-gray-500 mb-1">Research Tone</label>
                <select 
                  className="w-full bg-gray-50 border border-gray-700 rounded-md py-1.5 px-2 text-sm text-gray-700 focus:outline-none focus:ring-1 focus:ring-teal-500 focus:border-teal-500"
                  value={chatBoxSettings.tone}
                  onChange={(e) => setChatBoxSettings({...chatBoxSettings, tone: e.target.value})}
                >
                  <option value="Objective">Objective - Impartial and unbiased presentation of facts</option>
                  <option value="Formal">Formal - Adheres to academic standards</option>
                  <option value="Analytical">Analytical - Critical evaluation of data</option>
                  <option value="Persuasive">Persuasive - Convincing viewpoint</option>
                  <option value="Informative">Informative - Clear, comprehensive information</option>
                  <option value="Simple">Simple - Basic vocabulary and clear explanations</option>
                  <option value="Casual">Casual - Conversational style</option>
                </select>
              </div>
              
              <div>
                <label className="block text-xs text-gray-500 mb-1">Layout</label>
                <select 
                  className="w-full bg-gray-50 border border-gray-700 rounded-md py-1.5 px-2 text-sm text-gray-700 focus:outline-none focus:ring-1 focus:ring-teal-500 focus:border-teal-500"
                  value={chatBoxSettings.layoutType}
                  onChange={(e) => setChatBoxSettings({...chatBoxSettings, layoutType: e.target.value})}
                >
                  <option value="copilot">Copilot - Chat style layout</option>
                  <option value="document">Document - Traditional report layout</option>
                </select>
              </div>
            </div>
          </div>
        )}
      </header>
      
      {/* Main content area */}
      <div 
        ref={contentRef}
        className="flex-1 pt-14" /* Matches header height */
      >
        {children}
      </div>
      
      {/* Footer */}
      <footer className="mt-auto py-3 px-4 text-center border-t border-gray-300/40 bg-gray-50/80 backdrop-blur-sm">
        <div className="flex items-center justify-center gap-5 mb-3">
          <a href="https://gptr.dev" target="_blank" className="text-gray-500 hover:text-blue-600 transition-colors">
            <svg 
              xmlns="http://www.w3.org/2000/svg" 
              viewBox="0 0 24 24" 
              fill="none" 
              stroke="currentColor" 
              strokeWidth="2" 
              strokeLinecap="round" 
              strokeLinejoin="round" 
              className="w-5 h-5"
            >
              <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z" />
              <polyline points="9 22 9 12 15 12 15 22" />
            </svg>
          </a>
          <a href="https://github.com/assafelovic/gpt-researcher" target="_blank" className="text-gray-500 hover:text-gray-700 transition-colors">
            <img
              src="/img/github.svg"
              alt="GitHub"
              width={20}
              height={20}
              className="w-5 h-5"
            />
          </a>
          <a href="https://discord.gg/QgZXvJAccX" target="_blank" className="text-gray-500 hover:text-gray-700 transition-colors">
            <img
              src="/img/discord.svg"
              alt="Discord"
              width={20}
              height={20}
              className="w-5 h-5"
            />
          </a>
          <a href="https://hub.docker.com/r/gptresearcher/gpt-researcher" target="_blank" className="text-gray-500 hover:text-gray-700 transition-colors">
            <img
              src="/img/docker.svg"
              alt="Docker"
              width={20}
              height={20}
              className="w-5 h-5"
            />
          </a>
        </div>
        <div className="text-xs text-gray-500">
          Â© {new Date().getFullYear()} GPT Researcher. All rights reserved.
        </div>
      </footer>
      
      {/* Custom animations */}
      <style jsx global>{`
        .animate-slide-down {
          animation: slideDown 0.3s ease-out forwards;
        }
        
        @keyframes slideDown {
          from {
            opacity: 0;
            transform: translateY(-10px);
          }
          to {
            opacity: 1;
            transform: translateY(0);
          }
        }
        
        /* Custom scrollbar for history panel */
        .custom-scrollbar::-webkit-scrollbar {
          width: 4px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(17, 24, 39, 0.1);
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(75, 85, 99, 0.5);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(75, 85, 99, 0.7);
        }
        
        .line-clamp-1 {
          overflow: hidden;
          display: -webkit-box;
          -webkit-box-orient: vertical;
          -webkit-line-clamp: 1;
        }
        
        @media (display-mode: standalone) {
          /* PWA specific styles */
          body {
            overscroll-behavior-y: contain;
            -webkit-tap-highlight-color: transparent;
            -webkit-touch-callout: none;
            user-select: none;
          }
        }
      `}</style>
    </main>
  );
} 


================================================
FILE: frontend/nextjs/components/layouts/ResearchPageLayout.tsx
================================================
import { ReactNode, useRef, useCallback, useEffect, Dispatch, SetStateAction } from "react";
import { Toaster } from "react-hot-toast";
import Header from "@/components/Header";
import Footer from "@/components/Footer";
import { ChatBoxSettings } from "@/types/data";

interface ResearchPageLayoutProps {
  children: ReactNode;
  loading: boolean;
  isStopped: boolean;
  showResult: boolean;
  onStop?: () => void;
  onNewResearch: () => void;
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: Dispatch<SetStateAction<ChatBoxSettings>>;
  mainContentRef?: React.RefObject<HTMLDivElement>;
  showScrollButton?: boolean;
  onScrollToBottom?: () => void;
  toastOptions?: object;
}

export default function ResearchPageLayout({
  children,
  loading,
  isStopped,
  showResult,
  onStop,
  onNewResearch,
  chatBoxSettings,
  setChatBoxSettings,
  mainContentRef,
  showScrollButton = false,
  onScrollToBottom,
  toastOptions = {}
}: ResearchPageLayoutProps) {
  const defaultRef = useRef<HTMLDivElement>(null);
  const contentRef = mainContentRef || defaultRef;

  return (
    <main className="flex min-h-screen flex-col">
      <Toaster 
        position="bottom-center" 
        toastOptions={toastOptions}
      />
      
      <Header 
        loading={loading}
        isStopped={isStopped}
        showResult={showResult}
        onStop={onStop || (() => {})}
        onNewResearch={onNewResearch}
      />
      
      <div 
        ref={contentRef}
        className="min-h-[100vh] pt-[120px]"
      >
        {children}
      </div>
      
      {showScrollButton && showResult && (
        <button
          onClick={onScrollToBottom}
          className="fixed bottom-8 right-8 flex items-center justify-center w-12 h-12 text-gray-800	800	 bg-gradient-to-br from-teal-500 to-teal-600 rounded-full hover:from-teal-600 hover:to-teal-700 transform hover:scale-105 transition-all duration-200 shadow-lg z-50 backdrop-blur-sm border border-teal-400/20"
        >
          <svg 
            xmlns="http://www.w3.org/2000/svg" 
            className="h-6 w-6" 
            fill="none" 
            viewBox="0 0 24 24" 
            stroke="currentColor"
          >
            <path 
              strokeLinecap="round" 
              strokeLinejoin="round" 
              strokeWidth={2} 
              d="M19 14l-7 7m0 0l-7-7m7 7V3" 
            />
          </svg>
        </button>
      )}
      
      <Footer setChatBoxSettings={setChatBoxSettings} chatBoxSettings={chatBoxSettings} />
    </main>
  );
} 


================================================
FILE: frontend/nextjs/components/mobile/MobileChatPanel.tsx
================================================
import React, { useState, useRef, useEffect, useCallback, memo, useMemo } from 'react';
import LoadingDots from '@/components/LoadingDots';
import { Data, ChatData } from '@/types/data';
import { markdownToHtml } from '@/helpers/markdownHelper';
import '@/styles/markdown.css';
import Link from "next/link";
// Simple classname utility function to replace cn from @/lib/utils
const cn = (...classes: (string | undefined)[]) => classes.filter(Boolean).join(' ');
import { toast } from 'react-hot-toast';
import { motion, AnimatePresence } from "framer-motion";
// Remove SendIcon import and use inline SVG instead

interface MobileChatPanelProps {
  question: string;
  chatPromptValue: string;
  setChatPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleChat: (message: string) => void;
  orderedData: Data[];
  loading: boolean;
  isProcessingChat: boolean;
  isStopped: boolean;
  onNewResearch?: () => void;
  className?: string;
}

// Memoize the chat message component to prevent re-rendering all messages
const ChatMessage = memo(({ 
  type, 
  content, 
  html, 
  metadata 
}: { 
  type: string, 
  content: string, 
  html: string, 
  metadata?: any 
}) => {
  if (type === 'question') {
    // User question - now with teal/turquoise color to match theme
    return (
      <div className="flex items-start justify-end space-x-2 py-1 max-w-full animate-fade-in">
        <div className="flex-1 bg-blue-500/80 border border-teal-500/50 rounded-2xl px-4 py-3 text-sm text-gray-800	800	 font-medium ml-10 shadow-md">
          {content}
        </div>
        <div className="w-8 h-8 rounded-full bg-blue-500 flex items-center justify-center flex-shrink-0 shadow-md">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="white" stroke="white" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
            <circle cx="12" cy="9" r="5" />
            <path d="M3,20 c0,-4 8,-4 8,-4 s8,0 8,4" />
            <path d="M8,10 a4,3 0 0,0 8,0" fill="none" />
          </svg>
        </div>
      </div>
    );
  } else if (type === 'chat') {
    // Check if we have sources from a web search tool call
    const hasWebSources = metadata?.tool_calls?.some(
      (tool: any) => tool.tool === 'quick_search' && tool.search_metadata?.sources?.length > 0
    );
    
    // Get all sources from web searches
    const webSources = metadata?.tool_calls
      ?.filter((tool: any) => tool.tool === 'quick_search')
      .flatMap((tool: any) => tool.search_metadata?.sources || [])
      .map((source: any) => ({
        name: source.title,
        url: source.url
      })) || [];
    
    // Also check for direct sources in metadata (for backward compatibility)
    const directSources = metadata?.sources?.map((source: any) => ({
      name: source.title || source.url,
      url: source.url
    })) || [];
    
    // Combine sources from both locations
    const allSources = [...webSources, ...directSources];
    const hasSources = allSources.length > 0;
    
    // AI response - with darker color
    return (
      <div className="flex flex-col space-y-2 py-1 max-w-full animate-fade-in">
        <div className="flex items-start space-x-2">
          <div className="w-8 h-8 rounded-full bg-gradient-to-br from-gray-700 to-gray-800 flex items-center justify-center flex-shrink-0 shadow-md">
            <img src="/img/gptr-logo.png" alt="AI" className="w-6 h-6" />
          </div>
          <div className="flex-1 ai-message-bubble rounded-2xl px-4 py-3 text-sm text-gray-800	800	 mr-4 shadow-lg">
            <div className="markdown-content prose prose-sm prose-invert max-w-none">
              <div dangerouslySetInnerHTML={{ __html: html }} />
            </div>
            
            {/* Collapsed sources UI (old way) - still included for toggle behavior */}
            {metadata && 
             metadata.sources && 
             metadata.sources.length > 0 && (
              <div className="mt-2 pt-2 border-t border-gray-700/50 text-xs text-gray-700">
                <details className="group">
                  <summary className="cursor-pointer hover:text-gray-800	800	 flex items-center">
                    <span className="mr-1">Sources</span>
                    <svg className="h-3 w-3 transition-transform group-open:rotate-180" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M19 9l-7 7-7-7" />
                    </svg>
                  </summary>
                  <ul className="mt-1 ml-4 space-y-1 list-disc">
                    {metadata.sources.map((source: any, i: number) => (
                      <li key={i}>
                        <a 
                          href={source.url} 
                          target="_blank" 
                          rel="noopener noreferrer"
                          className="text-blue-600 hover:text-blue-600 hover:underline truncate block"
                        >
                          {source.title || source.url}
                        </a>
                      </li>
                    ))}
                  </ul>
                </details>
              </div>
            )}
          </div>
        </div>
        
        {/* Source cards display - similar to Sources component with compact=true */}
        {hasSources && (
          <div className="ml-10 mr-4">
            <div className="flex items-center gap-2 mb-2">
              <div className="flex items-center justify-center w-5 h-5 rounded-md bg-blue-500/20 border border-blue-500/30">
                <svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="text-blue-400">
                  <circle cx="12" cy="12" r="10"></circle>
                  <line x1="2" y1="12" x2="22" y2="12"></line>
                  <path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path>
                </svg>
              </div>
              <span className="text-xs font-medium text-blue-300">Sources</span>
            </div>
            <div className="max-h-[180px] overflow-y-auto scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-gray-300/10">
              <div className="flex w-full flex-wrap content-center items-center gap-2">
                {allSources.map((source: {name: string; url: string}, index: number) => {
                  // Extract domain from URL
                  let displayUrl = source.url;
                  try {
                    const urlObj = new URL(source.url);
                    displayUrl = urlObj.hostname.replace(/^www\./, '');
                  } catch (e) {
                    // If URL parsing fails, use the original URL
                  }
                  
                  return (
                    <a 
                      key={`${source.url}-${index}`} 
                      href={source.url} 
                      target="_blank" 
                      rel="noopener noreferrer" 
                      className="inline-flex items-center gap-1 px-1.5 py-0.5 text-xs bg-gray-100/60 text-gray-700 hover:text-blue-600 hover:bg-gray-100/90 rounded border border-gray-700/40 transition-colors"
                      title={source.name}
                    >
                      <svg xmlns="http://www.w3.org/2000/svg" width="8" height="8" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                        <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                        <polyline points="15 3 21 3 21 9"></polyline>
                        <line x1="10" y1="14" x2="21" y2="3"></line>
                      </svg>
                      {displayUrl}
                    </a>
                  );
                })}
              </div>
            </div>
          </div>
        )}
      </div>
    );
  }
  return null;
});

// Set display name for memo component
ChatMessage.displayName = 'ChatMessage';

// Add markdown processing function if it doesn't exist
function processMarkdown(content: string): string {
  // Simple implementation - in a real app you would use a proper markdown parser
  return content
    .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
    .replace(/\*(.*?)\*/g, '<em>$1</em>')
    .replace(/```([\s\S]*?)```/g, '<pre><code>$1</code></pre>')
    .replace(/`([^`]+)`/g, '<code>$1</code>')
    .replace(/\n/g, '<br />');
}

const MobileChatPanel: React.FC<MobileChatPanelProps> = ({
  question,
  chatPromptValue,
  setChatPromptValue,
  handleChat,
  orderedData,
  loading,
  isProcessingChat,
  isStopped,
  onNewResearch,
  className
}) => {
  const [inputFocused, setInputFocused] = useState(false);
  const [showPreferences, setShowPreferences] = useState(false);
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLTextAreaElement>(null);
  const [renderedMessages, setRenderedMessages] = useState<{id: string, content: string, html: string, type: string, metadata?: any}[]>([]);
  const [isSubmitting, setIsSubmitting] = useState(false);
  const prevOrderedDataLengthRef = useRef(0);
  
  // Process markdown in messages - memoized for performance
  useEffect(() => {
    // Only process if data has changed
    if (orderedData.length === prevOrderedDataLengthRef.current && !loading && !isProcessingChat) {
      return;
    }
    
    // Update reference for comparison
    prevOrderedDataLengthRef.current = orderedData.length;
    
    // Filter to only get chat messages (questions and responses)
    const chatMessages = orderedData.filter((data) => {
      return data.type === 'question' || data.type === 'chat';
    });
    
    const processMessages = async () => {
      try {
        // Process in batches if needed for large message sets
        const rendered = await Promise.all(
          chatMessages.map(async (msg, index) => {
            // For chat messages, convert markdown to HTML
            if (msg.type === 'chat') {
              try {
                const html = await markdownToHtml(msg.content);
                return {
                  id: `${msg.type}-${index}`,
                  content: msg.content,
                  html,
                  type: msg.type,
                  metadata: (msg as ChatData).metadata
                };
              } catch (error) {
                console.error('Error processing markdown:', error);
                // Provide fallback rendering
                return {
                  id: `${msg.type}-${index}`,
                  content: msg.content,
                  html: `<p>${msg.content}</p>`,
                  type: msg.type,
                  metadata: (msg as ChatData).metadata
                };
              }
            } else {
              // For questions, no markdown processing needed
              return {
                id: `${msg.type}-${index}`,
                content: msg.content,
                html: msg.content, // No markdown for questions
                type: msg.type
              };
            }
          })
        );
        
        // Use function form to ensure we're working with latest state
        setRenderedMessages(rendered);
      } catch (error) {
        console.error('Error processing messages:', error);
      }
    };
    
    processMessages();
  }, [orderedData, loading, isProcessingChat]);

  // Auto-resize textarea when content changes - memoized
  const handleTextAreaChange = useCallback((e: React.ChangeEvent<HTMLTextAreaElement>) => {
    setChatPromptValue(e.target.value);
    
    if (!inputRef.current) return;
    
    // Reset height to auto to accurately calculate the new height
    inputRef.current.style.height = 'auto';
    
    // Set the height based on the scroll height (content height)
    // with a maximum height
    const newHeight = Math.min(e.target.scrollHeight, 120);
    inputRef.current.style.height = `${newHeight}px`;
  }, [setChatPromptValue]);
  
  // Handle submitting chat messages - memoized
  const handleSubmit = useCallback(async (e?: React.FormEvent) => {
    if (e) e.preventDefault();
    
    if (!chatPromptValue.trim() || isProcessingChat || isSubmitting || isStopped) {
      return;
    }
    
    try {
      setIsSubmitting(true);
      await handleChat(chatPromptValue);
      setChatPromptValue('');
      
      // Focus back on input after sending
      setTimeout(() => {
        if (inputRef.current) {
          inputRef.current.focus();
        }
      }, 100);
    } catch (error) {
      console.error('Error submitting chat:', error);
      toast.error('Failed to send message. Please try again.');
    } finally {
      setIsSubmitting(false);
    }
  }, [chatPromptValue, isProcessingChat, isSubmitting, isStopped, setChatPromptValue, handleChat]);
  
  // Handle keyboard shortcuts - memoized
  const handleKeyDown = useCallback((e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if ((e.metaKey || e.ctrlKey) && e.key === 'Enter') {
      handleSubmit();
    }
  }, [handleSubmit]);
  
  // Function to scroll to bottom - memoized
  const scrollToBottom = useCallback(() => {
    if (chatContainerRef.current) {
      const { scrollHeight, clientHeight } = chatContainerRef.current;
      // Only scroll if we're not already at the bottom
      // Add a small buffer to prevent unnecessary scrolls
      const shouldScroll = scrollHeight - chatContainerRef.current.scrollTop - clientHeight > 20;
      
      if (shouldScroll) {
        chatContainerRef.current.scrollTop = scrollHeight;
      }
    }
  }, []);

  // Scroll when messages change or loading/processing state changes
  useEffect(() => {
    // Use requestAnimationFrame to ensure DOM has updated
    requestAnimationFrame(() => {
      scrollToBottom();
    });
  }, [renderedMessages.length, loading, isProcessingChat, scrollToBottom]);

  // Also handle mutations in the DOM that might affect scroll height
  useEffect(() => {
    if (!chatContainerRef.current) return;
    
    const observer = new MutationObserver(() => {
      requestAnimationFrame(scrollToBottom);
    });
    
    observer.observe(chatContainerRef.current, {
      childList: true,
      subtree: true,
      characterData: true
    });
    
    return () => observer.disconnect();
  }, [scrollToBottom]);

  // Determine if we need to show intro message
  const showIntroMessage = orderedData.length === 0 || (orderedData.length === 1 && orderedData[0].type === 'question');

  // Optimize the message rendering with better chunking
  const processedMessages = useMemo(() => {
    if (!renderedMessages || renderedMessages.length === 0) return [];
    
    // Process in smaller batches to prevent UI freeze
    // Limit the size of message content to prevent memory issues
    return renderedMessages.map(message => {
      // Trim very long messages to prevent rendering issues
      const processedContent = message.content.length > 50000 
        ? message.content.substring(0, 50000) + "... (message truncated for performance)"
        : message.content;
        
      return {
        ...message,
        content: processedContent,
        processedContent: processMarkdown(processedContent)
      };
    });
  }, [renderedMessages]);

  // Animation variants for preferences modal
  const fadeIn = {
    hidden: { opacity: 0 },
    visible: { opacity: 1, transition: { duration: 0.3 } }
  };

  const slideUp = {
    hidden: { opacity: 0, y: 20 },
    visible: { opacity: 1, y: 0, transition: { duration: 0.3, ease: "easeOut" } }
  };

  return (
    <div className={cn("flex flex-col h-full bg-gradient-to-b from-gray-900 to-gray-950", className)}>
      {/* Chat Messages Area */}
      <div 
        ref={chatContainerRef}
        className="flex-1 overflow-y-auto px-3 py-2 space-y-3 custom-scrollbar"
      >
        {/* Welcome/Intro message when no content */}
        {showIntroMessage && !loading && (
          <div className="flex items-start space-x-2 py-2 animate-fade-in">
            <div className="w-8 h-8 rounded-full bg-gradient-to-br from-gray-700 to-gray-800 flex items-center justify-center flex-shrink-0 shadow-md">
              <img src="/img/gptr-logo.png" alt="AI" className="w-6 h-6" />
            </div>
            <div className="flex-1 ai-message-bubble rounded-2xl p-4 text-sm text-gray-800	800	 shadow-lg">
              <p>Hi there! I&apos;m your research assistant. Type your question and I&apos;ll help you find information and insights.</p>
            </div>
          </div>
        )}
        
        {/* Research in progress message */}
        {loading && renderedMessages.length === 0 && (
          <div className="flex items-start space-x-2 py-2 animate-fade-in">
            <div className="w-8 h-8 rounded-full bg-gradient-to-br from-gray-700 to-gray-800 flex items-center justify-center flex-shrink-0 shadow-md">
              <img src="/img/gptr-logo.png" alt="AI" className="w-6 h-6" />
            </div>
            <div className="flex-1 ai-message-bubble rounded-2xl p-4 text-sm text-gray-800	800	 shadow-lg">
              <p>I&apos;m researching your question. This may take a moment...</p>
              <div className="mt-2 flex justify-center">
                <LoadingDots />
              </div>
            </div>
          </div>
        )}
        
        {/* Render chat messages */}
        {processedMessages.map((message) => (
          <ChatMessage 
            key={message.id}
            type={message.type}
            content={message.content}
            html={message.html}
            metadata={message.metadata}
          />
        ))}
        
        {/* Show typing indicator when processing */}
        {isProcessingChat && (
          <div className="flex items-start space-x-2 py-1 animate-fade-in">
            <div className="w-8 h-8 rounded-full bg-gradient-to-br from-gray-700 to-gray-800 flex items-center justify-center flex-shrink-0 shadow-md">
              <img src="/img/gptr-logo.png" alt="AI" className="w-6 h-6" />
            </div>
            <div className="flex-1 ai-message-bubble rounded-2xl px-4 py-3 text-gray-800	800	">
              <div className="flex space-x-2">
                <div className="w-2 h-2 bg-gray-300 rounded-full animate-pulse-slow"></div>
                <div className="w-2 h-2 bg-gray-300 rounded-full animate-pulse-slow animation-delay-200"></div>
                <div className="w-2 h-2 bg-gray-300 rounded-full animate-pulse-slow animation-delay-400"></div>
              </div>
            </div>
          </div>
        )}
      </div>
      
      {/* Input Area */}
      <div className={`px-3 py-3 border-t border-gray-300 ${inputFocused ? 'bg-gray-100/90' : 'bg-gray-50/90'} backdrop-blur-sm transition-colors duration-200 safe-bottom`}>
        {!isStopped ? (
          <form onSubmit={handleSubmit} className="relative">
            <textarea
              ref={inputRef}
              value={chatPromptValue}
              onChange={handleTextAreaChange}
              onKeyDown={handleKeyDown}
              onFocus={() => setInputFocused(true)}
              onBlur={() => setInputFocused(false)}
              placeholder="Ask a research question..."
              className="w-full px-4 py-3 pr-14 bg-gray-100/90 border border-gray-700 focus:border-teal-500 rounded-xl resize-none text-gray-800	800	 text-sm placeholder-gray-400 focus:outline-none focus:ring-1 focus:ring-teal-500/50 transition-all shadow-sm"
              style={{ minHeight: '48px', maxHeight: '120px' }}
              disabled={isProcessingChat || isSubmitting}
            />
            
            <button
              type="submit"
              disabled={!chatPromptValue.trim() || isProcessingChat || isSubmitting}
              className={`absolute right-3 bottom-[50%] translate-y-[50%] w-9 h-9 flex items-center justify-center rounded-full ${
                chatPromptValue.trim() && !isProcessingChat && !isSubmitting
                  ? 'bg-gradient-to-r from-teal-500 to-teal-600 hover:from-teal-400 hover:to-teal-500 text-gray-800	800	 shadow-md'
                  : 'bg-gray-700 text-gray-500 cursor-not-allowed'
              } transition-all duration-200`}
            >
              {isProcessingChat || isSubmitting ? (
                <div className="flex justify-center items-center">
                  <div className="w-4 h-4 border-2 border-gray-300 border-t-gray-600 rounded-full animate-spin"></div>
                </div>
              ) : (
                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <line x1="22" y1="2" x2="11" y2="13"></line>
                  <polygon points="22 2 15 22 11 13 2 9"></polygon>
                </svg>
              )}
            </button>
          </form>
        ) : (
          <div className="text-center p-3 text-gray-700 bg-gray-100/60 rounded-xl border border-gray-700/50 text-sm shadow-sm">
            Research has been stopped. 
            {onNewResearch && (
              <button 
                onClick={onNewResearch} 
                className="ml-2 text-blue-600 hover:text-blue-600 hover:underline font-medium"
              >
                Start new research
              </button>
            )}
          </div>
        )}
      </div>

      {/* Custom animations and styles */}
      <style jsx global>{`
        @keyframes fade-in {
          0% {
            opacity: 0;
            transform: translateY(8px);
          }
          100% {
            opacity: 1;
            transform: translateY(0);
          }
        }

        .animate-fade-in {
          animation: fade-in 0.3s ease-out forwards;
        }

        @keyframes pulse-slow {
          0%, 100% {
            opacity: 0.5;
          }
          50% {
            opacity: 1;
          }
        }
        
        .animate-pulse-slow {
          animation: pulse-slow 1.5s infinite;
        }
        
        .animation-delay-200 {
          animation-delay: 0.2s;
        }
        
        .animation-delay-400 {
          animation-delay: 0.4s;
        }
        
        /* Custom scrollbar */
        .custom-scrollbar::-webkit-scrollbar {
          width: 4px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(17, 24, 39, 0.1);
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(75, 85, 99, 0.5);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(75, 85, 99, 0.7);
        }
        
        /* AI message bubble with subtle gradient */
        .ai-message-bubble {
          background: linear-gradient(145deg, rgba(31, 41, 55, 0.95), rgba(17, 24, 39, 0.9));
          box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2), 0 0 0 1px rgba(255, 255, 255, 0.05);
          position: relative;
          overflow: hidden;
        }
        
        .ai-message-bubble::before {
          content: '';
          position: absolute;
          top: 0;
          left: 0;
          right: 0;
          height: 40%;
          background: linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));
          pointer-events: none;
          z-index: 0;
        }
        
        .ai-message-bubble > * {
          position: relative;
          z-index: 1;
        }
        
        /* Improved markdown content styling */
        .markdown-content {
          line-height: 1.6;
        }
        
        .markdown-content ul, .markdown-content ol {
          padding-left: 1.5rem;
        }
        
        .markdown-content code {
          background: rgba(0, 0, 0, 0.2);
          padding: 0.1em 0.3em;
          border-radius: 0.25rem;
          font-size: 0.875em;
        }
        
        .markdown-content pre {
          background: rgba(0, 0, 0, 0.2);
          padding: 0.75rem;
          border-radius: 0.5rem;
          overflow-x: auto;
          margin: 0.75rem 0;
        }
        
        .markdown-content pre code {
          background: transparent;
          padding: 0;
        }
        
        .markdown-content a {
          color: #5eead4;
          text-decoration: none;
        }
        
        .markdown-content a:hover {
          text-decoration: underline;
        }
      `}</style>
    </div>
  );
};

export default MobileChatPanel; 


================================================
FILE: frontend/nextjs/components/mobile/MobileHomeScreen.tsx
================================================
import React, { useState, useEffect, useRef, useCallback } from 'react';
import { ResearchHistoryItem } from '@/types/data';
import { useResearchHistoryContext } from '@/hooks/ResearchHistoryContext';
import LoadingDots from '@/components/LoadingDots';
import { toast } from "react-hot-toast";

interface MobileHomeScreenProps {
  promptValue: string;
  setPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleDisplayResult: (newQuestion: string) => Promise<void>;
  isLoading?: boolean;
  placeholder?: string;
  handleKeyDown?: (e: React.KeyboardEvent<HTMLTextAreaElement>) => void;
}

export default function MobileHomeScreen({
  promptValue,
  setPromptValue,
  handleDisplayResult,
  isLoading = false,
  placeholder = "What would you like to research today?",
  handleKeyDown
}: MobileHomeScreenProps) {
  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const { history } = useResearchHistoryContext();
  const [recentHistory, setRecentHistory] = useState<ResearchHistoryItem[]>([]);
  const [isFocused, setIsFocused] = useState(false);
  const [isSubmitting, setIsSubmitting] = useState(false);
  const submissionTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Get recent research history
  useEffect(() => {
    // Get the 3 most recent items
    if (history && history.length > 0) {
      setRecentHistory(history.slice(0, 3));
    }
  }, [history]);

  // Auto resize textarea
  useEffect(() => {
    if (textareaRef.current) {
      textareaRef.current.style.height = 'auto';
      textareaRef.current.style.height = textareaRef.current.scrollHeight + 'px';
    }
  }, [promptValue]);

  // Clean up any timeouts on unmount
  useEffect(() => {
    return () => {
      if (submissionTimeoutRef.current) {
        clearTimeout(submissionTimeoutRef.current);
      }
    };
  }, []);

  // Handle history item click
  const handleHistoryItemClick = useCallback((id: string) => {
    window.location.href = `/research/${id}`;
  }, []);

  const handlePromptChange = useCallback((e: React.ChangeEvent<HTMLTextAreaElement>) => {
    setPromptValue(e.target.value);
  }, [setPromptValue]);

  const handleSubmit = useCallback(async () => {
    // Don't submit if empty, already loading, or already submitting
    if (!promptValue.trim() || isLoading || isSubmitting) {
      return;
    }
    
    try {
      // Set submitting state for UI feedback
      setIsSubmitting(true);
      
      // Add a timeout as a safety measure to prevent infinite loading
      submissionTimeoutRef.current = setTimeout(() => {
        setIsSubmitting(false);
        toast.error("Research request took too long. Please try again.", {
          duration: 3000,
          position: "bottom-center"
        });
      }, 15000); // 15 second timeout
      
      // Create a new simplified direct API submission that won't use websockets
      try {
        // First show visual feedback
        const trimmedPrompt = promptValue.trim();
        
        // Call the display result handler from props
        await handleDisplayResult(trimmedPrompt);
        
        // Clear the timeout since we successfully completed
        if (submissionTimeoutRef.current) {
          clearTimeout(submissionTimeoutRef.current);
          submissionTimeoutRef.current = null;
        }
      } catch (apiError) {
        console.error("API error during research submission:", apiError);
        toast.error("There was a problem submitting your research. Please try again.", {
          duration: 3000,
          position: "bottom-center"
        });
        
        // Clear submission state
        setIsSubmitting(false);
      }
    } catch (error) {
      console.error("Error during research submission:", error);
      // Reset state in case of error
      setIsSubmitting(false);
      
      // Clear any existing timeout
      if (submissionTimeoutRef.current) {
        clearTimeout(submissionTimeoutRef.current);
        submissionTimeoutRef.current = null;
      }
    }
  }, [promptValue, isLoading, isSubmitting, handleDisplayResult]);

  // Handle enter key for submission
  const handleKeyPress = useCallback((e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if (handleKeyDown) {
      handleKeyDown(e);
    }
    
    // Submit on Enter (without shift)
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSubmit();
    }
  }, [handleKeyDown, handleSubmit]);

  return (
    <div className="flex flex-col h-full w-full bg-gradient-to-b from-gray-900 to-gray-950 pb-16">
      {/* Header with logo and title */}
      <div className="pt-10 px-6 text-center mb-8">
        <div className="flex justify-center mb-3">
          <img
            src="/img/gptr-logo.png"
            alt="GPT Researcher"
            width={60}
            height={60}
            className="rounded-xl"
          />
        </div>
        <p className="text-gray-500 text-sm">Say Hello to GPT Researcher, your AI partner for instant insights and comprehensive research</p>
      </div>

      {/* Search Box */}
      <div className="px-4 md:px-8 w-full max-w-lg mx-auto">
        <div 
          className={`relative bg-gray-100 border ${isFocused ? 'border-sky-500/70 input-glow-active' : 'border-gray-700/50 input-glow-subtle'} rounded-xl shadow-lg transition-all duration-300`}
        >
          <textarea
            ref={textareaRef}
            className="w-full bg-transparent text-gray-700 px-4 pt-4 pb-12 focus:outline-none resize-none rounded-xl"
            placeholder={placeholder}
            value={promptValue}
            onChange={handlePromptChange}
            onKeyDown={handleKeyPress}
            rows={1}
            onFocus={() => setIsFocused(true)}
            onBlur={() => setIsFocused(false)}
            disabled={isLoading || isSubmitting}
          />
          
          <div className="absolute bottom-3 right-3">
            <button
              onClick={handleSubmit}
              disabled={isLoading || isSubmitting || !promptValue.trim()}
              className={`rounded-full p-2 ${
                isLoading || isSubmitting || !promptValue.trim() 
                  ? 'bg-gray-700 text-gray-500' 
                  : 'bg-sky-600 text-gray-800	800	 hover:bg-sky-500'
              } transition-colors focus:outline-none focus:ring-2 focus:ring-sky-500 focus:ring-opacity-50`}
              aria-label="Start research"
            >
              {isLoading || isSubmitting ? (
                <div className="flex justify-center items-center">
                  <div className="w-4 h-4 border-2 border-gray-300 border-t-gray-600 rounded-full animate-spin"></div>
                </div>
              ) : (
                <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <path d="M5 12h14M12 5l7 7-7 7" />
                </svg>
              )}
            </button>
          </div>
        </div>
        <p className="text-xs text-gray-500 mt-2 text-center px-2">
          Enter any research topic or specific question
        </p>
      </div>

      {/* Recent research history */}
      {recentHistory.length > 0 && (
        <div className="mt-10 px-4">
          <h2 className="text-sm font-medium text-gray-500 mb-3 px-2">Recent Research</h2>
          <div className="space-y-2">
            {recentHistory.map((item) => (
              <button
                key={item.id}
                onClick={() => handleHistoryItemClick(item.id)}
                className="w-full bg-gray-100/60 hover:bg-gray-100 rounded-lg p-3 text-left transition-colors focus:outline-none focus:ring-2 focus:ring-gray-600"
              >
                <h3 className="text-sm font-medium text-gray-700 line-clamp-1">{item.question}</h3>
                <p className="text-xs text-gray-500 mt-1">
                  {new Date(item.timestamp || Date.now()).toLocaleString()}
                </p>
              </button>
            ))}
          </div>
          <div className="mt-3 text-center">
            <a
              href="/history"
              className="inline-block text-sm text-sky-400 hover:text-sky-300 transition-colors"
            >
              View all research
            </a>
          </div>
        </div>
      )}

      {/* Features or tips section */}
      <div className="mt-auto pb-6 pt-8 px-4">
        <div className="bg-gray-100/40 border border-gray-700/50 rounded-xl p-4">
          <h3 className="text-sm font-medium text-gray-700 mb-2">Research Tips</h3>
          <ul className="text-xs text-gray-500 space-y-1.5">
            <li className="flex items-start">
              <span className="text-sky-400 mr-1.5">â€¢</span>
              <span>Ask specific questions for better results</span>
            </li>
            <li className="flex items-start">
              <span className="text-sky-400 mr-1.5">â€¢</span>
              <span>Include key details like dates or context</span>
            </li>
            <li className="flex items-start">
              <span className="text-sky-400 mr-1.5">â€¢</span>
              <span>Chat with your research results for deeper insights</span>
            </li>
          </ul>
        </div>
      </div>

      {/* Styling for line clamp and input glow */}
      <style jsx global>{`
        .line-clamp-1 {
          overflow: hidden;
          display: -webkit-box;
          -webkit-box-orient: vertical;
          -webkit-line-clamp: 1;
        }
        
        .input-glow-subtle {
          box-shadow: 
            0 0 5px rgba(56, 189, 248, 0.2),
            0 0 12px rgba(14, 165, 233, 0.15),
            0 0 20px rgba(2, 132, 199, 0.1);
          animation: pulse-glow-subtle 3s infinite alternate;
        }
        
        @keyframes pulse-glow-subtle {
          0% {
            box-shadow: 
              0 0 5px rgba(56, 189, 248, 0.2),
              0 0 12px rgba(14, 165, 233, 0.15),
              0 0 20px rgba(2, 132, 199, 0.1);
          }
          100% {
            box-shadow: 
              0 0 8px rgba(56, 189, 248, 0.25),
              0 0 15px rgba(14, 165, 233, 0.2),
              0 0 25px rgba(2, 132, 199, 0.15);
          }
        }
        
        .input-glow-active {
          box-shadow: 
            0 0 5px rgba(56, 189, 248, 0.3),
            0 0 15px rgba(56, 189, 248, 0.3),
            0 0 25px rgba(14, 165, 233, 0.2),
            inset 0 0 3px rgba(186, 230, 253, 0.1);
          animation: pulse-glow-active 2s infinite alternate;
        }
        
        @keyframes pulse-glow-active {
          0% {
            box-shadow: 
              0 0 5px rgba(56, 189, 248, 0.3),
              0 0 15px rgba(56, 189, 248, 0.3),
              0 0 25px rgba(14, 165, 233, 0.2),
              inset 0 0 3px rgba(186, 230, 253, 0.1);
          }
          100% {
            box-shadow: 
              0 0 8px rgba(56, 189, 248, 0.4),
              0 0 20px rgba(14, 165, 233, 0.4),
              0 0 30px rgba(2, 132, 199, 0.3),
              inset 0 0 5px rgba(186, 230, 253, 0.2);
          }
        }
        
        @keyframes spin {
          to {
            transform: rotate(360deg);
          }
        }
        
        .animate-spin {
          animation: spin 1s linear infinite;
        }
      `}</style>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/mobile/MobileResearchContent.tsx
================================================
import React, { useRef, useState, useEffect } from "react";
import { useResearchHistoryContext } from "@/hooks/ResearchHistoryContext";
import MobileChatPanel from "@/components/mobile/MobileChatPanel";
import { ChatBoxSettings, Data, ChatData, QuestionData, ChatMessage } from "@/types/data";
import { toast } from "react-hot-toast";

interface MobileResearchContentProps {
  orderedData: Data[];
  answer: string;
  loading: boolean;
  isStopped: boolean;
  chatPromptValue: string;
  setChatPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleChat: (message: string) => void;
  isProcessingChat?: boolean;
  onNewResearch?: () => void;
  currentResearchId?: string;
  onShareClick?: () => void;
}

export default function MobileResearchContent({
  orderedData: initialOrderedData,
  answer: initialAnswer,
  loading: initialLoading,
  isStopped,
  chatPromptValue,
  setChatPromptValue,
  handleChat: parentHandleChat, // Renamed to clarify it's the parent's handler
  isProcessingChat: parentIsProcessing = false,
  onNewResearch,
  currentResearchId,
  onShareClick
}: MobileResearchContentProps) {
  // Access research history context for saving chat messages
  const { 
    addChatMessage, 
    updateResearch, 
    getChatMessages 
  } = useResearchHistoryContext();
  
  // Create local state to fully control the mobile experience
  const [localOrderedData, setLocalOrderedData] = useState<Data[]>(initialOrderedData);
  const [localAnswer, setLocalAnswer] = useState(initialAnswer);
  const [localLoading, setLocalLoading] = useState(initialLoading);
  const [localProcessing, setLocalProcessing] = useState(false);
  const bottomRef = useRef<HTMLDivElement>(null);
  
  // Sync with parent props when they change
  useEffect(() => {
    setLocalOrderedData(initialOrderedData);
  }, [initialOrderedData]);
  
  useEffect(() => {
    setLocalAnswer(initialAnswer);
  }, [initialAnswer]);
  
  useEffect(() => {
    setLocalLoading(initialLoading);
  }, [initialLoading]);
  
  // Handle chat message submission directly within the component
  const handleLocalChat = async (message: string) => {
    // Prevent processing if already in progress
    if (localProcessing) {
      return;
    }
    
    // Begin processing - show loading indicator
    setLocalProcessing(true);
    
    // Immediately add user question to UI for better UX
    const questionData: QuestionData = { 
      type: 'question', 
      content: message 
    };
    
    setLocalOrderedData(prev => [...prev, questionData]);
    
    // Create a user message object for history saving
    const userMessage: ChatMessage = {
      role: 'user',
      content: message,
      timestamp: Date.now()
    };
    
    // If we have a research ID, save the message to history
    if (currentResearchId) {
      try {
        await addChatMessage(currentResearchId, userMessage);
      } catch (error) {
        console.error('Error saving chat message to history:', error);
      }
    }
    
    try {
      // Get chat settings from localStorage or use defaults
      const reportSource = window.localStorage.getItem('chatBoxSettings') ? 
        JSON.parse(window.localStorage.getItem('chatBoxSettings') || '{}').report_source || 'web' :
        'web';
        
      const tone = window.localStorage.getItem('chatBoxSettings') ?
        JSON.parse(window.localStorage.getItem('chatBoxSettings') || '{}').tone || 'Objective' :
        'Objective';
      
      // Get all existing chat messages for context if we have a research ID
      const existingMessages = currentResearchId ? 
        getChatMessages(currentResearchId) : [];
      
      // Format messages to include just role and content
      const formattedMessages = [...existingMessages, userMessage].map(msg => ({
        role: msg.role,
        content: msg.content
      }));
      
      // Directly call the chat API
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: formattedMessages,
          report: localAnswer || '',
          report_source: reportSource,
          tone: tone
        }),
      });
      
      if (!response.ok) {
        throw new Error(`API request failed with status ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.response && data.response.content) {
        // Create the chat response object for history saving
        const aiMessage: ChatMessage = {
          role: 'assistant',
          content: data.response.content,
          timestamp: Date.now(),
          metadata: data.response.metadata
        };
        
        // Create the chat response data for UI
        const chatData: ChatData = { 
          type: 'chat', 
          content: data.response.content,
          metadata: data.response.metadata 
        };
        
        // Update local ordered data with the response
        setLocalOrderedData(prev => [...prev, chatData]);
        
        // If we have a research ID, save the AI message to history
        if (currentResearchId) {
          try {
            // Add AI message to chat history
            await addChatMessage(currentResearchId, aiMessage);
            
            // Update the research with both question and answer
            const updatedOrderedData = [...localOrderedData, questionData, chatData];
            await updateResearch(currentResearchId, localAnswer, updatedOrderedData);
          } catch (error) {
            console.error('Error saving AI response to history:', error);
          }
        }
      } else {
        // Show error for invalid or empty response
        const errorData: ChatData = {
          type: 'chat',
          content: 'Sorry, I couldn\'t generate a proper response. Please try again.'
        };
        
        setLocalOrderedData(prev => [...prev, errorData]);
        toast.error("Received an invalid response from the server", {
          duration: 3000,
          position: "bottom-center"
        });
      }
    } catch (error) {
      // Handle network or processing errors
      console.error('Error in mobile chat:', error);
      
      const errorData: ChatData = {
        type: 'chat',
        content: 'Sorry, there was an error processing your request. Please try again.'
      };
      
      setLocalOrderedData(prev => [...prev, errorData]);
      toast.error("Failed to communicate with the server", {
        duration: 3000,
        position: "bottom-center"
      });
    } finally {
      // Always end processing state
      setLocalProcessing(false);
      
      // Scroll to the bottom to show the new messages
      setTimeout(() => {
        if (bottomRef.current) {
          bottomRef.current.scrollIntoView({ behavior: 'smooth' });
        }
      }, 100);
    }
  };
  
  // Extract the initial question from ordered data
  const initialQuestion = localOrderedData.find(data => data.type === 'question');
  const questionText = initialQuestion?.content || '';

  return (
    <div className="flex flex-col h-[calc(100vh-3.5rem)] bg-gradient-to-b from-gray-900 to-gray-950">
      {/* Status Bar - Shows when researching or can show share button */}
      {(localLoading || localProcessing || (currentResearchId && onShareClick)) && (
        <div className="flex items-center justify-between px-4 py-2 bg-gray-100/90 border-b border-gray-700/50 backdrop-blur-sm">
          {/* Left side - status */}
          <div className="flex items-center">
            {(localLoading || localProcessing) && (
              <>
                <div className="w-2 h-2 rounded-full bg-amber-500 animate-pulse mr-2"></div>
                <span className="text-xs text-gray-700">
                  {localLoading ? "Researching..." : "Processing..."}
                </span>
              </>
            )}
            {!localLoading && !localProcessing && currentResearchId && (
              <>
                <div className="w-2 h-2 rounded-full bg-blue-500 mr-2"></div>
                <span className="text-xs text-gray-700">Research complete</span>
              </>
            )}
          </div>
          
          {/* Right side - share button */}
          {!localLoading && !localProcessing && currentResearchId && onShareClick && (
            <button 
              onClick={onShareClick}
              className="flex items-center text-xs px-3 py-1.5 rounded-md bg-gradient-to-r from-teal-700/70 to-teal-600/70 text-blue-600 border border-teal-600/40 shadow-sm"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="mr-1">
                <path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path>
                <polyline points="16 6 12 2 8 6"></polyline>
                <line x1="12" y1="2" x2="12" y2="15"></line>
              </svg>
              Share
            </button>
          )}
        </div>
      )}
      
      {/* Main Content - MobileChatPanel */}
      <div className="flex-1 overflow-hidden">
        <MobileChatPanel
          question={questionText}
          chatPromptValue={chatPromptValue}
          setChatPromptValue={setChatPromptValue}
          handleChat={handleLocalChat}
          orderedData={localOrderedData}
          loading={localLoading}
          isProcessingChat={localProcessing}
          isStopped={isStopped}
          onNewResearch={onNewResearch}
        />
      </div>
      
      {/* Reference element for scrolling */}
      <div ref={bottomRef} />
      
      {/* Subtle background pattern for premium feel */}
      <div className="absolute inset-0 bg-gradient-radial from-transparent to-transparent pointer-events-none" style={{ 
        backgroundImage: `radial-gradient(circle at 50% 10%, rgba(56, 189, 169, 0.03) 0%, transparent 70%), radial-gradient(circle at 80% 40%, rgba(56, 178, 169, 0.02) 0%, transparent 60%)` 
      }}></div>
      
      {/* Mobile-specific features/styles */}
      <style jsx global>{`
        /* Safe area insets for iPhone */
        @supports (padding: max(0px)) {
          .safe-bottom {
            padding-bottom: max(0.75rem, env(safe-area-inset-bottom));
          }
          .safe-top {
            padding-top: max(3.5rem, env(safe-area-inset-top) + 3.5rem);
          }
        }
        
        /* Remove tap highlight on mobile */
        * {
          -webkit-tap-highlight-color: transparent;
        }
        
        /* Better scrolling experience on mobile */
        .overflow-scroll {
          -webkit-overflow-scrolling: touch;
        }
        
        /* Animation for loading pulse */
        @keyframes pulse {
          0%, 100% { opacity: 1; }
          50% { opacity: 0.5; }
        }
        
        .animate-pulse {
          animation: pulse 1.5s cubic-bezier(0.4, 0, 0.6, 1) infinite;
        }
      `}</style>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/research/CopilotPanel.tsx
================================================
import React, { Dispatch, SetStateAction, useEffect, useRef } from 'react';
import ChatInput from '@/components/ResearchBlocks/elements/ChatInput';
import LoadingDots from '@/components/LoadingDots';
import { Data } from '@/types/data';
import Question from '@/components/ResearchBlocks/Question';
import ChatResponse from '@/components/ResearchBlocks/ChatResponse';
import Image from 'next/image';

interface CopilotPanelProps {
  question: string;
  chatPromptValue: string;
  setChatPromptValue: Dispatch<SetStateAction<string>>;
  handleChat: (message: string) => void;
  orderedData: Data[];
  loading: boolean;
  isProcessingChat: boolean;
  isStopped: boolean;
  bottomRef: React.RefObject<HTMLDivElement>;
  isCopilotVisible?: boolean;
  setIsCopilotVisible?: Dispatch<SetStateAction<boolean>>;
}

const CopilotPanel: React.FC<CopilotPanelProps> = ({
  question,
  chatPromptValue,
  setChatPromptValue,
  handleChat,
  orderedData,
  loading,
  isProcessingChat,
  isStopped,
  bottomRef,
  isCopilotVisible,
  setIsCopilotVisible
}) => {
  // Filter to only get chat messages (questions and responses) after the initial question
  const chatMessages = orderedData.filter((data, index) => {
    // Include all questions except the first one
    if (data.type === 'question') {
      return index > 0;
    }
    // Include all chat responses
    return data.type === 'chat';
  });

  // Reference to the chat container
  const chatContainerRef = useRef<HTMLDivElement>(null);
  
  // Function to scroll to bottom
  const scrollToBottom = () => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  };

  // Scroll when messages change or loading/processing state changes
  useEffect(() => {
    scrollToBottom();
  }, [chatMessages.length, loading, isProcessingChat]);

  // Also handle mutations in the DOM that might affect scroll height
  useEffect(() => {
    if (!chatContainerRef.current) return;
    
    const observer = new MutationObserver(scrollToBottom);
    
    observer.observe(chatContainerRef.current, {
      childList: true,
      subtree: true,
      characterData: true
    });
    
    return () => observer.disconnect();
  }, []);

  return (
    <>
      {/* Panel Header */}
      <div className="flex justify-between items-center px-2 py-3 border-b border-gray-300/60 bg-gray-50/40">
        {/* Left side */}
        <div className="flex items-center">
          <a href="/" className="mr-3">
            <img
              src="/img/gptr-logo.png"
              alt="logo"
              width={32}
              height={32}
              className="rounded-md"
            />
          </a>
          <h2 className="text-base font-medium text-gray-700">
            GPT Researcher
          </h2>
        </div>
        
        {/* Right side */}
        <div className="flex items-center gap-3">
          {/* Connection status indicator */}
          <div className="flex items-center">
            <div className={`w-1.5 h-1.5 rounded-full ${loading || isProcessingChat ? 'bg-amber-500 animate-pulse' : 'bg-blue-500'} mr-2`}></div>
            <span className="text-xs text-gray-500">{loading ? 'researching' : isProcessingChat ? 'thinking' : 'active'}</span>
          </div>
          
          {/* Toggle button */}
          {setIsCopilotVisible && (
            <button 
              onClick={(e) => {
                e.preventDefault();
                setIsCopilotVisible(false);
              }}
              className="flex items-center justify-center w-7 h-7 rounded-md hover:bg-gray-100 text-gray-500 hover:text-gray-700 transition-colors border border-transparent hover:border-gray-700/50"
              aria-label="Hide copilot panel"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
                <path d="M9 18l6-6-6-6" />
              </svg>
            </button>
          )}
        </div>
      </div>

      {/* Chat Messages - Scrollable */}
      <div 
        ref={chatContainerRef} 
        className="flex-1 overflow-y-auto py-2 px-2 custom-scrollbar bg-gray-50/20"
      >
        {/* Status message - conditional on research state */}
        <div className="mb-4">
          <div className="p-3 bg-gray-100/30 rounded-md border border-gray-700/40 shadow-sm">
            <div className="flex items-start gap-3">
              <div className="w-7 h-7 rounded-md bg-gray-100 flex items-center justify-center flex-shrink-0 text-gray-700 border border-gray-700/50">
                <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
                  <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
                </svg>
              </div>
              <div className="text-gray-700 text-sm">
                {loading ? (
                  <p>Working on your research... I&apos;ll analyze the results once they&apos;re complete.</p>
                ) : (
                  <p>I&apos;ve analyzed all the research results and can answer any questions about it. How can I help?</p>
                )}
              </div>
            </div>
          </div>
        </div>

        {/* Chat messages */}
        <div className="space-y-4">
          {chatMessages.map((data, index) => {
            if (data.type === 'question') {
              return (
                <div key={`chat-question-${index}`}>
                  <Question question={data.content} />
                </div>
              );
            } else if (data.type === 'chat') {
              return (
                <div key={`chat-answer-${index}`}>
                  <ChatResponse answer={data.content} metadata={data.metadata} />
                </div>
              );
            }
            return null;
          })}
        </div>

        {/* Loading indicator - always show during research or processing */}
        {(loading || isProcessingChat) && (
          <div className="flex justify-center">
            <div className="flex flex-col items-center">
              <LoadingDots />
            </div>
          </div>
        )}

        {/* Invisible element for scrolling */}
        <div ref={bottomRef} />
      </div>

      {/* Chat Input */}
      <div className="py-3 px-2 border-t border-gray-300/60 bg-gray-50/40">
        {!isStopped && (
          <ChatInput
            promptValue={chatPromptValue}
            setPromptValue={setChatPromptValue}
            handleSubmit={handleChat}
            disabled={loading || isProcessingChat}
          />
        )}
        {isStopped && (
          <div className="text-center p-2 text-gray-500 bg-gray-100/40 rounded-md border border-gray-700/40 text-sm">
            Research has been stopped. Start a new research to continue chatting.
          </div>
        )}
      </div>

      {/* Custom scrollbar styles */}
      <style jsx global>{`
        @keyframes pulse {
          0%, 100% { opacity: 1; }
          50% { opacity: 0.5; }
        }
        
        .animate-pulse {
          animation: pulse 1.5s cubic-bezier(0.4, 0, 0.6, 1) infinite;
        }
        
        .custom-scrollbar::-webkit-scrollbar {
          width: 4px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(17, 24, 39, 0.1);
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(75, 85, 99, 0.5);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(75, 85, 99, 0.7);
        }
      `}</style>
    </>
  );
};

export default CopilotPanel; 


================================================
FILE: frontend/nextjs/components/research/CopilotResearchContent.tsx
================================================
import { useRef, Dispatch, SetStateAction, useState, useCallback, useEffect } from "react";
import ResearchPanel from "@/components/research/ResearchPanel";
import CopilotPanel from "@/components/research/CopilotPanel";
import { ChatBoxSettings, Data } from "@/types/data";

interface CopilotResearchContentProps {
  orderedData: Data[];
  answer: string;
  allLogs: any[];
  chatBoxSettings: ChatBoxSettings;
  loading: boolean;
  isStopped: boolean;
  promptValue: string;
  chatPromptValue: string;
  setPromptValue: Dispatch<SetStateAction<string>>;
  setChatPromptValue: Dispatch<SetStateAction<string>>;
  handleDisplayResult: (question: string) => void;
  handleChat: (message: string) => void;
  handleClickSuggestion: (value: string) => void;
  currentResearchId?: string;
  onShareClick?: () => void;
  reset?: () => void;
  isProcessingChat?: boolean;
  onNewResearch?: () => void;
  toggleSidebar?: () => void;
}

export default function CopilotResearchContent({
  orderedData,
  answer,
  allLogs,
  chatBoxSettings,
  loading,
  isStopped,
  promptValue,
  chatPromptValue,
  setPromptValue,
  setChatPromptValue,
  handleDisplayResult,
  handleChat,
  handleClickSuggestion,
  currentResearchId,
  onShareClick,
  reset,
  isProcessingChat = false,
  onNewResearch,
  toggleSidebar
}: CopilotResearchContentProps) {
  const bottomRef = useRef<HTMLDivElement>(null);
  // Initialize copilot as hidden when loading
  const [isCopilotVisible, setIsCopilotVisible] = useState(false);
  const [showAnimation, setShowAnimation] = useState(false);
  // Track if user manually closed the copilot panel
  const [userClosedCopilot, setUserClosedCopilot] = useState(false);
  // State for split pane resizing
  const [resizingActive, setResizingActive] = useState(false);
  const [researchPanelWidth, setResearchPanelWidth] = useState(58); // percentage
  const [isMobile, setIsMobile] = useState(false);
  const containerRef = useRef<HTMLDivElement>(null);
  const widthRef = useRef(researchPanelWidth);
  const researchPanelRef = useRef<HTMLDivElement>(null);
  const chatPanelRef = useRef<HTMLDivElement>(null);
  const lastUpdateTimeRef = useRef(0);
  
  // Check if we're on mobile
  useEffect(() => {
    const checkIfMobile = () => {
      setIsMobile(window.innerWidth < 1024);
    };
    
    // Initial check
    checkIfMobile();
    
    // Add event listener for window resize
    window.addEventListener('resize', checkIfMobile);
    
    // Cleanup
    return () => window.removeEventListener('resize', checkIfMobile);
  }, []);
  
  // Create a memoized toggle function that's compatible with Dispatch<SetStateAction<boolean>>
  const toggleCopilotVisibility: Dispatch<SetStateAction<boolean>> = useCallback((value) => {
    // Handle both function and direct value cases
    const newValue = typeof value === 'function' ? value(isCopilotVisible) : value;
    
    // Set state without triggering scroll
    setIsCopilotVisible(newValue);
    
    // Track user's explicit action of closing the panel
    if (newValue === false) {
      setUserClosedCopilot(true);
    }
    
    // If we're showing the copilot, trigger the animation
    if (newValue && !isCopilotVisible) {
      setShowAnimation(true);
    }
    
    // Prevent scroll jumping by keeping current scroll position
    const currentScrollY = window.scrollY;
    
    // Use requestAnimationFrame to restore scroll position after the state update
    requestAnimationFrame(() => {
      window.scrollTo({
        top: currentScrollY,
        behavior: 'auto'
      });
    });
  }, [isCopilotVisible]);
  
  // Effect to handle initial state and research completion
  useEffect(() => {
    // Reset userClosedCopilot when new research starts
    if (loading) {
      setUserClosedCopilot(false);
    }
    
    // Automatically open the copilot when research completes BUT only if user hasn't manually closed it
    if (!loading && answer && !isCopilotVisible && !userClosedCopilot) {
      // Add a slight delay before showing the copilot for a better UX
      const timer = setTimeout(() => {
        setIsCopilotVisible(true);
        setShowAnimation(true);
      }, 800);
      
      return () => clearTimeout(timer);
    }
  }, [loading, answer, isCopilotVisible, userClosedCopilot]);
  
  // Extract the initial question from orderedData
  const initialQuestion = orderedData.find(data => data.type === 'question');
  const questionText = initialQuestion?.content || '';

  // Handle resize start
  const handleResizeStart = useCallback((e: React.MouseEvent) => {
    e.preventDefault();
    setResizingActive(true);
  }, []);

  // Handle resize move
  useEffect(() => {
    const handleResizeMove = (e: MouseEvent) => {
      if (!resizingActive || !containerRef.current) return;
      
      // Throttle updates to every 16ms (approx 60fps)
      const now = Date.now();
      if (now - lastUpdateTimeRef.current < 16) {
        return;
      }
      lastUpdateTimeRef.current = now;
      
      // Use requestAnimationFrame for smoother updates
      requestAnimationFrame(() => {
        if (!containerRef.current) return;
        
        const containerRect = containerRef.current.getBoundingClientRect();
        const containerWidth = containerRect.width;
        const mouseX = e.clientX - containerRect.left;
        
        // Calculate percentage width (with constraints)
        let newWidth = (mouseX / containerWidth) * 100;
        newWidth = Math.max(30, Math.min(70, newWidth)); // Constrain between 30% and 70%
        
        // Store width in ref without causing re-renders
        widthRef.current = newWidth;
        
        // Apply directly to DOM elements using refs
        if (researchPanelRef.current) {
          researchPanelRef.current.style.width = `${newWidth}%`;
        }
        if (chatPanelRef.current) {
          chatPanelRef.current.style.width = `${100 - newWidth}%`;
        }
      });
    };

    const handleResizeEnd = () => {
      // Only update state once dragging ends
      setResearchPanelWidth(widthRef.current);
      setResizingActive(false);
    };

    if (resizingActive) {
      document.addEventListener('mousemove', handleResizeMove);
      document.addEventListener('mouseup', handleResizeEnd);
    }

    return () => {
      document.removeEventListener('mousemove', handleResizeMove);
      document.removeEventListener('mouseup', handleResizeEnd);
    };
  }, [resizingActive]);

  return (
    <div 
      ref={containerRef}
      className="flex flex-col lg:flex-row w-full h-screen gap-1 px-2 lg:px-2 relative"
    >
      {/* Subtle background gradient */}
      <div className="absolute inset-0 bg-gradient-to-br from-gray-900/5 via-gray-800/5 to-gray-900/5 pointer-events-none"></div>
      
      {/* Research Results Panel (Left) */}
      <div 
        ref={researchPanelRef}
        data-panel="research"
        className={`w-full ${isCopilotVisible ? '' : 'lg:w-full'} h-full overflow-hidden flex flex-col bg-gray-50/30 backdrop-blur-sm rounded-lg border border-gray-300/50 shadow-lg ${!resizingActive ? 'transition-width duration-300' : ''}`}
        style={isCopilotVisible && !isMobile ? { width: `${researchPanelWidth}%` } : {}}
      >
        <ResearchPanel 
          orderedData={orderedData}
          answer={answer}
          allLogs={allLogs}
          chatBoxSettings={chatBoxSettings}
          handleClickSuggestion={handleClickSuggestion}
          currentResearchId={currentResearchId}
          onShareClick={onShareClick}
          isCopilotVisible={isCopilotVisible}
          setIsCopilotVisible={toggleCopilotVisibility}
          onNewResearch={onNewResearch}
          loading={loading}
          toggleSidebar={toggleSidebar}
        />
      </div>

      {/* Resizer handle */}
      {isCopilotVisible && (
        <div
          className={`hidden lg:flex flex-col items-center justify-center w-1 h-full cursor-col-resize ${resizingActive ? 'bg-blue-500/50' : 'bg-gray-700/30 hover:bg-blue-500/30'} transition-colors duration-150 active:bg-blue-500/50 z-10 mx-0.5`}
          onMouseDown={handleResizeStart}
        >
          <div className="flex flex-col items-center justify-center">
            <div className="w-0.5 h-16 bg-gray-500/80 rounded-full hover:bg-teal-400/80"></div>
          </div>
        </div>
      )}
      
      {/* Copilot Chat Panel (Right) */}
      {isCopilotVisible && (
        <div 
          ref={chatPanelRef}
          data-panel="chat"
          className={`w-full h-1/2 lg:h-full overflow-hidden flex flex-col bg-gray-50/30 backdrop-blur-sm rounded-lg border border-gray-300/50 shadow-lg ${!resizingActive ? 'transition-width duration-300' : ''} ${
            showAnimation ? 'animate-copilot-entrance' : ''
          }`}
          style={!isMobile ? { width: `${100 - researchPanelWidth}%` } : {}}
        >
          <CopilotPanel
            question={questionText}
            chatPromptValue={chatPromptValue}
            setChatPromptValue={setChatPromptValue}
            handleChat={handleChat}
            orderedData={orderedData}
            loading={loading}
            isProcessingChat={isProcessingChat}
            isStopped={isStopped}
            bottomRef={bottomRef}
            isCopilotVisible={isCopilotVisible}
            setIsCopilotVisible={toggleCopilotVisibility}
          />
        </div>
      )}
      
      {/* Custom styles for animations */}
      <style jsx global>{`
        @keyframes subtle-pulse {
          0% { opacity: 0.8; }
          50% { opacity: 1; }
          100% { opacity: 0.8; }
        }
        
        @keyframes spin {
          to { transform: rotate(360deg); }
        }
        
        @keyframes spin-slow {
          to { transform: rotate(-360deg); }
        }
        
        @keyframes spin-slower {
          to { transform: rotate(360deg); }
        }
        
        .animate-spin {
          animation: spin 1.5s linear infinite;
        }
        
        .animate-spin-slow {
          animation: spin-slow 3s linear infinite;
        }
        
        .animate-spin-slower {
          animation: spin-slower 4.5s linear infinite;
        }
        
        @keyframes copilot-entrance {
          0% { 
            opacity: 0; 
            transform: translateX(40px) scale(0.95);
            box-shadow: 0 0 0 rgba(17, 24, 39, 0);
          }
          70% {
            opacity: 1;
            transform: translateX(-5px) scale(1.02);
            box-shadow: 0 10px 25px rgba(17, 24, 39, 0.2);
          }
          100% { 
            opacity: 1; 
            transform: translateX(0) scale(1);
            box-shadow: 0 4px 12px rgba(17, 24, 39, 0.15);
          }
        }
        
        .animate-copilot-entrance {
          animation: copilot-entrance 0.6s cubic-bezier(0.22, 1, 0.36, 1) forwards;
        }
        
        .custom-scrollbar::-webkit-scrollbar {
          width: 4px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(17, 24, 39, 0.1);
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(75, 85, 99, 0.5);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(75, 85, 99, 0.7);
        }

        .transition-width {
          transition: width 0.3s ease;
        }
      `}</style>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/research/NotFoundContent.tsx
================================================
import React from 'react';

interface NotFoundContentProps {
  onNewResearch: () => void;
}

export default function NotFoundContent({ onNewResearch }: NotFoundContentProps) {
  return (
    <div className="min-h-[100vh] pt-[70px] flex flex-col items-center justify-center">
      <div className="text-center max-w-md mx-auto">
        <div className="w-24 h-24 mx-auto mb-6 rounded-full bg-gradient-to-br from-gray-800/60 to-gray-700/40 flex items-center justify-center">
          <svg xmlns="http://www.w3.org/2000/svg" className="h-12 w-12 text-gray-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M9.172 16.172a4 4 0 015.656 0M9 10h.01M15 10h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
          </svg>
        </div>
        <h2 className="text-2xl font-bold text-gray-700 mb-2">Research Not Found</h2>
        <p className="text-gray-500 mb-6">The research report you&apos;re looking for doesn&apos;t seem to exist or might have been deleted.</p>
        <button 
          onClick={onNewResearch}
          className="px-5 py-2.5 bg-blue-500 hover:bg-blue-600 text-gray-800	800	 rounded-md transition-colors"
        >
          Return to Home
        </button>
      </div>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/research/ResearchContent.tsx
================================================
import { useRef, Dispatch, SetStateAction } from "react";
import { ResearchResults } from "@/components/ResearchResults";
import InputArea from "@/components/ResearchBlocks/elements/InputArea";
import ChatInput from "@/components/ResearchBlocks/elements/ChatInput";
import LoadingDots from "@/components/LoadingDots";
import { ChatBoxSettings, Data } from "@/types/data";

interface ResearchContentProps {
  showResult: boolean;
  orderedData: Data[];
  answer: string;
  allLogs: any[];
  chatBoxSettings: ChatBoxSettings;
  loading: boolean;
  isInChatMode: boolean;
  isStopped: boolean;
  promptValue: string;
  chatPromptValue: string;
  setPromptValue: Dispatch<SetStateAction<string>>;
  setChatPromptValue: Dispatch<SetStateAction<string>>;
  handleDisplayResult: (question: string) => void;
  handleChat: (message: string) => void;
  handleClickSuggestion: (value: string) => void;
  currentResearchId?: string;
  onShareClick?: () => void;
  reset?: () => void;
  isProcessingChat?: boolean;
  bottomRef?: React.RefObject<HTMLDivElement>;
}

export default function ResearchContent({
  showResult,
  orderedData,
  answer,
  allLogs,
  chatBoxSettings,
  loading,
  isInChatMode,
  isStopped,
  promptValue,
  chatPromptValue,
  setPromptValue,
  setChatPromptValue,
  handleDisplayResult,
  handleChat,
  handleClickSuggestion,
  currentResearchId,
  onShareClick,
  reset,
  isProcessingChat = false,
  bottomRef
}: ResearchContentProps) {
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const internalBottomRef = useRef<HTMLDivElement>(null);
  const finalBottomRef = bottomRef || internalBottomRef;

  return (
    <div className="flex h-full w-full grow flex-col justify-between">
      <div className="container w-full space-y-2">
        {onShareClick && currentResearchId && (
          <div className="flex justify-end mb-4">
            <button 
              onClick={onShareClick}
              className="px-4 py-2 bg-blue-500 hover:bg-blue-600 text-gray-800	800	 rounded-md flex items-center gap-2 transition-colors"
            >
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M8.684 13.342C8.886 12.938 9 12.482 9 12c0-.482-.114-.938-.316-1.342m0 2.684a3 3 0 110-2.684m0 2.684l6.632 3.316m-6.632-6l6.632-3.316m0 0a3 3 0 105.367-2.684 3 3 0 00-5.367 2.684zm0 9.316a3 3 0 105.368 2.684 3 3 0 00-5.368-2.684z" />
              </svg>
              Share
            </button>
          </div>
        )}
        
        <div className="container space-y-2 task-components">
          <ResearchResults
            orderedData={orderedData}
            answer={answer}
            allLogs={allLogs}
            chatBoxSettings={chatBoxSettings}
            handleClickSuggestion={handleClickSuggestion}
            currentResearchId={currentResearchId}
            isProcessingChat={isProcessingChat}
            onShareClick={onShareClick}
          />
        </div>

        <div className="pt-1 sm:pt-2" ref={chatContainerRef}></div>
        {/* Invisible element for scrolling */}
        <div ref={finalBottomRef} />
      </div>
      
      <div id="input-area" className="container px-4 lg:px-0 mb-4">
        {loading || isProcessingChat ? (
          <div className="mt-4 flex justify-center">
            <LoadingDots />
          </div>
        ) : (
          <div>
            {isInChatMode && !isStopped ? (
              <ChatInput
                promptValue={chatPromptValue}
                setPromptValue={setChatPromptValue}
                handleSubmit={handleChat}
                disabled={loading || isProcessingChat}
              />
            ) : (
              showResult && reset ? (
                <InputArea
                  promptValue={promptValue}
                  setPromptValue={setPromptValue}
                  handleSubmit={handleDisplayResult}
                  disabled={loading}
                  reset={reset}
                  isStopped={isStopped}
                />
              ) : null
            )}
          </div>
        )}
      </div>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/research/ResearchPanel.tsx
================================================
import React, { useState } from 'react';
import { ResearchResults } from '@/components/ResearchResults';
import { Data, ChatBoxSettings } from '@/types/data';
import LoadingDots from '@/components/LoadingDots';
import Image from 'next/image';

interface ResearchPanelProps {
  orderedData: Data[];
  answer: string;
  allLogs: any[];
  chatBoxSettings: ChatBoxSettings;
  handleClickSuggestion: (value: string) => void;
  currentResearchId?: string;
  onShareClick?: () => void;
  isCopilotVisible?: boolean;
  setIsCopilotVisible?: React.Dispatch<React.SetStateAction<boolean>>;
  onNewResearch?: () => void;
  loading?: boolean;
  toggleSidebar?: () => void;
}

const ResearchPanel: React.FC<ResearchPanelProps> = ({
  orderedData,
  answer,
  allLogs,
  chatBoxSettings,
  handleClickSuggestion,
  currentResearchId,
  onShareClick,
  isCopilotVisible,
  setIsCopilotVisible,
  onNewResearch,
  loading,
  toggleSidebar
}) => {
  // Determine if research is complete (has answer) and copilot should be highlighted
  const researchComplete = Boolean(answer && answer.length > 0);
  const [isNotificationDismissed, setIsNotificationDismissed] = useState(false);
  
  return (
    <>
      {/* Panel Header */}
      <div className="flex justify-between items-center px-3 py-3 border-b border-gray-300/60 bg-gray-50/40">
        {/* Left side - Empty div to maintain flex layout */}
        <div className="flex items-center">
        </div>
        
        {/* Right side - Action buttons */}
        <div className="flex items-center gap-2">
          {/* New Research button */}
          {onNewResearch && (
            <button 
              onClick={onNewResearch}
              className="px-3 py-1.5 bg-sky-200/80 hover:bg-sky-300/80 text-sky-800 rounded-md flex items-center gap-1.5 transition-colors text-sm font-medium"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
                <line x1="12" y1="5" x2="12" y2="19"></line>
                <line x1="5" y1="12" x2="19" y2="12"></line>
              </svg>
              New Research
            </button>
          )}
          
          {/* Share button */}
          {onShareClick && currentResearchId && (
            <button 
              onClick={onShareClick}
              className="px-3 py-1.5 bg-blue-500 hover:bg-blue-600 text-gray-800	800	 rounded-md flex items-center gap-1.5 transition-colors border border-teal-500/50 text-sm shadow-sm hover:shadow-teal-500/20"
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
                <path d="M4 12v8a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2v-8"></path>
                <polyline points="16 6 12 2 8 6"></polyline>
                <line x1="12" y1="2" x2="12" y2="15"></line>
              </svg>
              Share
            </button>
          )}
          
          {/* Show Copilot button - only visible when copilot is hidden */}
          {!isCopilotVisible && setIsCopilotVisible && (
            <button 
              onClick={() => setIsCopilotVisible(true)}
              className={`px-3 py-1.5 bg-teal-800/70 hover:bg-blue-600 text-teal-100 rounded-md flex items-center gap-1.5 transition-colors border border-teal-700/60 text-sm ${researchComplete ? 'animate-chat-button-pulse' : ''}`}
            >
              <svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="1.5" strokeLinecap="round" strokeLinejoin="round">
                <path d="M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z"></path>
              </svg>
              Chat
            </button>
          )}
        </div>
      </div>
      
      <div className="flex-1 overflow-y-auto p-2 custom-scrollbar bg-gray-50/20">
        {/* Filter out chat messages so they only show in the chat panel */}
        <div className="space-y-4 relative">          
          <ResearchResults
            orderedData={orderedData.filter(data => {
              // Keep everything except chat responses
              if (data.type === 'chat') return false;
              
              // For questions, only keep the first/initial question
              if (data.type === 'question') {
                return orderedData.indexOf(data) === 0;
              }
              
              // Keep all other types
              return true;
            })}
            answer={answer}
            allLogs={allLogs}
            chatBoxSettings={chatBoxSettings}
            handleClickSuggestion={handleClickSuggestion}
            currentResearchId={currentResearchId}
          />
          
          {/* Loading indicator - show during research */}
          {loading && (
            <div className="flex justify-center mt-6">
              <div className="flex flex-col items-center">
                <LoadingDots />
              </div>
            </div>
          )}
        </div>
      </div>
      
      {/* Custom scrollbar styles */}
      <style jsx global>{`
        @keyframes chat-button-pulse {
          0%, 100% {
            box-shadow: 0 0 0 0 rgba(20, 184, 166, 0.4);
            transform: scale(1);
          }
          70% {
            box-shadow: 0 0 0 10px rgba(20, 184, 166, 0);
            transform: scale(1.02);
          }
        }
        
        .animate-chat-button-pulse {
          animation: chat-button-pulse 2s infinite cubic-bezier(0.66, 0, 0, 1);
        }
        
        @keyframes fade-in-up {
          0% {
            opacity: 0;
            transform: translateY(10px);
          }
          100% {
            opacity: 1;
            transform: translateY(0);
          }
        }
        
        .animate-fade-in-up {
          animation: fade-in-up 0.6s ease-out forwards;
        }
        
        .custom-scrollbar::-webkit-scrollbar {
          width: 4px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(17, 24, 39, 0.1);
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: rgba(75, 85, 99, 0.5);
          border-radius: 20px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: rgba(75, 85, 99, 0.7);
        }
      `}</style>
    </>
  );
};

export default ResearchPanel; 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/AccessReport.tsx
================================================
import React from 'react';
import {getHost} from '../../helpers/getHost'

interface AccessReportProps {
  accessData: {
    pdf?: string;
    docx?: string;
    json?: string;
  };
  chatBoxSettings: {
    report_type?: string;
  };
  report: string;
  onShareClick?: () => void;
}

const AccessReport: React.FC<AccessReportProps> = ({ accessData, chatBoxSettings, report, onShareClick }) => {
  const host = getHost();

  const getReportLink = (dataType: 'pdf' | 'docx' | 'json'): string => {
    // Early return if path is not available
    if (!accessData?.[dataType]) {
      console.warn(`No ${dataType} path provided`);
      return '#';
    }

    const path = accessData[dataType] as string;
    
    // Clean the path - remove leading/trailing slashes and handle outputs/ prefix
    const cleanPath = path
      .trim()
      .replace(/^\/+|\/+$/g, ''); // Remove leading/trailing slashes
    
    // Only prepend outputs/ if it's not already there
    const finalPath = cleanPath.startsWith('outputs/') 
      ? cleanPath 
      : `outputs/${cleanPath}`;
    
    return `${host}/${finalPath}`;
  };

  // Safety check for accessData
  if (!accessData || typeof accessData !== 'object') {
    return null;
  }

  return (
    <div className="container rounded-lg border border-solid border-gray-200 bg-black/30 backdrop-blur-md shadow-lg p-5 my-5">
      <div className="flex flex-col items-center">
        <h3 className="text-lg font-bold mb-4 text-gray-800	800	">Access Your Research Report</h3>
        
        <div className="flex flex-wrap justify-center gap-3">
          {accessData.pdf && (
            <a 
              href={getReportLink('pdf')} 
              className="bg-blue-500 text-gray-800	800	 font-medium uppercase text-sm px-6 py-3 rounded-lg shadow-md hover:shadow-lg hover:opacity-90 focus:outline-none focus:ring-2 focus:ring-teal-500/50 transform hover:scale-105 transition-all duration-200 flex items-center gap-2"
              target="_blank"
              rel="noopener noreferrer">
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
              </svg>
              View as PDF
            </a>
          )}
          
          {accessData.docx && (
            <a 
              href={getReportLink('docx')} 
              className="bg-blue-500 text-gray-800	800	 font-medium uppercase text-sm px-6 py-3 rounded-lg shadow-md hover:shadow-lg hover:opacity-90 focus:outline-none focus:ring-2 focus:ring-blue-400/50 transform hover:scale-105 transition-all duration-200 flex items-center gap-2"
              target="_blank"
              rel="noopener noreferrer">
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4" />
              </svg>
              Download DocX
            </a>
          )}
          
          {chatBoxSettings?.report_type === 'research_report' && accessData.json && (
            <a
              href={getReportLink('json')}
              className="bg-cyan-600 text-gray-800	800	 font-medium uppercase text-sm px-6 py-3 rounded-lg shadow-md hover:shadow-lg hover:opacity-90 focus:outline-none focus:ring-2 focus:ring-cyan-500/50 transform hover:scale-105 transition-all duration-200 flex items-center gap-2"
              target="_blank"
              rel="noopener noreferrer">
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 3v2m6-2v2M9 19v2m6-2v2M5 9H3m2 6H3m18-6h-2m2 6h-2M7 19h10a2 2 0 002-2V7a2 2 0 00-2-2H7a2 2 0 00-2 2v10a2 2 0 002 2zM9 9h6v6H9V9z" />
              </svg>
              Download Logs
            </a>
          )}
          
          {onShareClick && (
            <button
              onClick={onShareClick}
              className="bg-purple-600 text-gray-800	800	 font-medium uppercase text-sm px-6 py-3 rounded-lg shadow-md hover:shadow-lg hover:opacity-90 focus:outline-none focus:ring-2 focus:ring-purple-500/50 transform hover:scale-105 transition-all duration-200 flex items-center gap-2"
            >
              <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M8.684 13.342C8.886 12.938 9 12.482 9 12c0-.482-.114-.938-.316-1.342m0 2.684a3 3 0 110-2.684m0 2.684l6.632 3.316m-6.632-6l6.632-3.316m0 0a3 3 0 105.367-2.684 3 3 0 00-5.367 2.684zm0 9.316a3 3 0 105.368 2.684 3 3 0 00-5.368-2.684z" />
              </svg>
              Share Report
            </button>
          )}
        </div>
      </div>
    </div>
  );
};

export default AccessReport;


================================================
FILE: frontend/nextjs/components/ResearchBlocks/ChatInterface.tsx
================================================
import React, { useState, useEffect, useRef } from 'react';
import { ChatMessage } from '../../types/data';
import ChatInput from './elements/ChatInput';
import { markdownToHtml } from '../../helpers/markdownHelper';
import '../../styles/markdown.css';

interface ChatInterfaceProps {
  researchId: string;
  reportText: string;
  onAddMessage: (message: ChatMessage) => void;
  messages: ChatMessage[];
}

const ChatInterface: React.FC<ChatInterfaceProps> = ({ 
  researchId, 
  reportText, 
  onAddMessage, 
  messages 
}) => {
  const [isLoading, setIsLoading] = useState(false);
  const [promptValue, setPromptValue] = useState('');
  const [renderedMessages, setRenderedMessages] = useState<{content: string, html: string, role: string}[]>([]);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  // Convert markdown in messages to HTML
  useEffect(() => {
    const renderMessages = async () => {
      const rendered = await Promise.all(
        messages.map(async (msg) => {
          const html = await markdownToHtml(msg.content);
          return {
            content: msg.content,
            html,
            role: msg.role
          };
        })
      );
      setRenderedMessages(rendered);
    };
    
    renderMessages();
  }, [messages]);

  // Scroll to bottom when new messages are added
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [renderedMessages]);

  const handleSubmitPrompt = async (prompt: string) => {
    if (!prompt.trim()) return;
    
    // Add user message to the UI
    const userMessage: ChatMessage = {
      role: 'user',
      content: prompt,
      timestamp: Date.now()
    };
    onAddMessage(userMessage);
    
    // Show loading state
    setIsLoading(true);
    
    try {
      // Make API call to chat endpoint
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          report: reportText,
          messages: [...messages, userMessage]
        }),
      });
      
      if (!response.ok) {
        throw new Error('Failed to get chat response');
      }
      
      const data = await response.json();
      
      // Add assistant response to the UI
      if (data.response) {
        onAddMessage(data.response);
      }
    } catch (error) {
      console.error('Error during chat:', error);
      // Show error message in chat
      onAddMessage({
        role: 'assistant',
        content: 'Sorry, there was an error processing your request. Please try again.',
        timestamp: Date.now()
      });
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="flex flex-col h-full">
      <div className="flex-1 overflow-y-auto p-4 space-y-4 mb-4 custom-scrollbar">
        {renderedMessages.length === 0 ? (
          <div className="text-center p-8 rounded-lg bg-gradient-to-r from-gray-900/5 to-gray-800/5 border border-gray-300/20 backdrop-blur-sm relative overflow-hidden">
            {/* Ambient decoration */}
            <div className="absolute -inset-1 bg-gradient-to-r from-teal-500/5 via-purple-500/5 to-cyan-500/5 blur-xl opacity-30 animate-pulse"></div>
            
            {/* Icon */}
            <div className="w-16 h-16 mx-auto mb-4 relative">
              <div className="absolute inset-0 bg-gradient-to-br from-teal-400/30 to-cyan-400/30 rounded-full blur-md animate-pulse"></div>
              <div className="absolute inset-0 flex items-center justify-center">
                <svg xmlns="http://www.w3.org/2000/svg" className="h-10 w-10 text-teal-500" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={1.5} d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z" />
                </svg>
              </div>
            </div>
            
            <h3 className="text-lg font-medium text-gray-800	800	 mb-2">Ask a question about this research report</h3>
            <p className="text-sm text-gray-500 max-w-md mx-auto">
              The AI has analyzed all the content and is ready to help you explore the findings. 
              Ask anything about the research, request summaries, or dig deeper into specific topics.
            </p>
          </div>
        ) : (
          <>
            {renderedMessages.map((msg, index) => (
              <div 
                key={index} 
                className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
              >
                <div 
                  className={`max-w-[80%] p-3 rounded-lg shadow-md ${
                    msg.role === 'user' 
                      ? 'bg-gradient-to-r from-teal-500 to-teal-600 text-gray-800	800	 shadow-teal-500/20' 
                      : 'bg-gradient-to-r from-gray-800 to-gray-900 text-gray-800	800	 shadow-gray-700/30'
                  } relative overflow-hidden group transition-all duration-300 hover:shadow-lg`}
                >
                  {/* Add subtle animated gradient effect */}
                  <div className={`absolute inset-0 opacity-10 group-hover:opacity-20 transition-opacity duration-300 ${
                    msg.role === 'user' 
                      ? 'bg-gradient-to-br from-teal-300/40 via-transparent to-cyan-300/30' 
                      : 'bg-gradient-to-br from-indigo-400/30 via-transparent to-purple-400/20'
                  } animate-pulse pointer-events-none`}></div>
                  
                  <div 
                    className="markdown-content text-sm sm:text-base relative z-10"
                    dangerouslySetInnerHTML={{ __html: msg.html }}
                  />
                </div>
              </div>
            ))}
            
            {/* Skeleton loader for assistant response */}
            {isLoading && (
              <div className="flex justify-start">
                <div className="max-w-[80%] p-3 rounded-lg shadow-md bg-gradient-to-r from-gray-800 to-gray-900 text-gray-800	800	 shadow-gray-700/30 relative overflow-hidden">
                  <div className="absolute inset-0 bg-gradient-to-br from-indigo-400/30 via-transparent to-purple-400/20 opacity-20 animate-pulse"></div>
                  <div className="markdown-content text-sm sm:text-base relative z-10">
                    {/* Heading */}
                    <div className="h-7 bg-gray-700/50 rounded-md w-3/5 mb-4 animate-pulse"></div>
                    
                    {/* Paragraph */}
                    <div className="space-y-2 mb-4">
                      <div className="h-4 bg-gray-700/50 rounded-full w-full animate-pulse"></div>
                      <div className="h-4 bg-gray-700/50 rounded-full w-[95%] animate-pulse"></div>
                      <div className="h-4 bg-gray-700/50 rounded-full w-[98%] animate-pulse"></div>
                      <div className="h-4 bg-gray-700/50 rounded-full w-[90%] animate-pulse"></div>
                    </div>
                    
                    {/* List items */}
                    <div className="space-y-2 mb-4">
                      <div className="flex">
                        <div className="h-4 w-4 rounded-full bg-gray-700/50 mt-1 mr-2 animate-pulse"></div>
                        <div className="h-4 bg-gray-700/50 rounded-full w-[70%] animate-pulse"></div>
                      </div>
                      <div className="flex">
                        <div className="h-4 w-4 rounded-full bg-gray-700/50 mt-1 mr-2 animate-pulse"></div>
                        <div className="h-4 bg-gray-700/50 rounded-full w-[80%] animate-pulse"></div>
                      </div>
                      <div className="flex">
                        <div className="h-4 w-4 rounded-full bg-gray-700/50 mt-1 mr-2 animate-pulse"></div>
                        <div className="h-4 bg-gray-700/50 rounded-full w-[75%] animate-pulse"></div>
                      </div>
                    </div>
                    
                    {/* Code block */}
                    <div className="rounded-md bg-gray-100/70 p-3 mb-4">
                      <div className="space-y-2">
                        <div className="h-4 bg-gray-700/60 rounded-full w-[85%] animate-pulse"></div>
                        <div className="h-4 bg-gray-700/60 rounded-full w-[90%] animate-pulse"></div>
                        <div className="h-4 bg-gray-700/60 rounded-full w-[80%] animate-pulse"></div>
                      </div>
                    </div>
                    
                    {/* Final paragraph */}
                    <div className="space-y-2">
                      <div className="h-4 bg-gray-700/50 rounded-full w-[88%] animate-pulse"></div>
                      <div className="h-4 bg-gray-700/50 rounded-full w-[92%] animate-pulse"></div>
                      <div className="h-4 bg-gray-700/50 rounded-full w-[60%] animate-pulse"></div>
                    </div>
                  </div>
                </div>
              </div>
            )}
          </>
        )}
        <div ref={messagesEndRef} />
      </div>
      
      <div className="p-4 border-t border-gray-700">
        <ChatInput
          promptValue={promptValue}
          setPromptValue={setPromptValue}
          handleSubmit={handleSubmitPrompt}
          disabled={isLoading}
        />
      </div>
    </div>
  );
};

export default ChatInterface; 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/ChatResponse.tsx
================================================
import React, { useState, useEffect } from 'react';
import { toast } from "react-hot-toast";
import { markdownToHtml } from '../../helpers/markdownHelper';
import '../../styles/markdown.css';
import Sources from './Sources';

interface ChatResponseProps {
  answer: string;
  metadata?: {
    tool_calls?: Array<{
      tool: string;
      query: string;
      search_metadata: {
        query: string;
        sources: Array<{
          title: string;
          url: string;
          content: string;
        }>
      }
    }>
  }
}

export default function ChatResponse({ answer, metadata }: ChatResponseProps) {
    const [htmlContent, setHtmlContent] = useState('');
    
    // Check if we have sources from a web search tool call
    const hasWebSources = metadata?.tool_calls?.some(
      tool => tool.tool === 'quick_search' && tool.search_metadata?.sources?.length > 0
    );
    
    // Get all sources from web searches
    const webSources = metadata?.tool_calls
      ?.filter(tool => tool.tool === 'quick_search')
      .flatMap(tool => tool.search_metadata?.sources || [])
      .map(source => ({
        name: source.title,
        url: source.url
      })) || [];

    useEffect(() => {
      if (answer) {
        markdownToHtml(answer).then((html) => setHtmlContent(html));
      }
    }, [answer]);
    
    // Format the answer for display
    const formattedAnswer = answer.trim() || 'No answer available.';
    
    const copyToClipboard = () => {
        // Copy the plain text of the answer instead of the HTML
        navigator.clipboard.writeText(formattedAnswer)
            .then(() => {
                toast.success('Copied to clipboard!');
            })
            .catch((err) => {
                console.error('Failed to copy: ', err);
                toast.error('Failed to copy to clipboard');
            });
    };
  
    return (
      <div className="container flex h-auto w-full shrink-0 gap-4 bg-black/30 backdrop-blur-md shadow-lg rounded-lg border border-solid border-gray-700/40 p-5">
        <div className="w-full">
          <div className="flex items-center justify-between pb-3">
            <div className="flex items-center gap-3">
              <div className="flex items-center justify-center w-6 h-6 rounded-md bg-blue-500/20 border border-teal-500/30">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="12" height="12" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="text-blue-600">
                  <polyline points="20 6 9 17 4 12"></polyline>
                </svg>
              </div>
              <h3 className="text-sm font-medium text-blue-600">Answer</h3>
            </div>
            <button 
              onClick={copyToClipboard}
              className="hover:opacity-80 transition-opacity duration-200"
              aria-label="Copy to clipboard"
              title="Copy to clipboard"
            >
              <img
                src="/img/copy-white.svg"
                alt="copy"
                width={20}
                height={20}
                className="cursor-pointer text-gray-800	800	"
              />
            </button>
          </div>
          
          <div className="flex flex-wrap content-center items-center gap-[15px] pl-5 pr-5">
            <div className="w-full whitespace-pre-wrap text-base font-light leading-[152.5%] text-gray-800	800	 log-message">
              <div 
                className="markdown-content prose prose-invert max-w-none"
                dangerouslySetInnerHTML={{ __html: htmlContent }}
              />
            </div>
          </div>
          
          {/* Display web search sources if available */}
          {hasWebSources && webSources.length > 0 && (
            <div className="mt-4 pt-3 border-t border-gray-200">
              <div className="flex items-center gap-2 mb-2">
                <div className="flex items-center justify-center w-5 h-5 rounded-md bg-blue-500/20 border border-blue-500/30">
                  <svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" className="text-blue-400">
                    <circle cx="12" cy="12" r="10"></circle>
                    <line x1="2" y1="12" x2="22" y2="12"></line>
                    <path d="M12 2a15.3 15.3 0 0 1 4 10 15.3 15.3 0 0 1-4 10 15.3 15.3 0 0 1-4-10 15.3 15.3 0 0 1 4-10z"></path>
                  </svg>
                </div>
                <span className="text-xs font-medium text-blue-300">New Sources</span>
              </div>
              <Sources sources={webSources} compact={true} />
            </div>
          )}
        </div>
      </div>
    );
} 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/ImageSection.tsx
================================================
import Image from "next/image"; 
import React, { memo } from 'react';
import ImagesAlbum from '../Images/ImagesAlbum';

interface ImageSectionProps {
  metadata: any;
}

const ImageSection = ({ metadata }: ImageSectionProps) => {
  return (
    <div className="container h-auto w-full shrink-0 rounded-lg border border-solid border-gray-700/40 bg-black/30 backdrop-blur-md shadow-lg p-5">
      <div className="flex items-start gap-4 pb-3 lg:pb-3.5">
        <img src="/img/image.svg" alt="images" width={24} height={24} />
        <h3 className="text-base font-bold uppercase leading-[152.5%] text-gray-800	800	">
          Related Images
        </h3>
      </div>
      <div className="overflow-y-auto max-h-[500px] scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-gray-300/10">
        <ImagesAlbum images={metadata} />
      </div>
    </div>
  );
};

// Simple memo implementation that compares arrays properly
export default memo(ImageSection, (prevProps, nextProps) => {
  // If both are null/undefined or the same reference, they're equal
  if (prevProps.metadata === nextProps.metadata) return true;
  
  // If one is null/undefined but not the other, they're not equal
  if (!prevProps.metadata || !nextProps.metadata) return false;
  
  // Compare lengths
  if (prevProps.metadata.length !== nextProps.metadata.length) return false;
  
  // Compare each item
  for (let i = 0; i < prevProps.metadata.length; i++) {
    if (prevProps.metadata[i] !== nextProps.metadata[i]) return false;
  }
  
  return true;
}); 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/LogsSection.tsx
================================================
import Image from "next/image";
import LogMessage from './elements/LogMessage';
import { useEffect, useRef } from 'react';

interface Log {
  header: string;
  text: string;
  metadata: any;
  key: string;
}

interface OrderedLogsProps {
  logs: Log[];
}

const LogsSection = ({ logs }: OrderedLogsProps) => {
  const logsContainerRef = useRef<HTMLDivElement>(null);

  useEffect(() => {
    // Scroll to bottom whenever logs change
    if (logsContainerRef.current) {
      logsContainerRef.current.scrollTop = logsContainerRef.current.scrollHeight;
    }
  }, [logs]); // Dependency on logs array ensures this runs when new logs are added

  return (
    <div className="container h-auto w-full shrink-0 rounded-lg border border-solid border-gray-700/40 bg-black/30 backdrop-blur-md shadow-lg p-5 mt-5">
      <div className="flex items-start gap-4 pb-3 lg:pb-3.5">
        <img src="/img/chat-check.svg" alt="logs" width={24} height={24} />
        <h3 className="text-base font-bold uppercase leading-[152.5%] text-gray-800	800	">
          Agent Work
        </h3>
      </div>
      <div 
        ref={logsContainerRef}
        className="overflow-y-auto min-h-[200px] max-h-[500px] scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-gray-300/10"
      >
        <LogMessage logs={logs} />
      </div>
    </div>
  );
};

export default LogsSection; 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/Question.tsx
================================================
import React from 'react';
import Image from "next/image";

interface QuestionProps {
  question: string;
}

const Question: React.FC<QuestionProps> = ({ question }) => {
  return (
    <div className="container w-full flex flex-col sm:flex-row items-start gap-3 pt-5 mb-5 px-4 sm:px-6 py-4 rounded-lg border border-gray-200 backdrop-blur-sm bg-black/20 mt-5">
      <div className="flex items-center gap-2 sm:gap-4">
        <img
          src={"/img/message-question-circle.svg"}
          alt="message"
          width={24}
          height={24}
          className="w-6 h-6"
        />
        {/*<p className="font-bold uppercase leading-[152%] text-blue-600">
          Research Task:
        </p>*/}
      </div>
      <div className="grow text-gray-800	800	 break-words max-w-full log-message mt-1 sm:mt-0 font-medium">{question}</div>
    </div>
  );
};

export default Question;



================================================
FILE: frontend/nextjs/components/ResearchBlocks/Report.tsx
================================================
import Image from "next/image";
import React, { useState, useEffect } from 'react';
import { toast } from "react-hot-toast";
import { markdownToHtml } from '../../helpers/markdownHelper';
import '../../styles/markdown.css';
import { useResearchHistoryContext } from '../../hooks/ResearchHistoryContext';
import { ChatMessage } from '../../types/data';

export default function Report({ answer, researchId }: { answer: string, researchId?: string }) {
    const [htmlContent, setHtmlContent] = useState('');
    const { getChatMessages } = useResearchHistoryContext();
    // Memoize this value to prevent re-renders
    const chatMessages = researchId ? getChatMessages(researchId) : [];

    useEffect(() => {
      if (answer) {
        markdownToHtml(answer).then((html) => setHtmlContent(html));
      }
    }, [answer]);
    
    return (
      <div className="container flex h-auto w-full shrink-0 gap-4 bg-black/30 backdrop-blur-md shadow-lg rounded-lg border border-solid border-gray-700/40 p-5">
        <div className="w-full">
          <div className="flex items-center justify-between pb-3">
            <div className="flex items-center gap-3">
              <svg 
                xmlns="http://www.w3.org/2000/svg" 
                viewBox="0 0 24 24" 
                width={20}
                height={20}
                fill="none" 
                stroke="currentColor" 
                strokeWidth={1.5} 
                strokeLinecap="round" 
                strokeLinejoin="round" 
                className="text-blue-600"
              >
                <path d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
              </svg>
              <h3 className="text-sm font-medium text-blue-600">Research Report</h3>
            </div>
            {answer && (
              <div className="flex items-center gap-3">
                <button
                  onClick={() => {
                    navigator.clipboard.writeText(answer.trim());
                    toast("Report copied to clipboard", {
                      icon: "âœ‚ï¸",
                    });
                  }}
                  className="hover:opacity-80 transition-opacity duration-200"
                >
                  <img
                    src="/img/copy-white.svg"
                    alt="copy"
                    width={20}
                    height={20}
                    className="cursor-pointer text-gray-800	800	"
                  />
                </button>
              </div>
            )}
          </div>
          
          <div className="flex flex-wrap content-center items-center gap-[15px] pl-5 pr-5">
            <div className="w-full whitespace-pre-wrap text-base font-light leading-[152.5%] text-gray-800	800	 log-message">
              {answer ? (
                <div className="markdown-content prose prose-invert max-w-none" dangerouslySetInnerHTML={{ __html: htmlContent }} />
              ) : (
                <div className="flex w-full flex-col gap-2">
                  <div className="h-6 w-full animate-pulse rounded-md bg-gray-300/20" />
                  <div className="h-6 w-[85%] animate-pulse rounded-md bg-gray-300/20" />
                  <div className="h-6 w-[90%] animate-pulse rounded-md bg-gray-300/20" />
                  <div className="h-6 w-[70%] animate-pulse rounded-md bg-gray-300/20" />
                </div>
              )}
            </div>
          </div>
        </div>
      </div>
    );
} 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/Sources.tsx
================================================
import Image from "next/image";
import React from 'react';
import SourceCard from "./elements/SourceCard";

export default function Sources({
  sources,
  compact = false,
}: {
  sources: { name: string; url: string }[];
  compact?: boolean;
}) {
  if (compact) {
    // Compact version for chat responses
    return (
      <div className="max-h-[200px] overflow-y-auto scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-gray-300/10">
        <div className="flex w-full flex-wrap content-center items-center gap-2">
          {sources.map((source) => {
            // Extract domain from URL
            let displayUrl = source.url;
            try {
              const urlObj = new URL(source.url);
              displayUrl = urlObj.hostname.replace(/^www\./, '');
            } catch (e) {
              // If URL parsing fails, use the original URL
            }
            
            return (
              <a 
                key={source.url} 
                href={source.url} 
                target="_blank" 
                rel="noopener noreferrer" 
                className="inline-flex items-center gap-1.5 px-2 py-1 text-xs bg-gray-100/60 text-gray-700 hover:text-blue-600 hover:bg-gray-100/90 rounded border border-gray-700/40 transition-colors"
                title={source.name}
              >
                <svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
                  <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path>
                  <polyline points="15 3 21 3 21 9"></polyline>
                  <line x1="10" y1="14" x2="21" y2="3"></line>
                </svg>
                {displayUrl}
              </a>
            );
          })}
        </div>
      </div>
    );
  }

  // Full version for research results
  return (
    <div className="container h-auto w-full shrink-0 rounded-lg border border-solid border-gray-700/40 bg-black/30 backdrop-blur-md shadow-lg p-5">
      <div className="flex items-start gap-4 pb-3 lg:pb-3.5">
        <img src="/img/browser.svg" alt="sources" width={24} height={24} />
        <h3 className="text-base font-bold uppercase leading-[152.5%] text-gray-800	800	">
          {sources.length} Sources{" "}
        </h3>
      </div>
      <div className="overflow-y-auto max-h-[250px] scrollbar-thin scrollbar-thumb-gray-600 scrollbar-track-gray-300/10">
        <div className="flex w-full max-w-[890px] flex-wrap content-center items-center gap-[15px] pb-2">
          {sources.length > 0 ? (
            sources.map((source) => (
              <SourceCard source={source} key={source.url} />
            ))
          ) : (
            <>
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
              <div className="h-20 w-[260px] max-w-sm animate-pulse rounded-md bg-gray-300/20" />
            </>
          )}
        </div>
      </div>
    </div>
  );
}



================================================
FILE: frontend/nextjs/components/ResearchBlocks/elements/ChatInput.tsx
================================================
import Image from "next/image";
import React, { FC, useRef, useState, useEffect } from "react";
import TypeAnimation from "../../TypeAnimation";

type TChatInputProps = {
  promptValue: string;
  setPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleSubmit: (query: string) => void;
  disabled?: boolean;
};

// Debounce function to limit the rate at which a function can fire
function debounce(func: Function, wait: number) {
  let timeout: NodeJS.Timeout | undefined;
  return function executedFunction(...args: any[]) {
    const later = () => {
      clearTimeout(timeout);
      func(...args);
    };
    clearTimeout(timeout);
    timeout = setTimeout(later, wait);
  };
}

const ChatInput: FC<TChatInputProps> = ({
  promptValue,
  setPromptValue,
  handleSubmit,
  disabled,
}) => {
  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const [isFocused, setIsFocused] = useState(false);
  const placeholder = "Any questions about this report?";

  const resetHeight = () => {
    if (textareaRef.current) {
      textareaRef.current.style.height = '3em';
    }
  };

  const adjustHeight = debounce((target: HTMLTextAreaElement) => {
    target.style.height = 'auto';
    target.style.height = `${target.scrollHeight}px`;
  }, 100);

  const handleTextareaChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const target = e.target;
    adjustHeight(target);
    setPromptValue(target.value);
  };

  const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if (e.key === 'Enter') {
      if (e.shiftKey) {
        return;
      } else {
        e.preventDefault();
        if (!disabled && promptValue.trim()) {
          handleSubmit(promptValue);
          setPromptValue('');
          resetHeight();
        }
      }
    }
  };

  return (
    <div className="relative">
      {/* Gradient ring with balanced glow */}
      <div 
        className={`absolute -inset-0.5 rounded-lg bg-gradient-to-r from-[#0cdbb6]/50 via-[#1fd0f0]/40 to-[#06dbee]/50 blur-sm opacity-40 transition-opacity duration-300 ${isFocused || promptValue ? 'opacity-60' : 'opacity-30'}`}
      />
      
      {/* Ambient glow effect - balanced size and opacity */}
      <div 
        className="absolute -inset-3 rounded-xl opacity-25"
        style={{
          background: 'radial-gradient(circle at center, rgba(12, 219, 182, 0.15) 0%, rgba(6, 219, 238, 0.08) 40%, rgba(0, 0, 0, 0) 70%)',
        }}
      />
      
      <form
        className="mx-auto flex pt-2 pb-2 w-full items-center justify-between rounded-lg border border-gray-700/50 bg-gray-50/90 backdrop-blur-sm px-3 shadow-md relative overflow-hidden z-10"
        onSubmit={(e) => {
          e.preventDefault();
          if (!disabled && promptValue.trim()) {
            handleSubmit(promptValue);
            setPromptValue('');
            resetHeight();
          }
        }}
      >
        {/* Inner gradient blur effect - balanced opacity */}
        <div className="absolute -inset-1 bg-gradient-to-r from-teal-400/4 via-indigo-400/4 to-purple-400/4 blur-xl opacity-25 animate-pulse pointer-events-none"></div>
        
        <textarea
          placeholder={placeholder}
          ref={textareaRef}
          className="focus-visible::outline-0 my-1 w-full pl-5 font-light not-italic leading-[normal] 
          text-gray-700 placeholder-gray-400 outline-none focus-visible:ring-0 focus-visible:ring-offset-0 
          sm:text-base min-h-[4em] resize-none relative z-10 bg-transparent"
          disabled={disabled}
          value={promptValue}
          required
          rows={3}
          onKeyDown={handleKeyDown}
          onChange={handleTextareaChange}
          onFocus={() => setIsFocused(true)}
          onBlur={() => setIsFocused(false)}
        />
        
        <button
          disabled={disabled || !promptValue.trim()}
          type="submit"
          className="relative flex h-[45px] w-[45px] shrink-0 items-center justify-center rounded-md bg-blue-500 hover:bg-gradient-to-br hover:from-[#0cdbb6] hover:via-[#1fd0f0] hover:to-[#06dbee] transition-all duration-300 disabled:opacity-50 disabled:hover:bg-blue-500/75 z-10 before:absolute before:inset-0 before:rounded-md before:bg-gradient-to-r before:from-teal-300/15 before:to-cyan-300/15 before:opacity-0 before:transition-opacity before:hover:opacity-100 before:-z-10 disabled:before:opacity-0 group"
        >
          {disabled && (
            <div className="absolute inset-0 flex items-center justify-center">
              <TypeAnimation />
            </div>
          )}

          <div className="relative p-2 cursor-pointer overflow-hidden">
            {/* Glow effect on hover - balanced brightness */}
            <div className="absolute inset-0 opacity-0 group-hover:opacity-80 transition-opacity duration-300 bg-white/15 rounded-full blur-sm"></div>
            
            <img
              src={"/img/arrow-narrow-right.svg"}
              alt="send"
              width={20}
              height={20}
              className={`${disabled ? "invisible" : ""} transition-all duration-300 group-hover:scale-110 group-hover:brightness-110 group-hover:filter group-hover:drop-shadow-[0_0_2px_rgba(255,255,255,0.6)]`}
            />
          </div>
        </button>
      </form>
      
      {/* Animated glow effect at the bottom - balanced brightness */}
      <div 
        className="absolute bottom-0 left-0 right-0 h-[2.5px] opacity-35 overflow-hidden"
        style={{
          background: 'radial-gradient(ellipse at center, rgba(12, 219, 182, 0.5) 0%, rgba(6, 219, 238, 0.3) 25%, rgba(6, 219, 238, 0.08) 50%, rgba(0, 0, 0, 0) 75%)',
          boxShadow: '0 0 8px 1px rgba(12, 219, 182, 0.25), 0 0 15px 2px rgba(6, 219, 238, 0.08)'
        }}
      />
    </div>
  );
};

export default ChatInput; 


================================================
FILE: frontend/nextjs/components/ResearchBlocks/elements/InputArea.tsx
================================================
import Image from "next/image";
import React, { FC, useRef, useState, useEffect } from "react";
import TypeAnimation from "../../TypeAnimation";

type TInputAreaProps = {
  promptValue: string;
  setPromptValue: React.Dispatch<React.SetStateAction<string>>;
  handleSubmit: (query: string) => void;
  handleSecondary?: (query: string) => void;
  disabled?: boolean;
  reset?: () => void;
  isStopped?: boolean;
};

// Debounce function to limit the rate at which a function can fire
function debounce(func: Function, wait: number) {
  let timeout: NodeJS.Timeout | undefined;
  return function executedFunction(...args: any[]) {
    const later = () => {
      clearTimeout(timeout);
      func(...args);
    };
    clearTimeout(timeout);
    timeout = setTimeout(later, wait);
  };
}

const InputArea: FC<TInputAreaProps> = ({
  promptValue,
  setPromptValue,
  handleSubmit,
  handleSecondary,
  disabled,
  reset,
  isStopped,
}) => {
  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const [isFocused, setIsFocused] = useState(false);
  const placeholder = "Enter your topic, question, or area of interest...";

  // Auto-focus the textarea when component mounts
  useEffect(() => {
    if (textareaRef.current) {
      textareaRef.current.focus();
    }
  }, []);

  const resetHeight = () => {
    if (textareaRef.current) {
      textareaRef.current.style.height = '3em';
    }
  };

  const adjustHeight = debounce((target: HTMLTextAreaElement) => {
    target.style.height = 'auto';
    target.style.height = `${target.scrollHeight}px`;
  }, 100);

  const handleTextareaChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const target = e.target;
    adjustHeight(target);
    setPromptValue(target.value);
  };

  const handleKeyDown = (e: React.KeyboardEvent<HTMLTextAreaElement>) => {
    if (e.key === 'Enter') {
      if (e.shiftKey) {
        return;
      } else {
        e.preventDefault();
        if (!disabled) {
          if (reset) reset();
          handleSubmit(promptValue);
          setPromptValue('');
          resetHeight();
        }
      }
    }
  };

  if (isStopped) {
    return null;
  }

  return (
    <div className="relative">
      {/* Gradient ring - subtle effect */}
      <div 
        className={`absolute -inset-0.5 rounded-xl bg-gradient-to-r from-[#0cdbb6]/50 via-[#1fd0f0]/40 to-[#06dbee]/50 blur-md opacity-45 transition-opacity duration-300 ${isFocused || promptValue ? 'opacity-55' : 'opacity-35'}`}
      />
      
      {/* Ambient glow effect */}
      <div 
        className="absolute -inset-4 rounded-xl opacity-25"
        style={{
          background: 'radial-gradient(circle at center, rgba(12, 219, 182, 0.15) 0%, rgba(6, 219, 238, 0.08) 40%, rgba(0, 0, 0, 0) 70%)',
        }}
      />
    
      <form
        className="mx-auto flex pt-2 pb-2 w-full items-center justify-between rounded-xl border border-gray-700/50 bg-gray-50/90 backdrop-blur-sm px-3 shadow-md relative overflow-hidden z-10"
        onSubmit={(e) => {
          e.preventDefault();
          if (reset) reset();
          handleSubmit(promptValue);
          setPromptValue('');
          resetHeight();
        }}
      >
        {/* Inner gradient blur effect */}
        <div className="absolute -inset-1 bg-gradient-to-r from-teal-400/4 via-indigo-400/4 to-purple-400/4 blur-xl opacity-25 animate-pulse pointer-events-none"></div>
        
        <textarea
          placeholder={placeholder}
          ref={textareaRef}
          className="focus-visible::outline-0 my-1 w-full pl-2 pr-3 font-light not-italic leading-[normal] 
          text-gray-700 placeholder-gray-400 outline-none focus-visible:ring-0 focus-visible:ring-offset-0 
          text-lg sm:text-xl min-h-[4em] resize-none relative z-10 bg-transparent"
          disabled={disabled}
          value={promptValue}
          required
          rows={3}
          onKeyDown={handleKeyDown}
          onChange={handleTextareaChange}
          onFocus={() => setIsFocused(true)}
          onBlur={() => setIsFocused(false)}
        />
        
        <button
          disabled={disabled}
          type="submit"
          className="relative flex h-[45px] w-[45px] shrink-0 items-center justify-center rounded-md bg-blue-500 hover:bg-gradient-to-br hover:from-[#0cdbb6] hover:via-[#1fd0f0] hover:to-[#06dbee] transition-all duration-300 disabled:opacity-50 disabled:hover:bg-blue-500/75 z-10 before:absolute before:inset-0 before:rounded-md before:bg-gradient-to-r before:from-teal-300/20 before:to-cyan-300/20 before:opacity-0 before:transition-opacity before:hover:opacity-100 before:-z-10 disabled:before:opacity-0 group"
        >
          {disabled && (
            <div className="absolute inset-0 flex items-center justify-center">
              <TypeAnimation />
            </div>
          )}

          <div className="relative p-2 cursor-pointer overflow-hidden">
            {/* Glow effect on hover */}
            <div className="absolute inset-0 opacity-0 group-hover:opacity-100 transition-opacity duration-300 bg-white/20 rounded-full blur-md"></div>
            
            <img
              src={"/img/arrow-narrow-right.svg"}
              alt="search"
              width={20}
              height={20}
              className={`${disabled ? "invisible" : ""} transition-all duration-300 group-hover:scale-110 group-hover:brightness-110 group-hover:filter group-hover:drop-shadow-[0_0_3px_rgba(255,255,255,0.7)]`}
            />
          </div>
        </button>
      </form>
      
      {/* Animated glow effect at the bottom */}
      <div 
        className="absolute bottom-0 left-0 right-0 h-[3px] opacity-35 overflow-hidden"
        style={{
          background: 'radial-gradient(ellipse at center, rgba(12, 219, 182, 0.5) 0%, rgba(6, 219, 238, 0.3) 25%, rgba(6, 219, 238, 0.08) 50%, rgba(0, 0, 0, 0) 75%)',
          boxShadow: '0 0 8px 1px rgba(12, 219, 182, 0.25), 0 0 16px 2px rgba(6, 219, 238, 0.08)'
        }}
      />
    </div>
  );
};

export default InputArea;



================================================
FILE: frontend/nextjs/components/ResearchBlocks/elements/LogMessage.tsx
================================================
// LogMessage.tsx
import Accordion from '../../Task/Accordion';
import { useEffect, useState } from 'react';
import { markdownToHtml } from '../../../helpers/markdownHelper';
import ImagesAlbum from '../../Images/ImagesAlbum';
import Image from "next/image";

type ProcessedData = {
  field: string;
  htmlContent: string;
  isMarkdown: boolean;
};

type Log = {
  header: string;
  text: string;
  processedData?: ProcessedData[];
  metadata?: any;
};

interface LogMessageProps {
  logs: Log[];
}

const LogMessage: React.FC<LogMessageProps> = ({ logs }) => {
  const [processedLogs, setProcessedLogs] = useState<Log[]>([]);

  useEffect(() => {
    const processLogs = async () => {
      if (!logs) return;
      
      const newLogs = await Promise.all(
        logs.map(async (log) => {
          try {
            if (log.header === 'differences' && log.text) {
              const data = JSON.parse(log.text).data;
              const processedData = await Promise.all(
                Object.keys(data).map(async (field) => {
                  const fieldValue = data[field].after || data[field].before;
                  if (!plainTextFields.includes(field)) {
                    const htmlContent = await markdownToHtml(fieldValue);
                    return { field, htmlContent, isMarkdown: true };
                  }
                  return { field, htmlContent: fieldValue, isMarkdown: false };
                })
              );
              return { ...log, processedData };
            }
            return log;
          } catch (error) {
            console.error('Error processing log:', error);
            return log;
          }
        })
      );
      setProcessedLogs(newLogs);
    };

    processLogs();
  }, [logs]);

  return (
    <>
      {processedLogs.map((log, index) => {
        if (log.header === 'subquery_context_window' || log.header === 'differences') {
          return <Accordion key={index} logs={[log]} />;
        } else if (log.header !== 'selected_images' && log.header !== 'scraping_images') {
          return (
            <div
              key={index}
              className="w-full max-w-4xl mx-auto rounded-lg pt-2 mt-3 pb-2 px-4 bg-gray-50 shadow-md"
            >
              <p className="py-3 text-base leading-relaxed text-gray-800	800	 dtext-gray-800	ray-800	">
                {log.text}
              </p>
            </div>
          );
        }
        return null;
      })}
    </>
  );
};

const plainTextFields = ['task', 'sections', 'headers', 'sources', 'research_data'];

export default LogMessage;


================================================
FILE: frontend/nextjs/components/ResearchBlocks/elements/SourceCard.tsx
================================================
import Image from "next/image";
import { useState, useMemo } from "react";

const SourceCard = ({ source }: { source: { name: string; url: string } }) => {
  const [imageSrc, setImageSrc] = useState(`https://www.google.com/s2/favicons?domain=${source.url}&sz=128`);

  const handleImageError = () => {
    setImageSrc("/img/globe.svg");
  };
  
  // Extract and format the domain from the URL
  const formattedUrl = useMemo(() => {
    try {
      const urlObj = new URL(source.url);
      return urlObj.hostname.replace(/^www\./, '');
    } catch (e) {
      // If URL parsing fails, use the original URL but trim it
      return source.url.length > 50 ? source.url.substring(0, 50) + '...' : source.url;
    }
  }, [source.url]);

  return (
    <div className="flex h-[79px] w-full items-center gap-3 rounded-lg border border-solid border-gray-200 bg-gray-100/30 backdrop-blur-sm shadow-sm px-3 py-2 md:w-auto hover:border-teal-500/30 transition-colors duration-200">
      
        <img
          src={imageSrc}
          alt={source.url}
          className="p-1"
          width={44}
          height={44}
          onError={handleImageError}  // Update src on error
        />
      
      <div className="flex max-w-[192px] flex-col justify-center gap-[7px]">
        <h6 className="line-clamp-2 text-sm font-medium leading-[normal] text-gray-800	800	">
          {source.name}
        </h6>
        <a
          target="_blank"
          rel="noopener noreferrer"
          href={source.url}
          className="truncate text-sm font-light text-gray-700/60 hover:text-blue-600/80 transition-colors"
          title={source.url}
        >
          {formattedUrl}
        </a>
      </div>
    </div>
  );
};

export default SourceCard;



================================================
FILE: frontend/nextjs/components/ResearchBlocks/elements/SubQuestions.tsx
================================================
import Image from "next/image";

interface SubQuestionsProps {
  metadata: string[];
  handleClickSuggestion: (value: string) => void;
}

const SubQuestions: React.FC<SubQuestionsProps> = ({ metadata, handleClickSuggestion }) => {
  return (
    <div className="container flex w-full items-start gap-3 pt-5 pb-2">
      <div className="flex w-fit items-center gap-4">
        <img
          src={"/img/thinking.svg"}
          alt="thinking"
          width={30}
          height={30}
          className="size-[24px]"
        />
      </div>
      <div className="grow text-gray-800	800	">
        <p className="pr-5 font-bold leading-[152%] text-gray-800	800	 pb-[20px]">
          Pondering your question from several angles
        </p>
        <div className="flex flex-row flex-wrap items-center gap-2.5 pb-[20px]">
          {metadata.map((item, subIndex) => (
            <div
              className="flex cursor-pointer items-center justify-center gap-[5px] rounded-full border border-solid border-[#C1C1C1] bg-[#EDEDEA] px-2.5 py-2"
              onClick={() => handleClickSuggestion(item)}
              key={`${item}-${subIndex}`}
            >
              <span className="text-sm font-light leading-[normal] text-[#1B1B16]">
                {item}
              </span>
            </div>
          ))}
        </div>
      </div>
    </div>
  );
};

export default SubQuestions;


================================================
FILE: frontend/nextjs/components/Settings/ChatBox.tsx
================================================
import React, { useState, useEffect } from 'react';
import ResearchForm from '../Task/ResearchForm';
import Report from '../Task/Report';
import AgentLogs from '../Task/AgentLogs';
import AccessReport from '../ResearchBlocks/AccessReport';
import { getHost } from '../../helpers/getHost';
import { ChatBoxSettings } from '@/types/data';

interface ChatBoxProps {
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
}

interface OutputData {
  pdf?: string;
  docx?: string;
  json?: string;
}

interface WebSocketMessage {
  type: 'logs' | 'report' | 'path';
  output: string | OutputData;
}

export default function ChatBox({ chatBoxSettings, setChatBoxSettings }: ChatBoxProps) {

  const [agentLogs, setAgentLogs] = useState<any[]>([]);
  const [report, setReport] = useState("");
  const [accessData, setAccessData] = useState({});
  const [socket, setSocket] = useState<WebSocket | null>(null);

  return (
    <div>
      <main className="static-container" id="form">
        <ResearchForm 
          chatBoxSettings={chatBoxSettings} 
          setChatBoxSettings={setChatBoxSettings}
        />

        {agentLogs?.length > 0 ? <AgentLogs agentLogs={agentLogs} /> : ''}
        <div className="margin-div">
          {report ? <Report report={report} /> : ''}
          {Object.keys(accessData).length > 0 && 
            <AccessReport 
              accessData={accessData} 
              chatBoxSettings={chatBoxSettings} 
              report={report}
            />
          }
        </div>
      </main>
    </div>
  );
}


================================================
FILE: frontend/nextjs/components/Settings/FileUpload.tsx
================================================
import React, { useCallback, useEffect, useState } from "react";
import axios from 'axios';
import { useDropzone } from 'react-dropzone';
import {getHost} from "@/helpers/getHost"

const FileUpload = () => {
  const [files, setFiles] = useState([]);
  const host = getHost();

  const fetchFiles = useCallback(async () => {
    try {
      const response = await axios.get(`${host}/files/`);
      setFiles(response.data.files);
    } catch (error) {
      console.error('Error fetching files:', error);
    }
  }, [host]);

  useEffect(() => {
    fetchFiles();
  }, [fetchFiles]);

  const onDrop = async (acceptedFiles: any[]) => {
    const formData = new FormData();
    acceptedFiles.forEach(file => {
      formData.append('file', file);
    });
    
    try {
      await axios.post(`${host}/upload/`, formData, {
        headers: {
          'Content-Type': 'multipart/form-data'
        }
      });
      fetchFiles();
    } catch (error) {
      console.error('Error uploading files:', error);
    }
  };

  const deleteFile = async (filename: never) => {
    try {
      await axios.delete(`${host}/files/${filename}`);
      fetchFiles();
    } catch (error) {
      console.error('Error deleting file:', error);
    }
  };

  const { getRootProps, getInputProps } = useDropzone({ onDrop });

  return (
    <div className={"mb-4 w-full"}>
      <div {...getRootProps()} style={{ border: '2px dashed #cccccc', padding: '20px', textAlign: 'center' }}>
        <input {...getInputProps()} />
        <p>Drag &apos;n&apos; drop some files here, or click to select files</p>
      </div>
      {files.length > 0 && (
          <>
            <h2 className={"text-gray-900 mt-2 text-xl"}>Uploaded Files</h2>
            <ul role={"list"} className={"my-2 divide-y divide-gray-100"}>
              {files.map(file => (
                <li key={file} className={"flex justify-between gap-x-6 py-1"}>
                  <span className={"flex-1"}>{file}</span>
                  <button onClick={(e) => { e.preventDefault(); deleteFile(file) }}>
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" strokeWidth="1.5"
                        stroke="currentColor" className="size-6">
                      <path strokeLinecap="round" strokeLinejoin="round"
                            d="m14.74 9-.346 9m-4.788 0L9.26 9m9.968-3.21c.342.052.682.107 1.022.166m-1.022-.165L18.16 19.673a2.25 2.25 0 0 1-2.244 2.077H8.084a2.25 2.25 0 0 1-2.244-2.077L4.772 5.79m14.456 0a48.108 48.108 0 0 0-3.478-.397m-12 .562c.34-.059.68-.114 1.022-.165m0 0a48.11 48.11 0 0 1 3.478-.397m7.5 0v-.916c0-1.18-.91-2.164-2.09-2.201a51.964 51.964 0 0 0-3.32 0c-1.18.037-2.09 1.022-2.09 2.201v.916m7.5 0a48.667 48.667 0 0 0-7.5 0"/>
                    </svg>
                  </button>
                </li>
              ))}
            </ul>
          </>
        )}
    </div>
  );
};

export default FileUpload;


================================================
FILE: frontend/nextjs/components/Settings/LayoutSelector.tsx
================================================
import React, { ChangeEvent } from 'react';

interface LayoutSelectorProps {
  layoutType: string;
  onLayoutChange: (event: ChangeEvent<HTMLSelectElement>) => void;
}

export default function LayoutSelector({ layoutType, onLayoutChange }: LayoutSelectorProps) {
  return (
    <div className="form-group">
      <label htmlFor="layoutType" className="agent_question">Layout Type </label>
      <select 
        name="layoutType" 
        id="layoutType" 
        value={layoutType} 
        onChange={onLayoutChange} 
        className="form-control-static"
        required
      >
        <option value="research">Research - Traditional research layout with detailed results</option>
        <option value="copilot">Copilot - Side-by-side research and chat interface</option>
      </select>
    </div>
  );
} 


================================================
FILE: frontend/nextjs/components/Settings/MCPSelector.tsx
================================================
import React, { useState, useEffect } from 'react';

interface MCPConfig {
  name: string;
  command: string;
  args: string[];
  env: Record<string, string>;
}

interface MCPSelectorProps {
  mcpEnabled: boolean;
  mcpConfigs: MCPConfig[];
  onMCPChange: (enabled: boolean, configs: MCPConfig[]) => void;
}

const MCPSelector: React.FC<MCPSelectorProps> = ({
  mcpEnabled,
  mcpConfigs,
  onMCPChange,
}) => {
  const [enabled, setEnabled] = useState(mcpEnabled);
  const [configText, setConfigText] = useState(() => {
    // Initialize with the passed configs, handling empty array case
    if (Array.isArray(mcpConfigs) && mcpConfigs.length > 0) {
      return JSON.stringify(mcpConfigs, null, 2);
    }
    return '[]';
  });
  const [validationStatus, setValidationStatus] = useState<{
    isValid: boolean;
    message: string;
    serverCount?: number;
  }>({ isValid: true, message: 'Valid JSON âœ“' });
  const [showInfoModal, setShowInfoModal] = useState(false);

  useEffect(() => {
    validateConfig(configText);
  }, [configText]);

  // Sync with props when they change (for localStorage loading)
  useEffect(() => {
    setEnabled(mcpEnabled);
  }, [mcpEnabled]);

  useEffect(() => {
    if (Array.isArray(mcpConfigs)) {
      const newConfigText = mcpConfigs.length > 0 ? JSON.stringify(mcpConfigs, null, 2) : '[]';
      setConfigText(newConfigText);
    }
  }, [mcpConfigs]);

  const validateConfig = (text: string) => {
    if (!text.trim() || text.trim() === '[]') {
      setValidationStatus({ isValid: true, message: 'Empty configuration' });
      return true;
    }

    try {
      const parsed = JSON.parse(text);

      if (!Array.isArray(parsed)) {
        throw new Error('Configuration must be an array');
      }

      const errors: string[] = [];
      parsed.forEach((server: any, index: number) => {
        if (!server.name) {
          errors.push(`Server ${index + 1}: missing name`);
        }
        if (!server.command && !server.connection_url) {
          errors.push(`Server ${index + 1}: missing command or connection_url`);
        }
      });

      if (errors.length > 0) {
        throw new Error(errors.join('; '));
      }

      setValidationStatus({
        isValid: true,
        message: `Valid JSON âœ“ (${parsed.length} server${parsed.length !== 1 ? 's' : ''})`,
        serverCount: parsed.length
      });
      return true;
    } catch (error: any) {
      setValidationStatus({
        isValid: false,
        message: `Invalid JSON: ${error.message}`
      });
      return false;
    }
  };

  const handleEnabledChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    const newEnabled = e.target.checked;
    console.log('ğŸ” DEBUG: MCP enabled changed to:', newEnabled);
    setEnabled(newEnabled);

    if (newEnabled && validationStatus.isValid) {
      try {
        const configs = JSON.parse(configText || '[]');
        console.log('ğŸ” DEBUG: Calling onMCPChange with configs:', configs);
        onMCPChange(newEnabled, configs);
      } catch {
        console.log('ğŸ” DEBUG: JSON parse failed, calling with empty array');
        onMCPChange(newEnabled, []);
      }
    } else {
      console.log('ğŸ” DEBUG: Disabled or invalid, calling with empty array');
      onMCPChange(newEnabled, []);
    }
  };

  const handleConfigChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const newText = e.target.value;
    console.log('ğŸ” DEBUG: Config text changed to:', newText);
    setConfigText(newText);

    if (enabled && validateConfig(newText)) {
      try {
        const configs = JSON.parse(newText || '[]');
        console.log('ğŸ” DEBUG: Parsed configs from textarea:', configs);
        console.log('ğŸ” DEBUG: Calling onMCPChange from textarea with:', { enabled, configs });
        onMCPChange(enabled, configs);
      } catch {
        console.log('ğŸ” DEBUG: JSON parse failed in textarea change');
        // Invalid JSON, don't update
      }
    }
  };

  const formatJSON = () => {
    try {
      const parsed = JSON.parse(configText || '[]');
      const formatted = JSON.stringify(parsed, null, 2);
      setConfigText(formatted);
    } catch {
      // Invalid JSON, don't format
    }
  };

  // Helper function to check if a preset is currently selected
  const isPresetSelected = (presetName: string): boolean => {
    try {
      const currentText = configText.trim();
      if (!currentText || currentText === '[]') return false;
      
      const parsed = JSON.parse(currentText);
      if (!Array.isArray(parsed)) return false;
      
      return parsed.some(server => server.name === presetName);
    } catch {
      return false;
    }
  };

  const togglePreset = (preset: string) => {
    console.log('ğŸ” DEBUG: togglePreset called with:', preset);
    console.log('ğŸ” DEBUG: Current configText:', configText);
    console.log('ğŸ” DEBUG: MCP enabled:', enabled);
    
    const presets: Record<string, MCPConfig> = {
      github: {
        name: 'github',
        command: 'npx',
        args: ['-y', '@modelcontextprotocol/server-github'],
        env: {
          GITHUB_PERSONAL_ACCESS_TOKEN: 'your_github_token_here'
        }
      },
      tavily: {
        name: 'tavily',
        command: 'npx',
        args: ['-y', 'tavily-mcp@0.1.2'],
        env: {
          TAVILY_API_KEY: 'your_tavily_api_key_here'
        }
      },
      filesystem: {
        name: 'filesystem',
        command: 'npx',
        args: ['-y', '@modelcontextprotocol/server-filesystem', '/path/to/allowed/directory'],
        env: {}
      }
    };

    const config = presets[preset];
    if (!config) {
      console.log('ğŸ” DEBUG: Preset config not found for:', preset);
      return;
    }

    try {
      let currentConfig: MCPConfig[] = [];
      const currentText = configText.trim();

      if (currentText && currentText !== '[]') {
        currentConfig = JSON.parse(currentText);
      }
      console.log('ğŸ” DEBUG: Current parsed config:', currentConfig);

      const existingIndex = currentConfig.findIndex(server => server.name === config.name);
      console.log('ğŸ” DEBUG: Existing index for', config.name, ':', existingIndex);

      if (existingIndex !== -1) {
        // Remove the preset if it exists (deselect)
        console.log('ğŸ” DEBUG: Removing preset');
        currentConfig.splice(existingIndex, 1);
      } else {
        // Add the preset if it doesn't exist (select)
        console.log('ğŸ” DEBUG: Adding preset');
        currentConfig.push(config);
      }

      const newText = JSON.stringify(currentConfig, null, 2);
      console.log('ğŸ” DEBUG: New config text:', newText);
      console.log('ğŸ” DEBUG: Final config array:', currentConfig);
      
      setConfigText(newText);
      
      // IMPORTANT: Also call onMCPChange immediately with the new config
      if (enabled) {
        console.log('ğŸ” DEBUG: Calling onMCPChange from togglePreset with:', { enabled, currentConfig });
        onMCPChange(enabled, currentConfig);
      }
      
    } catch (error) {
      console.error('ğŸ” DEBUG: Error toggling preset:', error);
    }
  };

  const showExample = () => {
    const exampleConfig = [
      {
        name: 'github',
        command: 'npx',
        args: ['-y', '@modelcontextprotocol/server-github'],
        env: {
          GITHUB_PERSONAL_ACCESS_TOKEN: 'your_github_token_here'
        }
      },
      {
        name: 'filesystem',
        command: 'npx',
        args: ['-y', '@modelcontextprotocol/server-filesystem', '/path/to/allowed/directory'],
        env: {}
      }
    ];

    setConfigText(JSON.stringify(exampleConfig, null, 2));
  };

  return (
    <div className="form-group">
      <div className="settings mcp-section">
        <div className="settings mcp-header">
          <label className="agent_question">
            <input
              type="checkbox"
              className="settings mcp-toggle"
              checked={enabled}
              onChange={handleEnabledChange}
            />
            Enable MCP (Model Context Protocol)
          </label>
          <button
            type="button"
            className="settings mcp-info-btn"
            onClick={() => setShowInfoModal(true)}
            title="Learn about MCP"
          >
            â„¹ï¸
          </button>
        </div>
        <small className="text-muted" style={{ color: 'rgba(255, 255, 255, 0.6)', fontSize: '0.85rem', marginBottom: '15px', display: 'block' }}>
          Connect to external tools and data sources through MCP servers
        </small>

        {enabled && (
          <div className="settings mcp-config-section">
            <div className="settings mcp-presets">
              <label className="agent_question" style={{ marginBottom: '10px' }}>Quick Presets</label>
              <div className="settings preset-buttons">
                <button
                  type="button"
                  className={`settings preset-btn ${isPresetSelected('github') ? 'selected' : ''}`}
                  onClick={() => togglePreset('github')}
                >
                  <i className="fab fa-github"></i> GitHub
                </button>
                <button
                  type="button"
                  className={`settings preset-btn ${isPresetSelected('tavily') ? 'selected' : ''}`}
                  onClick={() => togglePreset('tavily')}
                >
                  <i className="fas fa-search"></i> Tavily Web Search
                </button>
                <button
                  type="button"
                  className={`settings preset-btn ${isPresetSelected('filesystem') ? 'selected' : ''}`}
                  onClick={() => togglePreset('filesystem')}
                >
                  <i className="fas fa-folder"></i> Local Files
                </button>
              </div>
              <small className="text-muted" style={{ color: 'rgba(255, 255, 255, 0.6)', fontSize: '0.85rem', marginTop: '8px', display: 'block' }}>
                Click a preset to toggle MCP servers in the configuration below. Selected presets are highlighted.
              </small>
            </div>

            <div className="settings mcp-config-group">
              <label className="agent_question" style={{ marginBottom: '10px' }}>MCP Servers Configuration</label>
              <textarea
                className={`settings mcp-config-textarea ${validationStatus.isValid ? 'valid' : 'invalid'}`}
                rows={12}
                placeholder="Paste your MCP servers configuration as JSON array..."
                value={configText}
                onChange={handleConfigChange}
                style={{ minHeight: '300px' }}
              />
              <div className="settings mcp-config-status">
                <span className={`settings mcp-status-text ${validationStatus.isValid ? 'valid' : 'invalid'}`}>
                  {validationStatus.message}
                </span>
                <button
                  type="button"
                  className="settings mcp-format-btn"
                  onClick={formatJSON}
                >
                  <i className="fas fa-code"></i> Format JSON
                </button>
              </div>
              <small className="text-muted" style={{ color: 'rgba(255, 255, 255, 0.6)', fontSize: '0.85rem', marginTop: '8px', display: 'block', lineHeight: '1.4' }}>
                Paste your MCP servers configuration as a JSON array. Each server should have properties like{' '}
                <code style={{ backgroundColor: 'rgba(255, 255, 255, 0.1)', padding: '2px 4px', borderRadius: '3px', color: '#0d9488' }}>name</code>,{' '}
                <code style={{ backgroundColor: 'rgba(255, 255, 255, 0.1)', padding: '2px 4px', borderRadius: '3px', color: '#0d9488' }}>command</code>,{' '}
                <code style={{ backgroundColor: 'rgba(255, 255, 255, 0.1)', padding: '2px 4px', borderRadius: '3px', color: '#0d9488' }}>args</code>, and optional{' '}
                <code style={{ backgroundColor: 'rgba(255, 255, 255, 0.1)', padding: '2px 4px', borderRadius: '3px', color: '#0d9488' }}>env</code> variables.{' '}
                <a
                  href="#"
                  className="settings mcp-example-link"
                  onClick={(e) => { e.preventDefault(); showExample(); }}
                  style={{ color: '#0d9488', textDecoration: 'none', fontWeight: '500' }}
                >
                  See example â†’
                </a>
              </small>
            </div>
          </div>
        )}

        {/* MCP Info Modal */}
        {showInfoModal && (
          <div className="settings mcp-info-modal visible">
            <div className="settings mcp-info-content">
              <button
                className="settings mcp-info-close"
                onClick={() => setShowInfoModal(false)}
              >
                <i className="fas fa-times"></i>
              </button>
              <h3>Model Context Protocol (MCP)</h3>
              <p>MCP enables GPT Researcher to connect with external tools and data sources through a standardized protocol.</p>

              <h4 className="highlight">Benefits:</h4>
              <ul>
                <li><span className="highlight">Access Local Data:</span> Connect to databases, file systems, and APIs</li>
                <li><span className="highlight">Use External Tools:</span> Integrate with web services and third-party tools</li>
                <li><span className="highlight">Extend Capabilities:</span> Add custom functionality through MCP servers</li>
                <li><span className="highlight">Maintain Security:</span> Controlled access with proper authentication</li>
              </ul>

              <h4 className="highlight">Quick Start:</h4>
              <ul>
                <li>Enable MCP using the checkbox above</li>
                <li>Click a preset to add pre-configured servers to the JSON</li>
                <li>Or paste your own MCP configuration as a JSON array</li>
                <li>Start your research - MCP will run with optimal settings</li>
              </ul>

              <h4 className="highlight">Configuration Format:</h4>
              <p>Each MCP server should be a JSON object with these properties:</p>
              <ul>
                <li><span className="highlight">name:</span> Unique identifier (e.g., &quot;github&quot;, &quot;filesystem&quot;)</li>
                <li><span className="highlight">command:</span> Command to run the server (e.g., &quot;npx&quot;, &quot;python&quot;)</li>
                <li><span className="highlight">args:</span> Array of arguments (e.g., [&quot;-y&quot;, &quot;@modelcontextprotocol/server-github&quot;])</li>
                <li><span className="highlight">env:</span> Object with environment variables (e.g., {JSON.stringify({API_KEY: "your_key"})})</li>
              </ul>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

export default MCPSelector;



================================================
FILE: frontend/nextjs/components/Settings/Modal.tsx
================================================
import React, { useState, useEffect } from "react";
import './Settings.css';
import ChatBox from './ChatBox';
import { ChatBoxSettings } from '@/types/data';
import { createPortal } from 'react-dom';
import { motion, AnimatePresence } from "framer-motion";

interface ChatBoxProps {
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
}

interface Domain {
  value: string;
}

const Modal: React.FC<ChatBoxProps> = ({ chatBoxSettings, setChatBoxSettings }) => {
  const [showModal, setShowModal] = useState(false);
  const [activeTab, setActiveTab] = useState('report_settings');
  const [mounted, setMounted] = useState(false);
  
  const [apiVariables, setApiVariables] = useState({
    DOC_PATH: './my-docs',
  });

  // Mount the component
  useEffect(() => {
    setMounted(true);
    return () => setMounted(false);
  }, []);

  useEffect(() => {
    const storedConfig = localStorage.getItem('apiVariables');
    if (storedConfig) {
      setApiVariables(JSON.parse(storedConfig));
    }

    // Handle body scroll when modal is shown/hidden
    if (showModal) {
      document.body.style.overflow = 'hidden';
      const header = document.querySelector('.settings .App-header');
      if (header) {
        header.classList.remove('App-header');
      }
    } else {
      document.body.style.overflow = '';
    }
    
    // Cleanup function
    return () => {
      document.body.style.overflow = '';
    };
  }, [showModal]);

  const handleSaveChanges = () => {
    setChatBoxSettings({
      ...chatBoxSettings
    });
    // Save both apiVariables AND chatBoxSettings to localStorage
    localStorage.setItem('apiVariables', JSON.stringify(apiVariables));
    localStorage.setItem('chatBoxSettings', JSON.stringify(chatBoxSettings));
    setShowModal(false);
  };

  const handleInputChange = (e: { target: { name: any; value: any; }; }) => {
    const { name, value } = e.target;
    setApiVariables(prevState => ({
      ...prevState,
      [name]: value
    }));
    localStorage.setItem('apiVariables', JSON.stringify({
      ...apiVariables,
      [name]: value
    }));
  };

  // Animation variants
  const fadeIn = {
    hidden: { opacity: 0 },
    visible: { opacity: 1, transition: { duration: 0.3 } }
  };

  const slideUp = {
    hidden: { opacity: 0, y: 20 },
    visible: { opacity: 1, y: 0, transition: { duration: 0.3, ease: "easeOut" } }
  };

  // Create modal content
  const modalContent = showModal && (
    <AnimatePresence>
      <motion.div 
        key="modal-overlay"
        className="fixed inset-0 z-[1000] flex items-center justify-center overflow-auto" 
        initial="hidden"
        animate="visible"
        exit="hidden"
        variants={fadeIn}
        style={{ backdropFilter: 'blur(5px)' }}
        onClick={(e) => {
          // Close when clicking the backdrop, not the modal content
          if (e.target === e.currentTarget) setShowModal(false);
        }}
      >
        <motion.div 
          className="relative w-auto max-w-3xl z-[1001] mx-6 my-8 md:mx-auto"
          variants={slideUp}
        >
          <div className="relative">
            {/* Subtle border with hint of glow */}
            <div className="absolute -inset-0.5 bg-gradient-to-r from-teal-500/20 via-cyan-500/15 to-blue-500/20 rounded-xl blur-sm opacity-50 shadow-sm"></div>
            
            {/* Modal content */}
            <div className="relative flex flex-col rounded-lg overflow-hidden bg-gray-50 border border-gray-300/60 shadow-md hover:shadow-teal-400/10 transition-shadow duration-300">
              {/* Header with subtler accent */}
              <div className="bg-gray-50 p-5 border-b border-gray-300">
                <div className="flex items-center justify-between">
                  <h3 className="text-xl font-semibold text-gray-800	800	">
                    <span className="mr-2">âš™ï¸</span>
                    <span className="text-blue-600">Preferences</span>
                  </h3>
                  <button
                    className="p-1 ml-auto text-gray-500 hover:text-gray-800	800	 transition-colors duration-200"
                    onClick={() => setShowModal(false)}
                  >
                    <svg className="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M6 18L18 6M6 6l12 12"></path>
                    </svg>
                  </button>
                </div>
              </div>
              
              {/* Body with content */}
              <div className="relative p-6 flex-auto bg-gray-50/95 modal-content">
                {false && (<div className="tabs mb-4">
                  <button onClick={() => setActiveTab('report_settings')} className={`tab-button ${activeTab === 'report_settings' ? 'active' : ''}`}>Report Settings</button>
                </div>)}

                {activeTab === 'report_settings' && (
                  <div className="App">
                    <header className="App-header">
                      <ChatBox setChatBoxSettings={setChatBoxSettings} chatBoxSettings={chatBoxSettings} />
                    </header>
                  </div>
                )}
              </div>
              
              {/* Footer with actions */}
              <div className="flex items-center justify-end p-4 border-t border-gray-300 bg-gray-50/80">
                <button
                  className="mr-3 px-4 py-2 text-sm font-medium text-gray-700 hover:text-gray-800	800	 bg-gray-100 hover:bg-gray-700 rounded-md transition-colors duration-200"
                  onClick={() => setShowModal(false)}
                >
                  Cancel
                </button>
                <button
                  className="px-6 py-2.5 text-sm font-medium rounded-md text-gray-800	800	 bg-blue-500 hover:bg-gradient-to-br hover:from-teal-500/95 hover:via-cyan-500/90 hover:to-teal-600/95 shadow-sm hover:shadow-teal-400/20 transition-all duration-300 focus:outline-none focus:ring-1 focus:ring-teal-500 focus:ring-offset-1 focus:ring-offset-gray-900"
                  onClick={handleSaveChanges}
                >
                  Save Changes
                </button>
              </div>
            </div>
          </div>
        </motion.div>
      </motion.div>
      <motion.div 
        key="modal-background"
        className="fixed inset-0 z-[999] bg-black"
        initial={{ opacity: 0 }}
        animate={{ opacity: 0.6 }}
        exit={{ opacity: 0 }}
      ></motion.div>
    </AnimatePresence>
  );

  return (
    <div className="settings">
      <button
        className="bg-gray-50 text-gray-800	800	 px-6 py-3 rounded-lg shadow-sm hover:shadow-teal-400/10 transition-all duration-300 border border-gray-300 hover:border-teal-500/30"
        type="button"
        onClick={() => setShowModal(true)}
      >
        <span className="flex items-center">
          <svg xmlns="http://www.w3.org/2000/svg" className="h-5 w-5 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z" />
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M15 12a3 3 0 11-6 0 3 3 0 016 0z" />
          </svg>
          Preferences
        </span>
      </button>
      {mounted && showModal && createPortal(modalContent, document.body)}
    </div>
  );
};

export default Modal;


================================================
FILE: frontend/nextjs/components/Settings/Settings.css
================================================
@keyframes gradientBG {
  0% {
    background-position: 0% 50%;
  }
  50% {
    background-position: 100% 50%;
  }
  100% {
    background-position: 0% 50%;
  }
}

.tabs {
  display: flex;
  justify-content: space-around;
  margin-bottom: 1rem;
}

.tab-button {
  padding: 0.5rem 1rem;
  border: none;
  background: none;
  cursor: pointer;
  font-size: 1rem;
  transition: all 0.3s ease-in-out;
}

.tab-button:hover {
  opacity: 0.8;
}

.tab-button.active {
  background-image: linear-gradient(to right, #0cdbb6, #1fd0f0);
  color: white;
  border-radius: 5px;
}

.settings html {
  scroll-behavior: smooth;
}

.settings body {
  font-family: 'Montserrat', sans-serif;
  color: #fff;
  line-height: 1.6;
  background-size: 200% 200%;
  background-image: linear-gradient(45deg, #151A2D, #2D284D, #151A2D);
  animation: gradientBG 10s ease infinite;
}

.settings .landing {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 50vh; /* Adjusted height */
  text-align: center;
}

.settings .landing h1 {
  font-size: 3.5rem;
  font-weight: 700;
  margin-bottom: 2rem;
}

.settings .landing p {
  font-size: 1.5rem;
  font-weight: 400;
  max-width: 500px;
  margin: auto;
  margin-bottom: 2rem;
}

.settings .container {
  max-width: 900px;
  margin: auto;
  padding: 20px;
  background-color: rgba(255, 255, 255, 0.1);
  border-radius: 12px;
  box-shadow: 0px 10px 25px rgba(0, 0, 0, 0.1);
  transition: all .3s ease-in-out;
  max-height: 80vh; /* Fixed maximum height */
  overflow-y: auto; /* Enable scrolling if content overflows */
}

.settings .container:hover {
  transform: scale(1.01);
  box-shadow: 0px 15px 30px rgba(0, 0, 0, 0.2);
}

.settings .static-container {
  max-width: 900px;
  margin: auto;
  padding: 20px;
  background-color: rgba(255, 255, 255, 0.1);
  border-radius: 12px;
  max-height: 80vh; /* Fixed maximum height */
  overflow-y: auto; /* Enable scrolling if content overflows */
}

.settings input, 
.settings select, 
.settings #output, 
.settings #reportContainer {
  background-color: rgba(0, 0, 0, 0.5); /* Darker background color */
  border: none;
  color: #fff; /* White text color */
  transition: all .3s ease-in-out;
}

.settings input:hover, 
.settings input:focus, 
.settings select:hover, 
.settings select:focus {
  background-color: #333; /* Darker hover/focus background color */
  border: 1px solid rgba(255, 255, 255, 0.5);
  box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease-in-out;
}

.settings .btn-primary {
  background: linear-gradient(to right, #0062cc, #007bff);
  border: none;
  transition: all .3s ease-in-out;
}

.settings .btn-secondary {
  background: linear-gradient(to right, #6c757d, #6c757d);
  border: none;
  transition: all .3s ease-in-out;
}

.settings .btn:hover {
  opacity: 0.8;
  transform: scale(1.1);
  box-shadow: 0px 10px 20px rgba(0, 0, 0, 0.3);
}

.settings .agent_question {
  font-size: 1.2rem;
  font-weight: 600;
  margin-bottom: 0.2rem;
  color: white; /* Clean white color for the dark background */
  letter-spacing: 0.01em;
}

.settings footer {
  position: fixed;
  left: 0;
  bottom: 0;
  width: 100%;
  background: linear-gradient(to right, #151A2D, #111827);
  color: white;
  text-align: center;
  padding: 10px 0;
}

.settings .margin-div {
  margin-top: 20px;
  margin-bottom: 20px;
  padding: 10px;
}

.settings .agent_response {
  background-color: #747d8c;
  margin: 10px;
  padding: 10px;
  border-radius: 12px;
}

.settings #output {
  height: 150px; /* Adjusted height */
  font-family: 'Times New Roman', Times, "Courier New", serif;
  overflow: auto;
  padding: 10px;
  margin-bottom: 10px;
  margin-top: 10px;
}

.settings #reportContainer {
  background-color: rgba(255, 255, 255, 0.1);
  border: none;
  color: #fff;
  transition: all .3s ease-in-out;
  padding: 10px;
  border-radius: 12px;
}

/* refactoring inline css */
.settings .sayGoodbye {
  background-image: linear-gradient(to right, #9867F0, #ED4E50);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.settings .form-group {
  display: flex;
  align-items: center;
  width: 100%;
  margin-bottom: 1rem;
}

.settings .form-group label {
  flex: 1;
  margin-right: 1rem;
}

.settings .form-group input,
.settings .form-group select {
  flex: 2;
  width: 100%;
  padding: 0.5rem;
  border-radius: 0.375rem; /* Rounded corners */
  border: 1px solid rgba(255, 255, 255, 0.5);
  background-color: rgba(0, 0, 0, 0.5); /* Darker background color */
  color: #fff; /* White text color */
  transition: all 0.3s ease-in-out;
}

.settings .form-group input:hover,
.settings .form-group input:focus,
.settings .form-group select:hover,
.settings .form-group select:focus {
  background-color: #333; /* Darker hover/focus background color */
  border: 1px solid rgba(255, 255, 255, 0.5);
  box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.1);
  transition: all 0.3s ease-in-out;
}

.report_settings_static {
  width: 100%;
  padding: 10px;
  border-radius: 8px;
  background-color: rgba(255, 255, 255, 0.1);
}

.form-control-static {
  width: 100%;
  padding: 0.5rem;
  border-radius: 0.375rem;
  border: 1px solid rgba(255, 255, 255, 0.3);
  background-color: rgba(0, 0, 0, 0.4);
  color: #fff;
  appearance: none; /* Remove default arrow */
  background-image: url("data:image/svg+xml;charset=UTF-8,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='none' stroke='rgba(255, 255, 255, 0.5)' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3e%3cpolyline points='6 9 12 15 18 9'%3e%3c/polyline%3e%3c/svg%3e");
  background-repeat: no-repeat;
  background-position: right 0.7rem center;
  background-size: 1em;
  transition: all 0.2s ease-in-out;
}

.form-control-static:hover {
  border-color: rgba(20, 184, 166, 0.5);
  box-shadow: 0 0 0 2px rgba(20, 184, 166, 0.1);
}

.form-control-static:focus {
  outline: none;
  border-color: rgba(20, 184, 166, 0.8);
  box-shadow: 0 0 0 3px rgba(20, 184, 166, 0.2);
}

.form-control-static option {
  background-color: #1f2937;
  color: white;
  padding: 8px;
}

.input-static {
  flex: 1;
  border-radius: 0.375rem;
  border: 1px solid rgba(255, 255, 255, 0.3);
  padding: 0.5rem 0.75rem;
  font-size: 0.875rem;
  background-color: rgba(0, 0, 0, 0.4);
  color: #fff;
}

.button-static {
  display: inline-flex;
  justify-content: center;
  align-items: center;
  border-radius: 0.375rem;
  border: 1px solid transparent;
  background-color: #0d9488;
  padding: 0.5rem 1rem;
  font-size: 0.875rem;
  font-weight: 500;
  color: white;
}

.domain-tag-static {
  display: inline-flex;
  align-items: center;
  border-radius: 9999px;
  background-color: #ede9fe;
  padding: 0.25rem 0.75rem;
  font-size: 0.875rem;
  margin-bottom: 0.75rem;
  margin-right: 0.5rem;
}

.domain-text-static {
  color: #0d9488;
}

.domain-button-static {
  margin-left: 0.5rem;
  color: #9f7aea;
  background: none;
  border: none;
  padding: 0;
  display: flex;
  align-items: center;
}

/* Add this after the existing .settings .agent_question rule */

.modal-content .agent_question {
  color: white;
  font-weight: 600;
  letter-spacing: 0.015em;
  margin-bottom: 0.2rem;
  display: block; /* Force block level for better spacing in the modal */
  position: relative;
}

/* Removing the ::after pseudo-element that created the gradient underline */

/* Adjust form-group styling in modal context */
.modal-content .form-group {
  display: flex;
  flex-direction: column;
  align-items: flex-start;
  margin-bottom: 1.5rem;
}

.modal-content .form-group label {
  margin-bottom: 0.5rem;
  margin-right: 0;
  width: 100%;
}

.modal-content .form-group select,
.modal-content .form-group input {
  width: 100%;
}

/* More specific rule that excludes MCP section */
.modal-content .form-group:not(.mcp-section) input,
.modal-content .form-group:not(.mcp-section) select {
  width: 100%;
}

/* Reset the width for MCP section to allow natural layout - make this more specific */
.modal-content .form-group.mcp-section input,
.modal-content .form-group.mcp-section select,
.modal-content .form-group.mcp-section textarea {
  width: auto !important;
}

/* Ensure MCP header layout stays horizontal */
.modal-content .mcp-section .mcp-header {
  display: flex !important;
  align-items: center !important;
  gap: 12px !important;
  margin-bottom: 15px !important;
  justify-content: flex-start !important;
}

.modal-content .mcp-section .mcp-header label {
  display: flex !important;
  align-items: center !important;
  gap: 10px !important;
  margin: 0 !important;
  flex: 1 !important;
}

.modal-content .mcp-section .mcp-toggle {
  width: auto !important;
  margin: 0 !important;
}

/* MCP Section Styles for Next.js Settings */
.settings .mcp-section {
  margin-bottom: 1.5rem;
  background-color: rgba(0, 0, 0, 0.3);
  border-radius: 10px;
  padding: 20px;
  border: 1px solid rgba(255, 255, 255, 0.1);
  transition: all 0.3s ease;
}

.settings .mcp-section:hover {
  border-color: rgba(13, 148, 136, 0.3);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
}

.settings .mcp-header {
  display: flex;
  align-items: center;
  gap: 12px;
  margin-bottom: 15px;
  justify-content: flex-start;
}

.settings .mcp-header label {
  display: flex;
  align-items: center;
  gap: 10px;
  font-weight: 600;
  margin: 0;
  color: #fff;
  font-size: 1.1rem;
  cursor: pointer;
  transition: color 0.3s ease;
  flex: 1;
}

.settings .mcp-header label:hover {
  color: #0d9488;
}

.settings .mcp-toggle {
  margin: 0;
  accent-color: #0d9488;
  width: 18px;
  height: 18px;
  cursor: pointer;
  transform: scale(1.2);
}

.settings .mcp-info-btn {
  background: rgba(13, 148, 136, 0.2);
  border: 1px solid rgba(13, 148, 136, 0.3);
  color: #0d9488;
  cursor: pointer;
  padding: 6px 8px;
  border-radius: 6px;
  transition: all 0.3s ease;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 14px;
  min-width: 28px;
  height: 28px;
  flex-shrink: 0;
}

.settings .mcp-info-btn:hover {
  background: rgba(13, 148, 136, 0.3);
  border-color: #0d9488;
  color: #0cdbb6;
  transform: scale(1.05);
}

.settings .mcp-config-section {
  margin-top: 20px;
  padding: 20px;
  border: 1px solid rgba(13, 148, 136, 0.2);
  border-radius: 10px;
  background: linear-gradient(145deg, rgba(0, 0, 0, 0.2), rgba(13, 148, 136, 0.05));
  backdrop-filter: blur(5px);
}

.settings .mcp-presets {
  margin-bottom: 20px;
}

.settings .preset-buttons {
  display: flex;
  gap: 12px;
  flex-wrap: wrap;
  margin-bottom: 12px;
}

.settings .preset-btn {
  display: flex;
  align-items: center;
  gap: 8px;
  border: 1px solid rgba(13, 148, 136, 0.4);
  color: #fff;
  background: linear-gradient(145deg, rgba(13, 148, 136, 0.2), rgba(13, 148, 136, 0.1));
  padding: 12px 20px;
  border-radius: 8px;
  font-size: 0.9rem;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.3s ease;
  backdrop-filter: blur(5px);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.settings .preset-btn:hover {
  border-color: #0d9488;
  color: #0cdbb6;
  background: linear-gradient(145deg, rgba(13, 148, 136, 0.3), rgba(13, 148, 136, 0.2));
  transform: translateY(-2px);
  box-shadow: 0 4px 16px rgba(13, 148, 136, 0.2);
}

.settings .preset-btn:active {
  transform: translateY(0);
  box-shadow: 0 2px 8px rgba(13, 148, 136, 0.3);
}

.settings .preset-btn i {
  font-size: 1rem;
  color: #0d9488;
}

.settings .preset-btn:hover i {
  color: #0cdbb6;
}

.settings .preset-btn.selected {
  border-color: #0d9488;
  color: #0cdbb6;
  background: linear-gradient(145deg, rgba(13, 148, 136, 0.4), rgba(13, 148, 136, 0.3));
  box-shadow: 0 4px 16px rgba(13, 148, 136, 0.3);
  transform: translateY(-1px);
}

.settings .preset-btn.selected i {
  color: #0cdbb6;
}

.settings .mcp-config-group {
  margin-bottom: 15px;
}

.settings .mcp-config-textarea {
  font-family: 'Monaco', 'Consolas', 'Courier New', monospace;
  font-size: 14px;
  line-height: 1.5;
  resize: vertical;
  border: 2px solid rgba(255, 255, 255, 0.2);
  border-radius: 8px;
  padding: 16px;
  background: rgba(0, 0, 0, 0.6);
  color: #fff;
  width: 100%;
  min-height: 300px;
  max-height: 500px;
  transition: all 0.3s ease;
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.3);
}

.settings .mcp-config-textarea::placeholder {
  color: rgba(255, 255, 255, 0.5);
  font-style: italic;
}

.settings .mcp-config-textarea:focus {
  border-color: #0d9488;
  background: rgba(0, 0, 0, 0.7);
  box-shadow: 
    inset 0 2px 8px rgba(0, 0, 0, 0.3),
    0 0 0 3px rgba(13, 148, 136, 0.2);
  outline: none;
}

.settings .mcp-config-textarea.invalid {
  border-color: #dc3545;
  background: rgba(220, 53, 69, 0.1);
  box-shadow: 
    inset 0 2px 8px rgba(0, 0, 0, 0.3),
    0 0 12px rgba(220, 53, 69, 0.3);
}

.settings .mcp-config-textarea.valid {
  border-color: #28a745;
  background: rgba(40, 167, 69, 0.1);
  box-shadow: 
    inset 0 2px 8px rgba(0, 0, 0, 0.3),
    0 0 12px rgba(40, 167, 69, 0.3);
}

.settings .mcp-config-status {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-top: 12px;
  margin-bottom: 10px;
  padding: 0 4px;
}

.settings .mcp-status-text {
  font-size: 0.9rem;
  font-weight: 600;
  color: rgba(255, 255, 255, 0.7);
  display: flex;
  align-items: center;
  gap: 6px;
}

.settings .mcp-status-text::before {
  content: '';
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background-color: #6c757d;
}

.settings .mcp-status-text.valid {
  color: #28a745;
}

.settings .mcp-status-text.valid::before {
  background-color: #28a745;
  box-shadow: 0 0 8px rgba(40, 167, 69, 0.5);
}

.settings .mcp-status-text.invalid {
  color: #dc3545;
}

.settings .mcp-status-text.invalid::before {
  background-color: #dc3545;
  box-shadow: 0 0 8px rgba(220, 53, 69, 0.5);
}

.settings .mcp-format-btn {
  font-size: 0.85rem;
  padding: 8px 16px;
  background: linear-gradient(145deg, rgba(108, 117, 125, 0.8), rgba(108, 117, 125, 0.6));
  border: 1px solid rgba(255, 255, 255, 0.3);
  color: #fff;
  border-radius: 6px;
  cursor: pointer;
  transition: all 0.3s ease;
  font-weight: 500;
  display: flex;
  align-items: center;
  gap: 6px;
}

.settings .mcp-format-btn:hover {
  background: linear-gradient(145deg, rgba(108, 117, 125, 1), rgba(108, 117, 125, 0.8));
  border-color: rgba(255, 255, 255, 0.5);
  transform: translateY(-1px);
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
}

.settings .mcp-format-btn i {
  color: #0d9488;
}

.settings .mcp-example-link {
  color: #0d9488;
  text-decoration: none;
  font-size: 0.9rem;
  font-weight: 500;
  transition: all 0.3s ease;
  display: inline-flex;
  align-items: center;
  gap: 4px;
}

.settings .mcp-example-link:hover {
  text-decoration: underline;
  color: #0cdbb6;
  transform: translateX(2px);
}

/* MCP Info Modal for Next.js */
.settings .mcp-info-modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.8);
  backdrop-filter: blur(8px);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
  opacity: 0;
  visibility: hidden;
  transition: all 0.4s ease;
}

.settings .mcp-info-modal.visible {
  opacity: 1;
  visibility: visible;
}

.settings .mcp-info-content {
  background: linear-gradient(145deg, #1a1a2e, #16213e);
  padding: 30px;
  border-radius: 16px;
  max-width: 650px;
  max-height: 85vh;
  overflow-y: auto;
  position: relative;
  margin: 20px;
  border: 1px solid rgba(13, 148, 136, 0.3);
  box-shadow: 
    0 20px 60px rgba(0, 0, 0, 0.6),
    0 0 0 1px rgba(255, 255, 255, 0.1);
  backdrop-filter: blur(10px);
  animation: modalSlideIn 0.4s ease;
}

@keyframes modalSlideIn {
  from {
    opacity: 0;
    transform: scale(0.9) translateY(-20px);
  }
  to {
    opacity: 1;
    transform: scale(1) translateY(0);
  }
}

.settings .mcp-info-close {
  position: absolute;
  top: 20px;
  right: 20px;
  background: rgba(255, 255, 255, 0.1);
  border: 1px solid rgba(255, 255, 255, 0.2);
  font-size: 1.2rem;
  cursor: pointer;
  color: rgba(255, 255, 255, 0.7);
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  transition: all 0.3s ease;
}

.settings .mcp-info-close:hover {
  background: rgba(220, 53, 69, 0.2);
  border-color: #dc3545;
  color: #dc3545;
  transform: scale(1.1);
}

.settings .mcp-info-content h3 {
  margin-top: 0;
  margin-bottom: 20px;
  color: #fff;
  border-bottom: 2px solid #0d9488;
  padding-bottom: 12px;
  font-size: 1.4rem;
  font-weight: 600;
}

.settings .mcp-info-content h4 {
  color: #0cdbb6;
  margin-top: 25px;
  margin-bottom: 15px;
  font-size: 1.1rem;
  font-weight: 600;
}

.settings .mcp-info-content p {
  line-height: 1.7;
  color: rgba(255, 255, 255, 0.85);
  margin-bottom: 15px;
}

.settings .mcp-info-content ul {
  padding-left: 25px;
  line-height: 1.7;
  margin-bottom: 20px;
}

.settings .mcp-info-content li {
  margin-bottom: 10px;
  color: rgba(255, 255, 255, 0.85);
}

.settings .mcp-info-content .highlight {
  color: #0cdbb6;
  font-weight: 600;
}

.settings .mcp-info-content::-webkit-scrollbar {
  width: 8px;
}

.settings .mcp-info-content::-webkit-scrollbar-track {
  background: rgba(0, 0, 0, 0.2);
  border-radius: 4px;
}

.settings .mcp-info-content::-webkit-scrollbar-thumb {
  background: #0d9488;
  border-radius: 4px;
}

.settings .mcp-info-content::-webkit-scrollbar-thumb:hover {
  background: #0cdbb6;
}

/* Mobile responsive for MCP section */
@media (max-width: 768px) {
  .settings .preset-buttons {
    flex-direction: column;
    gap: 8px;
  }

  .settings .preset-btn {
    justify-content: center;
    padding: 14px 20px;
    font-size: 0.95rem;
  }

  .settings .mcp-info-content {
    margin: 15px;
    padding: 25px;
    max-height: 90vh;
  }

  .settings .mcp-config-status {
    flex-direction: column;
    align-items: flex-start;
    gap: 10px;
  }

  .settings .mcp-config-textarea {
    min-height: 250px;
    font-size: 13px;
  }

  .settings .mcp-section {
    padding: 15px;
  }

  .settings .mcp-config-section {
    padding: 15px;
  }
}

.settings .text-muted {
  color: rgba(255, 255, 255, 0.6) !important;
  font-size: 0.85rem;
}

/* Modal positioning and height management fixes */
.modal-container {
  position: fixed !important;
  top: 50% !important;
  left: 50% !important;
  transform: translate(-50%, -50%) !important;
  max-height: 90vh !important;
  overflow-y: auto !important;
  width: auto !important;
  max-width: 95vw !important;
  z-index: 1001 !important;
}

/* Ensure modal content is scrollable when expanded */
.modal-content {
  max-height: 85vh !important;
  overflow-y: auto !important;
  position: relative !important;
}

/* Add scrollbar styling for modal content */
.modal-content::-webkit-scrollbar {
  width: 8px;
}

.modal-content::-webkit-scrollbar-track {
  background: rgba(0, 0, 0, 0.2);
  border-radius: 4px;
}

.modal-content::-webkit-scrollbar-thumb {
  background: #0d9488;
  border-radius: 4px;
}

.modal-content::-webkit-scrollbar-thumb:hover {
  background: #0cdbb6;
}

/* Ensure modal backdrop doesn't interfere with positioning */
.modal-backdrop {
  position: fixed !important;
  top: 0 !important;
  left: 0 !important;
  width: 100vw !important;
  height: 100vh !important;
  z-index: 999 !important;
}

/* For framer-motion modals, ensure proper centering */
[class*="fixed"][class*="inset-0"] {
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  padding: 20px !important;
  box-sizing: border-box !important;
}

/* Specific positioning for the settings modal */
.settings .fixed.inset-0 {
  align-items: center !important;
  justify-content: center !important;
  padding: 20px !important;
}

/* Ensure modal doesn't get too tall */
.settings .relative.w-auto {
  max-height: 90vh !important;
  overflow-y: auto !important;
  display: flex !important;
  flex-direction: column !important;
}

/* Make sure modal content scrolls instead of expanding beyond viewport */
@media (max-height: 800px) {
  .modal-container {
    max-height: 85vh !important;
  }

  .modal-content {
    max-height: 75vh !important;
  }

  .settings .mcp-config-textarea {
    min-height: 200px !important;
    max-height: 250px !important;
  }
}

/* For very small screens */
@media (max-height: 600px) {
  .modal-container {
    max-height: 95vh !important;
    top: 2.5vh !important;
    transform: translateX(-50%) !important;
  }

  .modal-content {
    max-height: 90vh !important;
  }

  .settings .mcp-config-textarea {
    min-height: 150px !important;
    max-height: 200px !important;
  }
}

/* Framer Motion Modal specific positioning fixes */
.settings [class*="fixed"][class*="inset-0"] {
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  padding: 20px !important;
  box-sizing: border-box !important;
  overflow-y: auto !important;
}

/* Ensure the modal content wrapper has proper constraints */
.settings [class*="relative"][class*="w-auto"] {
  max-height: 90vh !important;
  width: auto !important;
  max-width: min(95vw, 768px) !important;
  display: flex !important;
  flex-direction: column !important;
}

/* Modal content scrollable area */
.settings .modal-content {
  max-height: 70vh !important;
  overflow-y: auto !important;
  flex: 1 !important;
  min-height: 0 !important;
}

/* Prevent the modal from growing too tall */
.settings .bg-gray-50\/95 {
  max-height: 70vh !important;
  overflow-y: auto !important;
}

/* Ensure MCP textarea doesn't make modal too tall */
.settings .modal-content .mcp-config-textarea {
  max-height: 300px !important;
  overflow-y: auto !important;
}


================================================
FILE: frontend/nextjs/components/Settings/ToneSelector.tsx
================================================
import React, { ChangeEvent } from 'react';

interface ToneSelectorProps {
  tone: string;
  onToneChange: (event: ChangeEvent<HTMLSelectElement>) => void;
}
export default function ToneSelector({ tone, onToneChange }: ToneSelectorProps) {
  return (
    <div className="form-group">
      <label htmlFor="tone" className="agent_question">Tone </label>
      <select 
        name="tone" 
        id="tone" 
        value={tone} 
        onChange={onToneChange} 
        className="form-control-static"
        required
      >
        <option value="Objective">Objective - Impartial and unbiased presentation of facts and findings</option>
        <option value="Formal">Formal - Adheres to academic standards with sophisticated language and structure</option>
        <option value="Analytical">Analytical - Critical evaluation and detailed examination of data and theories</option>
        <option value="Persuasive">Persuasive - Convincing the audience of a particular viewpoint or argument</option>
        <option value="Informative">Informative - Providing clear and comprehensive information on a topic</option>
        <option value="Explanatory">Explanatory - Clarifying complex concepts and processes</option>
        <option value="Descriptive">Descriptive - Detailed depiction of phenomena, experiments, or case studies</option>
        <option value="Critical">Critical - Judging the validity and relevance of the research and its conclusions</option>
        <option value="Comparative">Comparative - Juxtaposing different theories, data, or methods to highlight differences and similarities</option>
        <option value="Speculative">Speculative - Exploring hypotheses and potential implications or future research directions</option>
        <option value="Reflective">Reflective - Considering the research process and personal insights or experiences</option>
        <option value="Narrative">Narrative - Telling a story to illustrate research findings or methodologies</option>
        <option value="Humorous">Humorous - Light-hearted and engaging, usually to make the content more relatable</option>
        <option value="Optimistic">Optimistic - Highlighting positive findings and potential benefits</option>
        <option value="Pessimistic">Pessimistic - Focusing on limitations, challenges, or negative outcomes</option>
        <option value="Simple">Simple - Written for young readers, using basic vocabulary and clear explanations</option>
        <option value="Casual">Casual - Conversational and relaxed style for easy, everyday reading</option>
      </select>
    </div>
  );
}


================================================
FILE: frontend/nextjs/components/Task/Accordion.tsx
================================================
// Accordion.tsx
import { useState, useEffect } from 'react';
import { addTargetBlankToLinks } from '../../helpers/markdownHelper';
import '../../styles/markdown.css'; // Import global markdown styles

type ProcessedData = {
  field: string;
  isMarkdown: boolean;
  htmlContent: string | object;
};

type Log = {
  header: string;
  text: string;
  processedData?: ProcessedData[];
};

interface AccordionProps {
  logs: Log[];
}

const plainTextFields = ['task', 'sections', 'headers', 'sources', 'research_data'];

const Accordion: React.FC<AccordionProps> = ({ logs }) => {
  // State to store processed HTML content
  const [processedLogs, setProcessedLogs] = useState<Log[]>(logs);

  useEffect(() => {
    // Process any markdown HTML content to add target="_blank" to links
    if (logs && logs.length > 0) {
      const newLogs = logs.map(log => {
        if (log.processedData) {
          const newProcessedData = log.processedData.map(data => {
            if (data.isMarkdown && typeof data.htmlContent === 'string') {
              return {
                ...data,
                htmlContent: addTargetBlankToLinks(data.htmlContent)
              };
            }
            return data;
          });
          return { ...log, processedData: newProcessedData };
        }
        return log;
      });
      setProcessedLogs(newLogs);
    }
  }, [logs]);

  const getLogHeaderText = (log: Log): string => {
    const regex = /ğŸ“ƒ Source: (https?:\/\/[^\s]+)/;
    const match = log.text.match(regex);
    let sourceUrl = '';

    if (match) {
      sourceUrl = match[1];
    }

    return log.header === 'differences'
      ? 'The following fields on the Langgraph were updated: ' + Object.keys(JSON.parse(log.text).data).join(', ')
      : `ğŸ“„ Retrieved relevant content from the source: ${sourceUrl}`;
  };

  const renderLogContent = (log: Log) => {
    if (log.header === 'differences' && log.processedData) {
      return log.processedData.map((data, index) => (
        <div key={index} className="mb-4">
          <h3 className="font-semibold text-lg text-body-color dark:text-dark-6">{data.field}:</h3>
          {data.isMarkdown ? (
            <div className="markdown-content" dangerouslySetInnerHTML={{ __html: data.htmlContent as string }} />
          ) : (
            <p className="text-body-color dark:text-dark-6">
              {typeof data.htmlContent === 'object' ? JSON.stringify(data.htmlContent) : data.htmlContent}
            </p>
          )}
        </div>
      ));
    } else {
      return <p className="mb-2 text-body-color dark:text-dark-6">{log.text}</p>;
    }
  };

  const [openIndex, setOpenIndex] = useState<number | null>(null);

  const handleToggle = (index: number) => {
    setOpenIndex(openIndex === index ? null : index);
  };

  return (
    <div id="accordion-collapse" data-accordion="collapse" className="mt-4 bg-gray-50 rounded-lg">
      {processedLogs.map((log, index) => (
        <div key={index}>
          <h2 id={`accordion-collapse-heading-${index}`}>
            <button
              type="button"
              className="flex items-center w-full p-5 font-medium rtl:text-right text-gray-800	800	 rounded-t-xl gap-3"
              onClick={() => handleToggle(index)}
              aria-expanded={openIndex === index}
              aria-controls={`accordion-collapse-body-${index}`}
            >
              <span className="flex-grow text-left">{getLogHeaderText(log)}</span>
              <svg
                data-accordion-icon
                className={`w-3 h-3 ${openIndex === index ? 'rotate-180' : ''} shrink-0`}
                aria-hidden="true"
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 10 6"
              >
                <path
                  stroke="currentColor"
                  strokeLinecap="round"
                  strokeLinejoin="round"
                  strokeWidth="2"
                  d="M9 5 5 1 1 5"
                />
              </svg>
            </button>
          </h2>
          <div
            id={`accordion-collapse-body-${index}`}
            className={`${openIndex === index ? '' : 'hidden'}`}
            aria-labelledby={`accordion-collapse-heading-${index}`}
          >
            <div className="p-5 border border-b-0 border-gray-900 dark:border-gray-900 dark:bg-gray-50 text-gray-800	800	">
              {renderLogContent(log)}
            </div>
          </div>
        </div>
      ))}
    </div>
  );
};

export default Accordion;


================================================
FILE: frontend/nextjs/components/Task/AgentLogs.tsx
================================================
export default function AgentLogs({agentLogs}:any){  
  const renderAgentLogs = (agentLogs:any)=>{
    return agentLogs && agentLogs.map((agentLog:any, index:number)=>{
      return (<div key={index}>{agentLog.output}</div>)
    })
  }

  return (
    <div className="margin-div">
        <h2>Agent Output</h2>
        <div id="output">
          {renderAgentLogs(agentLogs)}
        </div>
    </div>
  );
}


================================================
FILE: frontend/nextjs/components/Task/Report.tsx
================================================
import React, { useEffect, useState } from 'react';
import { markdownToHtml } from '../../helpers/markdownHelper';
import '../../styles/markdown.css';

export default function Report({report}:any) {
    const [htmlContent, setHtmlContent] = useState('');

    useEffect(() => {
        const convertMarkdownToHtml = async () => {
            try {
                const processedHtml = await markdownToHtml(report);
                setHtmlContent(processedHtml);
            } catch (error) {
                console.error('Error converting markdown to HTML:', error);
                setHtmlContent('<p>Error rendering content</p>');
            }
        };

        if (report) {
            convertMarkdownToHtml();
        }
    }, [report]);

    return (
        <div>
            <h2>Research Report</h2>
            <div id="reportContainer" className="markdown-content">
                <div dangerouslySetInnerHTML={{ __html: htmlContent }} />
            </div>
        </div>
    );
};


================================================
FILE: frontend/nextjs/components/Task/ResearchForm.tsx
================================================
import React, { useState, useEffect } from "react";
import FileUpload from "../Settings/FileUpload";
import ToneSelector from "../Settings/ToneSelector";
import MCPSelector from "../Settings/MCPSelector";
import LayoutSelector from "../Settings/LayoutSelector";
import { useAnalytics } from "../../hooks/useAnalytics";
import { ChatBoxSettings, Domain, MCPConfig } from '@/types/data';

interface ResearchFormProps {
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
  onFormSubmit?: (
    task: string,
    reportType: string,
    reportSource: string,
    domains: Domain[]
  ) => void;
}

export default function ResearchForm({
  chatBoxSettings,
  setChatBoxSettings,
  onFormSubmit,
}: ResearchFormProps) {
  const { trackResearchQuery } = useAnalytics();
  const [task, setTask] = useState("");
  const [newDomain, setNewDomain] = useState('');

  // Destructure necessary fields from chatBoxSettings
  let { report_type, report_source, tone, layoutType } = chatBoxSettings;

  const [domains, setDomains] = useState<Domain[]>(() => {
    if (typeof window !== 'undefined') {
      const saved = localStorage.getItem('domainFilters');
      return saved ? JSON.parse(saved) : [];
    }
    return [];
  });
  
  useEffect(() => {
    localStorage.setItem('domainFilters', JSON.stringify(domains));
    setChatBoxSettings(prev => ({
      ...prev,
      domains: domains.map(domain => domain.value)
    }));
  }, [domains, setChatBoxSettings]);

  const handleAddDomain = (e: React.FormEvent) => {
    e.preventDefault();
    if (newDomain.trim()) {
      setDomains([...domains, { value: newDomain.trim() }]);
      setNewDomain('');
    }
  };

  const handleRemoveDomain = (domainToRemove: string) => {
    setDomains(domains.filter(domain => domain.value !== domainToRemove));
  };

  const onFormChange = (e: { target: { name: any; value: any } }) => {
    const { name, value } = e.target;
    setChatBoxSettings((prevSettings: any) => ({
      ...prevSettings,
      [name]: value,
    }));
  };

  const onToneChange = (e: { target: { value: any } }) => {
    const { value } = e.target;
    setChatBoxSettings((prevSettings: any) => ({
      ...prevSettings,
      tone: value,
    }));
  };

  const onLayoutChange = (e: { target: { value: any } }) => {
    const { value } = e.target;
    setChatBoxSettings((prevSettings: any) => ({
      ...prevSettings,
      layoutType: value,
    }));
  };

  const onMCPChange = (enabled: boolean, configs: MCPConfig[]) => {
    setChatBoxSettings((prevSettings: any) => ({
      ...prevSettings,
      mcp_enabled: enabled,
      mcp_configs: configs,
    }));
  };

  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    if (onFormSubmit) {
      const updatedSettings = {
        ...chatBoxSettings,
        domains: domains.map(domain => domain.value)
      };
      setChatBoxSettings(updatedSettings);
      onFormSubmit(task, report_type, report_source, domains);
    }
  };

  return (
    <form
      method="POST"
      className="report_settings_static mt-3"
      onSubmit={handleSubmit}
    >
      <div className="form-group">
        <label htmlFor="report_type" className="agent_question">
          Report Type{" "}
        </label>
        <select
          name="report_type"
          value={report_type}
          onChange={onFormChange}
          className="form-control-static"
          required
        >
          <option value="research_report">
            Summary - Short and fast (~2 min)
          </option>
          <option value="deep">Deep Research Report</option>
          <option value="multi_agents">Multi Agents Report</option>
          <option value="detailed_report">
            Detailed - In depth and longer (~5 min)
          </option>
        </select>
      </div>

      <div className="form-group">
        <label htmlFor="report_source" className="agent_question">
          Report Source{" "}
        </label>
        <select
          name="report_source"
          value={report_source}
          onChange={onFormChange}
          className="form-control-static"
          required
        >
          <option value="web">The Internet</option>
          <option value="local">My Documents</option>
          <option value="hybrid">Hybrid</option>
        </select>
      </div>

      

      {report_source === "local" || report_source === "hybrid" ? (
        <FileUpload />
      ) : null}
      
      <ToneSelector tone={tone} onToneChange={onToneChange} />

      <MCPSelector 
        mcpEnabled={chatBoxSettings.mcp_enabled || false}
        mcpConfigs={chatBoxSettings.mcp_configs || []}
        onMCPChange={onMCPChange}
      />
      
      <LayoutSelector layoutType={layoutType || 'copilot'} onLayoutChange={onLayoutChange} />

      {/** TODO: move the below to its own component */}
      {(chatBoxSettings.report_source === "web" || chatBoxSettings.report_source === "hybrid") && (
        <div className="mt-4 domain_filters">
          <div className="flex gap-2 mb-4">
          <label htmlFor="domain_filters" className="agent_question">
          Filter by domain{" "}
        </label>
            <input
              type="text"
              value={newDomain}
              onChange={(e) => setNewDomain(e.target.value)}
              placeholder="Filter by domain (e.g., techcrunch.com)"
              className="input-static"
              onKeyPress={(e) => {
                if (e.key === 'Enter') {
                  e.preventDefault();
                  handleAddDomain(e);
                }
              }}
            />
            <button
              type="button"
              onClick={handleAddDomain}
              className="button-static"
            >
              Add Domain
            </button>
          </div>

          <div className="flex flex-wrap gap-2">
            {domains.map((domain, index) => (
              <div
                key={index}
                className="domain-tag-static"
              >
                <span className="domain-text-static">{domain.value}</span>
                <button
                  type="button"
                  onClick={() => handleRemoveDomain(domain.value)}
                  className="domain-button-static"
                >
                  X
                </button>
              </div>
            ))}
          </div>
        </div>
      )}
    </form>
  );
}



================================================
FILE: frontend/nextjs/config/task.ts
================================================
export const task = {
  "task": {
    "query": "Is AI in a hype cycle?",
    "include_human_feedback": false,
    "model": "gpt-4o",
    "max_sections": 3,
    "publish_formats": {
      "markdown": true,
      "pdf": true,
      "docx": true
    },
    "source": "web",
    "follow_guidelines": true,
    "guidelines": [
      "The report MUST fully answer the original question",
      "The report MUST be written in apa format",
      "The report MUST be written in english"
    ],
    "verbose": true
  },
  "initial_research": "Initial research data here",
  "sections": ["Section 1", "Section 2"],
  "research_data": "Research data here",
  "title": "Research Title",
  "headers": {
    "introduction": "Introduction header",
    "table_of_contents": "Table of Contents header",
    "conclusion": "Conclusion header",
    "sources": "Sources header"
  },
  "date": "2023-10-01",
  "table_of_contents": "- Introduction\n- Section 1\n- Section 2\n- Conclusion",
  "introduction": "Introduction content here",
  "conclusion": "Conclusion content here",
  "sources": ["Source 1", "Source 2"],
  "report": "Full report content here"
}


================================================
FILE: frontend/nextjs/helpers/findDifferences.ts
================================================
type Value = string | number | boolean | null | undefined | object | Value[]; // Possible value types
type Changes = { [key: string]: { before: Value; after: Value } | Changes }; // Recursive changes type

function findDifferences<T extends Record<string, any>>(obj1: T, obj2: T): Changes {
    // Helper function to check if a value is an object (excluding arrays)
    function isObject(obj: any): obj is Record<string, any> {
        return obj && typeof obj === 'object' && !Array.isArray(obj);
    }

    // Recursive function to compare two objects and return the differences
    function compareObjects(o1: Record<string, any>, o2: Record<string, any>): Changes {
        const changes: Changes = {};

        // Iterate over keys in the first object (o1)
        for (const key in o1) {
            if (isObject(o1[key]) && isObject(o2[key])) {
                // Recursively compare nested objects
                const nestedChanges = compareObjects(o1[key], o2[key]);
                if (Object.keys(nestedChanges).length > 0) {
                    changes[key] = nestedChanges; // Add nested changes if any
                }
            } else if (Array.isArray(o1[key]) && Array.isArray(o2[key])) {
                // Compare arrays
                if (o1[key].length !== o2[key].length || o1[key].some((val, index) => val !== o2[key][index])) {
                    changes[key] = { before: o1[key], after: o2[key] };
                }
            } else {
                // Compare primitive values (or any non-object, non-array values)
                if (o1[key] !== o2[key]) {
                    changes[key] = { before: o1[key], after: o2[key] };
                }
            }
        }

        // Iterate over keys in the second object (o2) to detect new keys
        for (const key in o2) {
            if (!(key in o1)) {
                changes[key] = { before: undefined, after: o2[key] };
            }
        }

        return changes; // Return the collected changes
    }

    return compareObjects(obj1, obj2); // Compare the two input objects
}

export default findDifferences;


================================================
FILE: frontend/nextjs/helpers/getHost.ts
================================================
interface GetHostParams {
  purpose?: string;
}

export const getHost = ({ purpose }: GetHostParams = {}): string => {
  if (typeof window !== 'undefined') {
    let { host } = window.location;
    const apiUrlInLocalStorage = localStorage.getItem("GPTR_API_URL");
    
    const urlParams = new URLSearchParams(window.location.search);
    const apiUrlInUrlParams = urlParams.get("GPTR_API_URL");
    
    if (apiUrlInLocalStorage) {
      return apiUrlInLocalStorage;
    } else if (apiUrlInUrlParams) {
      return apiUrlInUrlParams;
    } else if (process.env.NEXT_PUBLIC_GPTR_API_URL) {
      return process.env.NEXT_PUBLIC_GPTR_API_URL;
    } else if (process.env.REACT_APP_GPTR_API_URL) {
      return process.env.REACT_APP_GPTR_API_URL;
    } else if (purpose === 'langgraph-gui') {
      return host.includes('localhost') ? 'http%3A%2F%2F127.0.0.1%3A8123' : `https://${host}`;
    } else {
      return host.includes('localhost') ? 'http://localhost:8000' : `https://${host}`;
    }
  }
  return '';
};


================================================
FILE: frontend/nextjs/helpers/markdownHelper.ts
================================================
import { remark } from 'remark';
import html from 'remark-html';
import remarkGfm from 'remark-gfm';
import { Compatible } from "vfile";

/**
 * Adds target="_blank" and rel="noopener noreferrer" attributes to all links in HTML content
 * @param htmlContent - The HTML content containing links
 * @returns The processed HTML with target="_blank" added to all links
 */
export const addTargetBlankToLinks = (htmlContent: string): string => {
  return htmlContent.replace(
    /<a(.*?)href="(.*?)"(.*?)>/gi,
    '<a$1href="$2"$3 target="_blank" rel="noopener noreferrer">'
  );
};

/**
 * Fixes the list item paragraph issue in HTML content
 * This specifically addresses the problem where numbered list items with bold text
 * have an extra line break between the marker and content
 * @param htmlContent - The HTML content with possible list formatting issues
 * @returns The processed HTML with fixed list formatting
 */
export const fixListItemParagraphIssue = (htmlContent: string): string => {
  // This regex looks for list items with a paragraph immediately inside
  // and removes the paragraph tags while preserving the content
  return htmlContent.replace(
    /<li>\s*<p>([\s\S]*?)<\/p>/g,
    '<li>$1'
  );
};

/**
 * Converts markdown to HTML with GitHub Flavored Markdown support and adds target="_blank" to links
 * @param markdown - The markdown content to convert
 * @returns Promise with the HTML content
 */
export const markdownToHtml = async (markdown: Compatible | string): Promise<string> => {
  try {
    const result = await remark()
      .use(remarkGfm) // Add GitHub Flavored Markdown support (tables, strikethrough, etc.)
      .use(html, { sanitize: false })
      .process(markdown);
    
    // Get the HTML string
    let htmlString = result.toString();
    
    // Apply fixes
    htmlString = fixListItemParagraphIssue(htmlString);
    htmlString = addTargetBlankToLinks(htmlString);
    
    return htmlString;
  } catch (error) {
    console.error('Error converting Markdown to HTML:', error);
    return ''; // Handle error gracefully, return empty string or default content
  }
}; 


================================================
FILE: frontend/nextjs/hooks/ResearchHistoryContext.tsx
================================================
"use client";

import React, { createContext, useContext, ReactNode } from 'react';
import { useResearchHistory } from './useResearchHistory';
import { ResearchHistoryItem, Data, ChatMessage } from '../types/data';

// Define the shape of our context
interface ResearchHistoryContextType {
  history: ResearchHistoryItem[];
  loading: boolean;
  saveResearch: (question: string, answer: string, orderedData: Data[]) => Promise<string>;
  updateResearch: (id: string, answer: string, orderedData: Data[]) => Promise<boolean>;
  getResearchById: (id: string) => Promise<ResearchHistoryItem | null>;
  deleteResearch: (id: string) => Promise<boolean>;
  addChatMessage: (id: string, message: ChatMessage) => Promise<boolean>;
  getChatMessages: (id: string) => ChatMessage[];
  clearHistory: () => Promise<boolean>;
}

// Create the context with a default undefined value
const ResearchHistoryContext = createContext<ResearchHistoryContextType | undefined>(undefined);

// Provider component
export const ResearchHistoryProvider = ({ children }: { children: ReactNode }) => {
  // Use the hook only once here
  const researchHistory = useResearchHistory();
  
  return (
    <ResearchHistoryContext.Provider value={researchHistory}>
      {children}
    </ResearchHistoryContext.Provider>
  );
};

// Custom hook for consuming the context
export const useResearchHistoryContext = () => {
  const context = useContext(ResearchHistoryContext);
  
  if (context === undefined) {
    throw new Error('useResearchHistoryContext must be used within a ResearchHistoryProvider');
  }
  
  return context;
}; 


================================================
FILE: frontend/nextjs/hooks/useAnalytics.ts
================================================
import ReactGA from 'react-ga4';

interface ResearchData {
  query: string;
  report_type: string;
  report_source: string;
}

interface TrackResearchData {
  query: string;
  report_type: string;
  report_source: string;
}

export const useAnalytics = () => {
  const initGA = () => {
    if (typeof window !== 'undefined' && process.env.NEXT_PUBLIC_GA_MEASUREMENT_ID) {
      ReactGA.initialize(process.env.NEXT_PUBLIC_GA_MEASUREMENT_ID);
    }
  };

  const trackResearchQuery = (data: TrackResearchData) => {
    ReactGA.event({
      category: 'Research',
      action: 'Submit Query',
      label: JSON.stringify({
        query: data.query,
        report_type: data.report_type,
        report_source: data.report_source
      })
    });
  };

  return {
    initGA,
    trackResearchQuery
  };
};


================================================
FILE: frontend/nextjs/hooks/useResearchHistory.ts
================================================
import { useState, useEffect, useRef } from 'react';
import { toast } from 'react-hot-toast';
import { v4 as uuidv4 } from 'uuid';
import { ResearchHistoryItem, Data, ChatMessage } from '../types/data';

export const useResearchHistory = () => {
  const [history, setHistory] = useState<ResearchHistoryItem[]>([]);
  const [loading, setLoading] = useState<boolean>(true);
  const dataLoadedRef = useRef(false); // Track if data has been loaded
  
  // Fetch all research history on mount
  useEffect(() => {
    // Skip if data is already loaded to prevent excessive API calls
    if (dataLoadedRef.current) {
      return;
    }

    const fetchHistory = async () => {
      try {
        console.log('Fetching research history from server...');
        // First, load data from localStorage for immediate display
        const localHistory = loadFromLocalStorage();
        
        // Set local history immediately to show something to user
        if (localHistory && localHistory.length > 0) {
          setHistory(localHistory);
        }
        
        // Then try to fetch from server, but only for items we have locally
        if (localHistory && localHistory.length > 0) {
          // Extract IDs from local history to filter server results
          const localIds = localHistory.map((item: ResearchHistoryItem) => item.id).join(',');
          console.log(`Sending ${localHistory.length} local IDs to server for filtering`);
          
          const response = await fetch(`/api/reports?report_ids=${localIds}`);
          if (response.ok) {
            const data = await response.json();
            
            // Check if the response has the expected structure
            if (data.reports && Array.isArray(data.reports)) {
              console.log('Loaded research history from server:', data.reports.length, 'items');
              
              // Merge local and server history
              await syncLocalHistoryWithServer(localHistory, data.reports);
            } else {
              console.warn('Server response did not contain reports array', data);
              // Keep using the local history we already loaded
            }
          } else {
            console.warn('Failed to load history from server, status:', response.status);
            // We're already using local history from above
          }
        } else {
          console.log('No local history found, skipping server fetch');
        }
      } catch (error) {
        console.error('Error fetching research history:', error);
        // We're already using local history from above
      } finally {
        dataLoadedRef.current = true; // Mark data as loaded
        setLoading(false);
      }
    };
    
    // Helper to load from localStorage
    const loadFromLocalStorage = () => {
      const localHistoryStr = localStorage.getItem('researchHistory');
      if (localHistoryStr) {
        try {
          const parsedHistory = JSON.parse(localHistoryStr);
          if (Array.isArray(parsedHistory)) {
            console.log('Loaded research history from localStorage:', parsedHistory.length, 'items');
            return parsedHistory;
          } else {
            console.warn('localStorage history is not an array');
            return [];
          }
        } catch (error) {
          console.error('Error parsing localStorage history:', error);
          return [];
        }
      } else {
        return [];
      }
    };
    
    // Helper to sync local history with server
    const syncLocalHistoryWithServer = async (localHistory: ResearchHistoryItem[], serverHistory: ResearchHistoryItem[]) => {
      console.log('Syncing local history with server...');
      
      // Create a map of server history IDs for quick lookup
      const serverIds = new Set(serverHistory.map(item => item.id));
      
      // Find local reports that aren't on the server
      const localOnlyReports = localHistory.filter(item => !serverIds.has(item.id));
      console.log('Found local-only reports:', localOnlyReports.length);
      
      // Upload local-only reports to server
      for (const report of localOnlyReports) {
        try {
          // Skip reports without questions or answers
          if (!report.question || !report.answer) continue;
          
          console.log(`Uploading local report to server: ${report.id}`);
          
          const response = await fetch('/api/reports', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              id: report.id,
              question: report.question,
              answer: report.answer,
              orderedData: report.orderedData || [],
              chatMessages: report.chatMessages || []
            }),
          });
          
          if (!response.ok) {
            console.warn(`Failed to upload local report ${report.id} to server:`, response.status);
          }
        } catch (error) {
          console.error(`Error uploading local report ${report.id} to server:`, error);
        }
      }
      
      // Create a unified history with server data prioritized
      const combinedHistory = [...serverHistory];
      
      // Add local-only reports to the combined history
      for (const report of localOnlyReports) {
        if (!serverIds.has(report.id)) {
          combinedHistory.push(report);
        }
      }
      
      // Sort by timestamp if available, newest first
      const sortedHistory = combinedHistory.sort((a, b) => {
        const timeA = a.timestamp || 0;
        const timeB = b.timestamp || 0;
        return timeB - timeA;
      });
      
      setHistory(sortedHistory);
      
      // Update localStorage with the complete merged set
      localStorage.setItem('researchHistory', JSON.stringify(sortedHistory));
      
      console.log('History sync complete, total items:', sortedHistory.length);
    };
    
    fetchHistory();
  }, []); // Empty dependency array - only run once on mount
  
  // Save new research
  const saveResearch = async (question: string, answer: string, orderedData: Data[]) => {
    try {
      // Generate a unique ID
      const id = uuidv4();
      
      // Save to backend
      const response = await fetch('/api/reports', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          id,
          question,
          answer,
          orderedData,
          chatMessages: []
        }),
      });
      
      if (response.ok) {
        const data = await response.json();
        const newId = data.id;
        
        // Update local state
        const newResearch = {
          id: newId,
          question,
          answer,
          orderedData,
          chatMessages: [],
          timestamp: Date.now(),
        };
        
        setHistory(prev => [newResearch, ...prev]);
        
        // Also save to localStorage as fallback
        const localHistory = localStorage.getItem('researchHistory');
        const parsedHistory = localHistory ? JSON.parse(localHistory) : [];
        localStorage.setItem(
          'researchHistory',
          JSON.stringify([newResearch, ...parsedHistory])
        );
        
        return newId;
      } else {
        throw new Error(`API error: ${response.status}`);
      }
    } catch (error) {
      console.error('Error saving research:', error);
      toast.error('Failed to save research to server. Saved locally only.');
      
      // Fallback: save to localStorage only
      const newResearch = {
        id: uuidv4(),
        question,
        answer,
        orderedData,
        chatMessages: [],
        timestamp: Date.now(),
      };
      
      // Update local state
      setHistory(prev => [newResearch, ...prev]);
      
      // Save to localStorage
      const localHistory = localStorage.getItem('researchHistory');
      const parsedHistory = localHistory ? JSON.parse(localHistory) : [];
      localStorage.setItem(
        'researchHistory',
        JSON.stringify([newResearch, ...parsedHistory])
      );
      
      return newResearch.id;
    }
  };
  
  // Update existing research
  const updateResearch = async (id: string, answer: string, orderedData: Data[]) => {
    try {
      // Update in backend
      const response = await fetch(`/api/reports/${id}`, {
        method: 'PUT',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          answer,
          orderedData
        }),
      });
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }
      
      // Update local state
      setHistory(prev => 
        prev.map(item => 
          item.id === id ? { ...item, answer, orderedData, timestamp: Date.now() } : item
        )
      );
      
      // Also update localStorage as fallback
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const updatedHistory = parsedHistory.map((item: any) => 
          item.id === id ? { ...item, answer, orderedData, timestamp: Date.now() } : item
        );
        localStorage.setItem('researchHistory', JSON.stringify(updatedHistory));
      }
      
      return true;
    } catch (error) {
      console.error('Error updating research:', error);
      
      // Update local state anyway
      setHistory(prev => 
        prev.map(item => 
          item.id === id ? { ...item, answer, orderedData, timestamp: Date.now() } : item
        )
      );
      
      // Update localStorage
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const updatedHistory = parsedHistory.map((item: any) => 
          item.id === id ? { ...item, answer, orderedData, timestamp: Date.now() } : item
        );
        localStorage.setItem('researchHistory', JSON.stringify(updatedHistory));
      }
      
      return false;
    }
  };

  // Get research by ID
  const getResearchById = async (id: string) => {
    try {
      const response = await fetch(`/api/reports/${id}`);
      if (response.ok) {
        const data = await response.json();
        return data.report;
      } else if (response.status === 404) {
        // If not found on server, try localStorage
        const localHistory = localStorage.getItem('researchHistory');
        if (localHistory) {
          const parsedHistory = JSON.parse(localHistory);
          return parsedHistory.find((item: any) => item.id === id) || null;
        }
      } else {
        throw new Error(`API error: ${response.status}`);
      }
    } catch (error) {
      console.error('Error getting research by ID:', error);
      
      // Try localStorage as fallback
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        return parsedHistory.find((item: any) => item.id === id) || null;
      }
      
      return null;
    }
  };

  // Delete research
  const deleteResearch = async (id: string) => {
    try {
      const response = await fetch(`/api/reports/${id}`, {
        method: 'DELETE',
      });
      
      if (!response.ok && response.status !== 404) {
        throw new Error(`API error: ${response.status}`);
      }
      
      // Update local state
      setHistory(prev => prev.filter(item => item.id !== id));
      
      // Also update localStorage
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const filteredHistory = parsedHistory.filter((item: any) => item.id !== id);
        localStorage.setItem('researchHistory', JSON.stringify(filteredHistory));
      }
      
      return true;
    } catch (error) {
      console.error('Error deleting research:', error);
      
      // Update local state anyway
      setHistory(prev => prev.filter(item => item.id !== id));
      
      // Update localStorage
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const filteredHistory = parsedHistory.filter((item: any) => item.id !== id);
        localStorage.setItem('researchHistory', JSON.stringify(filteredHistory));
      }
      
      return false;
    }
  };

  // Add chat message
  const addChatMessage = async (id: string, message: ChatMessage) => {
    try {
      const response = await fetch(`/api/reports/${id}/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(message),
      });
      
      if (!response.ok) {
        throw new Error(`API error: ${response.status}`);
      }
      
      // Update local state
      setHistory(prev => 
        prev.map(item => {
          if (item.id === id) {
            const chatMessages = item.chatMessages || [];
            return { ...item, chatMessages: [...chatMessages, message] };
          }
          return item;
        })
      );
      
      // Also update localStorage
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const updatedHistory = parsedHistory.map((item: any) => {
          if (item.id === id) {
            const chatMessages = item.chatMessages || [];
            return { ...item, chatMessages: [...chatMessages, message] };
          }
          return item;
        });
        localStorage.setItem('researchHistory', JSON.stringify(updatedHistory));
      }
      
      return true;
    } catch (error) {
      console.error('Error adding chat message:', error);
      
      // Update local state anyway
      setHistory(prev => 
        prev.map(item => {
          if (item.id === id) {
            const chatMessages = item.chatMessages || [];
            return { ...item, chatMessages: [...chatMessages, message] };
          }
          return item;
        })
      );
      
      // Update localStorage
      const localHistory = localStorage.getItem('researchHistory');
      if (localHistory) {
        const parsedHistory = JSON.parse(localHistory);
        const updatedHistory = parsedHistory.map((item: any) => {
          if (item.id === id) {
            const chatMessages = item.chatMessages || [];
            return { ...item, chatMessages: [...chatMessages, message] };
          }
          return item;
        });
        localStorage.setItem('researchHistory', JSON.stringify(updatedHistory));
      }
      
      return false;
    }
  };

  // Get chat messages
  const getChatMessages = (id: string) => {
    // First try to get from local state
    // Add defensive check to ensure history is an array before using find
    if (Array.isArray(history)) {
      const research = history.find(item => item.id === id);
      if (research && research.chatMessages) {
        return research.chatMessages;
      }
    } else {
      console.warn('History is not an array when getting chat messages');
    }
    
    // Fallback to localStorage
    const localHistory = localStorage.getItem('researchHistory');
    if (localHistory) {
      try {
        const parsedHistory = JSON.parse(localHistory);
        // Check if parsedHistory is an array
        if (Array.isArray(parsedHistory)) {
          const research = parsedHistory.find((item: any) => item.id === id);
          if (research && research.chatMessages) {
            return research.chatMessages;
          }
        } else {
          console.warn('Parsed history from localStorage is not an array');
        }
      } catch (error) {
        console.error('Error parsing history from localStorage:', error);
      }
    }
    
    return [];
  };

  // Clear all history from local storage and server
  const clearHistory = async () => {
    try {
      // Not implementing bulk delete on the server for now
      // This would require a new API endpoint
      
      // Just clear local state and storage
      setHistory([]);
      localStorage.removeItem('researchHistory');
      
      return true;
    } catch (error) {
      console.error('Error clearing history:', error);
      return false;
    }
  };

  return {
    history,
    loading,
    saveResearch,
    updateResearch,
    getResearchById,
    deleteResearch,
    addChatMessage,
    getChatMessages,
    clearHistory
  };
}; 


================================================
FILE: frontend/nextjs/hooks/useScrollHandler.ts
================================================
import { useState, useEffect, useCallback, RefObject } from 'react';

export function useScrollHandler(
  mainContentRef: RefObject<HTMLDivElement>
) {
  const [showScrollButton, setShowScrollButton] = useState(false);

  const handleScroll = useCallback(() => {
    // Calculate if we're near bottom (within 100px)
    const scrollPosition = window.scrollY + window.innerHeight;
    const nearBottom = scrollPosition >= document.documentElement.scrollHeight - 100;
    
    // Show button if we're not near bottom and page is scrollable
    const isPageScrollable = document.documentElement.scrollHeight > window.innerHeight;
    setShowScrollButton(isPageScrollable && !nearBottom);
  }, []);

  const scrollToBottom = () => {
    window.scrollTo({
      top: document.documentElement.scrollHeight,
      behavior: 'smooth'
    });
  };

  // Add ResizeObserver to watch for content changes
  useEffect(() => {
    const mainContentElement = mainContentRef.current;
    const resizeObserver = new ResizeObserver(() => {
      handleScroll();
    });

    if (mainContentElement) {
      resizeObserver.observe(mainContentElement);
    }

    window.addEventListener('scroll', handleScroll);
    window.addEventListener('resize', handleScroll);
    
    return () => {
      if (mainContentElement) {
        resizeObserver.unobserve(mainContentElement);
      }
      resizeObserver.disconnect();
      window.removeEventListener('scroll', handleScroll);
      window.removeEventListener('resize', handleScroll);
    };
  }, [handleScroll, mainContentRef]);

  return {
    showScrollButton,
    scrollToBottom
  };
} 


================================================
FILE: frontend/nextjs/hooks/useWebSocket.ts
================================================
import { useRef, useState, useEffect, useCallback } from 'react';
import { Data, ChatBoxSettings, QuestionData } from '../types/data';
import { getHost } from '../helpers/getHost';

export const useWebSocket = (
  setOrderedData: React.Dispatch<React.SetStateAction<Data[]>>,
  setAnswer: React.Dispatch<React.SetStateAction<string>>, 
  setLoading: React.Dispatch<React.SetStateAction<boolean>>,
  setShowHumanFeedback: React.Dispatch<React.SetStateAction<boolean>>,
  setQuestionForHuman: React.Dispatch<React.SetStateAction<boolean | true>>
) => {
  const [socket, setSocket] = useState<WebSocket | null>(null);
  const heartbeatInterval = useRef<number>();

  // Cleanup function for heartbeat and socket on unmount
  useEffect(() => {
    return () => {
      // Clear heartbeat interval
      if (heartbeatInterval.current) {
        clearInterval(heartbeatInterval.current);
      }
      
      // Close socket on unmount if it exists and is open
      if (socket && socket.readyState === WebSocket.OPEN) {
        console.log('Closing WebSocket due to component unmount');
        socket.close(1000, "Component unmounted");
      }
    };
  }, [socket]);

  const startHeartbeat = (ws: WebSocket) => {
    // Clear any existing heartbeat
    if (heartbeatInterval.current) {
      clearInterval(heartbeatInterval.current);
    }
    
    // Start new heartbeat
    heartbeatInterval.current = window.setInterval(() => {
      if (ws.readyState === WebSocket.OPEN) {
        ws.send('ping');
      }
    }, 30000); // Send ping every 30 seconds
  };

  const initializeWebSocket = useCallback((
    promptValue: string, 
    chatBoxSettings: ChatBoxSettings
  ) => {
    // Close existing socket if any
    if (socket && socket.readyState === WebSocket.OPEN) {
      console.log('Closing existing WebSocket connection');
      socket.close(1000, "New connection requested");
    }

    const storedConfig = localStorage.getItem('apiVariables');
    const apiVariables = storedConfig ? JSON.parse(storedConfig) : {};

    if (typeof window !== 'undefined') {
      
      let fullHost = getHost()
      const protocol = fullHost.includes('https') ? 'wss:' : 'ws:'
      const cleanHost = fullHost.replace('http://', '').replace('https://', '')
      const ws_uri = `${protocol}//${cleanHost}/ws`

      console.log(`Creating new WebSocket connection to ${ws_uri}`);
      const newSocket = new WebSocket(ws_uri);
      setSocket(newSocket);

      // WebSocket connection opened handler
      newSocket.onopen = () => {
        console.log('WebSocket connection opened');
        
        const domainFilters = JSON.parse(localStorage.getItem('domainFilters') || '[]');
        const domains = domainFilters ? domainFilters.map((domain: any) => domain.value) : [];
        const { report_type, report_source, tone, mcp_enabled, mcp_configs, mcp_strategy } = chatBoxSettings;
        
        // Start a new research
        try {
          console.log(`Starting new research for: ${promptValue}`);
          const dataToSend = { 
            task: promptValue,
            report_type, 
            report_source, 
            tone,
            query_domains: domains,
            mcp_enabled: mcp_enabled || false,
            mcp_strategy: mcp_strategy || "fast",
            mcp_configs: mcp_configs || []
          };
          
          // Make sure we have a properly formatted command with a space after start
          const message = `start ${JSON.stringify(dataToSend)}`;
          console.log(`Sending start message, length: ${message.length}`);
          newSocket.send(message);
        } catch (error) {
          console.error("Error preparing start message:", error);
        }
        
        startHeartbeat(newSocket);
      };

      newSocket.onmessage = (event) => {
        try {
          // Handle ping response
          if (event.data === 'pong') return;

          // Try to parse JSON data
          console.log(`Received WebSocket message: ${event.data.substring(0, 100)}...`);
          const data = JSON.parse(event.data);
          
          if (data.type === 'error') {
            console.error(`Server error: ${data.output}`);
          } else if (data.type === 'human_feedback' && data.content === 'request') {
            setQuestionForHuman(data.output);
            setShowHumanFeedback(true);
          } else {
            const contentAndType = `${data.content}-${data.type}`;
            setOrderedData((prevOrder) => [...prevOrder, { ...data, contentAndType }]);

            if (data.type === 'report') {
              setAnswer((prev: string) => prev + data.output);
            } else if (data.type === 'path') {
              setLoading(false);
            }
          }
        } catch (error) {
          console.error('Error parsing WebSocket message:', error, event.data);
        }
      };

      newSocket.onclose = (event) => {
        console.log(`WebSocket connection closed: code=${event.code}, reason=${event.reason}`);
        if (heartbeatInterval.current) {
          clearInterval(heartbeatInterval.current);
        }
        setSocket(null);
      };

      newSocket.onerror = (error) => {
        console.error('WebSocket error:', error);
        if (heartbeatInterval.current) {
          clearInterval(heartbeatInterval.current);
        }
      };
    }
  }, [socket, setOrderedData, setAnswer, setLoading, setShowHumanFeedback, setQuestionForHuman]);

  return { socket, setSocket, initializeWebSocket };
};


================================================
FILE: frontend/nextjs/nginx/default.conf
================================================
server{
    listen 3000;

    location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
        try_files $uri $uri/ /index.html;
    }
}


================================================
FILE: frontend/nextjs/public/embed.js
================================================
(function () {
    window.GPTResearcher = {
        init: function () {
            const parentApiUrl = localStorage.getItem("GPTR_API_URL");

            // Create container
            const container = document.createElement("div");
            container.id = "gpt-researcher-container";
            container.style.width = "100%";
            container.style.height = "100vh";
            container.style.overflow = "hidden"; // Hide scrollbar

            // Create iframe
            const iframe = document.createElement("iframe");
            iframe.src = "https://gptr.app" + (parentApiUrl ? "?GPTR_API_URL=" + parentApiUrl : "");
            iframe.style.width = "100%";
            iframe.style.border = "none";
            iframe.style.height = "100%";
            iframe.style.overflow = "hidden";

            // Add custom styles to hide scrollbars
            const style = document.createElement("style");
            style.textContent = `
                #gpt-researcher-container {
                    -ms-overflow-style: none;  /* IE and Edge */
                    scrollbar-width: none;     /* Firefox */
                }
                #gpt-researcher-container::-webkit-scrollbar {
                    display: none;             /* Chrome, Safari and Opera */
                }
                #gpt-researcher-container iframe {
                    -ms-overflow-style: none;
                    scrollbar-width: none;
                }
                #gpt-researcher-container iframe::-webkit-scrollbar {
                    display: none;
                }
            `;
            document.head.appendChild(style);

            // Add iframe to container
            container.appendChild(iframe);
            document.currentScript.parentNode.insertBefore(container, document.currentScript);

            // Handle resize
            window.addEventListener("resize", () => {
                iframe.style.height = "100%";
            });

            // Ensure height is set after iframe loads
            iframe.addEventListener("load", () => {
                iframe.style.height = "100%";
            });
        },

        configure: function (options = {}) {
            if (options.height) {
                const iframe = document.querySelector("#gpt-researcher-container iframe");
                if (iframe) {
                    iframe.style.height = options.height + "px";
                }
            }
        },
    };

    // Initialize when script loads
    window.GPTResearcher.init();
})();


================================================
FILE: frontend/nextjs/public/manifest.json
================================================
{
  "name": "GPT Researcher",
  "short_name": "GPT Research",
  "description": "AI research assistant that helps you find information and answer questions.",
  "start_url": "/",
  "display": "standalone",
  "background_color": "#000000",
  "theme_color": "#111827",
  "orientation": "portrait",
  "icons": [
    {
      "src": "/img/gptr-black-logo.png",
      "sizes": "192x192",
      "type": "image/png",
      "purpose": "any maskable"
    },
    {
      "src": "/img/gptr-black-logo.png",
      "sizes": "512x512",
      "type": "image/png",
      "purpose": "any maskable"
    }
  ],
  "shortcuts": [
    {
      "name": "New Research",
      "short_name": "New",
      "description": "Start a new research query",
      "url": "/?source=pwa",
      "icons": [{ "src": "/img/gptr-black-logo.png", "sizes": "96x96" }]
    }
  ],
  "screenshots": [
    {
      "src": "/img/screenshots/mobile-chat.png",
      "sizes": "750x1334",
      "type": "image/png",
      "platform": "narrow",
      "label": "Mobile chat interface"
    },
    {
      "src": "/img/screenshots/mobile-home.png",
      "sizes": "750x1334",
      "type": "image/png",
      "platform": "narrow",
      "label": "Mobile home screen"
    }
  ],
  "prefer_related_applications": false,
  "dir": "ltr",
  "categories": ["productivity", "research", "education", "utilities"]
} 


================================================
FILE: frontend/nextjs/public/sw.js
================================================
if(!self.define){let e,s={};const i=(i,a)=>(i=new URL(i+".js",a).href,s[i]||new Promise(s=>{if("document"in self){const e=document.createElement("script");e.src=i,e.onload=s,document.head.appendChild(e)}else e=i,importScripts(i),s()}).then(()=>{let e=s[i];if(!e)throw new Error(`Module ${i} didnâ€™t register its module`);return e}));self.define=(a,c)=>{const n=e||("document"in self?document.currentScript.src:"")||location.href;if(s[n])return;let r={};const t=e=>i(e,n),f={module:{uri:n},exports:r,require:t};s[n]=Promise.all(a.map(e=>f[e]||t(e))).then(e=>(c(...e),r))}}define(["./workbox-f1770938"],function(e){"use strict";importScripts(),self.skipWaiting(),e.clientsClaim(),e.precacheAndRoute([{url:"/_next/static/G7IT8vcClgLfP_ydxxfW4/_buildManifest.js",revision:"c155cce658e53418dec34664328b51ac"},{url:"/_next/static/G7IT8vcClgLfP_ydxxfW4/_ssgManifest.js",revision:"b6652df95db52feb4daf4eca35380933"},{url:"/_next/static/chunks/280-320817e2ffab6569.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/434-baf8a8125c039bd0.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/560-d8dd596ffbff5b0a.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/653-5af4e210b86e256b.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/745-bbb285dc1df000c1.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/752-7cb70f1daee5e32d.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/997-79a992dba56a097f.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/app/_not-found/page-522e51c1efb44c8d.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/app/layout-ddf6317a96bddea3.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/app/page-e31dbd7384863db3.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/app/research/%5Bid%5D/page-0090a3dac8c1fda7.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/fd9d1056-4a9853f3fae16b4a.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/framework-f66176bb897dc684.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/main-512b1b24104322c5.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/main-app-6942d00797161b14.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/pages/_app-72b849fbd24ac258.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/pages/_error-7ba65e1336b92748.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/chunks/polyfills-42372ed130431b0a.js",revision:"846118c33b2c0e922d7b3a7676f81f6f"},{url:"/_next/static/chunks/webpack-bf8edad7b7764fb9.js",revision:"G7IT8vcClgLfP_ydxxfW4"},{url:"/_next/static/css/154871ea4058421a.css",revision:"154871ea4058421a"},{url:"/_next/static/css/29886ec2f5f5c8be.css",revision:"29886ec2f5f5c8be"},{url:"/_next/static/media/630e0b819503bca7-s.woff2",revision:"e3c313092df5d8ea3306d6b29bd44c00"},{url:"/_next/static/media/6eed223b32d97b82-s.woff2",revision:"a981ad4865c753b27f8c8e6feaaf8875"},{url:"/_next/static/media/793968fa3513f5d6-s.p.woff2",revision:"7e692144d823ca28415d410ba874b188"},{url:"/embed.js",revision:"0b3e41a99c8dfc5f1d62d0d671dda685"},{url:"/favicon.ico",revision:"7b22b406f80ee676dad48471e11227db"},{url:"/img/F.svg",revision:"864e14d8969075afeb81193720a5d81a"},{url:"/img/Info.svg",revision:"103713a9bf38d3899089f3a183465c74"},{url:"/img/W.svg",revision:"29ffc7f7899d212c10511aff379e5d28"},{url:"/img/agents/academicResearchAgentAvatar.png",revision:"5fdd1619642f9bcc42052fda16d9be2f"},{url:"/img/agents/businessAnalystAgentAvatar.png",revision:"c1c70436702666ca929509a43f3e8463"},{url:"/img/agents/computerSecurityanalystAvatar.png",revision:"c0b2092292bee3ee10b9142d8e176651"},{url:"/img/agents/defaultAgentAvatar.JPG",revision:"31e5569b41a8a71950bd11d10902d387"},{url:"/img/agents/financeAgentAvatar.png",revision:"8a2abf6022b4370c173ab5d640aee1f7"},{url:"/img/agents/mathAgentAvatar.png",revision:"158c0f4efeef63eea09970468a27012b"},{url:"/img/agents/travelAgentAvatar.png",revision:"bebef00873137dbff42c48a88ab68742"},{url:"/img/arrow-circle-up-right.svg",revision:"a5499f058a9d3691da804375c8e79516"},{url:"/img/arrow-narrow-right.svg",revision:"cd322b0eff9577fd0a09df5bb15ea8b1"},{url:"/img/browser.svg",revision:"3b8d111e94d135b5f9cffdfe6f132e34"},{url:"/img/chat-check.svg",revision:"d32099687f04f71e26271cd43cea3e93"},{url:"/img/chat.svg",revision:"f168b490d8217a2c2aeea457cd753e02"},{url:"/img/copy-white.svg",revision:"f2f284d5666bcc2da90e44a9e8989e3f"},{url:"/img/copy.svg",revision:"8d6ebbbc4873cc44e799c95f1a895dd5"},{url:"/img/dinosaur.svg",revision:"42e67ed2261ed92c9f76c159f0f258eb"},{url:"/img/discord.svg",revision:"5db80289a6e3af4ac7c6b6d4f0cdb554"},{url:"/img/docker-blue.svg",revision:"2904f8f3306ccf8037838262c45c24fc"},{url:"/img/docker.svg",revision:"f1e915ecef483e3bcc09c7353b8888a3"},{url:"/img/dunk.svg",revision:"b9d0c2dc95f767be7fdff0dc2689ec79"},{url:"/img/github-blue.svg",revision:"af3c1a6a8d0f9d7ec3cb9102b4048d99"},{url:"/img/github-footer.svg",revision:"d79823a21fbc7cc9917ac048a12e058a"},{url:"/img/github.svg",revision:"cc1a70d86f1925dc55b2a785587306c6"},{url:"/img/globe.svg",revision:"8fcfc462e0ff5e900ba8b4d006c9f680"},{url:"/img/gptr-black-logo.png",revision:"e17c36ddf6422aa3da2c70c8f51a9ab2"},{url:"/img/gptr-logo.png",revision:"1a143f183bfdc5dd622cb97326e32127"},{url:"/img/hiker.svg",revision:"f5500bb2627cd1e8007adc8072e8f48c"},{url:"/img/icon _atom_.svg",revision:"2f9ed381dc5036721f07fd49b173db9e"},{url:"/img/icon _dumbell_.svg",revision:"5f1dc06c26e33ca171f9c3318fc671ec"},{url:"/img/icon _leaf_.svg",revision:"111e97dee61cda7a8f97546be04f4797"},{url:"/img/image.svg",revision:"dec61be5c852ca6f27d7239e560e1185"},{url:"/img/indeed.svg",revision:"3efe5203c66766eefbb1c0c88b8b232d"},{url:"/img/link.svg",revision:"882ca6e69d6766c8d3970ed21a5ad3eb"},{url:"/img/message-question-circle.svg",revision:"d9403628fbc92456319e6760a29c795e"},{url:"/img/news.svg",revision:"b6928f2e0d6629fd7cd6e8cddec3aab9"},{url:"/img/search.svg",revision:"f82a218633229f8f820e669c3e094702"},{url:"/img/share.svg",revision:"1caaaa9c8251e1f8df78be44c0081216"},{url:"/img/similarTopics.svg",revision:"b0c4231a0b55d196586671632246b737"},{url:"/img/sources.svg",revision:"3e49c70e1ed372d878ef655e837b11ce"},{url:"/img/stock.svg",revision:"ae349f42cda11d3740feeb7f55090fe9"},{url:"/img/stock2.svg",revision:"2c13e5dcf1a278719062c0cc55f44d83"},{url:"/img/thinking.svg",revision:"cc051305099b3ad03476c735e5fcfd9f"},{url:"/img/white-books.svg",revision:"4730bdc4ebc81e77491db322ef1a57db"},{url:"/img/x.svg",revision:"31c64760085fa37405d771e179a4bdd0"},{url:"/manifest.json",revision:"75ef09fbbe2160222fffe8292d63cdd0"},{url:"/next.svg",revision:"8e061864f388b47f33a1c3780831193e"},{url:"/vercel.svg",revision:"61c6b19abff40ea7acd577be818f3976"}],{ignoreURLParametersMatching:[/^utm_/,/^fbclid$/]}),e.cleanupOutdatedCaches(),e.registerRoute("/",new e.NetworkFirst({cacheName:"start-url",plugins:[{cacheWillUpdate:async({response:e})=>e&&"opaqueredirect"===e.type?new Response(e.body,{status:200,statusText:"OK",headers:e.headers}):e}]}),"GET"),e.registerRoute(/^https:\/\/fonts\.(?:gstatic)\.com\/.*/i,new e.CacheFirst({cacheName:"google-fonts-webfonts",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:31536e3})]}),"GET"),e.registerRoute(/^https:\/\/fonts\.(?:googleapis)\.com\/.*/i,new e.StaleWhileRevalidate({cacheName:"google-fonts-stylesheets",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),"GET"),e.registerRoute(/\.(?:eot|otf|ttc|ttf|woff|woff2|font.css)$/i,new e.StaleWhileRevalidate({cacheName:"static-font-assets",plugins:[new e.ExpirationPlugin({maxEntries:4,maxAgeSeconds:604800})]}),"GET"),e.registerRoute(/\.(?:jpg|jpeg|gif|png|svg|ico|webp)$/i,new e.StaleWhileRevalidate({cacheName:"static-image-assets",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:2592e3})]}),"GET"),e.registerRoute(/\/_next\/static.+\.js$/i,new e.CacheFirst({cacheName:"next-static-js-assets",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\/_next\/image\?url=.+$/i,new e.StaleWhileRevalidate({cacheName:"next-image",plugins:[new e.ExpirationPlugin({maxEntries:64,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\.(?:mp3|wav|ogg)$/i,new e.CacheFirst({cacheName:"static-audio-assets",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\.(?:mp4|webm)$/i,new e.CacheFirst({cacheName:"static-video-assets",plugins:[new e.RangeRequestsPlugin,new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\.(?:js)$/i,new e.StaleWhileRevalidate({cacheName:"static-js-assets",plugins:[new e.ExpirationPlugin({maxEntries:48,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\.(?:css|less)$/i,new e.StaleWhileRevalidate({cacheName:"static-style-assets",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\/_next\/data\/.+\/.+\.json$/i,new e.StaleWhileRevalidate({cacheName:"next-data",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(/\.(?:json|xml|csv)$/i,new e.NetworkFirst({cacheName:"static-data-assets",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(({sameOrigin:e,url:{pathname:s}})=>!(!e||s.startsWith("/api/auth/callback")||!s.startsWith("/api/")),new e.NetworkFirst({cacheName:"apis",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:16,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(({request:e,url:{pathname:s},sameOrigin:i})=>"1"===e.headers.get("RSC")&&"1"===e.headers.get("Next-Router-Prefetch")&&i&&!s.startsWith("/api/"),new e.NetworkFirst({cacheName:"pages-rsc-prefetch",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(({request:e,url:{pathname:s},sameOrigin:i})=>"1"===e.headers.get("RSC")&&i&&!s.startsWith("/api/"),new e.NetworkFirst({cacheName:"pages-rsc",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(({url:{pathname:e},sameOrigin:s})=>s&&!e.startsWith("/api/"),new e.NetworkFirst({cacheName:"pages",plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:86400})]}),"GET"),e.registerRoute(({sameOrigin:e})=>!e,new e.NetworkFirst({cacheName:"cross-origin",networkTimeoutSeconds:10,plugins:[new e.ExpirationPlugin({maxEntries:32,maxAgeSeconds:3600})]}),"GET")});



================================================
FILE: frontend/nextjs/public/workbox-f1770938.js
================================================
define(["exports"],function(t){"use strict";try{self["workbox:core:7.0.0"]&&_()}catch(t){}const e=(t,...e)=>{let s=t;return e.length>0&&(s+=` :: ${JSON.stringify(e)}`),s};class s extends Error{constructor(t,s){super(e(t,s)),this.name=t,this.details=s}}try{self["workbox:routing:7.0.0"]&&_()}catch(t){}const n=t=>t&&"object"==typeof t?t:{handle:t};class r{constructor(t,e,s="GET"){this.handler=n(e),this.match=t,this.method=s}setCatchHandler(t){this.catchHandler=n(t)}}class i extends r{constructor(t,e,s){super(({url:e})=>{const s=t.exec(e.href);if(s&&(e.origin===location.origin||0===s.index))return s.slice(1)},e,s)}}class a{constructor(){this.t=new Map,this.i=new Map}get routes(){return this.t}addFetchListener(){self.addEventListener("fetch",t=>{const{request:e}=t,s=this.handleRequest({request:e,event:t});s&&t.respondWith(s)})}addCacheListener(){self.addEventListener("message",t=>{if(t.data&&"CACHE_URLS"===t.data.type){const{payload:e}=t.data,s=Promise.all(e.urlsToCache.map(e=>{"string"==typeof e&&(e=[e]);const s=new Request(...e);return this.handleRequest({request:s,event:t})}));t.waitUntil(s),t.ports&&t.ports[0]&&s.then(()=>t.ports[0].postMessage(!0))}})}handleRequest({request:t,event:e}){const s=new URL(t.url,location.href);if(!s.protocol.startsWith("http"))return;const n=s.origin===location.origin,{params:r,route:i}=this.findMatchingRoute({event:e,request:t,sameOrigin:n,url:s});let a=i&&i.handler;const o=t.method;if(!a&&this.i.has(o)&&(a=this.i.get(o)),!a)return;let c;try{c=a.handle({url:s,request:t,event:e,params:r})}catch(t){c=Promise.reject(t)}const h=i&&i.catchHandler;return c instanceof Promise&&(this.o||h)&&(c=c.catch(async n=>{if(h)try{return await h.handle({url:s,request:t,event:e,params:r})}catch(t){t instanceof Error&&(n=t)}if(this.o)return this.o.handle({url:s,request:t,event:e});throw n})),c}findMatchingRoute({url:t,sameOrigin:e,request:s,event:n}){const r=this.t.get(s.method)||[];for(const i of r){let r;const a=i.match({url:t,sameOrigin:e,request:s,event:n});if(a)return r=a,(Array.isArray(r)&&0===r.length||a.constructor===Object&&0===Object.keys(a).length||"boolean"==typeof a)&&(r=void 0),{route:i,params:r}}return{}}setDefaultHandler(t,e="GET"){this.i.set(e,n(t))}setCatchHandler(t){this.o=n(t)}registerRoute(t){this.t.has(t.method)||this.t.set(t.method,[]),this.t.get(t.method).push(t)}unregisterRoute(t){if(!this.t.has(t.method))throw new s("unregister-route-but-not-found-with-method",{method:t.method});const e=this.t.get(t.method).indexOf(t);if(!(e>-1))throw new s("unregister-route-route-not-registered");this.t.get(t.method).splice(e,1)}}let o;const c=()=>(o||(o=new a,o.addFetchListener(),o.addCacheListener()),o);function h(t,e,n){let a;if("string"==typeof t){const s=new URL(t,location.href);a=new r(({url:t})=>t.href===s.href,e,n)}else if(t instanceof RegExp)a=new i(t,e,n);else if("function"==typeof t)a=new r(t,e,n);else{if(!(t instanceof r))throw new s("unsupported-route-type",{moduleName:"workbox-routing",funcName:"registerRoute",paramName:"capture"});a=t}return c().registerRoute(a),a}try{self["workbox:strategies:7.0.0"]&&_()}catch(t){}const u={cacheWillUpdate:async({response:t})=>200===t.status||0===t.status?t:null},l={googleAnalytics:"googleAnalytics",precache:"precache-v2",prefix:"workbox",runtime:"runtime",suffix:"undefined"!=typeof registration?registration.scope:""},f=t=>[l.prefix,t,l.suffix].filter(t=>t&&t.length>0).join("-"),w=t=>t||f(l.precache),d=t=>t||f(l.runtime);function p(t,e){const s=new URL(t);for(const t of e)s.searchParams.delete(t);return s.href}class y{constructor(){this.promise=new Promise((t,e)=>{this.resolve=t,this.reject=e})}}const g=new Set;function m(t){return"string"==typeof t?new Request(t):t}class v{constructor(t,e){this.h={},Object.assign(this,e),this.event=e.event,this.u=t,this.l=new y,this.p=[],this.m=[...t.plugins],this.v=new Map;for(const t of this.m)this.v.set(t,{});this.event.waitUntil(this.l.promise)}async fetch(t){const{event:e}=this;let n=m(t);if("navigate"===n.mode&&e instanceof FetchEvent&&e.preloadResponse){const t=await e.preloadResponse;if(t)return t}const r=this.hasCallback("fetchDidFail")?n.clone():null;try{for(const t of this.iterateCallbacks("requestWillFetch"))n=await t({request:n.clone(),event:e})}catch(t){if(t instanceof Error)throw new s("plugin-error-request-will-fetch",{thrownErrorMessage:t.message})}const i=n.clone();try{let t;t=await fetch(n,"navigate"===n.mode?void 0:this.u.fetchOptions);for(const s of this.iterateCallbacks("fetchDidSucceed"))t=await s({event:e,request:i,response:t});return t}catch(t){throw r&&await this.runCallbacks("fetchDidFail",{error:t,event:e,originalRequest:r.clone(),request:i.clone()}),t}}async fetchAndCachePut(t){const e=await this.fetch(t),s=e.clone();return this.waitUntil(this.cachePut(t,s)),e}async cacheMatch(t){const e=m(t);let s;const{cacheName:n,matchOptions:r}=this.u,i=await this.getCacheKey(e,"read"),a=Object.assign(Object.assign({},r),{cacheName:n});s=await caches.match(i,a);for(const t of this.iterateCallbacks("cachedResponseWillBeUsed"))s=await t({cacheName:n,matchOptions:r,cachedResponse:s,request:i,event:this.event})||void 0;return s}async cachePut(t,e){const n=m(t);var r;await(r=0,new Promise(t=>setTimeout(t,r)));const i=await this.getCacheKey(n,"write");if(!e)throw new s("cache-put-with-no-response",{url:(a=i.url,new URL(String(a),location.href).href.replace(new RegExp(`^${location.origin}`),""))});var a;const o=await this.R(e);if(!o)return!1;const{cacheName:c,matchOptions:h}=this.u,u=await self.caches.open(c),l=this.hasCallback("cacheDidUpdate"),f=l?await async function(t,e,s,n){const r=p(e.url,s);if(e.url===r)return t.match(e,n);const i=Object.assign(Object.assign({},n),{ignoreSearch:!0}),a=await t.keys(e,i);for(const e of a)if(r===p(e.url,s))return t.match(e,n)}(u,i.clone(),["__WB_REVISION__"],h):null;try{await u.put(i,l?o.clone():o)}catch(t){if(t instanceof Error)throw"QuotaExceededError"===t.name&&await async function(){for(const t of g)await t()}(),t}for(const t of this.iterateCallbacks("cacheDidUpdate"))await t({cacheName:c,oldResponse:f,newResponse:o.clone(),request:i,event:this.event});return!0}async getCacheKey(t,e){const s=`${t.url} | ${e}`;if(!this.h[s]){let n=t;for(const t of this.iterateCallbacks("cacheKeyWillBeUsed"))n=m(await t({mode:e,request:n,event:this.event,params:this.params}));this.h[s]=n}return this.h[s]}hasCallback(t){for(const e of this.u.plugins)if(t in e)return!0;return!1}async runCallbacks(t,e){for(const s of this.iterateCallbacks(t))await s(e)}*iterateCallbacks(t){for(const e of this.u.plugins)if("function"==typeof e[t]){const s=this.v.get(e),n=n=>{const r=Object.assign(Object.assign({},n),{state:s});return e[t](r)};yield n}}waitUntil(t){return this.p.push(t),t}async doneWaiting(){let t;for(;t=this.p.shift();)await t}destroy(){this.l.resolve(null)}async R(t){let e=t,s=!1;for(const t of this.iterateCallbacks("cacheWillUpdate"))if(e=await t({request:this.request,response:e,event:this.event})||void 0,s=!0,!e)break;return s||e&&200!==e.status&&(e=void 0),e}}class R{constructor(t={}){this.cacheName=d(t.cacheName),this.plugins=t.plugins||[],this.fetchOptions=t.fetchOptions,this.matchOptions=t.matchOptions}handle(t){const[e]=this.handleAll(t);return e}handleAll(t){t instanceof FetchEvent&&(t={event:t,request:t.request});const e=t.event,s="string"==typeof t.request?new Request(t.request):t.request,n="params"in t?t.params:void 0,r=new v(this,{event:e,request:s,params:n}),i=this.q(r,s,e);return[i,this.D(i,r,s,e)]}async q(t,e,n){let r;await t.runCallbacks("handlerWillStart",{event:n,request:e});try{if(r=await this.U(e,t),!r||"error"===r.type)throw new s("no-response",{url:e.url})}catch(s){if(s instanceof Error)for(const i of t.iterateCallbacks("handlerDidError"))if(r=await i({error:s,event:n,request:e}),r)break;if(!r)throw s}for(const s of t.iterateCallbacks("handlerWillRespond"))r=await s({event:n,request:e,response:r});return r}async D(t,e,s,n){let r,i;try{r=await t}catch(i){}try{await e.runCallbacks("handlerDidRespond",{event:n,request:s,response:r}),await e.doneWaiting()}catch(t){t instanceof Error&&(i=t)}if(await e.runCallbacks("handlerDidComplete",{event:n,request:s,response:r,error:i}),e.destroy(),i)throw i}}function b(t){t.then(()=>{})}function q(){return q=Object.assign?Object.assign.bind():function(t){for(var e=1;e<arguments.length;e++){var s=arguments[e];for(var n in s)({}).hasOwnProperty.call(s,n)&&(t[n]=s[n])}return t},q.apply(null,arguments)}let D,U;const x=new WeakMap,L=new WeakMap,I=new WeakMap,C=new WeakMap,E=new WeakMap;let N={get(t,e,s){if(t instanceof IDBTransaction){if("done"===e)return L.get(t);if("objectStoreNames"===e)return t.objectStoreNames||I.get(t);if("store"===e)return s.objectStoreNames[1]?void 0:s.objectStore(s.objectStoreNames[0])}return k(t[e])},set:(t,e,s)=>(t[e]=s,!0),has:(t,e)=>t instanceof IDBTransaction&&("done"===e||"store"===e)||e in t};function O(t){return t!==IDBDatabase.prototype.transaction||"objectStoreNames"in IDBTransaction.prototype?(U||(U=[IDBCursor.prototype.advance,IDBCursor.prototype.continue,IDBCursor.prototype.continuePrimaryKey])).includes(t)?function(...e){return t.apply(B(this),e),k(x.get(this))}:function(...e){return k(t.apply(B(this),e))}:function(e,...s){const n=t.call(B(this),e,...s);return I.set(n,e.sort?e.sort():[e]),k(n)}}function T(t){return"function"==typeof t?O(t):(t instanceof IDBTransaction&&function(t){if(L.has(t))return;const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener("complete",r),t.removeEventListener("error",i),t.removeEventListener("abort",i)},r=()=>{e(),n()},i=()=>{s(t.error||new DOMException("AbortError","AbortError")),n()};t.addEventListener("complete",r),t.addEventListener("error",i),t.addEventListener("abort",i)});L.set(t,e)}(t),e=t,(D||(D=[IDBDatabase,IDBObjectStore,IDBIndex,IDBCursor,IDBTransaction])).some(t=>e instanceof t)?new Proxy(t,N):t);var e}function k(t){if(t instanceof IDBRequest)return function(t){const e=new Promise((e,s)=>{const n=()=>{t.removeEventListener("success",r),t.removeEventListener("error",i)},r=()=>{e(k(t.result)),n()},i=()=>{s(t.error),n()};t.addEventListener("success",r),t.addEventListener("error",i)});return e.then(e=>{e instanceof IDBCursor&&x.set(e,t)}).catch(()=>{}),E.set(e,t),e}(t);if(C.has(t))return C.get(t);const e=T(t);return e!==t&&(C.set(t,e),E.set(e,t)),e}const B=t=>E.get(t);const P=["get","getKey","getAll","getAllKeys","count"],M=["put","add","delete","clear"],W=new Map;function j(t,e){if(!(t instanceof IDBDatabase)||e in t||"string"!=typeof e)return;if(W.get(e))return W.get(e);const s=e.replace(/FromIndex$/,""),n=e!==s,r=M.includes(s);if(!(s in(n?IDBIndex:IDBObjectStore).prototype)||!r&&!P.includes(s))return;const i=async function(t,...e){const i=this.transaction(t,r?"readwrite":"readonly");let a=i.store;return n&&(a=a.index(e.shift())),(await Promise.all([a[s](...e),r&&i.done]))[0]};return W.set(e,i),i}N=(t=>q({},t,{get:(e,s,n)=>j(e,s)||t.get(e,s,n),has:(e,s)=>!!j(e,s)||t.has(e,s)}))(N);try{self["workbox:expiration:7.0.0"]&&_()}catch(t){}const S="cache-entries",K=t=>{const e=new URL(t,location.href);return e.hash="",e.href};class A{constructor(t){this._=null,this.L=t}I(t){const e=t.createObjectStore(S,{keyPath:"id"});e.createIndex("cacheName","cacheName",{unique:!1}),e.createIndex("timestamp","timestamp",{unique:!1})}C(t){this.I(t),this.L&&function(t,{blocked:e}={}){const s=indexedDB.deleteDatabase(t);e&&s.addEventListener("blocked",t=>e(t.oldVersion,t)),k(s).then(()=>{})}(this.L)}async setTimestamp(t,e){const s={url:t=K(t),timestamp:e,cacheName:this.L,id:this.N(t)},n=(await this.getDb()).transaction(S,"readwrite",{durability:"relaxed"});await n.store.put(s),await n.done}async getTimestamp(t){const e=await this.getDb(),s=await e.get(S,this.N(t));return null==s?void 0:s.timestamp}async expireEntries(t,e){const s=await this.getDb();let n=await s.transaction(S).store.index("timestamp").openCursor(null,"prev");const r=[];let i=0;for(;n;){const s=n.value;s.cacheName===this.L&&(t&&s.timestamp<t||e&&i>=e?r.push(n.value):i++),n=await n.continue()}const a=[];for(const t of r)await s.delete(S,t.id),a.push(t.url);return a}N(t){return this.L+"|"+K(t)}async getDb(){return this._||(this._=await function(t,e,{blocked:s,upgrade:n,blocking:r,terminated:i}={}){const a=indexedDB.open(t,e),o=k(a);return n&&a.addEventListener("upgradeneeded",t=>{n(k(a.result),t.oldVersion,t.newVersion,k(a.transaction),t)}),s&&a.addEventListener("blocked",t=>s(t.oldVersion,t.newVersion,t)),o.then(t=>{i&&t.addEventListener("close",()=>i()),r&&t.addEventListener("versionchange",t=>r(t.oldVersion,t.newVersion,t))}).catch(()=>{}),o}("workbox-expiration",1,{upgrade:this.C.bind(this)})),this._}}class F{constructor(t,e={}){this.O=!1,this.T=!1,this.k=e.maxEntries,this.B=e.maxAgeSeconds,this.P=e.matchOptions,this.L=t,this.M=new A(t)}async expireEntries(){if(this.O)return void(this.T=!0);this.O=!0;const t=this.B?Date.now()-1e3*this.B:0,e=await this.M.expireEntries(t,this.k),s=await self.caches.open(this.L);for(const t of e)await s.delete(t,this.P);this.O=!1,this.T&&(this.T=!1,b(this.expireEntries()))}async updateTimestamp(t){await this.M.setTimestamp(t,Date.now())}async isURLExpired(t){if(this.B){const e=await this.M.getTimestamp(t),s=Date.now()-1e3*this.B;return void 0===e||e<s}return!1}async delete(){this.T=!1,await this.M.expireEntries(1/0)}}try{self["workbox:range-requests:7.0.0"]&&_()}catch(t){}async function H(t,e){try{if(206===e.status)return e;const n=t.headers.get("range");if(!n)throw new s("no-range-header");const r=function(t){const e=t.trim().toLowerCase();if(!e.startsWith("bytes="))throw new s("unit-must-be-bytes",{normalizedRangeHeader:e});if(e.includes(","))throw new s("single-range-only",{normalizedRangeHeader:e});const n=/(\d*)-(\d*)/.exec(e);if(!n||!n[1]&&!n[2])throw new s("invalid-range-values",{normalizedRangeHeader:e});return{start:""===n[1]?void 0:Number(n[1]),end:""===n[2]?void 0:Number(n[2])}}(n),i=await e.blob(),a=function(t,e,n){const r=t.size;if(n&&n>r||e&&e<0)throw new s("range-not-satisfiable",{size:r,end:n,start:e});let i,a;return void 0!==e&&void 0!==n?(i=e,a=n+1):void 0!==e&&void 0===n?(i=e,a=r):void 0!==n&&void 0===e&&(i=r-n,a=r),{start:i,end:a}}(i,r.start,r.end),o=i.slice(a.start,a.end),c=o.size,h=new Response(o,{status:206,statusText:"Partial Content",headers:e.headers});return h.headers.set("Content-Length",String(c)),h.headers.set("Content-Range",`bytes ${a.start}-${a.end-1}/${i.size}`),h}catch(t){return new Response("",{status:416,statusText:"Range Not Satisfiable"})}}function $(t,e){const s=e();return t.waitUntil(s),s}try{self["workbox:precaching:7.0.0"]&&_()}catch(t){}function z(t){if(!t)throw new s("add-to-cache-list-unexpected-type",{entry:t});if("string"==typeof t){const e=new URL(t,location.href);return{cacheKey:e.href,url:e.href}}const{revision:e,url:n}=t;if(!n)throw new s("add-to-cache-list-unexpected-type",{entry:t});if(!e){const t=new URL(n,location.href);return{cacheKey:t.href,url:t.href}}const r=new URL(n,location.href),i=new URL(n,location.href);return r.searchParams.set("__WB_REVISION__",e),{cacheKey:r.href,url:i.href}}class G{constructor(){this.updatedURLs=[],this.notUpdatedURLs=[],this.handlerWillStart=async({request:t,state:e})=>{e&&(e.originalRequest=t)},this.cachedResponseWillBeUsed=async({event:t,state:e,cachedResponse:s})=>{if("install"===t.type&&e&&e.originalRequest&&e.originalRequest instanceof Request){const t=e.originalRequest.url;s?this.notUpdatedURLs.push(t):this.updatedURLs.push(t)}return s}}}class V{constructor({precacheController:t}){this.cacheKeyWillBeUsed=async({request:t,params:e})=>{const s=(null==e?void 0:e.cacheKey)||this.W.getCacheKeyForURL(t.url);return s?new Request(s,{headers:t.headers}):t},this.W=t}}let J,Q;async function X(t,e){let n=null;if(t.url){n=new URL(t.url).origin}if(n!==self.location.origin)throw new s("cross-origin-copy-response",{origin:n});const r=t.clone(),i={headers:new Headers(r.headers),status:r.status,statusText:r.statusText},a=e?e(i):i,o=function(){if(void 0===J){const t=new Response("");if("body"in t)try{new Response(t.body),J=!0}catch(t){J=!1}J=!1}return J}()?r.body:await r.blob();return new Response(o,a)}class Y extends R{constructor(t={}){t.cacheName=w(t.cacheName),super(t),this.j=!1!==t.fallbackToNetwork,this.plugins.push(Y.copyRedirectedCacheableResponsesPlugin)}async U(t,e){const s=await e.cacheMatch(t);return s||(e.event&&"install"===e.event.type?await this.S(t,e):await this.K(t,e))}async K(t,e){let n;const r=e.params||{};if(!this.j)throw new s("missing-precache-entry",{cacheName:this.cacheName,url:t.url});{const s=r.integrity,i=t.integrity,a=!i||i===s;n=await e.fetch(new Request(t,{integrity:"no-cors"!==t.mode?i||s:void 0})),s&&a&&"no-cors"!==t.mode&&(this.A(),await e.cachePut(t,n.clone()))}return n}async S(t,e){this.A();const n=await e.fetch(t);if(!await e.cachePut(t,n.clone()))throw new s("bad-precaching-response",{url:t.url,status:n.status});return n}A(){let t=null,e=0;for(const[s,n]of this.plugins.entries())n!==Y.copyRedirectedCacheableResponsesPlugin&&(n===Y.defaultPrecacheCacheabilityPlugin&&(t=s),n.cacheWillUpdate&&e++);0===e?this.plugins.push(Y.defaultPrecacheCacheabilityPlugin):e>1&&null!==t&&this.plugins.splice(t,1)}}Y.defaultPrecacheCacheabilityPlugin={cacheWillUpdate:async({response:t})=>!t||t.status>=400?null:t},Y.copyRedirectedCacheableResponsesPlugin={cacheWillUpdate:async({response:t})=>t.redirected?await X(t):t};class Z{constructor({cacheName:t,plugins:e=[],fallbackToNetwork:s=!0}={}){this.F=new Map,this.H=new Map,this.$=new Map,this.u=new Y({cacheName:w(t),plugins:[...e,new V({precacheController:this})],fallbackToNetwork:s}),this.install=this.install.bind(this),this.activate=this.activate.bind(this)}get strategy(){return this.u}precache(t){this.addToCacheList(t),this.G||(self.addEventListener("install",this.install),self.addEventListener("activate",this.activate),this.G=!0)}addToCacheList(t){const e=[];for(const n of t){"string"==typeof n?e.push(n):n&&void 0===n.revision&&e.push(n.url);const{cacheKey:t,url:r}=z(n),i="string"!=typeof n&&n.revision?"reload":"default";if(this.F.has(r)&&this.F.get(r)!==t)throw new s("add-to-cache-list-conflicting-entries",{firstEntry:this.F.get(r),secondEntry:t});if("string"!=typeof n&&n.integrity){if(this.$.has(t)&&this.$.get(t)!==n.integrity)throw new s("add-to-cache-list-conflicting-integrities",{url:r});this.$.set(t,n.integrity)}if(this.F.set(r,t),this.H.set(r,i),e.length>0){const t=`Workbox is precaching URLs without revision info: ${e.join(", ")}\nThis is generally NOT safe. Learn more at https://bit.ly/wb-precache`;console.warn(t)}}}install(t){return $(t,async()=>{const e=new G;this.strategy.plugins.push(e);for(const[e,s]of this.F){const n=this.$.get(s),r=this.H.get(e),i=new Request(e,{integrity:n,cache:r,credentials:"same-origin"});await Promise.all(this.strategy.handleAll({params:{cacheKey:s},request:i,event:t}))}const{updatedURLs:s,notUpdatedURLs:n}=e;return{updatedURLs:s,notUpdatedURLs:n}})}activate(t){return $(t,async()=>{const t=await self.caches.open(this.strategy.cacheName),e=await t.keys(),s=new Set(this.F.values()),n=[];for(const r of e)s.has(r.url)||(await t.delete(r),n.push(r.url));return{deletedURLs:n}})}getURLsToCacheKeys(){return this.F}getCachedURLs(){return[...this.F.keys()]}getCacheKeyForURL(t){const e=new URL(t,location.href);return this.F.get(e.href)}getIntegrityForCacheKey(t){return this.$.get(t)}async matchPrecache(t){const e=t instanceof Request?t.url:t,s=this.getCacheKeyForURL(e);if(s){return(await self.caches.open(this.strategy.cacheName)).match(s)}}createHandlerBoundToURL(t){const e=this.getCacheKeyForURL(t);if(!e)throw new s("non-precached-url",{url:t});return s=>(s.request=new Request(t),s.params=Object.assign({cacheKey:e},s.params),this.strategy.handle(s))}}const tt=()=>(Q||(Q=new Z),Q);class et extends r{constructor(t,e){super(({request:s})=>{const n=t.getURLsToCacheKeys();for(const r of function*(t,{ignoreURLParametersMatching:e=[/^utm_/,/^fbclid$/],directoryIndex:s="index.html",cleanURLs:n=!0,urlManipulation:r}={}){const i=new URL(t,location.href);i.hash="",yield i.href;const a=function(t,e=[]){for(const s of[...t.searchParams.keys()])e.some(t=>t.test(s))&&t.searchParams.delete(s);return t}(i,e);if(yield a.href,s&&a.pathname.endsWith("/")){const t=new URL(a.href);t.pathname+=s,yield t.href}if(n){const t=new URL(a.href);t.pathname+=".html",yield t.href}if(r){const t=r({url:i});for(const e of t)yield e.href}}(s.url,e)){const e=n.get(r);if(e){return{cacheKey:e,integrity:t.getIntegrityForCacheKey(e)}}}},t.strategy)}}t.CacheFirst=class extends R{async U(t,e){let n,r=await e.cacheMatch(t);if(!r)try{r=await e.fetchAndCachePut(t)}catch(t){t instanceof Error&&(n=t)}if(!r)throw new s("no-response",{url:t.url,error:n});return r}},t.ExpirationPlugin=class{constructor(t={}){this.cachedResponseWillBeUsed=async({event:t,request:e,cacheName:s,cachedResponse:n})=>{if(!n)return null;const r=this.V(n),i=this.J(s);b(i.expireEntries());const a=i.updateTimestamp(e.url);if(t)try{t.waitUntil(a)}catch(t){}return r?n:null},this.cacheDidUpdate=async({cacheName:t,request:e})=>{const s=this.J(t);await s.updateTimestamp(e.url),await s.expireEntries()},this.X=t,this.B=t.maxAgeSeconds,this.Y=new Map,t.purgeOnQuotaError&&function(t){g.add(t)}(()=>this.deleteCacheAndMetadata())}J(t){if(t===d())throw new s("expire-custom-caches-only");let e=this.Y.get(t);return e||(e=new F(t,this.X),this.Y.set(t,e)),e}V(t){if(!this.B)return!0;const e=this.Z(t);if(null===e)return!0;return e>=Date.now()-1e3*this.B}Z(t){if(!t.headers.has("date"))return null;const e=t.headers.get("date"),s=new Date(e).getTime();return isNaN(s)?null:s}async deleteCacheAndMetadata(){for(const[t,e]of this.Y)await self.caches.delete(t),await e.delete();this.Y=new Map}},t.NetworkFirst=class extends R{constructor(t={}){super(t),this.plugins.some(t=>"cacheWillUpdate"in t)||this.plugins.unshift(u),this.tt=t.networkTimeoutSeconds||0}async U(t,e){const n=[],r=[];let i;if(this.tt){const{id:s,promise:a}=this.et({request:t,logs:n,handler:e});i=s,r.push(a)}const a=this.st({timeoutId:i,request:t,logs:n,handler:e});r.push(a);const o=await e.waitUntil((async()=>await e.waitUntil(Promise.race(r))||await a)());if(!o)throw new s("no-response",{url:t.url});return o}et({request:t,logs:e,handler:s}){let n;return{promise:new Promise(e=>{n=setTimeout(async()=>{e(await s.cacheMatch(t))},1e3*this.tt)}),id:n}}async st({timeoutId:t,request:e,logs:s,handler:n}){let r,i;try{i=await n.fetchAndCachePut(e)}catch(t){t instanceof Error&&(r=t)}return t&&clearTimeout(t),!r&&i||(i=await n.cacheMatch(e)),i}},t.RangeRequestsPlugin=class{constructor(){this.cachedResponseWillBeUsed=async({request:t,cachedResponse:e})=>e&&t.headers.has("range")?await H(t,e):e}},t.StaleWhileRevalidate=class extends R{constructor(t={}){super(t),this.plugins.some(t=>"cacheWillUpdate"in t)||this.plugins.unshift(u)}async U(t,e){const n=e.fetchAndCachePut(t).catch(()=>{});e.waitUntil(n);let r,i=await e.cacheMatch(t);if(i);else try{i=await n}catch(t){t instanceof Error&&(r=t)}if(!i)throw new s("no-response",{url:t.url,error:r});return i}},t.cleanupOutdatedCaches=function(){self.addEventListener("activate",t=>{const e=w();t.waitUntil((async(t,e="-precache-")=>{const s=(await self.caches.keys()).filter(s=>s.includes(e)&&s.includes(self.registration.scope)&&s!==t);return await Promise.all(s.map(t=>self.caches.delete(t))),s})(e).then(t=>{}))})},t.clientsClaim=function(){self.addEventListener("activate",()=>self.clients.claim())},t.precacheAndRoute=function(t,e){!function(t){tt().precache(t)}(t),function(t){const e=tt();h(new et(e,t))}(e)},t.registerRoute=h});



================================================
FILE: frontend/nextjs/public/img/agents/defaultAgentAvatar.JPG
================================================
[Non-text file]


================================================
FILE: frontend/nextjs/src/GPTResearcher.tsx
================================================
import './index.css';

import React from 'react';
import { useRef, useState, useEffect, useCallback } from "react";
import { useWebSocket } from '../hooks/useWebSocket';

import { Data, ChatBoxSettings, QuestionData } from '../types/data';
import { preprocessOrderedData } from '../utils/dataProcessing';
import { ResearchResults } from '../components/ResearchResults';

import Header from "../components/Header";
import Hero from "../components/Hero";
import Footer from "../components/Footer";
import InputArea from "../components/ResearchBlocks/elements/InputArea";
import HumanFeedback from "../components/HumanFeedback";
import LoadingDots from "../components/LoadingDots";

export interface GPTResearcherProps {
  apiUrl?: string;
  apiKey?: string;
  defaultPrompt?: string;
  onResultsChange?: (results: any) => void;
  theme?: any;
}

export const GPTResearcher = ({
  apiUrl = '',
  apiKey = '',
  defaultPrompt = '',
  onResultsChange,
  theme = {}
}: GPTResearcherProps) => {

  localStorage.setItem('apiURL', apiUrl);

  const [promptValue, setPromptValue] = useState(defaultPrompt);
  const [showResult, setShowResult] = useState(false);
  const [answer, setAnswer] = useState("");
  const [loading, setLoading] = useState(false);
  const [chatBoxSettings, setChatBoxSettings] = useState<ChatBoxSettings>({ 
    report_source: 'web', 
    report_type: 'research_report', 
    tone: 'Objective',
    domains: [],
    defaultReportType: 'research_report',
    layoutType: 'default',
    mcp_enabled: false,
    mcp_configs: [],
    mcp_strategy: 'fast',
  });
  const [question, setQuestion] = useState("");
  const [orderedData, setOrderedData] = useState<Data[]>([]);
  const [showHumanFeedback, setShowHumanFeedback] = useState(false);
  const [questionForHuman, setQuestionForHuman] = useState<true | false>(false);
  const [allLogs, setAllLogs] = useState<any[]>([]);
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const [isStopped, setIsStopped] = useState(false);
  const [showScrollButton, setShowScrollButton] = useState(false);
  const mainContentRef = useRef<HTMLDivElement>(null);

  // Store apiUrl in state to ensure consistency
  const [currentApiUrl, setCurrentApiUrl] = useState(apiUrl);

  // Update currentApiUrl when prop changes
  useEffect(() => {
    setCurrentApiUrl(apiUrl);
  }, [apiUrl]);

  // Update callback when results change
  useEffect(() => {
    if (onResultsChange && orderedData.length > 0) {
      onResultsChange(orderedData);
    }
  }, [orderedData, onResultsChange]);

  const { socket, initializeWebSocket } = useWebSocket(
    setOrderedData,
    setAnswer,
    setLoading,
    setShowHumanFeedback,
    setQuestionForHuman
  );

  const handleFeedbackSubmit = (feedback: string | null) => {
    if (socket) {
      socket.send(JSON.stringify({ type: 'human_feedback', content: feedback }));
    }
    setShowHumanFeedback(false);
  };

  const handleChat = async (message: string) => {
    setShowResult(true);
    setQuestion(message);
    setLoading(true);
    setPromptValue("");
    
    // Create a user message
    const userMessage = {
      role: 'user',
      content: message,
      timestamp: Date.now()
    };
    
    // Add question to display in research results
    const questionData: QuestionData = { type: 'question', content: message };
    setOrderedData(prevOrder => [...prevOrder, questionData]);
    
    try {
      // Format message to ensure it only contains role and content
      const formattedMessage = {
        role: userMessage.role,
        content: userMessage.content
      };
      
      // Call the chat API
      const apiBaseUrl = process.env.NEXT_PUBLIC_GPTR_API_URL || '';
      const response = await fetch(`${apiBaseUrl}/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          report: answer,
          messages: [formattedMessage]
        }),
      });
      
      if (!response.ok) {
        throw new Error('Failed to get chat response');
      }
      
      const data = await response.json();
      
      if (data.response) {
        // Add AI response to display in research results
        setOrderedData(prevOrder => [...prevOrder, { 
          type: 'chat', 
          content: data.response.content 
        }]);
      }
    } catch (error) {
      console.error('Error during chat:', error);
      
      // Add error message to display
      setOrderedData(prevOrder => [...prevOrder, { 
        type: 'chat', 
        content: 'Sorry, there was an error processing your request. Please try again.' 
      }]);
    } finally {
      setLoading(false);
    }
  };

  const handleDisplayResult = async (newQuestion: string) => {
    setShowResult(true);
    setLoading(true);
    setQuestion(newQuestion);
    setPromptValue("");
    setAnswer("");
    setOrderedData((prevOrder) => [...prevOrder, { type: 'question', content: newQuestion }]);

    const storedConfig = localStorage.getItem('apiVariables');
    const apiVariables = storedConfig ? JSON.parse(storedConfig) : { LANGGRAPH_HOST_URL: '' };
    
    // Use provided apiUrl if available
    if (apiUrl) {
      apiVariables.API_URL = apiUrl;
    }
    
    // Use provided apiKey if available
    if (apiKey) {
      apiVariables.API_KEY = apiKey;
    }
    
    initializeWebSocket(newQuestion, chatBoxSettings);

  };

  const reset = () => {
    setShowResult(false);
    setPromptValue("");
    setQuestion("");
    setAnswer("");
  };

  const handleClickSuggestion = (value: string) => {
    setPromptValue(value);
    const element = document.getElementById('input-area');
    if (element) {
      element.scrollIntoView({ behavior: 'smooth' });
    }
  };

  const handleStopResearch = () => {
    if (socket) {
      socket.close();
    }
    setLoading(false);
    setIsStopped(true);
    
    // Reload the page to completely reset the socket connection
    window.location.reload();
  };

  const handleStartNewResearch = () => {
    setShowResult(false);
    setPromptValue("");
    setIsStopped(false);
    
    setQuestion("");
    setAnswer("");
    setOrderedData([]);
    setAllLogs([]);
    
    setShowHumanFeedback(false);
    setQuestionForHuman(false);
    
    if (socket) {
      socket.close();
    }
    setLoading(false);
  };

  useEffect(() => {
    const groupedData = preprocessOrderedData(orderedData);
    const statusReports = ["agent_generated", "starting_research", "planning_research", "error"];
    
    const newLogs = groupedData.reduce((acc: any[], data) => {
      if (data.type === 'accordionBlock') {
        const logs = data.items.map((item: any, subIndex: any) => ({
          header: item.content,
          text: item.output,
          metadata: item.metadata,
          key: `${item.type}-${item.content}-${subIndex}`,
        }));
        return [...acc, ...logs];
      } 
      else if (statusReports.includes(data.content)) {
        return [...acc, {
          header: data.content,
          text: data.output,
          metadata: data.metadata,
          key: `${data.type}-${data.content}`,
        }];
      }
      return acc;
    }, []);
    
    setAllLogs(newLogs);
  }, [orderedData]);

  const handleScroll = useCallback(() => {
    const scrollPosition = window.scrollY + window.innerHeight;
    const nearBottom = scrollPosition >= document.documentElement.scrollHeight - 100;
    
    const isPageScrollable = document.documentElement.scrollHeight > window.innerHeight;
    setShowScrollButton(isPageScrollable && !nearBottom);
  }, []);

  useEffect(() => {
    const mainContentElement = mainContentRef.current;
    const resizeObserver = new ResizeObserver(() => {
      handleScroll();
    });

    if (mainContentElement) {
      resizeObserver.observe(mainContentElement);
    }

    window.addEventListener('scroll', handleScroll);
    window.addEventListener('resize', handleScroll);
    
    return () => {
      if (mainContentElement) {
        resizeObserver.unobserve(mainContentElement);
      }
      resizeObserver.disconnect();
      window.removeEventListener('scroll', handleScroll);
      window.removeEventListener('resize', handleScroll);
    };
  }, [handleScroll]);

  const scrollToBottom = () => {
    window.scrollTo({
      top: document.documentElement.scrollHeight,
      behavior: 'smooth'
    });
  };

  return (
    <>
      <Header 
        loading={loading}
        isStopped={isStopped}
        showResult={showResult}
        onStop={handleStopResearch}
        onNewResearch={handleStartNewResearch}
      />
      <main ref={mainContentRef} className="min-h-[100vh] pt-[70px]">
        {!showResult && (
          <Hero
            promptValue={promptValue}
            setPromptValue={setPromptValue}
            handleDisplayResult={handleDisplayResult}
          />
        )}

        {showResult && (
          <div className="flex h-full w-full grow flex-col justify-between">
            <div className="container w-full space-y-2">
              <div className="container space-y-2 task-components">
                <ResearchResults
                  orderedData={orderedData}
                  answer={answer}
                  allLogs={allLogs}
                  chatBoxSettings={chatBoxSettings}
                  handleClickSuggestion={handleClickSuggestion}
                  currentResearchId={undefined}
                />
              </div>

              {showHumanFeedback && (
                <HumanFeedback
                  questionForHuman={questionForHuman}
                  websocket={socket}
                  onFeedbackSubmit={handleFeedbackSubmit}
                />
              )}

              <div className="pt-1 sm:pt-2" ref={chatContainerRef}></div>
            </div>
            <div id="input-area" className="container px-4 lg:px-0">
              {loading ? (
                <LoadingDots />
              ) : (
                <InputArea
                  promptValue={promptValue}
                  setPromptValue={setPromptValue}
                  handleSubmit={handleChat}
                  disabled={loading}
                  reset={reset}
                  isStopped={isStopped}
                />
              )}
            </div>
          </div>
        )}
      </main>
      {showScrollButton && showResult && (
        <button
          onClick={scrollToBottom}
          className="fixed bottom-8 right-8 flex items-center justify-center w-12 h-12 text-gray-800	800	 bg-[rgb(168,85,247)] rounded-full hover:bg-[rgb(147,51,234)] transform hover:scale-105 transition-all duration-200 shadow-lg z-50"
        >
          <svg 
            xmlns="http://www.w3.org/2000/svg" 
            className="h-6 w-6" 
            fill="none" 
            viewBox="0 0 24 24" 
            stroke="currentColor"
          >
            <path 
              strokeLinecap="round" 
              strokeLinejoin="round" 
              strokeWidth={2} 
              d="M19 14l-7 7m0 0l-7-7m7 7V3" 
            />
          </svg>
        </button>
      )}
      <Footer setChatBoxSettings={setChatBoxSettings} chatBoxSettings={chatBoxSettings} />
    </>
  );
};

export default GPTResearcher;


================================================
FILE: frontend/nextjs/src/index.css
================================================
/* frontend/nextjs/src/index.css */
@import './styles/markdown.css';
@import './app/globals.css';
@import './components/Settings/Settings.css';

/* Include your Tailwind directives */
@tailwind base;
@tailwind components;
@tailwind utilities;


================================================
FILE: frontend/nextjs/src/index.d.ts
================================================
declare module 'gpt-researcher-ui' {
  import React from 'react';

  export interface GPTResearcherProps {
    apiUrl?: string;
    apiKey?: string;
    defaultPrompt?: string;
    onResultsChange?: (results: any) => void;
    theme?: any;
  }

  export const GPTResearcher: React.FC<GPTResearcherProps>;
}


================================================
FILE: frontend/nextjs/src/index.ts
================================================
// src/index.ts
import { GPTResearcher } from './GPTResearcher';

export { GPTResearcher };
export type { GPTResearcherProps } from './GPTResearcher';


================================================
FILE: frontend/nextjs/src/utils/imageTransformPlugin.js
================================================
// imageTransformPlugin.js
export default function imageTransformPlugin() {
  return {
    name: 'image-transform',
    transform(code) {
      // Add more patterns to catch different image path formats
      return code.replace(
        /['"]\/img\/([^'"]+)['"]/g,  // Also catch paths starting with /
        "'https://gptr.app/img/$1'"
      ).replace(
        /['"]img\/([^'"]+)['"]/g,    // Catch relative paths
        "'https://gptr.app/img/$1'"
      );
    }
  };
}


================================================
FILE: frontend/nextjs/styles/markdown.css
================================================
.markdown-content {
  /* Base styles */
  color: white;
  font-family: Georgia, 'Times New Roman', Times, serif;
  font-size: 18px;
  line-height: 1.6;

  /* Headings */
  h1, h2, h3, h4, h5, h6 {
    line-height: 1.2;
    font-weight: 500;
  }

  h1 { font-size: 2.5em; }
  h2 { font-size: 2em; }
  h3 { font-size: 1.5em; }
  h4 { font-size: 1.2em; }
  h5 { font-size: 1.1em; }
  h6 { font-size: 1em; }

  /* Paragraphs and spacing */
  p {
    margin: 0;
    line-height: 1.6;
  }

  /* Text formatting */
  strong, b {
    font-weight: 600;
  }

  em, i {
    font-style: italic;
  }

  /* Strikethrough - GFM feature */
  del, s {
    text-decoration: line-through;
  }

  /* Lists */
  ul, ol {
    margin: 0;
    padding-left: 2em;
    line-height: 1.6;
  }

  li {
    margin: 0;
    padding-left: 0.5em;
  }

  ul li {
    list-style-type: disc;
  }

  ol li {
    list-style-type: decimal;
  }

  /* Task lists - GFM feature */
  ul.contains-task-list {
    list-style-type: none;
    padding-left: 0;
  }

  .task-list-item {
    list-style-type: none;
    padding-left: 1.5em;
    position: relative;
  }

  .task-list-item-checkbox {
    position: absolute;
    left: 0;
    top: 0.25em;
    margin-right: 0.5em;
  }

  /* Links */
  a {
    color: rgb(20, 184, 166);
    text-decoration: underline;
    font-weight: 500;
    
    &:hover {
      opacity: 0.8;
      color: rgb(13, 148, 136);
    }
  }

  /* Code blocks */
  pre {
    background-color: #1e1e1e;
    padding: 1em;
    border-radius: 4px;
    overflow-x: auto;
    margin: 1em 0;
  }

  code {
    font-family: 'Courier New', Courier, monospace;
    font-size: 0.9em;
    padding: 0 0.4em;
    background-color: #1e1e1e;
    border-radius: 3px;
  }

  /* Blockquotes */
  blockquote {
    border-left: 4px solid rgb(20, 184, 166);
    margin: 0;
    padding-left: 1em;
    font-style: italic;
    background-color: rgba(20, 184, 166, 0.1);
    border-radius: 0 4px 4px 0;
  }

  /* Tables - Theme-matching styling */
  table {
    border-collapse: collapse;
    width: 100%;
    margin: 1em 0;
    background-color: #111827; /* Dark background matching the site theme */
    border-radius: 4px;
    overflow: hidden;
    border: 2px solid #4B5563; /* Slightly thicker and lighter border for better visibility */
    box-shadow: 0 0 0 1px rgba(20, 184, 166, 0.1); /* Changed from purple to teal */
  }

  th, td {
    border: 1px solid #4B5563; /* Lighter border color for better contrast */
    padding: 0.75em;
    text-align: left;
  }

  /* Add a slightly more visible outer border to the table */
  tr:first-child th {
    border-top: 2px solid #4B5563;
  }
  
  tr:last-child td {
    border-bottom: 2px solid #4B5563;
  }
  
  th:first-child, td:first-child {
    border-left: 2px solid #4B5563;
  }
  
  th:last-child, td:last-child {
    border-right: 2px solid #4B5563;
  }

  th {
    background-color: #1f2937; /* Darker header background */
    font-weight: 600;
    color: white; /* Changed from purple to white to match rest of content */
  }

  tr:nth-child(even) {
    background-color: #1a202c; /* Slightly lighter for alternating rows */
  }

  tr:hover {
    background-color: #282c34; /* Highlight on hover */
  }

  /* Horizontal rule */
  hr {
    border: 0;
    border-top: 1px solid #444;
    margin: 1.5em 0;
  }

  /* Images */
  img {
    max-width: 100%;
    height: auto;
    border-radius: 4px;
    margin: 1em 0;
  }

  /* Definition Lists */
  dl {
    margin: 1em 0;
  }

  dt {
    font-weight: bold;
    margin-top: 0.5em;
  }

  dd {
    margin-left: 2em;
    margin-bottom: 0.5em;
  }
} 


================================================
FILE: frontend/nextjs/types/data.ts
================================================
export interface BaseData {
  type: string;
}

export interface BasicData extends BaseData {
  type: 'basic';
  content: string;
}

export interface LanggraphButtonData extends BaseData {
  type: 'langgraphButton';
  link: string;
}

export interface DifferencesData extends BaseData {
  type: 'differences';
  content: string;
  output: string;
}

export interface QuestionData extends BaseData {
  type: 'question';
  content: string;
}

export interface ChatData extends BaseData {
  type: 'chat';
  content: string;
  metadata?: any; // For storing search results and other contextual information
}

export type Data = BasicData | LanggraphButtonData | DifferencesData | QuestionData | ChatData;

export interface MCPConfig {
  name: string;
  command: string;
  args: string[];
  env: Record<string, string>;
}

export interface ChatBoxSettings {
  report_type: string;
  report_source: string;
  tone: string;
  domains: string[];
  defaultReportType: string;
  layoutType: string;
  mcp_enabled: boolean;
  mcp_configs: MCPConfig[];
  mcp_strategy?: string;
}

export interface Domain {
  value: string;
}

export interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
  timestamp?: number;
  metadata?: any; // For storing search results and other contextual information
}

export interface ResearchHistoryItem {
  id: string;
  question: string;
  answer: string;
  timestamp: number;
  orderedData: Data[];
  chatMessages?: ChatMessage[];
} 


================================================
FILE: frontend/nextjs/types/react-ga4.d.ts
================================================
declare module 'react-ga4' {
    export interface InitOptions {
      gaOptions?: any;
      gtagOptions?: any;
      testMode?: boolean;
    }
  
    export function initialize(
      measurementId: string | string[],
      options?: InitOptions
    ): void;
  
    export function event(options: {
      category: string;
      action: string;
      label?: string;
      value?: number;
      nonInteraction?: boolean;
      transport?: 'beacon' | 'xhr' | 'image';
      [key: string]: any;
    }): void;
  
    // Add other methods as needed
    export default {
      initialize,
      event
    };
  }


================================================
FILE: frontend/nextjs/utils/consolidateBlocks.ts
================================================
export const consolidateSourceAndImageBlocks = (groupedData: any[]) => {
  // Consolidate sourceBlocks
  const consolidatedSourceBlock = {
    type: 'sourceBlock',
    items: groupedData
      .filter(item => item.type === 'sourceBlock')
      .flatMap(block => block.items || [])
      .filter((item, index, self) => 
        index === self.findIndex(t => t.url === item.url)
      )
  };

  // Consolidate imageBlocks
  const consolidatedImageBlock = {
    type: 'imagesBlock',
    metadata: groupedData
      .filter(item => item.type === 'imagesBlock')
      .flatMap(block => block.metadata || [])
  };

  // Remove all existing sourceBlocks and imageBlocks
  groupedData = groupedData.filter(item => 
    item.type !== 'sourceBlock' && item.type !== 'imagesBlock'
  );

  // Add consolidated blocks if they have items
  if (consolidatedSourceBlock.items.length > 0) {
    groupedData.push(consolidatedSourceBlock);
  }
  if (consolidatedImageBlock.metadata.length > 0) {
    groupedData.push(consolidatedImageBlock);
  }

  return groupedData;
};


================================================
FILE: frontend/nextjs/utils/dataProcessing.ts
================================================
import { Data } from '../types/data';
import { consolidateSourceAndImageBlocks } from './consolidateBlocks';

export const preprocessOrderedData = (data: Data[]) => {
  let groupedData: any[] = [];
  let currentAccordionGroup: any = null;
  let currentSourceGroup: any = null;
  let currentReportGroup: any = null;
  let finalReportGroup: any = null;
  let sourceBlockEncountered = false;
  let lastSubqueriesIndex = -1;
  const seenUrls = new Set<string>();
  // console.log('websocket data before its processed',data)

  data.forEach((item: any) => {
    const { type, content, metadata, output, link } = item;

    if (type === 'question') {
      groupedData.push({ type: 'question', content });
    } else if (type === 'report') {
      // Start a new report group if we don't have one
      if (!currentReportGroup) {
        currentReportGroup = { type: 'reportBlock', content: '' };
        groupedData.push(currentReportGroup);
      }
      currentReportGroup.content += output;
    } else if (content === 'selected_images') {
      groupedData.push({ type: 'imagesBlock', metadata });
    } else if (type === 'logs' && content === 'research_report') {
      if (!finalReportGroup) {
        finalReportGroup = { type: 'reportBlock', content: '' };
        groupedData.push(finalReportGroup);
      }
      finalReportGroup.content += output.report;
    } else if (type === 'langgraphButton') {
      groupedData.push({ type: 'langgraphButton', link });
    } else if (type === 'chat') {
      groupedData.push({ type: 'chat', content: content });
    } else {
      if (currentReportGroup) {
        currentReportGroup = null;
      }

      if (content === 'subqueries') {
        if (currentAccordionGroup) {
          currentAccordionGroup = null;
        }
        if (currentSourceGroup) {
          groupedData.push(currentSourceGroup);
          currentSourceGroup = null;
        }
        groupedData.push(item);
        lastSubqueriesIndex = groupedData.length - 1;
      } else if (type === 'sourceBlock') {
        currentSourceGroup = item;
        if (lastSubqueriesIndex !== -1) {
          groupedData.splice(lastSubqueriesIndex + 1, 0, currentSourceGroup);
          lastSubqueriesIndex = -1;
        } else {
          groupedData.push(currentSourceGroup);
        }
        sourceBlockEncountered = true;
        currentSourceGroup = null;
      } else if (content === 'added_source_url') {
        if (!currentSourceGroup) {
          currentSourceGroup = { type: 'sourceBlock', items: [] };
        }
      
        if (!seenUrls.has(metadata)) {
          seenUrls.add(metadata);
          let hostname = "";
          try {
            if (typeof metadata === 'string') {
              hostname = new URL(metadata).hostname.replace('www.', '');
            }
          } catch (e) {
            hostname = "unknown";
          }
          currentSourceGroup.items.push({ name: hostname, url: metadata });
        }
      
        // Add this block to ensure the source group is added to groupedData
        if (currentSourceGroup.items.length > 0 && !groupedData.includes(currentSourceGroup)) {
          groupedData.push(currentSourceGroup);
          sourceBlockEncountered = true;
        }
      } else if (type !== 'path' && content !== '') {
        if (sourceBlockEncountered) {
          if (!currentAccordionGroup) {
            currentAccordionGroup = { type: 'accordionBlock', items: [] };
            groupedData.push(currentAccordionGroup);
          }
          currentAccordionGroup.items.push(item);
        } else {
          groupedData.push(item);
        }
      } else {
        if (currentAccordionGroup) {
          currentAccordionGroup = null;
        }
        if (currentSourceGroup) {
          currentSourceGroup = null;
        }
        if (currentReportGroup) {
          // Find and remove the previous reportBlock
          const reportBlockIndex = groupedData.findIndex(
            item => item === currentReportGroup
          );
          if (reportBlockIndex !== -1) {
            groupedData.splice(reportBlockIndex, 1);
          }
          currentReportGroup = null;  // Reset the current report group
        }
        groupedData.push(item);
      }
    }
  });

  groupedData = consolidateSourceAndImageBlocks(groupedData);
  return groupedData;
}; 


================================================
FILE: frontend/nextjs/utils/getLayout.tsx
================================================
import React, { useState, useEffect } from 'react';
import ResearchPageLayout from '@/components/layouts/ResearchPageLayout';
import CopilotLayout from '@/components/layouts/CopilotLayout';
import MobileLayout from '@/components/layouts/MobileLayout';
import { ChatBoxSettings } from '@/types/data';

interface LayoutProps {
  children: React.ReactNode;
  loading: boolean;
  isStopped: boolean;
  showResult: boolean;
  onStop?: () => void;
  onNewResearch?: () => void;
  chatBoxSettings: ChatBoxSettings;
  setChatBoxSettings: React.Dispatch<React.SetStateAction<ChatBoxSettings>>;
  mainContentRef?: React.RefObject<HTMLDivElement>;
  showScrollButton?: boolean;
  onScrollToBottom?: () => void;
  toastOptions?: Record<string, any>;
  toggleSidebar?: () => void;
  isProcessingChat?: boolean;
}

export const getAppropriateLayout = ({
  children,
  loading,
  isStopped,
  showResult,
  onStop,
  onNewResearch,
  chatBoxSettings,
  setChatBoxSettings,
  mainContentRef,
  showScrollButton = false,
  onScrollToBottom,
  toastOptions = {},
  toggleSidebar,
  isProcessingChat = false
}: LayoutProps) => {
  const [isMobile, setIsMobile] = useState(false);
  
  // Check if we're on mobile on client-side
  useEffect(() => {
    const checkIfMobile = () => {
      setIsMobile(window.innerWidth < 768);
    };
    
    // Initial check
    checkIfMobile();
    
    // Add event listener for window resize
    window.addEventListener('resize', checkIfMobile);
    
    // Cleanup
    return () => window.removeEventListener('resize', checkIfMobile);
  }, []);
  
  // If on mobile, use the mobile layout
  if (isMobile) {
    return (
      <MobileLayout
        loading={loading}
        isStopped={isStopped}
        showResult={showResult}
        onStop={onStop}
        onNewResearch={onNewResearch}
        chatBoxSettings={chatBoxSettings}
        setChatBoxSettings={setChatBoxSettings}
        mainContentRef={mainContentRef}
        toastOptions={toastOptions}
        toggleSidebar={toggleSidebar}
      >
        {children}
      </MobileLayout>
    );
  }
  
  // For desktop, use either the copilot or research layout based on settings
  if (chatBoxSettings.layoutType === 'copilot') {
    return (
      <CopilotLayout
        loading={loading}
        isStopped={isStopped}
        showResult={showResult}
        onStop={onStop}
        onNewResearch={onNewResearch}
        chatBoxSettings={chatBoxSettings}
        setChatBoxSettings={setChatBoxSettings}
        mainContentRef={mainContentRef}
        toastOptions={toastOptions}
        toggleSidebar={toggleSidebar}
      >
        {children}
      </CopilotLayout>
    );
  }
  
  // Default to ResearchPageLayout for desktop with standard layout
  return (
    <ResearchPageLayout
      loading={loading}
      isStopped={isStopped}
      showResult={showResult}
      onStop={onStop}
      onNewResearch={onNewResearch || (() => {})}
      chatBoxSettings={chatBoxSettings}
      setChatBoxSettings={setChatBoxSettings}
      mainContentRef={mainContentRef}
      showScrollButton={showScrollButton}
      onScrollToBottom={onScrollToBottom}
      toastOptions={toastOptions}
    >
      {children}
    </ResearchPageLayout>
  );
}; 


================================================
FILE: frontend/static/defaultAgentAvatar.JPG
================================================
[Non-text file]


================================================
FILE: gpt_researcher/__init__.py
================================================
from .agent import GPTResearcher

__all__ = ['GPTResearcher']


================================================
FILE: gpt_researcher/agent.py
================================================
from typing import Any, Optional
import json
import os

from .config import Config
from .memory import Memory
from .utils.enum import ReportSource, ReportType, Tone
from .llm_provider import GenericLLMProvider
from .prompts import get_prompt_family
from .vector_store import VectorStoreWrapper

# Research skills
from .skills.researcher import ResearchConductor
from .skills.writer import ReportGenerator
from .skills.context_manager import ContextManager
from .skills.browser import BrowserManager
from .skills.curator import SourceCurator
from .skills.deep_research import DeepResearchSkill

from .actions import (
    add_references,
    extract_headers,
    extract_sections,
    table_of_contents,
    get_search_results,
    get_retrievers,
    choose_agent
)


class GPTResearcher:
    def __init__(
        self,
        query: str,
        report_type: str = ReportType.ResearchReport.value,
        report_format: str = "markdown",
        report_source: str = ReportSource.Web.value,
        tone: Tone = Tone.Objective,
        source_urls: list[str] | None = None,
        document_urls: list[str] | None = None,
        complement_source_urls: bool = False,
        query_domains: list[str] | None = None,
        documents=None,
        vector_store=None,
        vector_store_filter=None,
        config_path=None,
        websocket=None,
        agent=None,
        role=None,
        parent_query: str = "",
        subtopics: list | None = None,
        visited_urls: set | None = None,
        verbose: bool = True,
        context=None,
        headers: dict | None = None,
        max_subtopics: int = 5,
        log_handler=None,
        prompt_family: str | None = None,
        mcp_configs: list[dict] | None = None,
        mcp_max_iterations: int | None = None,
        mcp_strategy: str | None = None,
        **kwargs
    ):
        """
        Initialize a GPT Researcher instance.
        
        Args:
            query (str): The research query or question.
            report_type (str): Type of report to generate.
            report_format (str): Format of the report (markdown, pdf, etc).
            report_source (str): Source of information for the report (web, local, etc).
            tone (Tone): Tone of the report.
            source_urls (list[str], optional): List of specific URLs to use as sources.
            document_urls (list[str], optional): List of document URLs to use as sources.
            complement_source_urls (bool): Whether to complement source URLs with web search.
            query_domains (list[str], optional): List of domains to restrict search to.
            documents: Document objects for LangChain integration.
            vector_store: Vector store for document retrieval.
            vector_store_filter: Filter for vector store queries.
            config_path: Path to configuration file.
            websocket: WebSocket for streaming output.
            agent: Pre-defined agent type.
            role: Pre-defined agent role.
            parent_query: Parent query for subtopic reports.
            subtopics: List of subtopics to research.
            visited_urls: Set of already visited URLs.
            verbose (bool): Whether to output verbose logs.
            context: Pre-loaded research context.
            headers (dict, optional): Additional headers for requests and configuration.
            max_subtopics (int): Maximum number of subtopics to generate.
            log_handler: Handler for logging events.
            prompt_family: Family of prompts to use.
            mcp_configs (list[dict], optional): List of MCP server configurations.
                Each dictionary can contain:
                - name (str): Name of the MCP server
                - command (str): Command to start the server
                - args (list[str]): Arguments for the server command
                - tool_name (str): Specific tool to use on the MCP server
                - env (dict): Environment variables for the server
                - connection_url (str): URL for WebSocket or HTTP connection
                - connection_type (str): Connection type (stdio, websocket, http)
                - connection_token (str): Authentication token for remote connections
                
                Example:
                ```python
                mcp_configs=[{
                    "command": "python",
                    "args": ["my_mcp_server.py"],
                    "name": "search"
                }]
                ```
            mcp_strategy (str, optional): MCP execution strategy. Options:
                - "fast" (default): Run MCP once with original query for best performance
                - "deep": Run MCP for all sub-queries for maximum thoroughness  
                - "disabled": Skip MCP entirely, use only web retrievers
        """
        self.kwargs = kwargs
        self.query = query
        self.report_type = report_type
        self.cfg = Config(config_path)
        self.cfg.set_verbose(verbose)
        self.report_source = report_source if report_source else getattr(self.cfg, 'report_source', None)
        self.report_format = report_format
        self.max_subtopics = max_subtopics
        self.tone = tone if isinstance(tone, Tone) else Tone.Objective
        self.source_urls = source_urls
        self.document_urls = document_urls
        self.complement_source_urls = complement_source_urls
        self.query_domains = query_domains or []
        self.research_sources = []  # The list of scraped sources including title, content and images
        self.research_images = []  # The list of selected research images
        self.documents = documents
        self.vector_store = VectorStoreWrapper(vector_store) if vector_store else None
        self.vector_store_filter = vector_store_filter
        self.websocket = websocket
        self.agent = agent
        self.role = role
        self.parent_query = parent_query
        self.subtopics = subtopics or []
        self.visited_urls = visited_urls or set()
        self.verbose = verbose
        self.context = context or []
        self.headers = headers or {}
        self.research_costs = 0.0
        self.log_handler = log_handler
        self.prompt_family = get_prompt_family(prompt_family or self.cfg.prompt_family, self.cfg)
        
        # Process MCP configurations if provided
        self.mcp_configs = mcp_configs
        if mcp_configs:
            self._process_mcp_configs(mcp_configs)
        
        self.retrievers = get_retrievers(self.headers, self.cfg)
        self.memory = Memory(
            self.cfg.embedding_provider, self.cfg.embedding_model, **self.cfg.embedding_kwargs
        )
        
        # Set default encoding to utf-8
        self.encoding = kwargs.get('encoding', 'utf-8')
        self.kwargs.pop('encoding', None)  # Remove encoding from kwargs to avoid passing it to LLM calls

        # Initialize components
        self.research_conductor: ResearchConductor = ResearchConductor(self)
        self.report_generator: ReportGenerator = ReportGenerator(self)
        self.context_manager: ContextManager = ContextManager(self)
        self.scraper_manager: BrowserManager = BrowserManager(self)
        self.source_curator: SourceCurator = SourceCurator(self)
        self.deep_researcher: Optional[DeepResearchSkill] = None
        if report_type == ReportType.DeepResearch.value:
            self.deep_researcher = DeepResearchSkill(self)

        # Handle MCP strategy configuration with backwards compatibility
        self.mcp_strategy = self._resolve_mcp_strategy(mcp_strategy, mcp_max_iterations)

    def _resolve_mcp_strategy(self, mcp_strategy: str | None, mcp_max_iterations: int | None) -> str:
        """
        Resolve MCP strategy from various sources with backwards compatibility.
        
        Priority:
        1. Parameter mcp_strategy (new approach)
        2. Parameter mcp_max_iterations (backwards compatibility)  
        3. Config MCP_STRATEGY
        4. Default "fast"
        
        Args:
            mcp_strategy: New strategy parameter
            mcp_max_iterations: Legacy parameter for backwards compatibility
            
        Returns:
            str: Resolved strategy ("fast", "deep", or "disabled")
        """
        # Priority 1: Use mcp_strategy parameter if provided
        if mcp_strategy is not None:
            # Support new strategy names
            if mcp_strategy in ["fast", "deep", "disabled"]:
                return mcp_strategy
            # Support old strategy names for backwards compatibility
            elif mcp_strategy == "optimized":
                import logging
                logging.getLogger(__name__).warning("mcp_strategy 'optimized' is deprecated, use 'fast' instead")
                return "fast"
            elif mcp_strategy == "comprehensive":
                import logging
                logging.getLogger(__name__).warning("mcp_strategy 'comprehensive' is deprecated, use 'deep' instead")
                return "deep"
            else:
                import logging
                logging.getLogger(__name__).warning(f"Invalid mcp_strategy '{mcp_strategy}', defaulting to 'fast'")
                return "fast"
        
        # Priority 2: Convert mcp_max_iterations for backwards compatibility
        if mcp_max_iterations is not None:
            import logging
            logging.getLogger(__name__).warning("mcp_max_iterations is deprecated, use mcp_strategy instead")
            
            if mcp_max_iterations == 0:
                return "disabled"
            elif mcp_max_iterations == 1:
                return "fast"
            elif mcp_max_iterations == -1:
                return "deep"
            else:
                # Treat any other number as fast mode
                return "fast"
        
        # Priority 3: Use config setting
        if hasattr(self.cfg, 'mcp_strategy'):
            config_strategy = self.cfg.mcp_strategy
            # Support new strategy names
            if config_strategy in ["fast", "deep", "disabled"]:
                return config_strategy
            # Support old strategy names for backwards compatibility
            elif config_strategy == "optimized":
                return "fast"
            elif config_strategy == "comprehensive":
                return "deep"
            
        # Priority 4: Default to fast
        return "fast"

    def _process_mcp_configs(self, mcp_configs: list[dict]) -> None:
        """
        Process MCP configurations from a list of configuration dictionaries.
        
        This method validates the MCP configurations. It only adds MCP to retrievers
        if no explicit retriever configuration is provided via environment variables.
        
        Args:
            mcp_configs (list[dict]): List of MCP server configuration dictionaries.
        """
        # Check if user explicitly set RETRIEVER environment variable
        user_set_retriever = os.getenv("RETRIEVER") is not None
        
        if not user_set_retriever:
            # Only auto-add MCP if user hasn't explicitly set retrievers
            if hasattr(self.cfg, 'retrievers') and self.cfg.retrievers:
                # If retrievers is set in config (but not via env var)
                current_retrievers = set(self.cfg.retrievers.split(",")) if isinstance(self.cfg.retrievers, str) else set(self.cfg.retrievers)
                if "mcp" not in current_retrievers:
                    current_retrievers.add("mcp")
                    self.cfg.retrievers = ",".join(filter(None, current_retrievers))
            else:
                # No retrievers configured, use mcp as default
                self.cfg.retrievers = "mcp"
        # If user explicitly set RETRIEVER, respect their choice and don't auto-add MCP
        
        # Store the mcp_configs for use by the MCP retriever
        self.mcp_configs = mcp_configs

    async def _log_event(self, event_type: str, **kwargs):
        """Helper method to handle logging events"""
        if self.log_handler:
            try:
                if event_type == "tool":
                    await self.log_handler.on_tool_start(kwargs.get('tool_name', ''), **kwargs)
                elif event_type == "action":
                    await self.log_handler.on_agent_action(kwargs.get('action', ''), **kwargs)
                elif event_type == "research":
                    await self.log_handler.on_research_step(kwargs.get('step', ''), kwargs.get('details', {}))

                # Add direct logging as backup
                import logging
                research_logger = logging.getLogger('research')
                research_logger.info(f"{event_type}: {json.dumps(kwargs, default=str)}")

            except Exception as e:
                import logging
                logging.getLogger('research').error(f"Error in _log_event: {e}", exc_info=True)

    async def conduct_research(self, on_progress=None):
        await self._log_event("research", step="start", details={
            "query": self.query,
            "report_type": self.report_type,
            "agent": self.agent,
            "role": self.role
        })

        # Handle deep research separately
        if self.report_type == ReportType.DeepResearch.value and self.deep_researcher:
            return await self._handle_deep_research(on_progress)

        if not (self.agent and self.role):
            await self._log_event("action", action="choose_agent")
            # Filter out encoding parameter as it's not supported by LLM APIs
            # filtered_kwargs = {k: v for k, v in self.kwargs.items() if k != 'encoding'}
            self.agent, self.role = await choose_agent(
                query=self.query,
                cfg=self.cfg,
                parent_query=self.parent_query,
                cost_callback=self.add_costs,
                headers=self.headers,
                prompt_family=self.prompt_family,
                **self.kwargs,
                # **filtered_kwargs
            )
            await self._log_event("action", action="agent_selected", details={
                "agent": self.agent,
                "role": self.role
            })

        await self._log_event("research", step="conducting_research", details={
            "agent": self.agent,
            "role": self.role
        })
        self.context = await self.research_conductor.conduct_research()

        await self._log_event("research", step="research_completed", details={
            "context_length": len(self.context)
        })
        return self.context

    async def _handle_deep_research(self, on_progress=None):
        """Handle deep research execution and logging."""
        # Log deep research configuration
        await self._log_event("research", step="deep_research_initialize", details={
            "type": "deep_research",
            "breadth": self.deep_researcher.breadth,
            "depth": self.deep_researcher.depth,
            "concurrency": self.deep_researcher.concurrency_limit
        })

        # Log deep research start
        await self._log_event("research", step="deep_research_start", details={
            "query": self.query,
            "breadth": self.deep_researcher.breadth,
            "depth": self.deep_researcher.depth,
            "concurrency": self.deep_researcher.concurrency_limit
        })

        # Run deep research and get context
        self.context = await self.deep_researcher.run(on_progress=on_progress)

        # Get total research costs
        total_costs = self.get_costs()

        # Log deep research completion with costs
        await self._log_event("research", step="deep_research_complete", details={
            "context_length": len(self.context),
            "visited_urls": len(self.visited_urls),
            "total_costs": total_costs
        })

        # Log final cost update
        await self._log_event("research", step="cost_update", details={
            "cost": total_costs,
            "total_cost": total_costs,
            "research_type": "deep_research"
        })

        # Return the research context
        return self.context

    async def write_report(self, existing_headers: list = [], relevant_written_contents: list = [], ext_context=None, custom_prompt="") -> str:
        await self._log_event("research", step="writing_report", details={
            "existing_headers": existing_headers,
            "context_source": "external" if ext_context else "internal"
        })

        report = await self.report_generator.write_report(
            existing_headers=existing_headers,
            relevant_written_contents=relevant_written_contents,
            ext_context=ext_context or self.context,
            custom_prompt=custom_prompt
        )

        await self._log_event("research", step="report_completed", details={
            "report_length": len(report)
        })
        return report

    async def write_report_conclusion(self, report_body: str) -> str:
        await self._log_event("research", step="writing_conclusion")
        conclusion = await self.report_generator.write_report_conclusion(report_body)
        await self._log_event("research", step="conclusion_completed")
        return conclusion

    async def write_introduction(self):
        await self._log_event("research", step="writing_introduction")
        intro = await self.report_generator.write_introduction()
        await self._log_event("research", step="introduction_completed")
        return intro

    async def quick_search(self, query: str, query_domains: list[str] = None) -> list[Any]:
        return await get_search_results(query, self.retrievers[0], query_domains=query_domains)

    async def get_subtopics(self):
        return await self.report_generator.get_subtopics()

    async def get_draft_section_titles(self, current_subtopic: str):
        return await self.report_generator.get_draft_section_titles(current_subtopic)

    async def get_similar_written_contents_by_draft_section_titles(
        self,
        current_subtopic: str,
        draft_section_titles: list[str],
        written_contents: list[dict],
        max_results: int = 10
    ) -> list[str]:
        return await self.context_manager.get_similar_written_contents_by_draft_section_titles(
            current_subtopic,
            draft_section_titles,
            written_contents,
            max_results
        )

    # Utility methods
    def get_research_images(self, top_k=10) -> list[dict[str, Any]]:
        return self.research_images[:top_k]

    def add_research_images(self, images: list[dict[str, Any]]) -> None:
        self.research_images.extend(images)

    def get_research_sources(self) -> list[dict[str, Any]]:
        return self.research_sources

    def add_research_sources(self, sources: list[dict[str, Any]]) -> None:
        self.research_sources.extend(sources)

    def add_references(self, report_markdown: str, visited_urls: set) -> str:
        return add_references(report_markdown, visited_urls)

    def extract_headers(self, markdown_text: str) -> list[dict]:
        return extract_headers(markdown_text)

    def extract_sections(self, markdown_text: str) -> list[dict]:
        return extract_sections(markdown_text)

    def table_of_contents(self, markdown_text: str) -> str:
        return table_of_contents(markdown_text)

    def get_source_urls(self) -> list:
        return list(self.visited_urls)

    def get_research_context(self) -> list:
        return self.context

    def get_costs(self) -> float:
        return self.research_costs

    def set_verbose(self, verbose: bool):
        self.verbose = verbose

    def add_costs(self, cost: float) -> None:
        if not isinstance(cost, (float, int)):
            raise ValueError("Cost must be an integer or float")
        self.research_costs += cost
        if self.log_handler:
            self._log_event("research", step="cost_update", details={
                "cost": cost,
                "total_cost": self.research_costs
            })



================================================
FILE: gpt_researcher/prompts.py
================================================
import warnings
from datetime import date, datetime, timezone

from langchain.docstore.document import Document

from .config import Config
from .utils.enum import ReportSource, ReportType, Tone
from .utils.enum import PromptFamily as PromptFamilyEnum
from typing import Callable, List, Dict, Any


## Prompt Families #############################################################

class PromptFamily:
    """General purpose class for prompt formatting.

    This may be overwritten with a derived class that is model specific. The
    methods are broken down into two groups:

    1. Prompt Generators: These follow a standard format and are correlated with
        the ReportType enum. They should be accessed via
        get_prompt_by_report_type

    2. Prompt Methods: These are situation-specific methods that do not have a
        standard signature and are accessed directly in the agent code.

    All derived classes must retain the same set of method names, but may
    override individual methods.
    """

    def __init__(self, config: Config):
        """Initialize with a config instance. This may be used by derived
        classes to select the correct prompting based on configured models and/
        or providers
        """
        self.cfg = config

    # MCP-specific prompts
    @staticmethod
    def generate_mcp_tool_selection_prompt(query: str, tools_info: List[Dict], max_tools: int = 3) -> str:
        """
        Generate prompt for LLM-based MCP tool selection.
        
        Args:
            query: The research query
            tools_info: List of available tools with their metadata
            max_tools: Maximum number of tools to select
            
        Returns:
            str: The tool selection prompt
        """
        import json
        
        return f"""You are a research assistant helping to select the most relevant tools for a research query.

RESEARCH QUERY: "{query}"

AVAILABLE TOOLS:
{json.dumps(tools_info, indent=2)}

TASK: Analyze the tools and select EXACTLY {max_tools} tools that are most relevant for researching the given query.

SELECTION CRITERIA:
- Choose tools that can provide information, data, or insights related to the query
- Prioritize tools that can search, retrieve, or access relevant content
- Consider tools that complement each other (e.g., different data sources)
- Exclude tools that are clearly unrelated to the research topic

Return a JSON object with this exact format:
{{
  "selected_tools": [
    {{
      "index": 0,
      "name": "tool_name",
      "relevance_score": 9,
      "reason": "Detailed explanation of why this tool is relevant"
    }}
  ],
  "selection_reasoning": "Overall explanation of the selection strategy"
}}

Select exactly {max_tools} tools, ranked by relevance to the research query.
"""

    @staticmethod
    def generate_mcp_research_prompt(query: str, selected_tools: List) -> str:
        """
        Generate prompt for MCP research execution with selected tools.
        
        Args:
            query: The research query
            selected_tools: List of selected MCP tools
            
        Returns:
            str: The research execution prompt
        """
        # Handle cases where selected_tools might be strings or objects with .name attribute
        tool_names = []
        for tool in selected_tools:
            if hasattr(tool, 'name'):
                tool_names.append(tool.name)
            else:
                tool_names.append(str(tool))
        
        return f"""You are a research assistant with access to specialized tools. Your task is to research the following query and provide comprehensive, accurate information.

RESEARCH QUERY: "{query}"

INSTRUCTIONS:
1. Use the available tools to gather relevant information about the query
2. Call multiple tools if needed to get comprehensive coverage
3. If a tool call fails or returns empty results, try alternative approaches
4. Synthesize information from multiple sources when possible
5. Focus on factual, relevant information that directly addresses the query

AVAILABLE TOOLS: {tool_names}

Please conduct thorough research and provide your findings. Use the tools strategically to gather the most relevant and comprehensive information."""

    @staticmethod
    def generate_search_queries_prompt(
        question: str,
        parent_query: str,
        report_type: str,
        max_iterations: int = 3,
        context: List[Dict[str, Any]] = [],
    ):
        """Generates the search queries prompt for the given question.
        Args:
            question (str): The question to generate the search queries prompt for
            parent_query (str): The main question (only relevant for detailed reports)
            report_type (str): The report type
            max_iterations (int): The maximum number of search queries to generate
            context (str): Context for better understanding of the task with realtime web information

        Returns: str: The search queries prompt for the given question
        """

        if (
            report_type == ReportType.DetailedReport.value
            or report_type == ReportType.SubtopicReport.value
        ):
            task = f"{parent_query} - {question}"
        else:
            task = question

        context_prompt = f"""
You are a seasoned research assistant tasked with generating search queries to find relevant information for the following task: "{task}".
Context: {context}

Use this context to inform and refine your search queries. The context provides real-time web information that can help you generate more specific and relevant queries. Consider any current events, recent developments, or specific details mentioned in the context that could enhance the search queries.
""" if context else ""

        dynamic_example = ", ".join([f'"query {i+1}"' for i in range(max_iterations)])

        return f"""Write {max_iterations} google search queries to search online that form an objective opinion from the following task: "{task}"

Assume the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')} if required.

{context_prompt}
You must respond with a list of strings in the following format: [{dynamic_example}].
The response should contain ONLY the list.
"""

    @staticmethod
    def generate_report_prompt(
        question: str,
        context,
        report_source: str,
        report_format="apa",
        total_words=1000,
        tone=None,
        language="english",
    ):
        """Generates the report prompt for the given question and research summary.
        Args: question (str): The question to generate the report prompt for
                research_summary (str): The research summary to generate the report prompt for
        Returns: str: The report prompt for the given question and research summary
        """

        reference_prompt = ""
        if report_source == ReportSource.Web.value:
            reference_prompt = f"""
You MUST write all used source urls at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.
Every url should be hyperlinked: [url website](url)
Additionally, you MUST include hyperlinks to the relevant URLs wherever they are referenced in the report:

eg: Author, A. A. (Year, Month Date). Title of web page. Website Name. [url website](url)
"""
        else:
            reference_prompt = f"""
You MUST write all used source document names at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each."
"""

        tone_prompt = f"Write the report in a {tone.value} tone." if tone else ""

        return f"""
Information: "{context}"
---
Using the above information, answer the following query or task: "{question}" in a detailed report --
The report should focus on the answer to the query, should be well structured, informative,
in-depth, and comprehensive, with facts and numbers if available and at least {total_words} words.
You should strive to write the report as long as you can using all relevant and necessary information provided.

Please follow all of the following guidelines in your report:
- You MUST determine your own concrete and valid opinion based on the given information. Do NOT defer to general and meaningless conclusions.
- You MUST write the report with markdown syntax and {report_format} format.
- Structure your report with clear markdown headers: use # for the main title, ## for major sections, and ### for subsections.
- Use markdown tables when presenting structured data or comparisons to enhance readability.
- You MUST prioritize the relevance, reliability, and significance of the sources you use. Choose trusted sources over less reliable ones.
- You must also prioritize new articles over older articles if the source can be trusted.
- You MUST NOT include a table of contents, but DO include proper markdown headers (# ## ###) to structure your report clearly.
- Use in-text citation references in {report_format} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).
- Don't forget to add a reference list at the end of the report in {report_format} format and full url links without hyperlinks.
- {reference_prompt}
- {tone_prompt}

You MUST write the report in the following language: {language}.
Please do your best, this is very important to my career.
Assume that the current date is {date.today()}.
"""

    @staticmethod
    def curate_sources(query, sources, max_results=10):
        return f"""Your goal is to evaluate and curate the provided scraped content for the research task: "{query}"
    while prioritizing the inclusion of relevant and high-quality information, especially sources containing statistics, numbers, or concrete data.

The final curated list will be used as context for creating a research report, so prioritize:
- Retaining as much original information as possible, with extra emphasis on sources featuring quantitative data or unique insights
- Including a wide range of perspectives and insights
- Filtering out only clearly irrelevant or unusable content

EVALUATION GUIDELINES:
1. Assess each source based on:
   - Relevance: Include sources directly or partially connected to the research query. Err on the side of inclusion.
   - Credibility: Favor authoritative sources but retain others unless clearly untrustworthy.
   - Currency: Prefer recent information unless older data is essential or valuable.
   - Objectivity: Retain sources with bias if they provide a unique or complementary perspective.
   - Quantitative Value: Give higher priority to sources with statistics, numbers, or other concrete data.
2. Source Selection:
   - Include as many relevant sources as possible, up to {max_results}, focusing on broad coverage and diversity.
   - Prioritize sources with statistics, numerical data, or verifiable facts.
   - Overlapping content is acceptable if it adds depth, especially when data is involved.
   - Exclude sources only if they are entirely irrelevant, severely outdated, or unusable due to poor content quality.
3. Content Retention:
   - DO NOT rewrite, summarize, or condense any source content.
   - Retain all usable information, cleaning up only clear garbage or formatting issues.
   - Keep marginally relevant or incomplete sources if they contain valuable data or insights.

SOURCES LIST TO EVALUATE:
{sources}

You MUST return your response in the EXACT sources JSON list format as the original sources.
The response MUST not contain any markdown format or additional text (like ```json), just the JSON list!
"""

    @staticmethod
    def generate_resource_report_prompt(
        question, context, report_source: str, report_format="apa", tone=None, total_words=1000, language="english"
    ):
        """Generates the resource report prompt for the given question and research summary.

        Args:
            question (str): The question to generate the resource report prompt for.
            context (str): The research summary to generate the resource report prompt for.

        Returns:
            str: The resource report prompt for the given question and research summary.
        """

        reference_prompt = ""
        if report_source == ReportSource.Web.value:
            reference_prompt = f"""
            You MUST include all relevant source urls.
            Every url should be hyperlinked: [url website](url)
            """
        else:
            reference_prompt = f"""
            You MUST write all used source document names at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each."
        """

        return (
            f'"""{context}"""\n\nBased on the above information, generate a bibliography recommendation report for the following'
            f' question or topic: "{question}". The report should provide a detailed analysis of each recommended resource,'
            " explaining how each source can contribute to finding answers to the research question.\n"
            "Focus on the relevance, reliability, and significance of each source.\n"
            "Ensure that the report is well-structured, informative, in-depth, and follows Markdown syntax.\n"
            "Use markdown tables and other formatting features when appropriate to organize and present information clearly.\n"
            "Include relevant facts, figures, and numbers whenever available.\n"
            f"The report should have a minimum length of {total_words} words.\n"
            f"You MUST write the report in the following language: {language}.\n"
            "You MUST include all relevant source urls."
            "Every url should be hyperlinked: [url website](url)"
            f"{reference_prompt}"
        )

    @staticmethod
    def generate_custom_report_prompt(
        query_prompt, context, report_source: str, report_format="apa", tone=None, total_words=1000, language: str = "english"
    ):
        return f'"{context}"\n\n{query_prompt}'

    @staticmethod
    def generate_outline_report_prompt(
        question, context, report_source: str, report_format="apa", tone=None,  total_words=1000, language: str = "english"
    ):
        """Generates the outline report prompt for the given question and research summary.
        Args: question (str): The question to generate the outline report prompt for
                research_summary (str): The research summary to generate the outline report prompt for
        Returns: str: The outline report prompt for the given question and research summary
        """

        return (
            f'"""{context}""" Using the above information, generate an outline for a research report in Markdown syntax'
            f' for the following question or topic: "{question}". The outline should provide a well-structured framework'
            " for the research report, including the main sections, subsections, and key points to be covered."
            f" The research report should be detailed, informative, in-depth, and a minimum of {total_words} words."
            " Use appropriate Markdown syntax to format the outline and ensure readability."
            " Consider using markdown tables and other formatting features where they would enhance the presentation of information."
        )

    @staticmethod
    def generate_deep_research_prompt(
        question: str,
        context: str,
        report_source: str,
        report_format="apa",
        tone=None,
        total_words=2000,
        language: str = "english"
    ):
        """Generates the deep research report prompt, specialized for handling hierarchical research results.
        Args:
            question (str): The research question
            context (str): The research context containing learnings with citations
            report_source (str): Source of the research (web, etc.)
            report_format (str): Report formatting style
            tone: The tone to use in writing
            total_words (int): Minimum word count
            language (str): Output language
        Returns:
            str: The deep research report prompt
        """
        reference_prompt = ""
        if report_source == ReportSource.Web.value:
            reference_prompt = f"""
You MUST write all used source urls at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each.
Every url should be hyperlinked: [url website](url)
Additionally, you MUST include hyperlinks to the relevant URLs wherever they are referenced in the report:

eg: Author, A. A. (Year, Month Date). Title of web page. Website Name. [url website](url)
"""
        else:
            reference_prompt = f"""
You MUST write all used source document names at the end of the report as references, and make sure to not add duplicated sources, but only one reference for each."
"""

        tone_prompt = f"Write the report in a {tone.value} tone." if tone else ""

        return f"""
Using the following hierarchically researched information and citations:

"{context}"

Write a comprehensive research report answering the query: "{question}"

The report should:
1. Synthesize information from multiple levels of research depth
2. Integrate findings from various research branches
3. Present a coherent narrative that builds from foundational to advanced insights
4. Maintain proper citation of sources throughout
5. Be well-structured with clear sections and subsections
6. Have a minimum length of {total_words} words
7. Follow {report_format} format with markdown syntax
8. Use markdown tables, lists and other formatting features when presenting comparative data, statistics, or structured information

Additional requirements:
- Prioritize insights that emerged from deeper levels of research
- Highlight connections between different research branches
- Include relevant statistics, data, and concrete examples
- You MUST determine your own concrete and valid opinion based on the given information. Do NOT defer to general and meaningless conclusions.
- You MUST prioritize the relevance, reliability, and significance of the sources you use. Choose trusted sources over less reliable ones.
- You must also prioritize new articles over older articles if the source can be trusted.
- Use in-text citation references in {report_format} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).
- {tone_prompt}
- Write in {language}

{reference_prompt}

Please write a thorough, well-researched report that synthesizes all the gathered information into a cohesive whole.
Assume the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')}.
"""

    @staticmethod
    def auto_agent_instructions():
        return """
This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific server, defined by its type and role, with each server requiring distinct instructions.
Agent
The server is determined by the field of the topic and the specific name of the server that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each server type is associated with a corresponding emoji.

examples:
task: "should I invest in apple stocks?"
response:
{
    "server": "ğŸ’° Finance Agent",
    "agent_role_prompt: "You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends."
}
task: "could reselling sneakers become profitable?"
response:
{
    "server":  "ğŸ“ˆ Business Analyst Agent",
    "agent_role_prompt": "You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis."
}
task: "what are the most interesting sites in Tel Aviv?"
response:
{
    "server":  "ğŸŒ Travel Agent",
    "agent_role_prompt": "You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights."
}
"""

    @staticmethod
    def generate_summary_prompt(query, data):
        """Generates the summary prompt for the given question and text.
        Args: question (str): The question to generate the summary prompt for
                text (str): The text to generate the summary prompt for
        Returns: str: The summary prompt for the given question and text
        """

        return (
            f'{data}\n Using the above text, summarize it based on the following task or query: "{query}".\n If the '
            f"query cannot be answered using the text, YOU MUST summarize the text in short.\n Include all factual "
            f"information such as numbers, stats, quotes, etc if available. "
        )

    @staticmethod
    def pretty_print_docs(docs: list[Document], top_n: int | None = None) -> str:
        """Compress the list of documents into a context string"""
        return f"\n".join(f"Source: {d.metadata.get('source')}\n"
                          f"Title: {d.metadata.get('title')}\n"
                          f"Content: {d.page_content}\n"
                          for i, d in enumerate(docs)
                          if top_n is None or i < top_n)

    @staticmethod
    def join_local_web_documents(docs_context: str, web_context: str) -> str:
        """Joins local web documents with context scraped from the internet"""
        return f"Context from local documents: {docs_context}\n\nContext from web sources: {web_context}"

    ################################################################################################

    # DETAILED REPORT PROMPTS

    @staticmethod
    def generate_subtopics_prompt() -> str:
        return """
Provided the main topic:

{task}

and research data:

{data}

- Construct a list of subtopics which indicate the headers of a report document to be generated on the task.
- These are a possible list of subtopics : {subtopics}.
- There should NOT be any duplicate subtopics.
- Limit the number of subtopics to a maximum of {max_subtopics}
- Finally order the subtopics by their tasks, in a relevant and meaningful order which is presentable in a detailed report

"IMPORTANT!":
- Every subtopic MUST be relevant to the main topic and provided research data ONLY!

{format_instructions}
"""

    @staticmethod
    def generate_subtopic_report_prompt(
        current_subtopic,
        existing_headers: list,
        relevant_written_contents: list,
        main_topic: str,
        context,
        report_format: str = "apa",
        max_subsections=5,
        total_words=800,
        tone: Tone = Tone.Objective,
        language: str = "english",
    ) -> str:
        return f"""
Context:
"{context}"

Main Topic and Subtopic:
Using the latest information available, construct a detailed report on the subtopic: {current_subtopic} under the main topic: {main_topic}.
You must limit the number of subsections to a maximum of {max_subsections}.

Content Focus:
- The report should focus on answering the question, be well-structured, informative, in-depth, and include facts and numbers if available.
- Use markdown syntax and follow the {report_format.upper()} format.
- When presenting data, comparisons, or structured information, use markdown tables to enhance readability.

IMPORTANT:Content and Sections Uniqueness:
- This part of the instructions is crucial to ensure the content is unique and does not overlap with existing reports.
- Carefully review the existing headers and existing written contents provided below before writing any new subsections.
- Prevent any content that is already covered in the existing written contents.
- Do not use any of the existing headers as the new subsection headers.
- Do not repeat any information already covered in the existing written contents or closely related variations to avoid duplicates.
- If you have nested subsections, ensure they are unique and not covered in the existing written contents.
- Ensure that your content is entirely new and does not overlap with any information already covered in the previous subtopic reports.

"Existing Subtopic Reports":
- Existing subtopic reports and their section headers:

    {existing_headers}

- Existing written contents from previous subtopic reports:

    {relevant_written_contents}

"Structure and Formatting":
- As this sub-report will be part of a larger report, include only the main body divided into suitable subtopics without any introduction or conclusion section.

- You MUST include markdown hyperlinks to relevant source URLs wherever referenced in the report, for example:

    ### Section Header

    This is a sample text ([in-text citation](url)).

- Use H2 for the main subtopic header (##) and H3 for subsections (###).
- Use smaller Markdown headers (e.g., H2 or H3) for content structure, avoiding the largest header (H1) as it will be used for the larger report's heading.
- Organize your content into distinct sections that complement but do not overlap with existing reports.
- When adding similar or identical subsections to your report, you should clearly indicate the differences between and the new content and the existing written content from previous subtopic reports. For example:

    ### New header (similar to existing header)

    While the previous section discussed [topic A], this section will explore [topic B]."

"Date":
Assume the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')} if required.

"IMPORTANT!":
- You MUST write the report in the following language: {language}.
- The focus MUST be on the main topic! You MUST Leave out any information un-related to it!
- Must NOT have any introduction, conclusion, summary or reference section.
- You MUST use in-text citation references in {report_format.upper()} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).
- You MUST mention the difference between the existing content and the new content in the report if you are adding the similar or same subsections wherever necessary.
- The report should have a minimum length of {total_words} words.
- Use an {tone.value} tone throughout the report.

Do NOT add a conclusion section.
"""

    @staticmethod
    def generate_draft_titles_prompt(
        current_subtopic: str,
        main_topic: str,
        context: str,
        max_subsections: int = 5
    ) -> str:
        return f"""
"Context":
"{context}"

"Main Topic and Subtopic":
Using the latest information available, construct a draft section title headers for a detailed report on the subtopic: {current_subtopic} under the main topic: {main_topic}.

"Task":
1. Create a list of draft section title headers for the subtopic report.
2. Each header should be concise and relevant to the subtopic.
3. The header should't be too high level, but detailed enough to cover the main aspects of the subtopic.
4. Use markdown syntax for the headers, using H3 (###) as H1 and H2 will be used for the larger report's heading.
5. Ensure the headers cover main aspects of the subtopic.

"Structure and Formatting":
Provide the draft headers in a list format using markdown syntax, for example:

### Header 1
### Header 2
### Header 3

"IMPORTANT!":
- The focus MUST be on the main topic! You MUST Leave out any information un-related to it!
- Must NOT have any introduction, conclusion, summary or reference section.
- Focus solely on creating headers, not content.
"""

    @staticmethod
    def generate_report_introduction(question: str, research_summary: str = "", language: str = "english", report_format: str = "apa") -> str:
        return f"""{research_summary}\n
Using the above latest information, Prepare a detailed report introduction on the topic -- {question}.
- The introduction should be succinct, well-structured, informative with markdown syntax.
- As this introduction will be part of a larger report, do NOT include any other sections, which are generally present in a report.
- The introduction should be preceded by an H1 heading with a suitable topic for the entire report.
- You must use in-text citation references in {report_format.upper()} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).
Assume that the current date is {datetime.now(timezone.utc).strftime('%B %d, %Y')} if required.
- The output must be in {language} language.
"""


    @staticmethod
    def generate_report_conclusion(query: str, report_content: str, language: str = "english", report_format: str = "apa") -> str:
        """
        Generate a concise conclusion summarizing the main findings and implications of a research report.

        Args:
            query (str): The research task or question.
            report_content (str): The content of the research report.
            language (str): The language in which the conclusion should be written.

        Returns:
            str: A concise conclusion summarizing the report's main findings and implications.
        """
        prompt = f"""
    Based on the research report below and research task, please write a concise conclusion that summarizes the main findings and their implications:

    Research task: {query}

    Research Report: {report_content}

    Your conclusion should:
    1. Recap the main points of the research
    2. Highlight the most important findings
    3. Discuss any implications or next steps
    4. Be approximately 2-3 paragraphs long

    If there is no "## Conclusion" section title written at the end of the report, please add it to the top of your conclusion.
    You must use in-text citation references in {report_format.upper()} format and make it with markdown hyperlink placed at the end of the sentence or paragraph that references them like this: ([in-text citation](url)).

    IMPORTANT: The entire conclusion MUST be written in {language} language.

    Write the conclusion:
    """

        return prompt


class GranitePromptFamily(PromptFamily):
    """Prompts for IBM's granite models"""


    def _get_granite_class(self) -> type[PromptFamily]:
        """Get the right granite prompt family based on the version number"""
        if "3.3" in self.cfg.smart_llm:
            return Granite33PromptFamily
        if "3" in self.cfg.smart_llm:
            return Granite3PromptFamily
        # If not a known version, return the default
        return PromptFamily

    def pretty_print_docs(self, *args, **kwargs) -> str:
        return self._get_granite_class().pretty_print_docs(*args, **kwargs)

    def join_local_web_documents(self, *args, **kwargs) -> str:
        return self._get_granite_class().join_local_web_documents(*args, **kwargs)


class Granite3PromptFamily(PromptFamily):
    """Prompts for IBM's granite 3.X models (before 3.3)"""

    _DOCUMENTS_PREFIX = "<|start_of_role|>documents<|end_of_role|>\n"
    _DOCUMENTS_SUFFIX = "\n<|end_of_text|>"

    @classmethod
    def pretty_print_docs(cls, docs: list[Document], top_n: int | None = None) -> str:
        if not docs:
            return ""
        all_documents = "\n\n".join([
            f"Document {doc.metadata.get('source', i)}\n" + \
            f"Title: {doc.metadata.get('title')}\n" + \
            doc.page_content
            for i, doc in enumerate(docs)
            if top_n is None or i < top_n
        ])
        return "".join([cls._DOCUMENTS_PREFIX, all_documents, cls._DOCUMENTS_SUFFIX])

    @classmethod
    def join_local_web_documents(cls, docs_context: str | list, web_context: str | list) -> str:
        """Joins local web documents using Granite's preferred format"""
        if isinstance(docs_context, str) and docs_context.startswith(cls._DOCUMENTS_PREFIX):
            docs_context = docs_context[len(cls._DOCUMENTS_PREFIX):]
        if isinstance(web_context, str) and web_context.endswith(cls._DOCUMENTS_SUFFIX):
            web_context = web_context[:-len(cls._DOCUMENTS_SUFFIX)]
        all_documents = "\n\n".join([docs_context, web_context])
        return "".join([cls._DOCUMENTS_PREFIX, all_documents, cls._DOCUMENTS_SUFFIX])


class Granite33PromptFamily(PromptFamily):
    """Prompts for IBM's granite 3.3 models"""

    _DOCUMENT_TEMPLATE = """<|start_of_role|>document {{"document_id": "{document_id}"}}<|end_of_role|>
{document_content}<|end_of_text|>
"""

    @staticmethod
    def _get_content(doc: Document) -> str:
        doc_content = doc.page_content
        if title := doc.metadata.get("title"):
            doc_content = f"Title: {title}\n{doc_content}"
        return doc_content.strip()

    @classmethod
    def pretty_print_docs(cls, docs: list[Document], top_n: int | None = None) -> str:
        return "\n".join([
            cls._DOCUMENT_TEMPLATE.format(
                document_id=doc.metadata.get("source", i),
                document_content=cls._get_content(doc),
            )
            for i, doc in enumerate(docs)
            if top_n is None or i < top_n
        ])

    @classmethod
    def join_local_web_documents(cls, docs_context: str | list, web_context: str | list) -> str:
        """Joins local web documents using Granite's preferred format"""
        return "\n\n".join([docs_context, web_context])

## Factory ######################################################################

# This is the function signature for the various prompt generator functions
PROMPT_GENERATOR = Callable[
    [
        str,        # question
        str,        # context
        str,        # report_source
        str,        # report_format
        str | None, # tone
        int,        # total_words
        str,        # language
    ],
    str,
]

report_type_mapping = {
    ReportType.ResearchReport.value: "generate_report_prompt",
    ReportType.ResourceReport.value: "generate_resource_report_prompt",
    ReportType.OutlineReport.value: "generate_outline_report_prompt",
    ReportType.CustomReport.value: "generate_custom_report_prompt",
    ReportType.SubtopicReport.value: "generate_subtopic_report_prompt",
    ReportType.DeepResearch.value: "generate_deep_research_prompt",
}


def get_prompt_by_report_type(
    report_type: str,
    prompt_family: type[PromptFamily] | PromptFamily,
):
    prompt_by_type = getattr(prompt_family, report_type_mapping.get(report_type, ""), None)
    default_report_type = ReportType.ResearchReport.value
    if not prompt_by_type:
        warnings.warn(
            f"Invalid report type: {report_type}.\n"
            f"Please use one of the following: {', '.join([enum_value for enum_value in report_type_mapping.keys()])}\n"
            f"Using default report type: {default_report_type} prompt.",
            UserWarning,
        )
        prompt_by_type = getattr(prompt_family, report_type_mapping.get(default_report_type))
    return prompt_by_type


prompt_family_mapping = {
    PromptFamilyEnum.Default.value: PromptFamily,
    PromptFamilyEnum.Granite.value: GranitePromptFamily,
    PromptFamilyEnum.Granite3.value: Granite3PromptFamily,
    PromptFamilyEnum.Granite31.value: Granite3PromptFamily,
    PromptFamilyEnum.Granite32.value: Granite3PromptFamily,
    PromptFamilyEnum.Granite33.value: Granite33PromptFamily,
}


def get_prompt_family(
    prompt_family_name: PromptFamilyEnum | str, config: Config,
) -> PromptFamily:
    """Get a prompt family by name or value."""
    if isinstance(prompt_family_name, PromptFamilyEnum):
        prompt_family_name = prompt_family_name.value
    if prompt_family := prompt_family_mapping.get(prompt_family_name):
        return prompt_family(config)
    warnings.warn(
        f"Invalid prompt family: {prompt_family_name}.\n"
        f"Please use one of the following: {', '.join([enum_value for enum_value in prompt_family_mapping.keys()])}\n"
        f"Using default prompt family: {PromptFamilyEnum.Default.value} prompt.",
        UserWarning,
    )
    return PromptFamily()



================================================
FILE: gpt_researcher/actions/__init__.py
================================================
from .retriever import get_retriever, get_retrievers
from .query_processing import plan_research_outline, get_search_results
from .agent_creator import extract_json_with_regex, choose_agent
from .web_scraping import scrape_urls
from .report_generation import write_conclusion, summarize_url, generate_draft_section_titles, generate_report, write_report_introduction
from .markdown_processing import extract_headers, extract_sections, table_of_contents, add_references
from .utils import stream_output

__all__ = [
    "get_retriever",
    "get_retrievers",
    "get_search_results",
    "plan_research_outline",
    "extract_json_with_regex",
    "scrape_urls",
    "write_conclusion",
    "summarize_url",
    "generate_draft_section_titles",
    "generate_report",
    "write_report_introduction",
    "extract_headers",
    "extract_sections",
    "table_of_contents",
    "add_references",
    "stream_output",
    "choose_agent"
]


================================================
FILE: gpt_researcher/actions/agent_creator.py
================================================
import json
import re
import json_repair
from ..utils.llm import create_chat_completion
from ..prompts import PromptFamily

async def choose_agent(
    query,
    cfg,
    parent_query=None,
    cost_callback: callable = None,
    headers=None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
):
    """
    Chooses the agent automatically
    Args:
        parent_query: In some cases the research is conducted on a subtopic from the main query.
            The parent query allows the agent to know the main context for better reasoning.
        query: original query
        cfg: Config
        cost_callback: callback for calculating llm costs
        prompt_family: Family of prompts

    Returns:
        agent: Agent name
        agent_role_prompt: Agent role prompt
    """
    query = f"{parent_query} - {query}" if parent_query else f"{query}"
    response = None  # Initialize response to ensure it's defined

    try:
        response = await create_chat_completion(
            model=cfg.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{prompt_family.auto_agent_instructions()}"},
                {"role": "user", "content": f"task: {query}"},
            ],
            temperature=0.15,
            llm_provider=cfg.smart_llm_provider,
            llm_kwargs=cfg.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )

        agent_dict = json.loads(response)
        return agent_dict["server"], agent_dict["agent_role_prompt"]

    except Exception as e:
        return await handle_json_error(response)


async def handle_json_error(response):
    try:
        agent_dict = json_repair.loads(response)
        if agent_dict.get("server") and agent_dict.get("agent_role_prompt"):
            return agent_dict["server"], agent_dict["agent_role_prompt"]
    except Exception as e:
        print(f"âš ï¸ Error in reading JSON and failed to repair with json_repair: {e}")
        print(f"âš ï¸ LLM Response: `{response}`")

    json_string = extract_json_with_regex(response)
    if json_string:
        try:
            json_data = json.loads(json_string)
            return json_data["server"], json_data["agent_role_prompt"]
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")

    print("No JSON found in the string. Falling back to Default Agent.")
    return "Default Agent", (
        "You are an AI critical thinker research assistant. Your sole purpose is to write well written, "
        "critically acclaimed, objective and structured reports on given text."
    )


def extract_json_with_regex(response):
    json_match = re.search(r"{.*?}", response, re.DOTALL)
    if json_match:
        return json_match.group(0)
    return None



================================================
FILE: gpt_researcher/actions/markdown_processing.py
================================================
import re
import markdown
from typing import List, Dict

def extract_headers(markdown_text: str) -> List[Dict]:
    """
    Extract headers from markdown text.

    Args:
        markdown_text (str): The markdown text to process.

    Returns:
        List[Dict]: A list of dictionaries representing the header structure.
    """
    headers = []
    parsed_md = markdown.markdown(markdown_text)
    lines = parsed_md.split("\n")

    stack = []
    for line in lines:
        if line.startswith("<h") and len(line) > 2 and line[2].isdigit():
            level = int(line[2])
            header_text = line[line.index(">") + 1 : line.rindex("<")]

            while stack and stack[-1]["level"] >= level:
                stack.pop()

            header = {
                "level": level,
                "text": header_text,
            }
            if stack:
                stack[-1].setdefault("children", []).append(header)
            else:
                headers.append(header)

            stack.append(header)

    return headers

def extract_sections(markdown_text: str) -> List[Dict[str, str]]:
    """
    Extract all written sections from subtopic report.

    Args:
        markdown_text (str): Subtopic report text.

    Returns:
        List[Dict[str, str]]: List of sections, each section is a dictionary containing
        'section_title' and 'written_content'.
    """
    sections = []
    parsed_md = markdown.markdown(markdown_text)
    
    pattern = r'<h\d>(.*?)</h\d>(.*?)(?=<h\d>|$)'
    matches = re.findall(pattern, parsed_md, re.DOTALL)
    
    for title, content in matches:
        clean_content = re.sub(r'<.*?>', '', content).strip()
        if clean_content:
            sections.append({
                "section_title": title.strip(),
                "written_content": clean_content
            })
    
    return sections

def table_of_contents(markdown_text: str) -> str:
    """
    Generate a table of contents for the given markdown text.

    Args:
        markdown_text (str): The markdown text to process.

    Returns:
        str: The generated table of contents.
    """
    def generate_table_of_contents(headers, indent_level=0):
        toc = ""
        for header in headers:
            toc += " " * (indent_level * 4) + "- " + header["text"] + "\n"
            if "children" in header:
                toc += generate_table_of_contents(header["children"], indent_level + 1)
        return toc

    try:
        headers = extract_headers(markdown_text)
        toc = "## Table of Contents\n\n" + generate_table_of_contents(headers)
        return toc
    except Exception as e:
        print("table_of_contents Exception : ", e)
        return markdown_text

def add_references(report_markdown: str, visited_urls: set) -> str:
    """
    Add references to the markdown report.

    Args:
        report_markdown (str): The existing markdown report.
        visited_urls (set): A set of URLs that have been visited during research.

    Returns:
        str: The updated markdown report with added references.
    """
    try:
        url_markdown = "\n\n\n## References\n\n"
        url_markdown += "".join(f"- [{url}]({url})\n" for url in visited_urls)
        updated_markdown_report = report_markdown + url_markdown
        return updated_markdown_report
    except Exception as e:
        print(f"Encountered exception in adding source urls : {e}")
        return report_markdown


================================================
FILE: gpt_researcher/actions/query_processing.py
================================================
import json_repair

from gpt_researcher.llm_provider.generic.base import ReasoningEfforts
from ..utils.llm import create_chat_completion
from ..prompts import PromptFamily
from typing import Any, List, Dict
from ..config import Config
import logging

logger = logging.getLogger(__name__)

async def get_search_results(query: str, retriever: Any, query_domains: List[str] = None, researcher=None) -> List[Dict[str, Any]]:
    """
    Get web search results for a given query.

    Args:
        query: The search query
        retriever: The retriever instance
        query_domains: Optional list of domains to search
        researcher: The researcher instance (needed for MCP retrievers)

    Returns:
        A list of search results
    """
    # Check if this is an MCP retriever and pass the researcher instance
    if "mcpretriever" in retriever.__name__.lower():
        search_retriever = retriever(
            query, 
            query_domains=query_domains,
            researcher=researcher  # Pass researcher instance for MCP retrievers
        )
    else:
        search_retriever = retriever(query, query_domains=query_domains)
    
    return search_retriever.search()

async def generate_sub_queries(
    query: str,
    parent_query: str,
    report_type: str,
    context: List[Dict[str, Any]],
    cfg: Config,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> List[str]:
    """
    Generate sub-queries using the specified LLM model.

    Args:
        query: The original query
        parent_query: The parent query
        report_type: The type of report
        max_iterations: Maximum number of research iterations
        context: Search results context
        cfg: Configuration object
        cost_callback: Callback for cost calculation
        prompt_family: Family of prompts

    Returns:
        A list of sub-queries
    """
    gen_queries_prompt = prompt_family.generate_search_queries_prompt(
        query,
        parent_query,
        report_type,
        max_iterations=cfg.max_iterations or 3,
        context=context,
    )

    try:
        response = await create_chat_completion(
            model=cfg.strategic_llm_model,
            messages=[{"role": "user", "content": gen_queries_prompt}],
            llm_provider=cfg.strategic_llm_provider,
            max_tokens=None,
            llm_kwargs=cfg.llm_kwargs,
            reasoning_effort=ReasoningEfforts.Medium.value,
            cost_callback=cost_callback,
            **kwargs
        )
    except Exception as e:
        logger.warning(f"Error with strategic LLM: {e}. Retrying with max_tokens={cfg.strategic_token_limit}.")
        logger.warning(f"See https://github.com/assafelovic/gpt-researcher/issues/1022")
        try:
            response = await create_chat_completion(
                model=cfg.strategic_llm_model,
                messages=[{"role": "user", "content": gen_queries_prompt}],
                max_tokens=cfg.strategic_token_limit,
                llm_provider=cfg.strategic_llm_provider,
                llm_kwargs=cfg.llm_kwargs,
                cost_callback=cost_callback,
                **kwargs
            )
            logger.warning(f"Retrying with max_tokens={cfg.strategic_token_limit} successful.")
        except Exception as e:
            logger.warning(f"Retrying with max_tokens={cfg.strategic_token_limit} failed.")
            logger.warning(f"Error with strategic LLM: {e}. Falling back to smart LLM.")
            response = await create_chat_completion(
                model=cfg.smart_llm_model,
                messages=[{"role": "user", "content": gen_queries_prompt}],
                temperature=cfg.temperature,
                max_tokens=cfg.smart_token_limit,
                llm_provider=cfg.smart_llm_provider,
                llm_kwargs=cfg.llm_kwargs,
                cost_callback=cost_callback,
                **kwargs
            )

    return json_repair.loads(response)

async def plan_research_outline(
    query: str,
    search_results: List[Dict[str, Any]],
    agent_role_prompt: str,
    cfg: Config,
    parent_query: str,
    report_type: str,
    cost_callback: callable = None,
    retriever_names: List[str] = None,
    **kwargs
) -> List[str]:
    """
    Plan the research outline by generating sub-queries.

    Args:
        query: Original query
        search_results: Initial search results
        agent_role_prompt: Agent role prompt
        cfg: Configuration object
        parent_query: Parent query
        report_type: Report type
        cost_callback: Callback for cost calculation
        retriever_names: Names of the retrievers being used

    Returns:
        A list of sub-queries
    """
    # Handle the case where retriever_names is not provided
    if retriever_names is None:
        retriever_names = []
    
    # For MCP retrievers, we may want to skip sub-query generation
    # Check if MCP is the only retriever or one of multiple retrievers
    if retriever_names and ("mcp" in retriever_names or "MCPRetriever" in retriever_names):
        mcp_only = (len(retriever_names) == 1 and 
                   ("mcp" in retriever_names or "MCPRetriever" in retriever_names))
        
        if mcp_only:
            # If MCP is the only retriever, skip sub-query generation
            logger.info("Using MCP retriever only - skipping sub-query generation")
            # Return the original query to prevent additional search iterations
            return [query]
        else:
            # If MCP is one of multiple retrievers, generate sub-queries for the others
            logger.info("Using MCP with other retrievers - generating sub-queries for non-MCP retrievers")

    # Generate sub-queries for research outline
    sub_queries = await generate_sub_queries(
        query,
        parent_query,
        report_type,
        search_results,
        cfg,
        cost_callback,
        **kwargs
    )

    return sub_queries



================================================
FILE: gpt_researcher/actions/report_generation.py
================================================
import asyncio
from typing import List, Dict, Any
from ..config.config import Config
from ..utils.llm import create_chat_completion
from ..utils.logger import get_formatted_logger
from ..prompts import PromptFamily, get_prompt_by_report_type
from ..utils.enum import Tone

logger = get_formatted_logger()


async def write_report_introduction(
    query: str,
    context: str,
    agent_role_prompt: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> str:
    """
    Generate an introduction for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        str: The generated introduction.
    """
    try:
        introduction = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {"role": "user", "content": prompt_family.generate_report_introduction(
                    question=query,
                    research_summary=context,
                    language=config.language
                )},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return introduction
    except Exception as e:
        logger.error(f"Error in generating report introduction: {e}")
    return ""


async def write_conclusion(
    query: str,
    context: str,
    agent_role_prompt: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> str:
    """
    Write a conclusion for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        str: The generated conclusion.
    """
    try:
        conclusion = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {
                    "role": "user",
                    "content": prompt_family.generate_report_conclusion(query=query,
                                                                        report_content=context,
                                                                        language=config.language),
                },
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return conclusion
    except Exception as e:
        logger.error(f"Error in writing conclusion: {e}")
    return ""


async def summarize_url(
    url: str,
    content: str,
    role: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    **kwargs
) -> str:
    """
    Summarize the content of a URL.

    Args:
        url (str): The URL to summarize.
        content (str): The content of the URL.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.

    Returns:
        str: The summarized content.
    """
    try:
        summary = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{role}"},
                {"role": "user", "content": f"Summarize the following content from {url}:\n\n{content}"},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return summary
    except Exception as e:
        logger.error(f"Error in summarizing URL: {e}")
    return ""


async def generate_draft_section_titles(
    query: str,
    current_subtopic: str,
    context: str,
    role: str,
    config: Config,
    websocket=None,
    cost_callback: callable = None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> List[str]:
    """
    Generate draft section titles for the report.

    Args:
        query (str): The research query.
        context (str): Context for the report.
        role (str): The role of the agent.
        config (Config): Configuration object.
        websocket: WebSocket connection for streaming output.
        cost_callback (callable, optional): Callback for calculating LLM costs.
        prompt_family: Family of prompts

    Returns:
        List[str]: A list of generated section titles.
    """
    try:
        section_titles = await create_chat_completion(
            model=config.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{role}"},
                {"role": "user", "content": prompt_family.generate_draft_titles_prompt(
                    current_subtopic, query, context)},
            ],
            temperature=0.25,
            llm_provider=config.smart_llm_provider,
            stream=True,
            websocket=None,
            max_tokens=config.smart_token_limit,
            llm_kwargs=config.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
        return section_titles.split("\n")
    except Exception as e:
        logger.error(f"Error in generating draft section titles: {e}")
    return []


async def generate_report(
    query: str,
    context,
    agent_role_prompt: str,
    report_type: str,
    tone: Tone,
    report_source: str,
    websocket,
    cfg,
    main_topic: str = "",
    existing_headers: list = [],
    relevant_written_contents: list = [],
    cost_callback: callable = None,
    custom_prompt: str = "", # This can be any prompt the user chooses with the context
    headers=None,
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
):
    """
    generates the final report
    Args:
        query:
        context:
        agent_role_prompt:
        report_type:
        websocket:
        tone:
        cfg:
        main_topic:
        existing_headers:
        relevant_written_contents:
        cost_callback:
        prompt_family: Family of prompts

    Returns:
        report:

    """
    generate_prompt = get_prompt_by_report_type(report_type, prompt_family)
    report = ""

    if report_type == "subtopic_report":
        content = f"{generate_prompt(query, existing_headers, relevant_written_contents, main_topic, context, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}"
    elif custom_prompt:
        content = f"{custom_prompt}\n\nContext: {context}"
    else:
        content = f"{generate_prompt(query, context, report_source, report_format=cfg.report_format, tone=tone, total_words=cfg.total_words, language=cfg.language)}"
    try:
        report = await create_chat_completion(
            model=cfg.smart_llm_model,
            messages=[
                {"role": "system", "content": f"{agent_role_prompt}"},
                {"role": "user", "content": content},
            ],
            temperature=0.35,
            llm_provider=cfg.smart_llm_provider,
            stream=True,
            websocket=websocket,
            max_tokens=cfg.smart_token_limit,
            llm_kwargs=cfg.llm_kwargs,
            cost_callback=cost_callback,
            **kwargs
        )
    except:
        try:
            report = await create_chat_completion(
                model=cfg.smart_llm_model,
                messages=[
                    {"role": "user", "content": f"{agent_role_prompt}\n\n{content}"},
                ],
                temperature=0.35,
                llm_provider=cfg.smart_llm_provider,
                stream=True,
                websocket=websocket,
                max_tokens=cfg.smart_token_limit,
                llm_kwargs=cfg.llm_kwargs,
                cost_callback=cost_callback,
                **kwargs
            )
        except Exception as e:
            print(f"Error in generate_report: {e}")

    return report



================================================
FILE: gpt_researcher/actions/retriever.py
================================================
def get_retriever(retriever: str):
    """
    Gets the retriever
    Args:
        retriever (str): retriever name

    Returns:
        retriever: Retriever class

    """
    match retriever:
        case "google":
            from gpt_researcher.retrievers import GoogleSearch

            return GoogleSearch
        case "searx":
            from gpt_researcher.retrievers import SearxSearch

            return SearxSearch
        case "searchapi":
            from gpt_researcher.retrievers import SearchApiSearch

            return SearchApiSearch
        case "serpapi":
            from gpt_researcher.retrievers import SerpApiSearch

            return SerpApiSearch
        case "serper":
            from gpt_researcher.retrievers import SerperSearch

            return SerperSearch
        case "duckduckgo":
            from gpt_researcher.retrievers import Duckduckgo

            return Duckduckgo
        case "bing":
            from gpt_researcher.retrievers import BingSearch

            return BingSearch
        case "arxiv":
            from gpt_researcher.retrievers import ArxivSearch

            return ArxivSearch
        case "tavily":
            from gpt_researcher.retrievers import TavilySearch

            return TavilySearch
        case "exa":
            from gpt_researcher.retrievers import ExaSearch

            return ExaSearch
        case "semantic_scholar":
            from gpt_researcher.retrievers import SemanticScholarSearch

            return SemanticScholarSearch
        case "pubmed_central":
            from gpt_researcher.retrievers import PubMedCentralSearch

            return PubMedCentralSearch
        case "custom":
            from gpt_researcher.retrievers import CustomRetriever

            return CustomRetriever
        case "mcp":
            from gpt_researcher.retrievers import MCPRetriever

            return MCPRetriever

        case _:
            return None


def get_retrievers(headers: dict[str, str], cfg):
    """
    Determine which retriever(s) to use based on headers, config, or default.

    Args:
        headers (dict): The headers dictionary
        cfg: The configuration object

    Returns:
        list: A list of retriever classes to be used for searching.
    """
    # Check headers first for multiple retrievers
    if headers.get("retrievers"):
        retrievers = headers.get("retrievers").split(",")
    # If not found, check headers for a single retriever
    elif headers.get("retriever"):
        retrievers = [headers.get("retriever")]
    # If not in headers, check config for multiple retrievers
    elif cfg.retrievers:
        # Handle both list and string formats for config retrievers
        if isinstance(cfg.retrievers, str):
            retrievers = cfg.retrievers.split(",")
        else:
            retrievers = cfg.retrievers
        # Strip whitespace from each retriever name
        retrievers = [r.strip() for r in retrievers]
    # If not found, check config for a single retriever
    elif cfg.retriever:
        retrievers = [cfg.retriever]
    # If still not set, use default retriever
    else:
        retrievers = [get_default_retriever().__name__]

    # Convert retriever names to actual retriever classes
    # Use get_default_retriever() as a fallback for any invalid retriever names
    retriever_classes = [get_retriever(r) or get_default_retriever() for r in retrievers]
    
    return retriever_classes


def get_default_retriever():
    from gpt_researcher.retrievers import TavilySearch

    return TavilySearch


================================================
FILE: gpt_researcher/actions/utils.py
================================================
from typing import Dict, Any, Callable
from ..utils.logger import get_formatted_logger

logger = get_formatted_logger()


async def stream_output(
    type, content, output, websocket=None, output_log=True, metadata=None
):
    """
    Streams output to the websocket
    Args:
        type:
        content:
        output:

    Returns:
        None
    """
    if (not websocket or output_log) and type != "images":
        try:
            logger.info(f"{output}")
        except UnicodeEncodeError:
            # Option 1: Replace problematic characters with a placeholder
            logger.error(output.encode(
                'cp1252', errors='replace').decode('cp1252'))

    if websocket:
        await websocket.send_json(
            {"type": type, "content": content,
                "output": output, "metadata": metadata}
        )


async def safe_send_json(websocket: Any, data: Dict[str, Any]) -> None:
    """
    Safely send JSON data through a WebSocket connection.

    Args:
        websocket (WebSocket): The WebSocket connection to send data through.
        data (Dict[str, Any]): The data to send as JSON.

    Returns:
        None
    """
    try:
        await websocket.send_json(data)
    except Exception as e:
        logger.error(f"Error sending JSON through WebSocket: {e}")


def calculate_cost(
    prompt_tokens: int,
    completion_tokens: int,
    model: str
) -> float:
    """
    Calculate the cost of API usage based on the number of tokens and the model used.

    Args:
        prompt_tokens (int): Number of tokens in the prompt.
        completion_tokens (int): Number of tokens in the completion.
        model (str): The model used for the API call.

    Returns:
        float: The calculated cost in USD.
    """
    # Define cost per 1k tokens for different models
    costs = {
        "gpt-3.5-turbo": 0.002,
        "gpt-4": 0.03,
        "gpt-4-32k": 0.06,
        "gpt-4o": 0.00001,
        "gpt-4o-mini": 0.000001,
        "o3-mini": 0.0000005,
        # Add more models and their costs as needed
    }

    model = model.lower()
    if model not in costs:
        logger.warning(
            f"Unknown model: {model}. Cost calculation may be inaccurate.")
        return 0.0001 # Default avg cost if model is unknown

    cost_per_1k = costs[model]
    total_tokens = prompt_tokens + completion_tokens
    return (total_tokens / 1000) * cost_per_1k


def format_token_count(count: int) -> str:
    """
    Format the token count with commas for better readability.

    Args:
        count (int): The token count to format.

    Returns:
        str: The formatted token count.
    """
    return f"{count:,}"


async def update_cost(
    prompt_tokens: int,
    completion_tokens: int,
    model: str,
    websocket: Any
) -> None:
    """
    Update and send the cost information through the WebSocket.

    Args:
        prompt_tokens (int): Number of tokens in the prompt.
        completion_tokens (int): Number of tokens in the completion.
        model (str): The model used for the API call.
        websocket (WebSocket): The WebSocket connection to send data through.

    Returns:
        None
    """
    cost = calculate_cost(prompt_tokens, completion_tokens, model)
    total_tokens = prompt_tokens + completion_tokens

    await safe_send_json(websocket, {
        "type": "cost",
        "data": {
            "total_tokens": format_token_count(total_tokens),
            "prompt_tokens": format_token_count(prompt_tokens),
            "completion_tokens": format_token_count(completion_tokens),
            "total_cost": f"${cost:.4f}"
        }
    })


def create_cost_callback(websocket: Any) -> Callable:
    """
    Create a callback function for updating costs.

    Args:
        websocket (WebSocket): The WebSocket connection to send data through.

    Returns:
        Callable: A callback function that can be used to update costs.
    """
    async def cost_callback(
        prompt_tokens: int,
        completion_tokens: int,
        model: str
    ) -> None:
        await update_cost(prompt_tokens, completion_tokens, model, websocket)

    return cost_callback



================================================
FILE: gpt_researcher/actions/web_scraping.py
================================================
from typing import Any
from colorama import Fore, Style

from gpt_researcher.utils.workers import WorkerPool
from ..scraper import Scraper
from ..config.config import Config
from ..utils.logger import get_formatted_logger

logger = get_formatted_logger()


async def scrape_urls(
    urls, cfg: Config, worker_pool: WorkerPool
) -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
    """
    Scrapes the urls
    Args:
        urls: List of urls
        cfg: Config (optional)

    Returns:
        tuple[list[dict[str, Any]], list[dict[str, Any]]]: tuple containing scraped content and images

    """
    scraped_data = []
    images = []
    user_agent = (
        cfg.user_agent
        if cfg
        else "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
    )

    try:
        scraper = Scraper(urls, user_agent, cfg.scraper, worker_pool=worker_pool)
        scraped_data = await scraper.run()
        for item in scraped_data:
            if 'image_urls' in item:
                images.extend(item['image_urls'])
    except Exception as e:
        print(f"{Fore.RED}Error in scrape_urls: {e}{Style.RESET_ALL}")

    return scraped_data, images


async def filter_urls(urls: list[str], config: Config) -> list[str]:
    """
    Filter URLs based on configuration settings.

    Args:
        urls (list[str]): List of URLs to filter.
        config (Config): Configuration object.

    Returns:
        list[str]: Filtered list of URLs.
    """
    filtered_urls = []
    for url in urls:
        # Add your filtering logic here
        # For example, you might want to exclude certain domains or URL patterns
        if not any(excluded in url for excluded in config.excluded_domains):
            filtered_urls.append(url)
    return filtered_urls

async def extract_main_content(html_content: str) -> str:
    """
    Extract the main content from HTML.

    Args:
        html_content (str): Raw HTML content.

    Returns:
        str: Extracted main content.
    """
    # Implement content extraction logic here
    # This could involve using libraries like BeautifulSoup or custom parsing logic
    # For now, we'll just return the raw HTML as a placeholder
    return html_content

async def process_scraped_data(scraped_data: list[dict[str, Any]], config: Config) -> list[dict[str, Any]]:
    """
    Process the scraped data to extract and clean the main content.

    Args:
        scraped_data (list[dict[str, Any]]): List of dictionaries containing scraped data.
        config (Config): Configuration object.

    Returns:
        list[dict[str, Any]]: Processed scraped data.
    """
    processed_data = []
    for item in scraped_data:
        if item['status'] == 'success':
            main_content = await extract_main_content(item['content'])
            processed_data.append({
                'url': item['url'],
                'content': main_content,
                'status': 'success'
            })
        else:
            processed_data.append(item)
    return processed_data



================================================
FILE: gpt_researcher/config/__init__.py
================================================
from .config import Config
from .variables.base import BaseConfig
from .variables.default import DEFAULT_CONFIG as DefaultConfig

__all__ = ["Config", "BaseConfig", "DefaultConfig"]



================================================
FILE: gpt_researcher/config/config.py
================================================
import json
import os
import warnings
from typing import Dict, Any, List, Union, Type, get_origin, get_args

from gpt_researcher.llm_provider.generic.base import ReasoningEfforts
from .variables.default import DEFAULT_CONFIG
from .variables.base import BaseConfig


class Config:
    """Config class for GPT Researcher."""

    CONFIG_DIR = os.path.join(os.path.dirname(__file__), "variables")

    def __init__(self, config_path: str | None = None):
        """Initialize the config class."""
        self.config_path = config_path
        self.llm_kwargs: Dict[str, Any] = {}
        self.embedding_kwargs: Dict[str, Any] = {}

        config_to_use = self.load_config(config_path)
        self._set_attributes(config_to_use)
        self._set_embedding_attributes()
        self._set_llm_attributes()
        self._handle_deprecated_attributes()
        if config_to_use['REPORT_SOURCE'] != 'web':
          self._set_doc_path(config_to_use)

        # MCP support configuration
        self.mcp_servers = []  # List of MCP server configurations
        self.mcp_allowed_root_paths = []  # Allowed root paths for MCP servers

        # Read from config
        if hasattr(self, 'mcp_servers'):
            self.mcp_servers = self.mcp_servers
        if hasattr(self, 'mcp_allowed_root_paths'):
            self.mcp_allowed_root_paths = self.mcp_allowed_root_paths

    def _set_attributes(self, config: Dict[str, Any]) -> None:
        for key, value in config.items():
            env_value = os.getenv(key)
            if env_value is not None:
                value = self.convert_env_value(key, env_value, BaseConfig.__annotations__[key])
            setattr(self, key.lower(), value)

        # Handle RETRIEVER with default value
        retriever_env = os.environ.get("RETRIEVER", config.get("RETRIEVER", "tavily"))
        try:
            self.retrievers = self.parse_retrievers(retriever_env)
        except ValueError as e:
            print(f"Warning: {str(e)}. Defaulting to 'tavily' retriever.")
            self.retrievers = ["tavily"]

    def _set_embedding_attributes(self) -> None:
        self.embedding_provider, self.embedding_model = self.parse_embedding(
            self.embedding
        )

    def _set_llm_attributes(self) -> None:
        self.fast_llm_provider, self.fast_llm_model = self.parse_llm(self.fast_llm)
        self.smart_llm_provider, self.smart_llm_model = self.parse_llm(self.smart_llm)
        self.strategic_llm_provider, self.strategic_llm_model = self.parse_llm(self.strategic_llm)
        self.reasoning_effort = self.parse_reasoning_effort(os.getenv("REASONING_EFFORT"))

    def _handle_deprecated_attributes(self) -> None:
        if os.getenv("EMBEDDING_PROVIDER") is not None:
            warnings.warn(
                "EMBEDDING_PROVIDER is deprecated and will be removed soon. Use EMBEDDING instead.",
                FutureWarning,
                stacklevel=2,
            )
            self.embedding_provider = (
                os.environ["EMBEDDING_PROVIDER"] or self.embedding_provider
            )

            embedding_provider = os.environ["EMBEDDING_PROVIDER"]
            if embedding_provider == "ollama":
                self.embedding_model = os.environ["OLLAMA_EMBEDDING_MODEL"]
            elif embedding_provider == "custom":
                self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "custom")
            elif embedding_provider == "openai":
                self.embedding_model = "text-embedding-3-large"
            elif embedding_provider == "azure_openai":
                self.embedding_model = "text-embedding-3-large"
            elif embedding_provider == "huggingface":
                self.embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
            elif embedding_provider == "gigachat":
                self.embedding_model = "Embeddings"
            elif embedding_provider == "google_genai":
                self.embedding_model = "text-embedding-004"
            else:
                raise Exception("Embedding provider not found.")

        _deprecation_warning = (
            "LLM_PROVIDER, FAST_LLM_MODEL and SMART_LLM_MODEL are deprecated and "
            "will be removed soon. Use FAST_LLM and SMART_LLM instead."
        )
        if os.getenv("LLM_PROVIDER") is not None:
            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)
            self.fast_llm_provider = (
                os.environ["LLM_PROVIDER"] or self.fast_llm_provider
            )
            self.smart_llm_provider = (
                os.environ["LLM_PROVIDER"] or self.smart_llm_provider
            )
        if os.getenv("FAST_LLM_MODEL") is not None:
            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)
            self.fast_llm_model = os.environ["FAST_LLM_MODEL"] or self.fast_llm_model
        if os.getenv("SMART_LLM_MODEL") is not None:
            warnings.warn(_deprecation_warning, FutureWarning, stacklevel=2)
            self.smart_llm_model = os.environ["SMART_LLM_MODEL"] or self.smart_llm_model

    def _set_doc_path(self, config: Dict[str, Any]) -> None:
        self.doc_path = config['DOC_PATH']
        if self.doc_path:
            try:
                self.validate_doc_path()
            except Exception as e:
                print(f"Warning: Error validating doc_path: {str(e)}. Using default doc_path.")
                self.doc_path = DEFAULT_CONFIG['DOC_PATH']

    @classmethod
    def load_config(cls, config_path: str | None) -> Dict[str, Any]:
        """Load a configuration by name."""
        if config_path is None:
            return DEFAULT_CONFIG

        # config_path = os.path.join(cls.CONFIG_DIR, config_path)
        if not os.path.exists(config_path):
            if config_path and config_path != "default":
                print(f"Warning: Configuration not found at '{config_path}'. Using default configuration.")
                if not config_path.endswith(".json"):
                    print(f"Do you mean '{config_path}.json'?")
            return DEFAULT_CONFIG

        with open(config_path, "r") as f:
            custom_config = json.load(f)

        # Merge with default config to ensure all keys are present
        merged_config = DEFAULT_CONFIG.copy()
        merged_config.update(custom_config)
        return merged_config

    @classmethod
    def list_available_configs(cls) -> List[str]:
        """List all available configuration names."""
        configs = ["default"]
        for file in os.listdir(cls.CONFIG_DIR):
            if file.endswith(".json"):
                configs.append(file[:-5])  # Remove .json extension
        return configs

    def parse_retrievers(self, retriever_str: str) -> List[str]:
        """Parse the retriever string into a list of retrievers and validate them."""
        from ..retrievers.utils import get_all_retriever_names
        
        retrievers = [retriever.strip()
                      for retriever in retriever_str.split(",")]
        valid_retrievers = get_all_retriever_names() or []
        invalid_retrievers = [r for r in retrievers if r not in valid_retrievers]
        if invalid_retrievers:
            raise ValueError(
                f"Invalid retriever(s) found: {', '.join(invalid_retrievers)}. "
                f"Valid options are: {', '.join(valid_retrievers)}."
            )
        return retrievers

    @staticmethod
    def parse_llm(llm_str: str | None) -> tuple[str | None, str | None]:
        """Parse llm string into (llm_provider, llm_model)."""
        from gpt_researcher.llm_provider.generic.base import _SUPPORTED_PROVIDERS

        if llm_str is None:
            return None, None
        try:
            llm_provider, llm_model = llm_str.split(":", 1)
            assert llm_provider in _SUPPORTED_PROVIDERS, (
                f"Unsupported {llm_provider}.\nSupported llm providers are: "
                + ", ".join(_SUPPORTED_PROVIDERS)
            )
            return llm_provider, llm_model
        except ValueError:
            raise ValueError(
                "Set SMART_LLM or FAST_LLM = '<llm_provider>:<llm_model>' "
                "Eg 'openai:gpt-4o-mini'"
            )

    @staticmethod
    def parse_reasoning_effort(reasoning_effort_str: str | None) -> str | None:
        """Parse reasoning effort string into (reasoning_effort)."""
        if reasoning_effort_str is None:
            return ReasoningEfforts.Medium.value
        if reasoning_effort_str not in [effort.value for effort in ReasoningEfforts]:
            raise ValueError(f"Invalid reasoning effort: {reasoning_effort_str}. Valid options are: {', '.join([effort.value for effort in ReasoningEfforts])}")
        return reasoning_effort_str

    @staticmethod
    def parse_embedding(embedding_str: str | None) -> tuple[str | None, str | None]:
        """Parse embedding string into (embedding_provider, embedding_model)."""
        from gpt_researcher.memory.embeddings import _SUPPORTED_PROVIDERS

        if embedding_str is None:
            return None, None
        try:
            embedding_provider, embedding_model = embedding_str.split(":", 1)
            assert embedding_provider in _SUPPORTED_PROVIDERS, (
                f"Unsupported {embedding_provider}.\nSupported embedding providers are: "
                + ", ".join(_SUPPORTED_PROVIDERS)
            )
            return embedding_provider, embedding_model
        except ValueError:
            raise ValueError(
                "Set EMBEDDING = '<embedding_provider>:<embedding_model>' "
                "Eg 'openai:text-embedding-3-large'"
            )

    def validate_doc_path(self):
        """Ensure that the folder exists at the doc path"""
        os.makedirs(self.doc_path, exist_ok=True)

    @staticmethod
    def convert_env_value(key: str, env_value: str, type_hint: Type) -> Any:
        """Convert environment variable to the appropriate type based on the type hint."""
        origin = get_origin(type_hint)
        args = get_args(type_hint)

        if origin is Union:
            # Handle Union types (e.g., Union[str, None])
            for arg in args:
                if arg is type(None):
                    if env_value.lower() in ("none", "null", ""):
                        return None
                else:
                    try:
                        return Config.convert_env_value(key, env_value, arg)
                    except ValueError:
                        continue
            raise ValueError(f"Cannot convert {env_value} to any of {args}")

        if type_hint is bool:
            return env_value.lower() in ("true", "1", "yes", "on")
        elif type_hint is int:
            return int(env_value)
        elif type_hint is float:
            return float(env_value)
        elif type_hint in (str, Any):
            return env_value
        elif origin is list or origin is List:
            return json.loads(env_value)
        elif type_hint is dict:
            return json.loads(env_value)
        else:
            raise ValueError(f"Unsupported type {type_hint} for key {key}")


    def set_verbose(self, verbose: bool) -> None:
        """Set the verbosity level."""
        self.llm_kwargs["verbose"] = verbose

    def get_mcp_server_config(self, name: str) -> dict:
        """
        Get the configuration for an MCP server.
        
        Args:
            name (str): The name of the MCP server to get the config for.
                
        Returns:
            dict: The server configuration, or an empty dict if the server is not found.
        """
        if not name or not self.mcp_servers:
            return {}
        
        for server in self.mcp_servers:
            if isinstance(server, dict) and server.get("name") == name:
                return server
            
        return {}


================================================
FILE: gpt_researcher/config/variables/__init__.py
================================================



================================================
FILE: gpt_researcher/config/variables/base.py
================================================
from typing import Union, List, Dict, Any
from typing_extensions import TypedDict


class BaseConfig(TypedDict):
    RETRIEVER: str
    EMBEDDING: str
    SIMILARITY_THRESHOLD: float
    FAST_LLM: str
    SMART_LLM: str
    STRATEGIC_LLM: str
    FAST_TOKEN_LIMIT: int
    SMART_TOKEN_LIMIT: int
    STRATEGIC_TOKEN_LIMIT: int
    BROWSE_CHUNK_MAX_LENGTH: int
    SUMMARY_TOKEN_LIMIT: int
    TEMPERATURE: float
    USER_AGENT: str
    MAX_SEARCH_RESULTS_PER_QUERY: int
    MEMORY_BACKEND: str
    TOTAL_WORDS: int
    REPORT_FORMAT: str
    CURATE_SOURCES: bool
    MAX_ITERATIONS: int
    LANGUAGE: str
    AGENT_ROLE: Union[str, None]
    SCRAPER: str
    MAX_SCRAPER_WORKERS: int
    MAX_SUBTOPICS: int
    REPORT_SOURCE: Union[str, None]
    DOC_PATH: str
    PROMPT_FAMILY: str
    LLM_KWARGS: dict
    EMBEDDING_KWARGS: dict
    DEEP_RESEARCH_CONCURRENCY: int
    DEEP_RESEARCH_DEPTH: int
    DEEP_RESEARCH_BREADTH: int
    MCP_SERVERS: List[Dict[str, Any]]
    MCP_AUTO_TOOL_SELECTION: bool
    MCP_USE_LLM_ARGS: bool
    MCP_ALLOWED_ROOT_PATHS: List[str]
    MCP_STRATEGY: str
    REASONING_EFFORT: str



================================================
FILE: gpt_researcher/config/variables/default.py
================================================
from .base import BaseConfig

DEFAULT_CONFIG: BaseConfig = {
    "RETRIEVER": "tavily",
    "EMBEDDING": "openai:text-embedding-3-small",
    "SIMILARITY_THRESHOLD": 0.42,
    "FAST_LLM": "openai:gpt-4o-mini",
    "SMART_LLM": "openai:gpt-4.1",  # Has support for long responses (2k+ words).
    "STRATEGIC_LLM": "openai:o4-mini",  # Can be used with o1 or o3, please note it will make tasks slower.
    "FAST_TOKEN_LIMIT": 3000,
    "SMART_TOKEN_LIMIT": 6000,
    "STRATEGIC_TOKEN_LIMIT": 4000,
    "BROWSE_CHUNK_MAX_LENGTH": 8192,
    "CURATE_SOURCES": False,
    "SUMMARY_TOKEN_LIMIT": 700,
    "TEMPERATURE": 0.4,
    "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
    "MAX_SEARCH_RESULTS_PER_QUERY": 5,
    "MEMORY_BACKEND": "local",
    "TOTAL_WORDS": 1200,
    "REPORT_FORMAT": "APA",
    "MAX_ITERATIONS": 3,
    "AGENT_ROLE": None,
    "SCRAPER": "bs",
    "MAX_SCRAPER_WORKERS": 15,
    "MAX_SUBTOPICS": 3,
    "LANGUAGE": "english",
    "REPORT_SOURCE": "web",
    "DOC_PATH": "./my-docs",
    "PROMPT_FAMILY": "default",
    "LLM_KWARGS": {},
    "EMBEDDING_KWARGS": {},
    "VERBOSE": False,
    # Deep research specific settings
    "DEEP_RESEARCH_BREADTH": 3,
    "DEEP_RESEARCH_DEPTH": 2,
    "DEEP_RESEARCH_CONCURRENCY": 4,
    
    # MCP retriever specific settings
    "MCP_SERVERS": [],  # List of predefined MCP server configurations
    "MCP_AUTO_TOOL_SELECTION": True,  # Whether to automatically select the best tool for a query
    "MCP_ALLOWED_ROOT_PATHS": [],  # List of allowed root paths for local file access
    "MCP_STRATEGY": "fast",  # MCP execution strategy: "fast", "deep", "disabled"
    "REASONING_EFFORT": "medium",
}



================================================
FILE: gpt_researcher/config/variables/test_local.json
================================================
{
  "DOC_PATH": "tests/docs"
}



================================================
FILE: gpt_researcher/context/__init__.py
================================================
from .compression import ContextCompressor
from .retriever import SearchAPIRetriever

__all__ = ['ContextCompressor', 'SearchAPIRetriever']



================================================
FILE: gpt_researcher/context/compression.py
================================================
import os
import asyncio
from typing import Optional
from .retriever import SearchAPIRetriever, SectionRetriever
from langchain.retrievers import (
    ContextualCompressionRetriever,
)
from langchain.retrievers.document_compressors import (
    DocumentCompressorPipeline,
    EmbeddingsFilter,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from ..vector_store import VectorStoreWrapper
from ..utils.costs import estimate_embedding_cost
from ..memory.embeddings import OPENAI_EMBEDDING_MODEL
from ..prompts import PromptFamily


class VectorstoreCompressor:
    def __init__(
        self,
        vector_store: VectorStoreWrapper,
        max_results:int = 7,
        filter: Optional[dict] = None,
        prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
        **kwargs,
    ):

        self.vector_store = vector_store
        self.max_results = max_results
        self.filter = filter
        self.kwargs = kwargs
        self.prompt_family = prompt_family

    async def async_get_context(self, query, max_results=5):
        """Get relevant context from vector store"""
        results = await self.vector_store.asimilarity_search(query=query, k=max_results, filter=self.filter)
        return self.prompt_family.pretty_print_docs(results)


class ContextCompressor:
    def __init__(
        self,
        documents,
        embeddings,
        max_results=5,
        prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
        **kwargs,
    ):
        self.max_results = max_results
        self.documents = documents
        self.kwargs = kwargs
        self.embeddings = embeddings
        self.similarity_threshold = os.environ.get("SIMILARITY_THRESHOLD", 0.35)
        self.prompt_family = prompt_family

    def __get_contextual_retriever(self):
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,
                                            similarity_threshold=self.similarity_threshold)
        pipeline_compressor = DocumentCompressorPipeline(
            transformers=[splitter, relevance_filter]
        )
        base_retriever = SearchAPIRetriever(
            pages=self.documents
        )
        contextual_retriever = ContextualCompressionRetriever(
            base_compressor=pipeline_compressor, base_retriever=base_retriever
        )
        return contextual_retriever

    async def async_get_context(self, query, max_results=5, cost_callback=None):
        compressed_docs = self.__get_contextual_retriever()
        if cost_callback:
            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))
        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query, **self.kwargs)
        return self.prompt_family.pretty_print_docs(relevant_docs, max_results)


class WrittenContentCompressor:
    def __init__(self, documents, embeddings, similarity_threshold, **kwargs):
        self.documents = documents
        self.kwargs = kwargs
        self.embeddings = embeddings
        self.similarity_threshold = similarity_threshold

    def __get_contextual_retriever(self):
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        relevance_filter = EmbeddingsFilter(embeddings=self.embeddings,
                                            similarity_threshold=self.similarity_threshold)
        pipeline_compressor = DocumentCompressorPipeline(
            transformers=[splitter, relevance_filter]
        )
        base_retriever = SectionRetriever(
            sections=self.documents
        )
        contextual_retriever = ContextualCompressionRetriever(
            base_compressor=pipeline_compressor, base_retriever=base_retriever
        )
        return contextual_retriever

    def __pretty_docs_list(self, docs, top_n):
        return [f"Title: {d.metadata.get('section_title')}\nContent: {d.page_content}\n" for i, d in enumerate(docs) if i < top_n]

    async def async_get_context(self, query, max_results=5, cost_callback=None):
        compressed_docs = self.__get_contextual_retriever()
        if cost_callback:
            cost_callback(estimate_embedding_cost(model=OPENAI_EMBEDDING_MODEL, docs=self.documents))
        relevant_docs = await asyncio.to_thread(compressed_docs.invoke, query, **self.kwargs)
        return self.__pretty_docs_list(relevant_docs, max_results)



================================================
FILE: gpt_researcher/context/retriever.py
================================================
import os
from enum import Enum
from typing import Any, Dict, List, Optional

from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.schema import Document
from langchain.schema.retriever import BaseRetriever


class SearchAPIRetriever(BaseRetriever):
    """Search API retriever."""
    pages: List[Dict] = []

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:

        docs = [
            Document(
                page_content=page.get("raw_content", ""),
                metadata={
                    "title": page.get("title", ""),
                    "source": page.get("url", ""),
                },
            )
            for page in self.pages
        ]

        return docs

class SectionRetriever(BaseRetriever):
    """
    SectionRetriever:
    This class is used to retrieve sections while avoiding redundant subtopics.
    """
    sections: List[Dict] = []
    """
    sections example:
    [
        {
            "section_title": "Example Title",
            "written_content": "Example content"
        },
        ...
    ]
    """
    
    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:

        docs = [
            Document(
                page_content=page.get("written_content", ""),
                metadata={
                    "section_title": page.get("section_title", ""),
                },
            )
            for page in self.sections  # Changed 'self.pages' to 'self.sections'
        ]

        return docs


================================================
FILE: gpt_researcher/document/__init__.py
================================================
from .document import DocumentLoader
from .online_document import OnlineDocumentLoader
from .langchain_document import LangChainDocumentLoader

__all__ = ['DocumentLoader', 'OnlineDocumentLoader', 'LangChainDocumentLoader']



================================================
FILE: gpt_researcher/document/azure_document_loader.py
================================================
from azure.storage.blob import BlobServiceClient
import os
import tempfile

class AzureDocumentLoader:
    def __init__(self, container_name, connection_string):
        self.client = BlobServiceClient.from_connection_string(connection_string)
        self.container = self.client.get_container_client(container_name)

    async def load(self):
        """Download all blobs to temp files and return their paths."""
        temp_dir = tempfile.mkdtemp()
        blobs = self.container.list_blobs()
        file_paths = []
        for blob in blobs:
            blob_client = self.container.get_blob_client(blob.name)
            local_path = os.path.join(temp_dir, blob.name)
            with open(local_path, "wb") as f:
                blob_data = blob_client.download_blob()
                f.write(blob_data.readall())
            file_paths.append(local_path)
        return file_paths  # Pass to existing DocumentLoader


================================================
FILE: gpt_researcher/document/document.py
================================================
import asyncio
import os
from typing import List, Union
from langchain_community.document_loaders import (
    PyMuPDFLoader,
    TextLoader,
    UnstructuredCSVLoader,
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader
)
from langchain_community.document_loaders import BSHTMLLoader


class DocumentLoader:

    def __init__(self, path: Union[str, List[str]]):
        self.path = path

    async def load(self) -> list:
        tasks = []
        if isinstance(self.path, list):
            for file_path in self.path:
                if os.path.isfile(file_path):  # Ensure it's a valid file
                    filename = os.path.basename(file_path)
                    file_name, file_extension_with_dot = os.path.splitext(filename)
                    file_extension = file_extension_with_dot.strip(".").lower()
                    tasks.append(self._load_document(file_path, file_extension))
                    
        elif isinstance(self.path, (str, bytes, os.PathLike)):
            for root, dirs, files in os.walk(self.path):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_name, file_extension_with_dot = os.path.splitext(file)
                    file_extension = file_extension_with_dot.strip(".").lower()
                    tasks.append(self._load_document(file_path, file_extension))
                    
        else:
            raise ValueError("Invalid type for path. Expected str, bytes, os.PathLike, or list thereof.")

        # for root, dirs, files in os.walk(self.path):
        #     for file in files:
        #         file_path = os.path.join(root, file)
        #         file_name, file_extension_with_dot = os.path.splitext(file_path)
        #         file_extension = file_extension_with_dot.strip(".")
        #         tasks.append(self._load_document(file_path, file_extension))

        docs = []
        for pages in await asyncio.gather(*tasks):
            for page in pages:
                if page.page_content:
                    docs.append({
                        "raw_content": page.page_content,
                        "url": os.path.basename(page.metadata['source'])
                    })
                    
        if not docs:
            raise ValueError("ğŸ¤· Failed to load any documents!")

        return docs

    async def _load_document(self, file_path: str, file_extension: str) -> list:
        ret_data = []
        try:
            loader_dict = {
                "pdf": PyMuPDFLoader(file_path),
                "txt": TextLoader(file_path),
                "doc": UnstructuredWordDocumentLoader(file_path),
                "docx": UnstructuredWordDocumentLoader(file_path),
                "pptx": UnstructuredPowerPointLoader(file_path),
                "csv": UnstructuredCSVLoader(file_path, mode="elements"),
                "xls": UnstructuredExcelLoader(file_path, mode="elements"),
                "xlsx": UnstructuredExcelLoader(file_path, mode="elements"),
                "md": UnstructuredMarkdownLoader(file_path),
                "html": BSHTMLLoader(file_path),
                "htm": BSHTMLLoader(file_path)
            }

            loader = loader_dict.get(file_extension, None)
            if loader:
                try:
                    ret_data = loader.load()
                except Exception as e:
                    print(f"Failed to load HTML document : {file_path}")
                    print(e)

        except Exception as e:
            print(f"Failed to load document : {file_path}")
            print(e)

        return ret_data



================================================
FILE: gpt_researcher/document/langchain_document.py
================================================
import asyncio
import os

from langchain_core.documents import Document
from typing import List, Dict


# Supports the base Document class from langchain
# - https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/documents/base.py
class LangChainDocumentLoader:

    def __init__(self, documents: List[Document]):
        self.documents = documents

    async def load(self, metadata_source_index="title") -> List[Dict[str, str]]:
        docs = []
        for document in self.documents:
            docs.append(
                {
                    "raw_content": document.page_content,
                    "url": document.metadata.get(metadata_source_index, ""),
                }
            )
        return docs



================================================
FILE: gpt_researcher/document/online_document.py
================================================
import os
import aiohttp
import tempfile
from langchain_community.document_loaders import (
    PyMuPDFLoader,
    TextLoader,
    UnstructuredCSVLoader,
    UnstructuredExcelLoader,
    UnstructuredMarkdownLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader
)


class OnlineDocumentLoader:

    def __init__(self, urls):
        self.urls = urls

    async def load(self) -> list:
        docs = []
        for url in self.urls:
            pages = await self._download_and_process(url)
            for page in pages:
                if page.page_content:
                    docs.append({
                        "raw_content": page.page_content,
                        "url": page.metadata.get("source")
                    })

        if not docs:
            raise ValueError("ğŸ¤· Failed to load any documents!")

        return docs

    async def _download_and_process(self, url: str) -> list:
        try:
            headers = {
                "User-Agent": "Mozilla/5.0"
            }
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=6) as response:
                    if response.status != 200:
                        print(f"Failed to download {url}: HTTP {response.status}")
                        return []

                    content = await response.read()
                    with tempfile.NamedTemporaryFile(delete=False, suffix=self._get_extension(url)) as tmp_file:
                        tmp_file.write(content)
                        tmp_file_path = tmp_file.name

                    return await self._load_document(tmp_file_path, self._get_extension(url).strip('.'))
        except aiohttp.ClientError as e:
            print(f"Failed to process {url}")
            print(e)
            return []
        except Exception as e:
            print(f"Unexpected error processing {url}")
            print(e)
            return []

    async def _load_document(self, file_path: str, file_extension: str) -> list:
        ret_data = []
        try:
            loader_dict = {
                "pdf": PyMuPDFLoader(file_path),
                "txt": TextLoader(file_path),
                "doc": UnstructuredWordDocumentLoader(file_path),
                "docx": UnstructuredWordDocumentLoader(file_path),
                "pptx": UnstructuredPowerPointLoader(file_path),
                "csv": UnstructuredCSVLoader(file_path, mode="elements"),
                "xls": UnstructuredExcelLoader(file_path, mode="elements"),
                "xlsx": UnstructuredExcelLoader(file_path, mode="elements"),
                "md": UnstructuredMarkdownLoader(file_path)
            }

            loader = loader_dict.get(file_extension, None)
            if loader:
                ret_data = loader.load()

        except Exception as e:
            print(f"Failed to load document : {file_path}")
            print(e)
        finally:
            os.remove(file_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶

        return ret_data

    @staticmethod
    def _get_extension(url: str) -> str:
        return os.path.splitext(url.split("?")[0])[1]



================================================
FILE: gpt_researcher/llm_provider/__init__.py
================================================
from .generic import GenericLLMProvider

__all__ = [
    "GenericLLMProvider",
]



================================================
FILE: gpt_researcher/llm_provider/generic/__init__.py
================================================
from .base import GenericLLMProvider

__all__ = ["GenericLLMProvider"]


================================================
FILE: gpt_researcher/llm_provider/generic/base.py
================================================
import aiofiles
import asyncio
import importlib
import json
import subprocess
import sys
import traceback
from typing import Any
from colorama import Fore, Style, init
import os
from enum import Enum

_SUPPORTED_PROVIDERS = {
    "openai",
    "anthropic",
    "azure_openai",
    "cohere",
    "google_vertexai",
    "google_genai",
    "fireworks",
    "ollama",
    "together",
    "mistralai",
    "huggingface",
    "groq",
    "bedrock",
    "dashscope",
    "xai",
    "deepseek",
    "litellm",
    "gigachat",
    "openrouter",
    "vllm_openai",
    "aimlapi",
    "netmind",
}

NO_SUPPORT_TEMPERATURE_MODELS = [
    "deepseek/deepseek-reasoner",
    "o1-mini",
    "o1-mini-2024-09-12",
    "o1",
    "o1-2024-12-17",
    "o3-mini",
    "o3-mini-2025-01-31",
    "o1-preview",
    "o3",
    "o3-2025-04-16",
    "o4-mini",
    "o4-mini-2025-04-16",
    # GPT-5 family: OpenAI enforces default temperature only
    "gpt-5",
    "gpt-5-mini",
]

SUPPORT_REASONING_EFFORT_MODELS = [
    "o3-mini",
    "o3-mini-2025-01-31",
    "o3",
    "o3-2025-04-16",
    "o4-mini",
    "o4-mini-2025-04-16",
]

class ReasoningEfforts(Enum):
    High = "high"
    Medium = "medium"
    Low = "low"


class ChatLogger:
    """Helper utility to log all chat requests and their corresponding responses
    plus the stack trace leading to the call.
    """

    def __init__(self, fname: str):
        self.fname = fname
        self._lock = asyncio.Lock()

    async def log_request(self, messages, response):
        async with self._lock:
            async with aiofiles.open(self.fname, mode="a", encoding="utf-8") as handle:
                await handle.write(json.dumps({
                    "messages": messages,
                    "response": response,
                    "stacktrace": traceback.format_exc()
                }) + "\n")

class GenericLLMProvider:

    def __init__(self, llm, chat_log: str | None = None,  verbose: bool = True):
        self.llm = llm
        self.chat_logger = ChatLogger(chat_log) if chat_log else None
        self.verbose = verbose
    @classmethod
    def from_provider(cls, provider: str, chat_log: str | None = None, verbose: bool=True, **kwargs: Any):
        if provider == "openai":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI

            # Support custom OpenAI-compatible APIs via OPENAI_BASE_URL
            if "openai_api_base" not in kwargs and os.environ.get("OPENAI_BASE_URL"):
                kwargs["openai_api_base"] = os.environ["OPENAI_BASE_URL"]

            llm = ChatOpenAI(**kwargs)
        elif provider == "anthropic":
            _check_pkg("langchain_anthropic")
            from langchain_anthropic import ChatAnthropic

            llm = ChatAnthropic(**kwargs)
        elif provider == "azure_openai":
            _check_pkg("langchain_openai")
            from langchain_openai import AzureChatOpenAI

            if "model" in kwargs:
                model_name = kwargs.get("model", None)
                kwargs = {"azure_deployment": model_name, **kwargs}

            llm = AzureChatOpenAI(**kwargs)
        elif provider == "cohere":
            _check_pkg("langchain_cohere")
            from langchain_cohere import ChatCohere

            llm = ChatCohere(**kwargs)
        elif provider == "google_vertexai":
            _check_pkg("langchain_google_vertexai")
            from langchain_google_vertexai import ChatVertexAI

            llm = ChatVertexAI(**kwargs)
        elif provider == "google_genai":
            _check_pkg("langchain_google_genai")
            from langchain_google_genai import ChatGoogleGenerativeAI

            llm = ChatGoogleGenerativeAI(**kwargs)
        elif provider == "fireworks":
            _check_pkg("langchain_fireworks")
            from langchain_fireworks import ChatFireworks

            llm = ChatFireworks(**kwargs)
        elif provider == "ollama":
            _check_pkg("langchain_community")
            _check_pkg("langchain_ollama")
            from langchain_ollama import ChatOllama

            llm = ChatOllama(base_url=os.environ["OLLAMA_BASE_URL"], **kwargs)
        elif provider == "together":
            _check_pkg("langchain_together")
            from langchain_together import ChatTogether

            llm = ChatTogether(**kwargs)
        elif provider == "mistralai":
            _check_pkg("langchain_mistralai")
            from langchain_mistralai import ChatMistralAI

            llm = ChatMistralAI(**kwargs)
        elif provider == "huggingface":
            _check_pkg("langchain_huggingface")
            from langchain_huggingface import ChatHuggingFace

            if "model" in kwargs or "model_name" in kwargs:
                model_id = kwargs.pop("model", None) or kwargs.pop("model_name", None)
                kwargs = {"model_id": model_id, **kwargs}
            llm = ChatHuggingFace(**kwargs)
        elif provider == "groq":
            _check_pkg("langchain_groq")
            from langchain_groq import ChatGroq

            llm = ChatGroq(**kwargs)
        elif provider == "bedrock":
            _check_pkg("langchain_aws")
            from langchain_aws import ChatBedrock

            if "model" in kwargs or "model_name" in kwargs:
                model_id = kwargs.pop("model", None) or kwargs.pop("model_name", None)
                kwargs = {"model_id": model_id, "model_kwargs": kwargs}
            llm = ChatBedrock(**kwargs)
        elif provider == "dashscope":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(openai_api_base='https://dashscope.aliyuncs.com/compatible-mode/v1',
                     openai_api_key=os.environ["DASHSCOPE_API_KEY"],
                     **kwargs
                )
        elif provider == "xai":
            _check_pkg("langchain_xai")
            from langchain_xai import ChatXAI

            llm = ChatXAI(**kwargs)
        elif provider == "deepseek":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(openai_api_base='https://api.deepseek.com',
                     openai_api_key=os.environ["DEEPSEEK_API_KEY"],
                     **kwargs
                )
        elif provider == "litellm":
            _check_pkg("langchain_community")
            from langchain_community.chat_models.litellm import ChatLiteLLM

            llm = ChatLiteLLM(**kwargs)
        elif provider == "gigachat":
            _check_pkg("langchain_gigachat")
            from langchain_gigachat.chat_models import GigaChat

            kwargs.pop("model", None) # Use env GIGACHAT_MODEL=GigaChat-Max
            llm = GigaChat(**kwargs)
        elif provider == "openrouter":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI
            from langchain_core.rate_limiters import InMemoryRateLimiter

            rps = float(os.environ["OPENROUTER_LIMIT_RPS"]) if "OPENROUTER_LIMIT_RPS" in os.environ else 1.0

            rate_limiter = InMemoryRateLimiter(
                requests_per_second=rps,
                check_every_n_seconds=0.1,
                max_bucket_size=10,
            )

            llm = ChatOpenAI(openai_api_base='https://openrouter.ai/api/v1',
                     openai_api_key=os.environ["OPENROUTER_API_KEY"],
                     rate_limiter=rate_limiter,
                     **kwargs
                )
        elif provider == "vllm_openai":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(
                openai_api_key=os.environ["VLLM_OPENAI_API_KEY"],
                openai_api_base=os.environ["VLLM_OPENAI_API_BASE"],
                **kwargs
            )
        elif provider == "aimlapi":
            _check_pkg("langchain_openai")
            from langchain_openai import ChatOpenAI

            llm = ChatOpenAI(openai_api_base='https://api.aimlapi.com/v1',
                             openai_api_key=os.environ["AIMLAPI_API_KEY"],
                             **kwargs
                             )
        elif provider == 'netmind':
            _check_pkg("langchain_netmind")
            from langchain_netmind import ChatNetmind

            llm = ChatNetmind(**kwargs)
        else:
            supported = ", ".join(_SUPPORTED_PROVIDERS)
            raise ValueError(
                f"Unsupported {provider}.\n\nSupported model providers are: {supported}"
            )
        return cls(llm, chat_log, verbose=verbose)


    async def get_chat_response(self, messages, stream, websocket=None, **kwargs):
        if not stream:
            # Getting output from the model chain using ainvoke for asynchronous invoking
            output = await self.llm.ainvoke(messages, **kwargs)

            res = output.content

        else:
            res = await self.stream_response(messages, websocket, **kwargs)

        if self.chat_logger:
            await self.chat_logger.log_request(messages, res)

        return res

    async def stream_response(self, messages, websocket=None, **kwargs):
        paragraph = ""
        response = ""

        # Streaming the response using the chain astream method from langchain
        async for chunk in self.llm.astream(messages, **kwargs):
            content = chunk.content
            if content is not None:
                response += content
                paragraph += content
                if "\n" in paragraph:
                    await self._send_output(paragraph, websocket)
                    paragraph = ""

        if paragraph:
            await self._send_output(paragraph, websocket)

        return response

    async def _send_output(self, content, websocket=None):
        if websocket is not None:
            await websocket.send_json({"type": "report", "output": content})
        elif self.verbose:
            print(f"{Fore.GREEN}{content}{Style.RESET_ALL}")


def _check_pkg(pkg: str) -> None:
    if not importlib.util.find_spec(pkg):
        pkg_kebab = pkg.replace("_", "-")
        # Import colorama and initialize it
        init(autoreset=True)

        try:
            print(f"{Fore.YELLOW}Installing {pkg_kebab}...{Style.RESET_ALL}")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", pkg_kebab])
            print(f"{Fore.GREEN}Successfully installed {pkg_kebab}{Style.RESET_ALL}")

            # Try importing again after install
            importlib.import_module(pkg)

        except subprocess.CalledProcessError:
            raise ImportError(
                Fore.RED + f"Failed to install {pkg_kebab}. Please install manually with "
                f"`pip install -U {pkg_kebab}`"
            )



================================================
FILE: gpt_researcher/mcp/README.md
================================================
# GPT Researcher MCP Integration

This directory contains the comprehensive Model Context Protocol (MCP) integration for GPT Researcher. MCP enables GPT Researcher to seamlessly connect with and utilize external tools and data sources through a standardized protocol.

## ğŸ”§ What is MCP?

Model Context Protocol (MCP) is an open standard that enables secure connections between AI applications and external data sources and tools. With MCP, GPT Researcher can:

- **Access Local Data**: Connect to databases, file systems, and local APIs
- **Use External Tools**: Integrate with web services, APIs, and third-party tools
- **Extend Capabilities**: Add custom functionality through MCP servers
- **Maintain Security**: Controlled access with proper authentication and permissions

## ğŸ“ Module Structure

```
gpt_researcher/mcp/
â”œâ”€â”€ __init__.py           # Module initialization and imports
â”œâ”€â”€ client.py             # MCP client management and configuration
â”œâ”€â”€ tool_selector.py      # Intelligent tool selection using LLM
â”œâ”€â”€ research.py           # Research execution with selected tools
â”œâ”€â”€ streaming.py          # WebSocket streaming and logging utilities
â””â”€â”€ README.md            # This documentation
```

### Core Components

#### ğŸ¤– `client.py` - MCPClientManager
Handles MCP server connections and client lifecycle:
- Converts GPT Researcher configs to MCP format
- Manages MultiServerMCPClient instances
- Handles connection types (stdio, websocket, HTTP)
- Provides automatic cleanup and resource management

#### ğŸ§  `tool_selector.py` - MCPToolSelector
Intelligent tool selection using LLM analysis:
- Analyzes available tools against research queries
- Uses strategic LLM for optimal tool selection
- Provides fallback pattern-matching selection
- Limits tool selection to prevent overhead

#### ğŸ” `research.py` - MCPResearchSkill
Executes research using selected MCP tools:
- Binds tools to LLM for intelligent usage
- Manages tool execution and error handling
- Processes results into standard format
- Includes LLM analysis alongside tool results

#### ğŸ“¡ `streaming.py` - MCPStreamer
Real-time streaming and logging:
- WebSocket streaming for live updates
- Structured logging for debugging
- Progress tracking and status updates
- Error and warning management

## ğŸš€ Getting Started

### Prerequisites

1. **Install MCP Dependencies**:
   ```bash
   pip install langchain-mcp-adapters
   ```

2. **Setup MCP Server**: You need at least one MCP server to connect to. This could be:
   - A local server you develop
   - A third-party MCP server
   - A cloud-based MCP service

### Basic Usage

#### 1. Configure MCP in GPT Researcher

```python
from gpt_researcher import GPTResearcher

# MCP configuration for a local server
mcp_configs = [{
    "command": "python",
    "args": ["my_mcp_server.py"],
    "name": "local_server",
    "tool_name": "search"  # Optional: specify specific tool
}]

# Initialize researcher with MCP
researcher = GPTResearcher(
    query="What are the latest developments in AI?",
    mcp_configs=mcp_configs
)

# Conduct research using MCP tools
context = await researcher.conduct_research()
report = await researcher.write_report()
```

#### 2. WebSocket/HTTP Server Configuration

```python
# WebSocket MCP server
mcp_configs = [{
    "connection_url": "ws://localhost:8080/mcp",
    "connection_type": "websocket",
    "name": "websocket_server"
}]

# HTTP MCP server
mcp_configs = [{
    "connection_url": "https://api.example.com/mcp",
    "connection_type": "http",
    "connection_token": "your-auth-token",
    "name": "http_server"
}]
```

#### 3. Multiple Servers

```python
mcp_configs = [
    {
        "command": "python",
        "args": ["database_server.py"],
        "name": "database",
        "env": {"DB_HOST": "localhost"}
    },
    {
        "connection_url": "ws://localhost:8080/search",
        "name": "search_service"
    },
    {
        "connection_url": "https://api.knowledge.com/mcp",
        "connection_token": "token123",
        "name": "knowledge_base"
    }
]
```

## ğŸ”§ Configuration Options

### MCP Server Configuration

Each MCP server configuration supports the following options:

| Field              | Type | Description | Example |
|--------------------|------|-------------|---------|
| `name`             | `str` | Unique name for the server | `"my_server"` |
| `command`          | `str` | Command to start stdio server | `"python"` |
| `args`             | `list[str]` | Arguments for the command | `["server.py", "--port", "8080"]` |
| `connection_url`   | `str` | URL for websocket/HTTP connection | `"ws://localhost:8080/mcp"` |
| `connection_type`  | `str` | Connection type | `"stdio"`, `"websocket"`, `"http"` |
| `connection_token` | `str` | Authentication token | `"your-token"` |
| `tool_name`        | `str` | Specific tool to use (optional) | `"search"` |
| `env`              | `dict` | Environment variables | `{"API_KEY": "secret"}` |

### Auto-Detection Features

The MCP client automatically detects connection types:
- URLs starting with `ws://` or `wss://` â†’ WebSocket
- URLs starting with `http://` or `https://` â†’ HTTP  
- No URL provided â†’ stdio (default)

## ğŸ—ï¸ Development

### Adding New Components

1. **Create your component** in the appropriate file
2. **Add it to `__init__.py`** for easy importing
3. **Update this README** with documentation
4. **Add tests** in the tests directory

### Extending Tool Selection

To customize tool selection logic, extend `MCPToolSelector`:

```python
from gpt_researcher.mcp import MCPToolSelector

class CustomToolSelector(MCPToolSelector):
    def _fallback_tool_selection(self, all_tools, max_tools):
        # Custom fallback logic
        return super()._fallback_tool_selection(all_tools, max_tools)
```

### Custom Result Processing

Extend `MCPResearchSkill` for custom result processing:

```python
from gpt_researcher.mcp import MCPResearchSkill

class CustomResearchSkill(MCPResearchSkill):
    def _process_tool_result(self, tool_name, result):
        # Custom result processing
        return super()._process_tool_result(tool_name, result)
```

## ğŸ”’ Security Considerations

- **Token Management**: Store authentication tokens securely
- **Server Validation**: Only connect to trusted MCP servers
- **Environment Variables**: Use env vars for sensitive configuration
- **Network Security**: Use HTTPS/WSS for remote connections
- **Access Control**: Implement proper permission controls

## ğŸ› Troubleshooting

### Common Issues

1. **Import Error**: `langchain-mcp-adapters not installed`
   ```bash
   pip install langchain-mcp-adapters
   ```

2. **Connection Failed**: Check server URL and authentication
   - Verify server is running
   - Check connection URL format
   - Validate authentication tokens

3. **No Tools Available**: Server may not be exposing tools
   - Check server implementation
   - Verify tool registration
   - Review server logs

4. **Tool Selection Issues**: LLM may not select appropriate tools
   - Review tool descriptions
   - Check query relevance
   - Consider custom selection logic

### Debug Logging

Enable debug logging for detailed information:

```python
import logging
logging.getLogger('gpt_researcher.mcp').setLevel(logging.DEBUG)
```

## ğŸ“š Resources

- **MCP Specification**: [Model Context Protocol Docs](https://spec.modelcontextprotocol.io/)
- **langchain-mcp-adapters**: [GitHub Repository](https://github.com/modelcontextprotocol/langchain-mcp-adapters)
- **GPT Researcher Docs**: [Documentation](https://docs.gptr.dev/)
- **Example MCP Servers**: [MCP Examples](https://github.com/modelcontextprotocol/servers)

## ğŸ¤ Contributing

Contributions to the MCP integration are welcome! Please:

1. **Follow the project structure** outlined above
2. **Add comprehensive tests** for new functionality  
3. **Update documentation** including this README
4. **Follow coding standards** consistent with the project
5. **Consider backwards compatibility** when making changes

---

*This MCP integration brings powerful extensibility to GPT Researcher, enabling connections to virtually any data source or tool through the standardized MCP protocol.* ğŸ™‚ 


================================================
FILE: gpt_researcher/mcp/__init__.py
================================================
"""
MCP (Model Context Protocol) Integration for GPT Researcher

This module provides comprehensive MCP integration including:
- Client management for MCP servers
- Tool selection and execution
- Research execution with MCP tools
- Streaming support for real-time updates
"""

import logging

logger = logging.getLogger(__name__)

try:
    # Check if langchain-mcp-adapters is available
    from langchain_mcp_adapters.client import MultiServerMCPClient
    HAS_MCP_ADAPTERS = True
    logger.debug("langchain-mcp-adapters is available")
    
    # Import core MCP components
    from .client import MCPClientManager
    from .tool_selector import MCPToolSelector
    from .research import MCPResearchSkill
    from .streaming import MCPStreamer
    
    __all__ = [
        "MCPClientManager",
        "MCPToolSelector", 
        "MCPResearchSkill",
        "MCPStreamer",
        "HAS_MCP_ADAPTERS"
    ]
    
except ImportError as e:
    logger.warning(f"MCP dependencies not available: {e}")
    HAS_MCP_ADAPTERS = False
    __all__ = ["HAS_MCP_ADAPTERS"]
    
except Exception as e:
    logger.error(f"Unexpected error importing MCP components: {e}")
    HAS_MCP_ADAPTERS = False
    __all__ = ["HAS_MCP_ADAPTERS"] 


================================================
FILE: gpt_researcher/mcp/client.py
================================================
"""
MCP Client Management Module

Handles MCP client creation, configuration conversion, and connection management.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

try:
    from langchain_mcp_adapters.client import MultiServerMCPClient
    HAS_MCP_ADAPTERS = True
except ImportError:
    HAS_MCP_ADAPTERS = False

logger = logging.getLogger(__name__)


class MCPClientManager:
    """
    Manages MCP client lifecycle and configuration.
    
    Responsible for:
    - Converting GPT Researcher MCP configs to langchain format
    - Creating and managing MultiServerMCPClient instances
    - Handling client cleanup and resource management
    """

    def __init__(self, mcp_configs: List[Dict[str, Any]]):
        """
        Initialize the MCP client manager.
        
        Args:
            mcp_configs: List of MCP server configurations from GPT Researcher
        """
        self.mcp_configs = mcp_configs or []
        self._client = None
        self._client_lock = asyncio.Lock()

    def convert_configs_to_langchain_format(self) -> Dict[str, Dict[str, Any]]:
        """
        Convert GPT Researcher MCP configs to langchain-mcp-adapters format.
        
        Returns:
            Dict[str, Dict[str, Any]]: Server configurations for MultiServerMCPClient
        """
        server_configs = {}
        
        for i, config in enumerate(self.mcp_configs):
            # Generate server name
            server_name = config.get("name", f"mcp_server_{i+1}")
            
            # Build the server config
            server_config = {}
            
            # Auto-detect transport type from URL if provided
            connection_url = config.get("connection_url")
            if connection_url:
                if connection_url.startswith(("wss://", "ws://")):
                    server_config["transport"] = "websocket"
                    server_config["url"] = connection_url
                elif connection_url.startswith(("https://", "http://")):
                    server_config["transport"] = "streamable_http"
                    server_config["url"] = connection_url
                else:
                    # Fallback to specified connection_type or stdio
                    connection_type = config.get("connection_type", "stdio")
                    server_config["transport"] = connection_type
                    if connection_type in ["websocket", "streamable_http", "http"]:
                        server_config["url"] = connection_url
            else:
                # No URL provided, use stdio (default) or specified connection_type
                connection_type = config.get("connection_type", "stdio")
                server_config["transport"] = connection_type
            
            # Handle stdio transport configuration
            if server_config.get("transport") == "stdio":
                if config.get("command"):
                    server_config["command"] = config["command"]
                    
                    # Handle server_args
                    server_args = config.get("args", [])
                    if isinstance(server_args, str):
                        server_args = server_args.split()
                    server_config["args"] = server_args
                    
                    # Handle environment variables
                    server_env = config.get("env", {})
                    if server_env:
                        server_config["env"] = server_env
                        
            # Add authentication if provided
            if config.get("connection_token"):
                server_config["token"] = config["connection_token"]
                
            server_configs[server_name] = server_config
            
        return server_configs

    async def get_or_create_client(self) -> Optional[object]:
        """
        Get or create a MultiServerMCPClient with proper lifecycle management.
        
        Returns:
            MultiServerMCPClient: The client instance or None if creation fails
        """
        async with self._client_lock:
            if self._client is not None:
                return self._client
                
            if not HAS_MCP_ADAPTERS:
                logger.error("langchain-mcp-adapters not installed")
                return None
                
            if not self.mcp_configs:
                logger.error("No MCP server configurations found")
                return None
                
            try:
                # Convert configs to langchain format
                server_configs = self.convert_configs_to_langchain_format()
                logger.info(f"Creating MCP client for {len(server_configs)} server(s)")
                
                # Initialize the MultiServerMCPClient
                self._client = MultiServerMCPClient(server_configs)
                
                return self._client
                
            except Exception as e:
                logger.error(f"Error creating MCP client: {e}")
                return None

    async def close_client(self):
        """
        Properly close the MCP client and clean up resources.
        """
        async with self._client_lock:
            if self._client is not None:
                try:
                    # Since MultiServerMCPClient doesn't support context manager
                    # or explicit close methods in langchain-mcp-adapters 0.1.0,
                    # we just clear the reference and let garbage collection handle it
                    logger.debug("Releasing MCP client reference")
                except Exception as e:
                    logger.error(f"Error during MCP client cleanup: {e}")
                finally:
                    # Always clear the reference
                    self._client = None

    async def get_all_tools(self) -> List:
        """
        Get all available tools from MCP servers.
        
        Returns:
            List: All available MCP tools
        """
        client = await self.get_or_create_client()
        if not client:
            return []
            
        try:
            # Get tools from all servers
            all_tools = await client.get_tools()
            
            if all_tools:
                logger.info(f"Loaded {len(all_tools)} total tools from MCP servers")
                return all_tools
            else:
                logger.warning("No tools available from MCP servers")
                return []
                
        except Exception as e:
            logger.error(f"Error getting MCP tools: {e}")
            return [] 


================================================
FILE: gpt_researcher/mcp/research.py
================================================
"""
MCP Research Execution Skill

Handles research execution using selected MCP tools as a skill component.
"""
import asyncio
import logging
from typing import List, Dict, Any

logger = logging.getLogger(__name__)


class MCPResearchSkill:
    """
    Handles research execution using selected MCP tools.
    
    Responsible for:
    - Executing research with LLM and bound tools
    - Processing tool results into standard format
    - Managing tool execution and error handling
    """

    def __init__(self, cfg, researcher=None):
        """
        Initialize the MCP research skill.
        
        Args:
            cfg: Configuration object with LLM settings
            researcher: Researcher instance for cost tracking
        """
        self.cfg = cfg
        self.researcher = researcher

    async def conduct_research_with_tools(self, query: str, selected_tools: List) -> List[Dict[str, str]]:
        """
        Use LLM with bound tools to conduct intelligent research.
        
        Args:
            query: Research query
            selected_tools: List of selected MCP tools
            
        Returns:
            List[Dict[str, str]]: Research results in standard format
        """
        if not selected_tools:
            logger.warning("No tools available for research")
            return []
            
        logger.info(f"Conducting research using {len(selected_tools)} selected tools")
        
        try:
            from ..llm_provider.generic.base import GenericLLMProvider
            
            # Create LLM provider using the config
            provider_kwargs = {
                'model': self.cfg.strategic_llm_model,
                **self.cfg.llm_kwargs
            }
            
            llm_provider = GenericLLMProvider.from_provider(
                self.cfg.strategic_llm_provider, 
                **provider_kwargs
            )
            
            # Bind tools to LLM
            llm_with_tools = llm_provider.llm.bind_tools(selected_tools)
            
            # Import here to avoid circular imports
            from ..prompts import PromptFamily
            
            # Create research prompt
            research_prompt = PromptFamily.generate_mcp_research_prompt(query, selected_tools)

            # Create messages
            messages = [{"role": "user", "content": research_prompt}]
            
            # Invoke LLM with tools
            logger.info("LLM researching with bound tools...")
            response = await llm_with_tools.ainvoke(messages)
            
            # Process tool calls and results
            research_results = []
            
            # Check if the LLM made tool calls
            if hasattr(response, 'tool_calls') and response.tool_calls:
                logger.info(f"LLM made {len(response.tool_calls)} tool calls")
                
                # Process each tool call
                for i, tool_call in enumerate(response.tool_calls, 1):
                    tool_name = tool_call.get("name", "unknown")
                    tool_args = tool_call.get("args", {})
                    
                    logger.info(f"Executing tool {i}/{len(response.tool_calls)}: {tool_name}")
                    
                    # Log the tool arguments for transparency
                    if tool_args:
                        args_str = ", ".join([f"{k}={v}" for k, v in tool_args.items()])
                        logger.debug(f"Tool arguments: {args_str}")
                    
                    try:
                        # Find the tool by name
                        tool = next((t for t in selected_tools if t.name == tool_name), None)
                        if not tool:
                            logger.warning(f"Tool {tool_name} not found in selected tools")
                            continue
                        
                        # Execute the tool
                        if hasattr(tool, 'ainvoke'):
                            result = await tool.ainvoke(tool_args)
                        elif hasattr(tool, 'invoke'):
                            result = tool.invoke(tool_args)
                        else:
                            result = await tool(tool_args) if asyncio.iscoroutinefunction(tool) else tool(tool_args)
                        
                        # Log the actual tool response for debugging
                        if result:
                            result_preview = str(result)[:500] + "..." if len(str(result)) > 500 else str(result)
                            logger.debug(f"Tool {tool_name} response preview: {result_preview}")
                            
                            # Process the result
                            formatted_results = self._process_tool_result(tool_name, result)
                            research_results.extend(formatted_results)
                            logger.info(f"Tool {tool_name} returned {len(formatted_results)} formatted results")
                            
                            # Log details of each formatted result
                            for j, formatted_result in enumerate(formatted_results):
                                title = formatted_result.get("title", "No title")
                                content_preview = formatted_result.get("body", "")[:200] + "..." if len(formatted_result.get("body", "")) > 200 else formatted_result.get("body", "")
                                logger.debug(f"Result {j+1}: '{title}' - Content: {content_preview}")
                        else:
                            logger.warning(f"Tool {tool_name} returned empty result")
                            
                    except Exception as e:
                        logger.error(f"Error executing tool {tool_name}: {e}")
                        continue
                        
            # Also include the LLM's own analysis/response as a result
            if hasattr(response, 'content') and response.content:
                llm_analysis = {
                    "title": f"LLM Analysis: {query}",
                    "href": "mcp://llm_analysis",
                    "body": response.content
                }
                research_results.append(llm_analysis)
                
                # Log LLM analysis content
                analysis_preview = response.content[:300] + "..." if len(response.content) > 300 else response.content
                logger.debug(f"LLM Analysis: {analysis_preview}")
                logger.info("Added LLM analysis to results")
            
            logger.info(f"Research completed with {len(research_results)} total results")
            return research_results
            
        except Exception as e:
            logger.error(f"Error in LLM research with tools: {e}")
            return []

    def _process_tool_result(self, tool_name: str, result: Any) -> List[Dict[str, str]]:
        """
        Process tool result into search result format.
        
        Args:
            tool_name: Name of the tool that produced the result
            result: The tool result
            
        Returns:
            List[Dict[str, str]]: Formatted search results
        """
        search_results = []
        
        try:
            # 1) First: handle MCP result wrapper with structured_content/content
            if isinstance(result, dict) and ("structured_content" in result or "content" in result):
                search_results = []
                # Prefer structured_content when present
                structured = result.get("structured_content")
                if isinstance(structured, dict):
                    items = structured.get("results")
                    if isinstance(items, list):
                        for i, item in enumerate(items):
                            if isinstance(item, dict):
                                search_results.append({
                                    "title": item.get("title", f"Result from {tool_name} #{i+1}"),
                                    "href": item.get("href", item.get("url", f"mcp://{tool_name}/{i}")),
                                    "body": item.get("body", item.get("content", str(item)))
                                })
                    # If no items array but structured is dict, treat as single
                    elif isinstance(structured, dict):
                        search_results.append({
                            "title": structured.get("title", f"Result from {tool_name}"),
                            "href": structured.get("href", structured.get("url", f"mcp://{tool_name}")),
                            "body": structured.get("body", structured.get("content", str(structured)))
                        })
                # Fallback to content if provided (MCP spec: list of {type: text, text: ...})
                if not search_results:
                    content_field = result.get("content")
                    if isinstance(content_field, list):
                        texts = []
                        for part in content_field:
                            if isinstance(part, dict):
                                if part.get("type") == "text" and isinstance(part.get("text"), str):
                                    texts.append(part["text"])
                                elif "text" in part:
                                    texts.append(str(part.get("text")))
                                else:
                                    # unknown piece; stringify
                                    texts.append(str(part))
                            else:
                                texts.append(str(part))
                        body_text = "\n\n".join([t for t in texts if t])
                    elif isinstance(content_field, str):
                        body_text = content_field
                    else:
                        body_text = str(result)
                    search_results.append({
                        "title": f"Result from {tool_name}",
                        "href": f"mcp://{tool_name}",
                        "body": body_text,
                    })
                return search_results

            # 2) If the result is already a list, process each item normally
            if isinstance(result, list):
                # If the result is already a list, process each item
                for i, item in enumerate(result):
                    if isinstance(item, dict):
                        # Use the item as is if it has required fields
                        if "title" in item and ("content" in item or "body" in item):
                            search_result = {
                                "title": item.get("title", ""),
                                "href": item.get("href", item.get("url", f"mcp://{tool_name}/{i}")),
                                "body": item.get("body", item.get("content", str(item))),
                            }
                            search_results.append(search_result)
                        else:
                            # Create a search result with a generic title
                            search_result = {
                                "title": f"Result from {tool_name}",
                                "href": f"mcp://{tool_name}/{i}",
                                "body": str(item),
                            }
                            search_results.append(search_result)
            # 3) If the result is a dict (non-MCP wrapper), use it as a single search result
            elif isinstance(result, dict):
                # If the result is a dictionary, use it as a single search result
                search_result = {
                    "title": result.get("title", f"Result from {tool_name}"),
                    "href": result.get("href", result.get("url", f"mcp://{tool_name}")),
                    "body": result.get("body", result.get("content", str(result))),
                }
                search_results.append(search_result)
            else:
                # For any other type, convert to string and use as a single search result
                search_result = {
                    "title": f"Result from {tool_name}",
                    "href": f"mcp://{tool_name}",
                    "body": str(result),
                }
                search_results.append(search_result)
                
        except Exception as e:
            logger.error(f"Error processing tool result from {tool_name}: {e}")
            # Fallback: create a basic result
            search_result = {
                "title": f"Result from {tool_name}",
                "href": f"mcp://{tool_name}",
                "body": str(result),
            }
            search_results.append(search_result)
        
        return search_results 


================================================
FILE: gpt_researcher/mcp/streaming.py
================================================
"""
MCP Streaming Utilities Module

Handles websocket streaming and logging for MCP operations.
"""
import asyncio
import logging
from typing import Any, Optional

logger = logging.getLogger(__name__)


class MCPStreamer:
    """
    Handles streaming output for MCP operations.
    
    Responsible for:
    - Streaming logs to websocket
    - Synchronous/asynchronous logging
    - Error handling in streaming
    """

    def __init__(self, websocket=None):
        """
        Initialize the MCP streamer.
        
        Args:
            websocket: WebSocket for streaming output
        """
        self.websocket = websocket

    async def stream_log(self, message: str, data: Any = None):
        """Stream a log message to the websocket if available."""
        logger.info(message)
        
        if self.websocket:
            try:
                from ..actions.utils import stream_output
                await stream_output(
                    type="logs", 
                    content="mcp_retriever", 
                    output=message, 
                    websocket=self.websocket,
                    metadata=data
                )
            except Exception as e:
                logger.error(f"Error streaming log: {e}")
                
    def stream_log_sync(self, message: str, data: Any = None):
        """Synchronous version of stream_log for use in sync contexts."""
        logger.info(message)
        
        if self.websocket:
            try:
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        asyncio.create_task(self.stream_log(message, data))
                    else:
                        loop.run_until_complete(self.stream_log(message, data))
                except RuntimeError:
                    logger.debug("Could not stream log: no running event loop")
            except Exception as e:
                logger.error(f"Error in sync log streaming: {e}")

    async def stream_stage_start(self, stage: str, description: str):
        """Stream the start of a research stage."""
        await self.stream_log(f"ğŸ”§ {stage}: {description}")

    async def stream_stage_complete(self, stage: str, result_count: int = None):
        """Stream the completion of a research stage."""
        if result_count is not None:
            await self.stream_log(f"âœ… {stage} completed: {result_count} results")
        else:
            await self.stream_log(f"âœ… {stage} completed")

    async def stream_tool_selection(self, selected_count: int, total_count: int):
        """Stream tool selection information."""
        await self.stream_log(f"ğŸ§  Using LLM to select {selected_count} most relevant tools from {total_count} available")

    async def stream_tool_execution(self, tool_name: str, step: int, total: int):
        """Stream tool execution progress."""
        await self.stream_log(f"ğŸ” Executing tool {step}/{total}: {tool_name}")

    async def stream_research_results(self, result_count: int, total_chars: int = None):
        """Stream research results summary."""
        if total_chars:
            await self.stream_log(f"âœ… MCP research completed: {result_count} results obtained ({total_chars:,} chars)")
        else:
            await self.stream_log(f"âœ… MCP research completed: {result_count} results obtained")

    async def stream_error(self, error_msg: str):
        """Stream error messages."""
        await self.stream_log(f"âŒ {error_msg}")

    async def stream_warning(self, warning_msg: str):
        """Stream warning messages."""
        await self.stream_log(f"âš ï¸ {warning_msg}")

    async def stream_info(self, info_msg: str):
        """Stream informational messages."""
        await self.stream_log(f"â„¹ï¸ {info_msg}") 


================================================
FILE: gpt_researcher/mcp/tool_selector.py
================================================
"""
MCP Tool Selection Module

Handles intelligent tool selection using LLM analysis.
"""
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class MCPToolSelector:
    """
    Handles intelligent selection of MCP tools using LLM analysis.
    
    Responsible for:
    - Analyzing available tools with LLM
    - Selecting the most relevant tools for a query
    - Providing fallback selection mechanisms
    """

    def __init__(self, cfg, researcher=None):
        """
        Initialize the tool selector.
        
        Args:
            cfg: Configuration object with LLM settings
            researcher: Researcher instance for cost tracking
        """
        self.cfg = cfg
        self.researcher = researcher

    async def select_relevant_tools(self, query: str, all_tools: List, max_tools: int = 3) -> List:
        """
        Use LLM to select the most relevant tools for the research query.
        
        Args:
            query: Research query
            all_tools: List of all available tools
            max_tools: Maximum number of tools to select (default: 3)
            
        Returns:
            List: Selected tools most relevant for the query
        """
        if not all_tools:
            return []

        if len(all_tools) < max_tools:
            max_tools = len(all_tools)
            
        logger.info(f"Using LLM to select {max_tools} most relevant tools from {len(all_tools)} available")
        
        # Create tool descriptions for LLM analysis
        tools_info = []
        for i, tool in enumerate(all_tools):
            tool_info = {
                "index": i,
                "name": tool.name,
                "description": tool.description or "No description available"
            }
            tools_info.append(tool_info)
        
        # Import here to avoid circular imports
        from ..prompts import PromptFamily
        
        # Create prompt for intelligent tool selection
        prompt = PromptFamily.generate_mcp_tool_selection_prompt(query, tools_info, max_tools)

        try:
            # Call LLM for tool selection
            response = await self._call_llm_for_tool_selection(prompt)
            
            if not response:
                logger.warning("No LLM response for tool selection, using fallback")
                return self._fallback_tool_selection(all_tools, max_tools)
            
            # Log a preview of the LLM response for debugging
            response_preview = response[:500] + "..." if len(response) > 500 else response
            logger.debug(f"LLM tool selection response: {response_preview}")
            
            # Parse LLM response
            try:
                selection_result = json.loads(response)
            except json.JSONDecodeError:
                # Try to extract JSON from response
                import re
                json_match = re.search(r"\{.*\}", response, re.DOTALL)
                if json_match:
                    try:
                        selection_result = json.loads(json_match.group(0))
                    except json.JSONDecodeError:
                        logger.warning("Could not parse extracted JSON, using fallback")
                        return self._fallback_tool_selection(all_tools, max_tools)
                else:
                    logger.warning("No JSON found in LLM response, using fallback")
                    return self._fallback_tool_selection(all_tools, max_tools)
            
            selected_tools = []
            
            # Process selected tools
            for tool_selection in selection_result.get("selected_tools", []):
                tool_index = tool_selection.get("index")
                tool_name = tool_selection.get("name", "")
                reason = tool_selection.get("reason", "")
                relevance_score = tool_selection.get("relevance_score", 0)
                
                if tool_index is not None and 0 <= tool_index < len(all_tools):
                    selected_tools.append(all_tools[tool_index])
                    logger.info(f"Selected tool '{tool_name}' (score: {relevance_score}): {reason}")
            
            if len(selected_tools) == 0:
                logger.warning("No tools selected by LLM, using fallback selection")
                return self._fallback_tool_selection(all_tools, max_tools)
            
            # Log the overall selection reasoning
            selection_reasoning = selection_result.get("selection_reasoning", "No reasoning provided")
            logger.info(f"LLM selection strategy: {selection_reasoning}")
            
            logger.info(f"LLM selected {len(selected_tools)} tools for research")
            return selected_tools
            
        except Exception as e:
            logger.error(f"Error in LLM tool selection: {e}")
            logger.warning("Falling back to pattern-based selection")
            return self._fallback_tool_selection(all_tools, max_tools)

    async def _call_llm_for_tool_selection(self, prompt: str) -> str:
        """
        Call the LLM using the existing create_chat_completion function for tool selection.
        
        Args:
            prompt (str): The prompt to send to the LLM.
            
        Returns:
            str: The generated text response.
        """
        if not self.cfg:
            logger.warning("No config available for LLM call")
            return ""
            
        try:
            from ..utils.llm import create_chat_completion
            
            # Create messages for the LLM
            messages = [{"role": "user", "content": prompt}]
            
            # Use the strategic LLM for tool selection (as it's more complex reasoning)
            result = await create_chat_completion(
                model=self.cfg.strategic_llm_model,
                messages=messages,
                temperature=0.0,  # Low temperature for consistent tool selection
                llm_provider=self.cfg.strategic_llm_provider,
                llm_kwargs=self.cfg.llm_kwargs,
                cost_callback=self.researcher.add_costs if self.researcher and hasattr(self.researcher, 'add_costs') else None,
            )
            return result
        except Exception as e:
            logger.error(f"Error calling LLM for tool selection: {e}")
            return ""

    def _fallback_tool_selection(self, all_tools: List, max_tools: int) -> List:
        """
        Fallback tool selection using pattern matching if LLM selection fails.
        
        Args:
            all_tools: List of all available tools
            max_tools: Maximum number of tools to select
            
        Returns:
            List: Selected tools
        """
        # Define patterns for research-relevant tools
        research_patterns = [
            'search', 'get', 'read', 'fetch', 'find', 'list', 'query', 
            'lookup', 'retrieve', 'browse', 'view', 'show', 'describe'
        ]
        
        scored_tools = []
        
        for tool in all_tools:
            tool_name = tool.name.lower()
            tool_description = (tool.description or "").lower()
            
            # Calculate relevance score based on pattern matching
            score = 0
            for pattern in research_patterns:
                if pattern in tool_name:
                    score += 3
                if pattern in tool_description:
                    score += 1
            
            if score > 0:
                scored_tools.append((tool, score))
        
        # Sort by score and take top tools
        scored_tools.sort(key=lambda x: x[1], reverse=True)
        selected_tools = [tool for tool, score in scored_tools[:max_tools]]
        
        for i, (tool, score) in enumerate(scored_tools[:max_tools]):
            logger.info(f"Fallback selected tool {i+1}: {tool.name} (score: {score})")
        
        return selected_tools 


================================================
FILE: gpt_researcher/memory/__init__.py
================================================
from .embeddings import Memory



================================================
FILE: gpt_researcher/memory/embeddings.py
================================================
import os
from typing import Any

OPENAI_EMBEDDING_MODEL = os.environ.get(
    "OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"
)

_SUPPORTED_PROVIDERS = {
    "openai",
    "azure_openai",
    "cohere",
    "gigachat",
    "google_vertexai",
    "google_genai",
    "fireworks",
    "ollama",
    "together",
    "mistralai",
    "huggingface",
    "nomic",
    "voyageai",
    "dashscope",
    "custom",
    "bedrock",
    "aimlapi",
    "netmind",
}


class Memory:
    def __init__(self, embedding_provider: str, model: str, **embedding_kwargs: Any):
        _embeddings = None
        match embedding_provider:
            case "custom":
                from langchain_openai import OpenAIEmbeddings

                _embeddings = OpenAIEmbeddings(
                    model=model,
                    openai_api_key=os.getenv("OPENAI_API_KEY", "custom"),
                    openai_api_base=os.getenv(
                        "OPENAI_BASE_URL", "http://localhost:1234/v1"
                    ),  # default for lmstudio
                    check_embedding_ctx_length=False,
                    **embedding_kwargs,
                )  # quick fix for lmstudio
            case "openai":
                from langchain_openai import OpenAIEmbeddings

                # Support custom OpenAI-compatible APIs via OPENAI_BASE_URL
                if "openai_api_base" not in embedding_kwargs and os.environ.get("OPENAI_BASE_URL"):
                    embedding_kwargs["openai_api_base"] = os.environ["OPENAI_BASE_URL"]

                _embeddings = OpenAIEmbeddings(model=model, **embedding_kwargs)
            case "azure_openai":
                from langchain_openai import AzureOpenAIEmbeddings

                _embeddings = AzureOpenAIEmbeddings(
                    model=model,
                    azure_endpoint=os.environ["AZURE_OPENAI_ENDPOINT"],
                    openai_api_key=os.environ["AZURE_OPENAI_API_KEY"],
                    openai_api_version=os.environ["AZURE_OPENAI_API_VERSION"],
                    **embedding_kwargs,
                )
            case "cohere":
                from langchain_cohere import CohereEmbeddings

                _embeddings = CohereEmbeddings(model=model, **embedding_kwargs)
            case "google_vertexai":
                from langchain_google_vertexai import VertexAIEmbeddings

                _embeddings = VertexAIEmbeddings(model=model, **embedding_kwargs)
            case "google_genai":
                from langchain_google_genai import GoogleGenerativeAIEmbeddings

                _embeddings = GoogleGenerativeAIEmbeddings(
                    model=model, **embedding_kwargs
                )
            case "fireworks":
                from langchain_fireworks import FireworksEmbeddings

                _embeddings = FireworksEmbeddings(model=model, **embedding_kwargs)
            case "gigachat":
                from langchain_gigachat import GigaChatEmbeddings

                _embeddings = GigaChatEmbeddings(model=model, **embedding_kwargs)
            case "ollama":
                from langchain_ollama import OllamaEmbeddings

                _embeddings = OllamaEmbeddings(
                    model=model,
                    base_url=os.environ["OLLAMA_BASE_URL"],
                    **embedding_kwargs,
                )
            case "together":
                from langchain_together import TogetherEmbeddings

                _embeddings = TogetherEmbeddings(model=model, **embedding_kwargs)
            case "netmind":
                from langchain_netmind import NetmindEmbeddings

                _embeddings = NetmindEmbeddings(model=model, **embedding_kwargs)
            case "mistralai":
                from langchain_mistralai import MistralAIEmbeddings

                _embeddings = MistralAIEmbeddings(model=model, **embedding_kwargs)
            case "huggingface":
                from langchain_huggingface import HuggingFaceEmbeddings

                _embeddings = HuggingFaceEmbeddings(model_name=model, **embedding_kwargs)
            case "nomic":
                from langchain_nomic import NomicEmbeddings

                _embeddings = NomicEmbeddings(model=model, **embedding_kwargs)
            case "voyageai":
                from langchain_voyageai import VoyageAIEmbeddings

                _embeddings = VoyageAIEmbeddings(
                    voyage_api_key=os.environ["VOYAGE_API_KEY"],
                    model=model,
                    **embedding_kwargs,
                )
            case "dashscope":
                from langchain_community.embeddings import DashScopeEmbeddings

                _embeddings = DashScopeEmbeddings(model=model, **embedding_kwargs)
            case "bedrock":
                from langchain_aws.embeddings import BedrockEmbeddings

                _embeddings = BedrockEmbeddings(model_id=model, **embedding_kwargs)
            case "aimlapi":
                from langchain_openai import OpenAIEmbeddings

                _embeddings = OpenAIEmbeddings(
                    model=model,
                    openai_api_key=os.getenv("AIMLAPI_API_KEY"),
                    openai_api_base=os.getenv("AIMLAPI_BASE_URL", "https://api.aimlapi.com/v1"),
                    **embedding_kwargs,
                )
            case _:
                raise Exception("Embedding not found.")

        self._embeddings = _embeddings

    def get_embeddings(self):
        return self._embeddings



================================================
FILE: gpt_researcher/retrievers/__init__.py
================================================
from .arxiv.arxiv import ArxivSearch
from .bing.bing import BingSearch
from .custom.custom import CustomRetriever
from .duckduckgo.duckduckgo import Duckduckgo
from .google.google import GoogleSearch
from .pubmed_central.pubmed_central import PubMedCentralSearch
from .searx.searx import SearxSearch
from .semantic_scholar.semantic_scholar import SemanticScholarSearch
from .searchapi.searchapi import SearchApiSearch
from .serpapi.serpapi import SerpApiSearch
from .serper.serper import SerperSearch
from .tavily.tavily_search import TavilySearch
from .exa.exa import ExaSearch
from .mcp import MCPRetriever

__all__ = [
    "TavilySearch",
    "CustomRetriever",
    "Duckduckgo",
    "SearchApiSearch",
    "SerperSearch",
    "SerpApiSearch",
    "GoogleSearch",
    "SearxSearch",
    "BingSearch",
    "ArxivSearch",
    "SemanticScholarSearch",
    "PubMedCentralSearch",
    "ExaSearch",
    "MCPRetriever"
]



================================================
FILE: gpt_researcher/retrievers/utils.py
================================================
import importlib.util
import logging
import os
import sys

logger = logging.getLogger(__name__)

async def stream_output(log_type, step, content, websocket=None, with_data=False, data=None):
    """
    Stream output to the client.
    
    Args:
        log_type (str): The type of log
        step (str): The step being performed
        content (str): The content to stream
        websocket: The websocket to stream to
        with_data (bool): Whether to include data
        data: Additional data to include
    """
    if websocket:
        try:
            if with_data:
                await websocket.send_json({
                    "type": log_type,
                    "step": step,
                    "content": content,
                    "data": data
                })
            else:
                await websocket.send_json({
                    "type": log_type,
                    "step": step,
                    "content": content
                })
        except Exception as e:
            logger.error(f"Error streaming output: {e}")

def check_pkg(pkg: str) -> None:
    """
    Checks if a package is installed and raises an error if not.
    
    Args:
        pkg (str): The package name
    
    Raises:
        ImportError: If the package is not installed
    """
    if not importlib.util.find_spec(pkg):
        pkg_kebab = pkg.replace("_", "-")
        raise ImportError(
            f"Unable to import {pkg_kebab}. Please install with "
            f"`pip install -U {pkg_kebab}`"
        )

# Valid retrievers for fallback
VALID_RETRIEVERS = [
    "tavily",
    "custom",
    "duckduckgo",
    "searchapi",
    "serper",
    "serpapi",
    "google",
    "searx",
    "bing",
    "arxiv",
    "semantic_scholar",
    "pubmed_central",
    "exa",
    "mcp",
    "mock"
]

def get_all_retriever_names():
    """
    Get all available retriever names
    :return: List of all available retriever names
    :rtype: list
    """
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Get all items in the current directory
        all_items = os.listdir(current_dir)
        
        # Filter out only the directories, excluding __pycache__
        retrievers = [
            item for item in all_items 
            if os.path.isdir(os.path.join(current_dir, item)) and not item.startswith('__')
        ]
        
        return retrievers
    except Exception as e:
        logger.error(f"Error getting retrievers: {e}")
        return VALID_RETRIEVERS



================================================
FILE: gpt_researcher/retrievers/arxiv/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/arxiv/arxiv.py
================================================
import arxiv


class ArxivSearch:
    """
    Arxiv API Retriever
    """
    def __init__(self, query, sort='Relevance', query_domains=None):
        self.arxiv = arxiv
        self.query = query
        assert sort in ['Relevance', 'SubmittedDate'], "Invalid sort criterion"
        self.sort = arxiv.SortCriterion.SubmittedDate if sort == 'SubmittedDate' else arxiv.SortCriterion.Relevance
        

    def search(self, max_results=5):
        """
        Performs the search
        :param query:
        :param max_results:
        :return:
        """

        arxiv_gen = list(arxiv.Client().results(
        self.arxiv.Search(
            query= self.query, #+
            max_results=max_results,
            sort_by=self.sort,
        )))

        search_result = []

        for result in arxiv_gen:

            search_result.append({
                "title": result.title,
                "href": result.pdf_url,
                "body": result.summary,
            })
        
        return search_result


================================================
FILE: gpt_researcher/retrievers/bing/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/bing/bing.py
================================================
# Bing Search Retriever

# libraries
import os
import requests
import json
import logging


class BingSearch():
    """
    Bing Search Retriever
    """

    def __init__(self, query, query_domains=None):
        """
        Initializes the BingSearch object
        Args:
            query:
        """
        self.query = query
        self.query_domains = query_domains or None
        self.api_key = self.get_api_key()
        self.logger = logging.getLogger(__name__)

    def get_api_key(self):
        """
        Gets the Bing API key
        Returns:

        """
        try:
            api_key = os.environ["BING_API_KEY"]
        except:
            raise Exception(
                "Bing API key not found. Please set the BING_API_KEY environment variable.")
        return api_key

    def search(self, max_results=7) -> list[dict[str]]:
        """
        Searches the query
        Returns:

        """
        print("Searching with query {0}...".format(self.query))
        """Useful for general internet search queries using the Bing API."""

        # Search the query
        url = "https://api.bing.microsoft.com/v7.0/search"

        headers = {
            'Ocp-Apim-Subscription-Key': self.api_key,
            'Content-Type': 'application/json'
        }
        # TODO: Add support for query domains
        params = {
            "responseFilter": "Webpages",
            "q": self.query,
            "count": max_results,
            "setLang": "en-GB",
            "textDecorations": False,
            "textFormat": "HTML",
            "safeSearch": "Strict"
        }

        resp = requests.get(url, headers=headers, params=params)

        # Preprocess the results
        if resp is None:
            return []
        try:
            search_results = json.loads(resp.text)
            results = search_results["webPages"]["value"]
        except Exception as e:
            self.logger.error(
                f"Error parsing Bing search results: {e}. Resulting in empty response.")
            return []
        if search_results is None:
            self.logger.warning(f"No search results found for query: {self.query}")
            return []
        search_results = []

        # Normalize the results to match the format of the other search APIs
        for result in results:
            # skip youtube results
            if "youtube.com" in result["url"]:
                continue
            search_result = {
                "title": result["name"],
                "href": result["url"],
                "body": result["snippet"],
            }
            search_results.append(search_result)

        return search_results



================================================
FILE: gpt_researcher/retrievers/custom/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/custom/custom.py
================================================
from typing import Any, Dict, List, Optional
import requests
import os


class CustomRetriever:
    """
    Custom API Retriever
    """

    def __init__(self, query: str, query_domains=None):
        self.endpoint = os.getenv('RETRIEVER_ENDPOINT')
        if not self.endpoint:
            raise ValueError("RETRIEVER_ENDPOINT environment variable not set")

        self.params = self._populate_params()
        self.query = query

    def _populate_params(self) -> Dict[str, Any]:
        """
        Populates parameters from environment variables prefixed with 'RETRIEVER_ARG_'
        """
        return {
            key[len('RETRIEVER_ARG_'):].lower(): value
            for key, value in os.environ.items()
            if key.startswith('RETRIEVER_ARG_')
        }

    def search(self, max_results: int = 5) -> Optional[List[Dict[str, Any]]]:
        """
        Performs the search using the custom retriever endpoint.

        :param max_results: Maximum number of results to return (not currently used)
        :return: JSON response in the format:
            [
              {
                "url": "http://example.com/page1",
                "raw_content": "Content of page 1"
              },
              {
                "url": "http://example.com/page2",
                "raw_content": "Content of page 2"
              }
            ]
        """
        try:
            response = requests.get(self.endpoint, params={**self.params, 'query': self.query})
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"Failed to retrieve search results: {e}")
            return None


================================================
FILE: gpt_researcher/retrievers/duckduckgo/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/duckduckgo/duckduckgo.py
================================================
from itertools import islice
from ..utils import check_pkg


class Duckduckgo:
    """
    Duckduckgo API Retriever
    """
    def __init__(self, query, query_domains=None):
        check_pkg('ddgs')
        from ddgs import DDGS
        self.ddg = DDGS()
        self.query = query
        self.query_domains = query_domains or None

    def search(self, max_results=5):
        """
        Performs the search
        :param query:
        :param max_results:
        :return:
        """
        # TODO: Add support for query domains
        try:
            search_response = self.ddg.text(self.query, region='wt-wt', max_results=max_results)
        except Exception as e:
            print(f"Error: {e}. Failed fetching sources. Resulting in empty response.")
            search_response = []
        return search_response



================================================
FILE: gpt_researcher/retrievers/exa/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/exa/exa.py
================================================
import os
from ..utils import check_pkg


class ExaSearch:
    """
    Exa API Retriever
    """

    def __init__(self, query, query_domains=None):
        """
        Initializes the ExaSearch object.
        Args:
            query: The search query.
        """
        # This validation is necessary since exa_py is optional
        check_pkg("exa_py")
        from exa_py import Exa
        self.query = query
        self.api_key = self._retrieve_api_key()
        self.client = Exa(api_key=self.api_key)
        self.query_domains = query_domains or None

    def _retrieve_api_key(self):
        """
        Retrieves the Exa API key from environment variables.
        Returns:
            The API key.
        Raises:
            Exception: If the API key is not found.
        """
        try:
            api_key = os.environ["EXA_API_KEY"]
        except KeyError:
            raise Exception(
                "Exa API key not found. Please set the EXA_API_KEY environment variable. "
                "You can obtain your key from https://exa.ai/"
            )
        return api_key

    def search(
        self, max_results=10, use_autoprompt=False, search_type="neural", **filters
    ):
        """
        Searches the query using the Exa API.
        Args:
            max_results: The maximum number of results to return.
            use_autoprompt: Whether to use autoprompting.
            search_type: The type of search (e.g., "neural", "keyword").
            **filters: Additional filters (e.g., date range, domains).
        Returns:
            A list of search results.
        """
        results = self.client.search(
            self.query,
            type=search_type,
            use_autoprompt=use_autoprompt,
            num_results=max_results,
            include_domains=self.query_domains,
            **filters
        )

        search_response = [
            {"href": result.url, "body": result.text} for result in results.results
        ]
        return search_response

    def find_similar(self, url, exclude_source_domain=False, **filters):
        """
        Finds similar documents to the provided URL using the Exa API.
        Args:
            url: The URL to find similar documents for.
            exclude_source_domain: Whether to exclude the source domain in the results.
            **filters: Additional filters.
        Returns:
            A list of similar documents.
        """
        results = self.client.find_similar(
            url, exclude_source_domain=exclude_source_domain, **filters
        )

        similar_response = [
            {"href": result.url, "body": result.text} for result in results.results
        ]
        return similar_response

    def get_contents(self, ids, **options):
        """
        Retrieves the contents of the specified IDs using the Exa API.
        Args:
            ids: The IDs of the documents to retrieve.
            **options: Additional options for content retrieval.
        Returns:
            A list of document contents.
        """
        results = self.client.get_contents(ids, **options)

        contents_response = [
            {"id": result.id, "content": result.text} for result in results.results
        ]
        return contents_response



================================================
FILE: gpt_researcher/retrievers/google/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/google/google.py
================================================
# Tavily API Retriever

# libraries
import os
import requests
import json


class GoogleSearch:
    """
    Google API Retriever
    """
    def __init__(self, query, headers=None, query_domains=None):
        """
        Initializes the GoogleSearch object
        Args:
            query:
        """
        self.query = query
        self.headers = headers or {}
        self.query_domains = query_domains or None
        self.api_key = self.headers.get("google_api_key") or self.get_api_key()  # Use the passed api_key or fallback to environment variable
        self.cx_key = self.headers.get("google_cx_key") or self.get_cx_key()  # Use the passed cx_key or fallback to environment variable

    def get_api_key(self):
        """
        Gets the Google API key
        Returns:

        """
        # Get the API key
        try:
            api_key = os.environ["GOOGLE_API_KEY"]
        except:
            raise Exception("Google API key not found. Please set the GOOGLE_API_KEY environment variable. "
                            "You can get a key at https://developers.google.com/custom-search/v1/overview")
        return api_key

    def get_cx_key(self):
        """
        Gets the Google CX key
        Returns:

        """
        # Get the API key
        try:
            api_key = os.environ["GOOGLE_CX_KEY"]
        except:
            raise Exception("Google CX key not found. Please set the GOOGLE_CX_KEY environment variable. "
                            "You can get a key at https://developers.google.com/custom-search/v1/overview")
        return api_key

    def search(self, max_results=7):
        """
        Searches the query using Google Custom Search API, optionally restricting to specific domains
        Returns:
            list: List of search results with title, href and body
        """
        # Build query with domain restrictions if specified
        search_query = self.query
        if self.query_domains and len(self.query_domains) > 0:
            domain_query = " OR ".join([f"site:{domain}" for domain in self.query_domains])
            search_query = f"({domain_query}) {self.query}"

        print("Searching with query {0}...".format(search_query))

        url = f"https://www.googleapis.com/customsearch/v1?key={self.api_key}&cx={self.cx_key}&q={search_query}&start=1"
        resp = requests.get(url)

        if resp.status_code < 200 or resp.status_code >= 300:
            print("Google search: unexpected response status: ", resp.status_code)

        if resp is None:
            return
        try:
            search_results = json.loads(resp.text)
        except Exception:
            return
        if search_results is None:
            return

        results = search_results.get("items", [])
        search_results = []

        # Normalizing results to match the format of the other search APIs
        for result in results:
            # skip youtube results
            if "youtube.com" in result["link"]:
                continue
            try:
                search_result = {
                    "title": result["title"],
                    "href": result["link"],
                    "body": result["snippet"],
                }
            except:
                continue
            search_results.append(search_result)

        return search_results[:max_results]



================================================
FILE: gpt_researcher/retrievers/mcp/__init__.py
================================================
"""
MCP Retriever Module

This module contains only the MCP retriever implementation.
The core MCP functionality has been moved to gpt_researcher.mcp module.
"""
import logging

logger = logging.getLogger(__name__)

try:
    # Check if langchain-mcp-adapters is available
    from langchain_mcp_adapters.client import MultiServerMCPClient
    HAS_MCP_ADAPTERS = True
    logger.debug("langchain-mcp-adapters is available")
    
    # Import the retriever
    from .retriever import MCPRetriever
    __all__ = ["MCPRetriever"]
    logger.debug("MCPRetriever imported successfully")
    
except ImportError as e:
    # Log the specific import error for debugging
    logger.warning(f"Failed to import MCPRetriever: {e}")
    # MCP package not installed or other import error, provide a placeholder
    MCPRetriever = None
    __all__ = []
except Exception as e:
    # Catch any other exception that might occur
    logger.error(f"Unexpected error importing MCPRetriever: {e}")
    MCPRetriever = None
    __all__ = [] 


================================================
FILE: gpt_researcher/retrievers/mcp/retriever.py
================================================
"""
MCP-Based Research Retriever

A retriever that uses Model Context Protocol (MCP) tools for intelligent research.
This retriever implements a two-stage approach:
1. Tool Selection: LLM selects 2-3 most relevant tools from all available MCP tools
2. Research Execution: LLM uses the selected tools to conduct intelligent research
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

try:
    from langchain_mcp_adapters.client import MultiServerMCPClient
    HAS_MCP_ADAPTERS = True
except ImportError:
    HAS_MCP_ADAPTERS = False

from ...mcp.client import MCPClientManager
from ...mcp.tool_selector import MCPToolSelector
from ...mcp.research import MCPResearchSkill
from ...mcp.streaming import MCPStreamer

logger = logging.getLogger(__name__)


class MCPRetriever:
    """
    Model Context Protocol (MCP) Retriever for GPT Researcher.
    
    This retriever implements a two-stage approach:
    1. Tool Selection: LLM selects 2-3 most relevant tools from all available MCP tools
    2. Research Execution: LLM with bound tools conducts intelligent research
    
    This approach is more efficient than calling all tools and provides better, 
    more targeted research results.
    
    The retriever requires a researcher instance to access:
    - mcp_configs: List of MCP server configurations
    - cfg: Configuration object with LLM settings and parameters
    - add_costs: Method for tracking research costs
    """

    def __init__(
        self, 
        query: str, 
        headers: Optional[Dict[str, str]] = None,
        query_domains: Optional[List[str]] = None,
        websocket=None,
        researcher=None,
        **kwargs
    ):
        """
        Initialize the MCP Retriever.
        
        Args:
            query (str): The search query string.
            headers (dict, optional): Headers containing MCP configuration.
            query_domains (list, optional): List of domains to search (not used in MCP).
            websocket: WebSocket for stream logging.
            researcher: Researcher instance containing mcp_configs and cfg.
            **kwargs: Additional arguments (for compatibility).
        """
        self.query = query
        self.headers = headers or {}
        self.query_domains = query_domains or []
        self.websocket = websocket
        self.researcher = researcher
        
        # Extract mcp_configs and config from the researcher instance
        self.mcp_configs = self._get_mcp_configs()
        self.cfg = self._get_config()
        
        # Initialize modular components
        self.client_manager = MCPClientManager(self.mcp_configs)
        self.tool_selector = MCPToolSelector(self.cfg, self.researcher)
        self.mcp_researcher = MCPResearchSkill(self.cfg, self.researcher)
        self.streamer = MCPStreamer(self.websocket)
        
        # Initialize caching
        self._all_tools_cache = None
        
        # Log initialization
        if self.mcp_configs:
            self.streamer.stream_log_sync(f"ğŸ”§ Initializing MCP retriever for query: {self.query}")
            self.streamer.stream_log_sync(f"ğŸ”§ Found {len(self.mcp_configs)} MCP server configurations")
        else:
            logger.error("No MCP server configurations found. The retriever will fail during search.")
            self.streamer.stream_log_sync("âŒ CRITICAL: No MCP server configurations found. Please check documentation.")

    def _get_mcp_configs(self) -> List[Dict[str, Any]]:
        """
        Get MCP configurations from the researcher instance.
        
        Returns:
            List[Dict[str, Any]]: List of MCP server configurations.
        """
        if self.researcher and hasattr(self.researcher, 'mcp_configs'):
            return self.researcher.mcp_configs or []
        return []

    def _get_config(self):
        """
        Get configuration from the researcher instance.
        
        Returns:
            Config: Configuration object with LLM settings.
        """
        if self.researcher and hasattr(self.researcher, 'cfg'):
            return self.researcher.cfg
        
        # If no config available, this is a critical error
        logger.error("No config found in researcher instance. MCPRetriever requires a researcher instance with cfg attribute.")
        raise ValueError("MCPRetriever requires a researcher instance with cfg attribute containing LLM configuration")

    async def search_async(self, max_results: int = 10) -> List[Dict[str, str]]:
        """
        Perform an async search using MCP tools with intelligent two-stage approach.
        
        Args:
            max_results: Maximum number of results to return.
            
        Returns:
            List[Dict[str, str]]: The search results.
        """
        # Check if we have any server configurations
        if not self.mcp_configs:
            error_msg = "No MCP server configurations available. Please provide mcp_configs parameter to GPTResearcher."
            logger.error(error_msg)
            await self.streamer.stream_error("MCP retriever cannot proceed without server configurations.")
            return []  # Return empty instead of raising to allow research to continue
            
        # Log to help debug the integration flow
        logger.info(f"MCPRetriever.search_async called for query: {self.query}")
            
        try:
            # Stage 1: Get all available tools
            await self.streamer.stream_stage_start("Stage 1", "Getting all available MCP tools")
            all_tools = await self._get_all_tools()
            
            if not all_tools:
                await self.streamer.stream_warning("No MCP tools available, skipping MCP research")
                return []
            
            # Stage 2: Select most relevant tools
            await self.streamer.stream_stage_start("Stage 2", "Selecting most relevant tools")
            selected_tools = await self.tool_selector.select_relevant_tools(self.query, all_tools, max_tools=3)
            
            if not selected_tools:
                await self.streamer.stream_warning("No relevant tools selected, skipping MCP research")
                return []
            
            # Stage 3: Conduct research with selected tools
            await self.streamer.stream_stage_start("Stage 3", "Conducting research with selected tools")
            results = await self.mcp_researcher.conduct_research_with_tools(self.query, selected_tools)
            
            # Limit the number of results
            if len(results) > max_results:
                logger.info(f"Limiting {len(results)} MCP results to {max_results}")
                results = results[:max_results]
            
            # Log result summary with actual content samples
            logger.info(f"MCPRetriever returning {len(results)} results")
            
            # Calculate total content length for summary
            total_content_length = sum(len(result.get("body", "")) for result in results)
            await self.streamer.stream_research_results(len(results), total_content_length)
            
            # Log detailed content samples for debugging
            if results:
                # Show samples of the first few results
                for i, result in enumerate(results[:3]):  # Show first 3 results
                    title = result.get("title", "No title")
                    url = result.get("href", "No URL")
                    content = result.get("body", "")
                    content_length = len(content)
                    content_sample = content[:400] + "..." if len(content) > 400 else content
                    
                    logger.debug(f"Result {i+1}/{len(results)}: '{title}'")
                    logger.debug(f"URL: {url}")
                    logger.debug(f"Content ({content_length:,} chars): {content_sample}")
                    
                if len(results) > 3:
                    remaining_results = len(results) - 3
                    remaining_content = sum(len(result.get("body", "")) for result in results[3:])
                    logger.debug(f"... and {remaining_results} more results ({remaining_content:,} chars)")
                    
            return results
            
        except Exception as e:
            logger.error(f"Error in MCP search: {e}")
            await self.streamer.stream_error(f"Error in MCP search: {str(e)}")
            return []
        finally:
            # Ensure client cleanup after search completes
            try:
                await self.client_manager.close_client()
            except Exception as e:
                logger.error(f"Error during client cleanup: {e}")

    def search(self, max_results: int = 10) -> List[Dict[str, str]]:
        """
        Perform a search using MCP tools with intelligent two-stage approach.
        
        This is the synchronous interface required by GPT Researcher.
        It wraps the async search_async method.
        
        Args:
            max_results: Maximum number of results to return.
            
        Returns:
            List[Dict[str, str]]: The search results.
        """
        # Check if we have any server configurations
        if not self.mcp_configs:
            error_msg = "No MCP server configurations available. Please provide mcp_configs parameter to GPTResearcher."
            logger.error(error_msg)
            self.streamer.stream_log_sync("âŒ MCP retriever cannot proceed without server configurations.")
            return []  # Return empty instead of raising to allow research to continue
            
        # Log to help debug the integration flow
        logger.info(f"MCPRetriever.search called for query: {self.query}")
        
        try:
            # Handle the async/sync boundary properly
            try:
                # Try to get the current event loop
                loop = asyncio.get_running_loop()
                # If we're in an async context, we need to schedule the coroutine
                # This is a bit tricky - we'll create a task and let it run
                import concurrent.futures
                import threading
                
                # Create a new event loop in a separate thread
                def run_in_thread():
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)
                    try:
                        result = new_loop.run_until_complete(self.search_async(max_results))
                        return result
                    finally:
                        # Enhanced cleanup procedure for MCP connections
                        try:
                            # Cancel all pending tasks with a timeout
                            pending = asyncio.all_tasks(new_loop)
                            for task in pending:
                                task.cancel()
                            
                            # Wait for cancelled tasks to complete with timeout
                            if pending:
                                try:
                                    new_loop.run_until_complete(
                                        asyncio.wait_for(
                                            asyncio.gather(*pending, return_exceptions=True),
                                            timeout=5.0  # 5 second timeout for cleanup
                                        )
                                    )
                                except asyncio.TimeoutError:
                                    logger.debug("Timeout during task cleanup, continuing...")
                                except Exception:
                                    pass  # Ignore other cleanup errors
                        except Exception:
                            pass  # Ignore cleanup errors
                        finally:
                            try:
                                # Give the loop a moment to finish any final cleanup
                                import time
                                time.sleep(0.1)
                                
                                # Force garbage collection to clean up any remaining references
                                import gc
                                gc.collect()
                                
                                # Additional time for HTTP clients to finish their cleanup
                                time.sleep(0.2)
                                
                                # Close the loop
                                if not new_loop.is_closed():
                                    new_loop.close()
                            except Exception:
                                pass  # Ignore close errors
                
                # Run in a thread pool to avoid blocking the main event loop
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(run_in_thread)
                    results = future.result(timeout=300)  # 5 minute timeout
                    
            except RuntimeError:
                # No event loop is running, we can run directly
                results = asyncio.run(self.search_async(max_results))
            
            return results
            
        except Exception as e:
            logger.error(f"Error in MCP search: {e}")
            self.streamer.stream_log_sync(f"âŒ Error in MCP search: {str(e)}")
            # Return empty results instead of raising to allow research to continue
            return []

    async def _get_all_tools(self) -> List:
        """
        Get all available tools from MCP servers.
        
        Returns:
            List: All available MCP tools
        """
        if self._all_tools_cache is not None:
            return self._all_tools_cache
            
        try:
            all_tools = await self.client_manager.get_all_tools()
            
            if all_tools:
                await self.streamer.stream_log(f"ğŸ“‹ Loaded {len(all_tools)} total tools from MCP servers")
                self._all_tools_cache = all_tools
                return all_tools
            else:
                await self.streamer.stream_warning("No tools available from MCP servers")
                return []
                
        except Exception as e:
            logger.error(f"Error getting MCP tools: {e}")
            await self.streamer.stream_error(f"Error getting MCP tools: {str(e)}")
            return [] 


================================================
FILE: gpt_researcher/retrievers/pubmed_central/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/pubmed_central/pubmed_central.py
================================================
import os
import xml.etree.ElementTree as ET

import requests


class PubMedCentralSearch:
    """
    PubMed Central API Retriever
    """

    def __init__(self, query, query_domains=None):
        """
        Initializes the PubMedCentralSearch object.
        Args:
            query: The search query.
        """
        self.query = query
        self.api_key = self._retrieve_api_key()

    def _retrieve_api_key(self):
        """
        Retrieves the NCBI API key from environment variables.
        Returns:
            The API key.
        Raises:
            Exception: If the API key is not found.
        """
        try:
            api_key = os.environ["NCBI_API_KEY"]
        except KeyError:
            raise Exception(
                "NCBI API key not found. Please set the NCBI_API_KEY environment variable. "
                "You can obtain your key from https://www.ncbi.nlm.nih.gov/account/"
            )
        return api_key

    def search(self, max_results=10):
        """
        Searches the query using the PubMed Central API.
        Args:
            max_results: The maximum number of results to return.
        Returns:
            A list of search results.
        """
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            "db": "pmc",
            "term": f"{self.query} AND free fulltext[filter]",
            "retmax": max_results,
            "usehistory": "y",
            "api_key": self.api_key,
            "retmode": "json",
            "sort": "relevance"
        }
        response = requests.get(base_url, params=params)

        if response.status_code != 200:
            raise Exception(
                f"Failed to retrieve data: {response.status_code} - {response.text}"
            )

        results = response.json()
        ids = results["esearchresult"]["idlist"]

        search_response = []
        for article_id in ids:
            xml_content = self.fetch([article_id])
            if self.has_body_content(xml_content):
                article_data = self.parse_xml(xml_content)
                if article_data:
                    search_response.append(
                        {
                            "href": f"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{article_id}/",
                            "body": f"{article_data['title']}\n\n{article_data['abstract']}\n\n{article_data['body'][:500]}...",
                        }
                    )

            if len(search_response) >= max_results:
                break

        return search_response

    def fetch(self, ids):
        """
        Fetches the full text content for given article IDs.
        Args:
            ids: List of article IDs.
        Returns:
            XML content of the articles.
        """
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
        params = {
            "db": "pmc",
            "id": ",".join(ids),
            "retmode": "xml",
            "api_key": self.api_key,
        }
        response = requests.get(base_url, params=params)

        if response.status_code != 200:
            raise Exception(
                f"Failed to retrieve data: {response.status_code} - {response.text}"
            )

        return response.text

    def has_body_content(self, xml_content):
        """
        Checks if the XML content has a body section.
        Args:
            xml_content: XML content of the article.
        Returns:
            Boolean indicating presence of body content.
        """
        root = ET.fromstring(xml_content)
        ns = {
            "mml": "http://www.w3.org/1998/Math/MathML",
            "xlink": "http://www.w3.org/1999/xlink",
        }
        article = root.find("article", ns)
        if article is None:
            return False

        body_elem = article.find(".//body", namespaces=ns)
        if body_elem is not None:
            return True
        else:
            for sec in article.findall(".//sec", namespaces=ns):
                for p in sec.findall(".//p", namespaces=ns):
                    if p.text:
                        return True
        return False

    def parse_xml(self, xml_content):
        """
        Parses the XML content to extract title, abstract, and body.
        Args:
            xml_content: XML content of the article.
        Returns:
            Dictionary containing title, abstract, and body text.
        """
        root = ET.fromstring(xml_content)
        ns = {
            "mml": "http://www.w3.org/1998/Math/MathML",
            "xlink": "http://www.w3.org/1999/xlink",
        }

        article = root.find("article", ns)
        if article is None:
            return None

        title = article.findtext(
            ".//title-group/article-title", default="", namespaces=ns
        )

        abstract = article.find(".//abstract", namespaces=ns)
        abstract_text = (
            "".join(abstract.itertext()).strip() if abstract is not None else ""
        )

        body = []
        body_elem = article.find(".//body", namespaces=ns)
        if body_elem is not None:
            for p in body_elem.findall(".//p", namespaces=ns):
                if p.text:
                    body.append(p.text.strip())
        else:
            for sec in article.findall(".//sec", namespaces=ns):
                for p in sec.findall(".//p", namespaces=ns):
                    if p.text:
                        body.append(p.text.strip())

        return {"title": title, "abstract": abstract_text, "body": "\n".join(body)}



================================================
FILE: gpt_researcher/retrievers/searchapi/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/searchapi/searchapi.py
================================================
# SearchApi Retriever

# libraries
import os
import requests
import urllib.parse


class SearchApiSearch():
    """
    SearchApi Retriever
    """
    def __init__(self, query, query_domains=None):
        """
        Initializes the SearchApiSearch object
        Args:
            query:
        """
        self.query = query
        self.api_key = self.get_api_key()

    def get_api_key(self):
        """
        Gets the SearchApi API key
        Returns:

        """
        try:
            api_key = os.environ["SEARCHAPI_API_KEY"]
        except:
            raise Exception("SearchApi key not found. Please set the SEARCHAPI_API_KEY environment variable. "
                            "You can get a key at https://www.searchapi.io/")
        return api_key

    def search(self, max_results=7):
        """
        Searches the query
        Returns:

        """
        print("SearchApiSearch: Searching with query {0}...".format(self.query))
        """Useful for general internet search queries using SearchApi."""


        url = "https://www.searchapi.io/api/v1/search"
        params = {
            "q": self.query,
            "engine": "google",
        }

        headers = {
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}',
            'X-SearchApi-Source': 'gpt-researcher'
        }

        encoded_url = url + "?" + urllib.parse.urlencode(params)
        search_response = []

        try:
            response = requests.get(encoded_url, headers=headers, timeout=20)
            if response.status_code == 200:
                search_results = response.json()
                if search_results:
                    results = search_results["organic_results"]
                    results_processed = 0
                    for result in results:
                        # skip youtube results
                        if "youtube.com" in result["link"]:
                            continue
                        if results_processed >= max_results:
                            break
                        search_result = {
                            "title": result["title"],
                            "href": result["link"],
                            "body": result["snippet"],
                        }
                        search_response.append(search_result)
                        results_processed += 1
        except Exception as e:
            print(f"Error: {e}. Failed fetching sources. Resulting in empty response.")
            search_response = []

        return search_response



================================================
FILE: gpt_researcher/retrievers/searx/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/searx/searx.py
================================================
import os
import json
import requests
from typing import List, Dict
from urllib.parse import urljoin


class SearxSearch():
    """
    SearxNG API Retriever
    """
    def __init__(self, query: str, query_domains=None):
        """
        Initializes the SearxSearch object
        Args:
            query: Search query string
        """
        self.query = query
        self.query_domains = query_domains or None
        self.base_url = self.get_searxng_url()

    def get_searxng_url(self) -> str:
        """
        Gets the SearxNG instance URL from environment variables
        Returns:
            str: Base URL of SearxNG instance
        """
        try:
            base_url = os.environ["SEARX_URL"]
            if not base_url.endswith('/'):
                base_url += '/'
            return base_url
        except KeyError:
            raise Exception(
                "SearxNG URL not found. Please set the SEARX_URL environment variable. "
                "You can find public instances at https://searx.space/"
            )

    def search(self, max_results: int = 10) -> List[Dict[str, str]]:
        """
        Searches the query using SearxNG API
        Args:
            max_results: Maximum number of results to return
        Returns:
            List of dictionaries containing search results
        """
        search_url = urljoin(self.base_url, "search")
        # TODO: Add support for query domains
        params = {
            # The search query. 
            'q': self.query, 
            # Output format of results. Format needs to be activated in searxng config.
            'format': 'json'
        }

        try:
            response = requests.get(
                search_url,
                params=params,
                headers={'Accept': 'application/json'}
            )
            response.raise_for_status()
            results = response.json()

            # Normalize results to match the expected format
            search_response = []
            for result in results.get('results', [])[:max_results]:
                search_response.append({
                    "href": result.get('url', ''),
                    "body": result.get('content', '')
                })

            return search_response

        except requests.exceptions.RequestException as e:
            raise Exception(f"Error querying SearxNG: {str(e)}")
        except json.JSONDecodeError:
            raise Exception("Error parsing SearxNG response")



================================================
FILE: gpt_researcher/retrievers/semantic_scholar/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/semantic_scholar/semantic_scholar.py
================================================
from typing import Dict, List

import requests


class SemanticScholarSearch:
    """
    Semantic Scholar API Retriever
    """

    BASE_URL = "https://api.semanticscholar.org/graph/v1/paper/search"
    VALID_SORT_CRITERIA = ["relevance", "citationCount", "publicationDate"]

    def __init__(self, query: str, sort: str = "relevance", query_domains=None):
        """
        Initialize the SemanticScholarSearch class with a query and sort criterion.

        :param query: Search query string
        :param sort: Sort criterion ('relevance', 'citationCount', 'publicationDate')
        """
        self.query = query
        assert sort in self.VALID_SORT_CRITERIA, "Invalid sort criterion"
        self.sort = sort.lower()

    def search(self, max_results: int = 20) -> List[Dict[str, str]]:
        """
        Perform the search on Semantic Scholar and return results.

        :param max_results: Maximum number of results to retrieve
        :return: List of dictionaries containing title, href, and body of each paper
        """
        params = {
            "query": self.query,
            "limit": max_results,
            "fields": "title,abstract,url,venue,year,authors,isOpenAccess,openAccessPdf",
            "sort": self.sort,
        }

        try:
            response = requests.get(self.BASE_URL, params=params)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"An error occurred while accessing Semantic Scholar API: {e}")
            return []

        results = response.json().get("data", [])
        search_result = []

        for result in results:
            if result.get("isOpenAccess") and result.get("openAccessPdf"):
                search_result.append(
                    {
                        "title": result.get("title", "No Title"),
                        "href": result["openAccessPdf"].get("url", "No URL"),
                        "body": result.get("abstract", "Abstract not available"),
                    }
                )

        return search_result



================================================
FILE: gpt_researcher/retrievers/serpapi/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/serpapi/serpapi.py
================================================
# SerpApi Retriever

# libraries
import os
import requests
import urllib.parse


class SerpApiSearch():
    """
    SerpApi Retriever
    """
    def __init__(self, query, query_domains=None):
        """
        Initializes the SerpApiSearch object
        Args:
            query:
        """
        self.query = query
        self.query_domains = query_domains or None
        self.api_key = self.get_api_key()

    def get_api_key(self):
        """
        Gets the SerpApi API key
        Returns:

        """
        try:
            api_key = os.environ["SERPAPI_API_KEY"]
        except:
            raise Exception("SerpApi API key not found. Please set the SERPAPI_API_KEY environment variable. "
                            "You can get a key at https://serpapi.com/")
        return api_key

    def search(self, max_results=7):
        """
        Searches the query
        Returns:

        """
        print("SerpApiSearch: Searching with query {0}...".format(self.query))
        """Useful for general internet search queries using SerpApi."""

        url = "https://serpapi.com/search.json"

        search_query = self.query
        if self.query_domains:
            # Add site:domain1 OR site:domain2 OR ... to the search query
            search_query += " site:" + " OR site:".join(self.query_domains)

        params = {
            "q": search_query,
            "api_key": self.api_key
        }
        encoded_url = url + "?" + urllib.parse.urlencode(params)
        search_response = []
        try:
            response = requests.get(encoded_url, timeout=10)
            if response.status_code == 200:
                search_results = response.json()
                if search_results:
                    results = search_results["organic_results"]
                    results_processed = 0
                    for result in results:
                        # skip youtube results
                        if "youtube.com" in result["link"]:
                            continue
                        if results_processed >= max_results:
                            break
                        search_result = {
                            "title": result["title"],
                            "href": result["link"],
                            "body": result["snippet"],
                        }
                        search_response.append(search_result)
                        results_processed += 1
        except Exception as e:
            print(f"Error: {e}. Failed fetching sources. Resulting in empty response.")
            search_response = []

        return search_response



================================================
FILE: gpt_researcher/retrievers/serper/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/serper/serper.py
================================================
# Google Serper Retriever

# libraries
import os
import requests
import json


class SerperSearch():
    """
    Google Serper Retriever with support for country, language, and date filtering
    """
    def __init__(self, query, query_domains=None, country=None, language=None, time_range=None, exclude_sites=None):
        """
        Initializes the SerperSearch object
        Args:
            query (str): The search query string.
            query_domains (list, optional): List of domains to include in the search. Defaults to None.
            country (str, optional): Country code for search results (e.g., 'us', 'kr', 'jp'). Defaults to None.
            language (str, optional): Language code for search results (e.g., 'en', 'ko', 'ja'). Defaults to None.
            time_range (str, optional): Time range filter (e.g., 'qdr:h', 'qdr:d', 'qdr:w', 'qdr:m', 'qdr:y'). Defaults to None.
            exclude_sites (list, optional): List of sites to exclude from search results. Defaults to None.
        """
        self.query = query
        self.query_domains = query_domains or None
        self.country = country or os.getenv("SERPER_REGION")
        self.language = language or os.getenv("SERPER_LANGUAGE")
        self.time_range = time_range or os.getenv("SERPER_TIME_RANGE")
        self.exclude_sites = exclude_sites or self._get_exclude_sites_from_env()
        self.api_key = self.get_api_key()

    def _get_exclude_sites_from_env(self):
        """
        Gets the list of sites to exclude from environment variables
        Returns:
            list: List of sites to exclude
        """
        exclude_sites_env = os.getenv("SERPER_EXCLUDE_SITES", "")
        if exclude_sites_env:
            # Split by comma and strip whitespace
            return [site.strip() for site in exclude_sites_env.split(",") if site.strip()]
        return []

    def get_api_key(self):
        """
        Gets the Serper API key
        Returns:

        """
        try:
            api_key = os.environ["SERPER_API_KEY"]
        except:
            raise Exception("Serper API key not found. Please set the SERPER_API_KEY environment variable. "
                            "You can get a key at https://serper.dev/")
        return api_key

    def search(self, max_results=7):
        """
        Searches the query with optional country, language, and time filtering
        Returns:
            list: List of search results with title, href, and body
        """
        print("Searching with query {0}...".format(self.query))
        """Useful for general internet search queries using the Serper API."""

        # Search the query (see https://serper.dev/playground for the format)
        url = "https://google.serper.dev/search"

        headers = {
            'X-API-KEY': self.api_key,
            'Content-Type': 'application/json'
        }

        # Build search parameters
        query_with_filters = self.query

        # Exclude sites using Google search syntax
        if self.exclude_sites:
            for site in self.exclude_sites:
                query_with_filters += f" -site:{site}"

        # Add domain filtering if specified
        if self.query_domains:
            # Add site:domain1 OR site:domain2 OR ... to the search query
            domain_query = " site:" + " OR site:".join(self.query_domains)
            query_with_filters += domain_query

        search_params = {
            "q": query_with_filters,
            "num": max_results
        }

        # Add optional parameters if they exist
        if self.country:
            search_params["gl"] = self.country  # Geographic location (country)

        if self.language:
            search_params["hl"] = self.language  # Host language

        if self.time_range:
            search_params["tbs"] = self.time_range  # Time-based search

        data = json.dumps(search_params)

        resp = requests.request("POST", url, timeout=10, headers=headers, data=data)

        # Preprocess the results
        if resp is None:
            return
        try:
            search_results = json.loads(resp.text)
        except Exception:
            return
        if search_results is None:
            return

        results = search_results.get("organic", [])
        search_results = []

        # Normalize the results to match the format of the other search APIs
        # Excluded sites should already be filtered out by the query parameters
        for result in results:
            search_result = {
                "title": result["title"],
                "href": result["link"],
                "body": result["snippet"],
            }
            search_results.append(search_result)

        return search_results



================================================
FILE: gpt_researcher/retrievers/tavily/__init__.py
================================================



================================================
FILE: gpt_researcher/retrievers/tavily/tavily_search.py
================================================
# Tavily API Retriever

# libraries
import os
from typing import Literal, Sequence, Optional
import requests
import json


class TavilySearch:
    """
    Tavily API Retriever
    """

    def __init__(self, query, headers=None, topic="general", query_domains=None):
        """
        Initializes the TavilySearch object.

        Args:
            query (str): The search query string.
            headers (dict, optional): Additional headers to include in the request. Defaults to None.
            topic (str, optional): The topic for the search. Defaults to "general".
            query_domains (list, optional): List of domains to include in the search. Defaults to None.
        """
        self.query = query
        self.headers = headers or {}
        self.topic = topic
        self.base_url = "https://api.tavily.com/search"
        self.api_key = self.get_api_key()
        self.headers = {
            "Content-Type": "application/json",
        }
        self.query_domains = query_domains or None

    def get_api_key(self):
        """
        Gets the Tavily API key
        Returns:

        """
        api_key = self.headers.get("tavily_api_key")
        if not api_key:
            try:
                api_key = os.environ["TAVILY_API_KEY"]
            except KeyError:
                print(
                    "Tavily API key not found, set to blank. If you need a retriver, please set the TAVILY_API_KEY environment variable."
                )
                return ""
        return api_key


    def _search(
        self,
        query: str,
        search_depth: Literal["basic", "advanced"] = "basic",
        topic: str = "general",
        days: int = 2,
        max_results: int = 10,
        include_domains: Sequence[str] = None,
        exclude_domains: Sequence[str] = None,
        include_answer: bool = False,
        include_raw_content: bool = False,
        include_images: bool = False,
        use_cache: bool = True,
    ) -> dict:
        """
        Internal search method to send the request to the API.
        """

        data = {
            "query": query,
            "search_depth": search_depth,
            "topic": topic,
            "days": days,
            "include_answer": include_answer,
            "include_raw_content": include_raw_content,
            "max_results": max_results,
            "include_domains": include_domains,
            "exclude_domains": exclude_domains,
            "include_images": include_images,
            "api_key": self.api_key,
            "use_cache": use_cache,
        }

        response = requests.post(
            self.base_url, data=json.dumps(data), headers=self.headers, timeout=100
        )

        if response.status_code == 200:
            return response.json()
        else:
            # Raises a HTTPError if the HTTP request returned an unsuccessful status code
            response.raise_for_status()

    def search(self, max_results=10):
        """
        Searches the query
        Returns:

        """
        try:
            # Search the query
            results = self._search(
                self.query,
                search_depth="basic",
                max_results=max_results,
                topic=self.topic,
                include_domains=self.query_domains,
            )
            sources = results.get("results", [])
            if not sources:
                raise Exception("No results found with Tavily API search.")
            # Return the results
            search_response = [
                {"href": obj["url"], "body": obj["content"]} for obj in sources
            ]
        except Exception as e:
            print(f"Error: {e}. Failed fetching sources. Resulting in empty response.")
            search_response = []
        return search_response



================================================
FILE: gpt_researcher/scraper/__init__.py
================================================
from .beautiful_soup.beautiful_soup import BeautifulSoupScraper
from .web_base_loader.web_base_loader import WebBaseLoaderScraper
from .arxiv.arxiv import ArxivScraper
from .pymupdf.pymupdf import PyMuPDFScraper
from .browser.browser import BrowserScraper
from .browser.nodriver_scraper import NoDriverScraper
from .tavily_extract.tavily_extract import TavilyExtract
from .firecrawl.firecrawl import FireCrawl
from .scraper import Scraper

__all__ = [
    "BeautifulSoupScraper",
    "WebBaseLoaderScraper",
    "ArxivScraper",
    "PyMuPDFScraper",
    "BrowserScraper",
    "NoDriverScraper",
    "TavilyExtract",
    "Scraper",
    "FireCrawl",
]



================================================
FILE: gpt_researcher/scraper/scraper.py
================================================
import asyncio
from colorama import Fore, init

import requests
import subprocess
import sys
import importlib
import logging

from gpt_researcher.utils.workers import WorkerPool

from . import (
    ArxivScraper,
    BeautifulSoupScraper,
    PyMuPDFScraper,
    WebBaseLoaderScraper,
    BrowserScraper,
    NoDriverScraper,
    TavilyExtract,
    FireCrawl,
)


class Scraper:
    """
    Scraper class to extract the content from the links
    """

    def __init__(self, urls, user_agent, scraper, worker_pool: WorkerPool):
        """
        Initialize the Scraper class.
        Args:
            urls:
        """
        self.urls = urls
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": user_agent})
        self.scraper = scraper
        if self.scraper == "tavily_extract":
            self._check_pkg(self.scraper)
        if self.scraper == "firecrawl":
            self._check_pkg(self.scraper)
        self.logger = logging.getLogger(__name__)
        self.worker_pool = worker_pool

    async def run(self):
        """
        Extracts the content from the links
        """
        contents = await asyncio.gather(
            *(self.extract_data_from_url(url, self.session) for url in self.urls)
        )

        res = [content for content in contents if content["raw_content"] is not None]
        return res

    def _check_pkg(self, scrapper_name: str) -> None:
        """
        Checks and ensures required Python packages are available for scrapers that need
        dependencies beyond requirements.txt. When adding a new scraper to the repo, update `pkg_map`
        with its required information and call check_pkg() during initialization.
        """
        pkg_map = {
            "tavily_extract": {
                "package_installation_name": "tavily-python",
                "import_name": "tavily",
            },
            "firecrawl": {
                "package_installation_name": "firecrawl-py",
                "import_name": "firecrawl",
            },
        }
        pkg = pkg_map[scrapper_name]
        if not importlib.util.find_spec(pkg["import_name"]):
            pkg_inst_name = pkg["package_installation_name"]
            init(autoreset=True)
            print(Fore.YELLOW + f"{pkg_inst_name} not found. Attempting to install...")
            try:
                subprocess.check_call(
                    [sys.executable, "-m", "pip", "install", pkg_inst_name]
                )
                print(Fore.GREEN + f"{pkg_inst_name} installed successfully.")
            except subprocess.CalledProcessError:
                raise ImportError(
                    Fore.RED
                    + f"Unable to install {pkg_inst_name}. Please install manually with "
                    f"`pip install -U {pkg_inst_name}`"
                )

    async def extract_data_from_url(self, link, session):
        """
        Extracts the data from the link with logging
        """
        async with self.worker_pool.throttle():
            try:
                Scraper = self.get_scraper(link)
                scraper = Scraper(link, session)

                # Get scraper name
                scraper_name = scraper.__class__.__name__
                self.logger.info(f"\n=== Using {scraper_name} ===")

                # Get content
                if hasattr(scraper, "scrape_async"):
                    content, image_urls, title = await scraper.scrape_async()
                else:
                    (
                        content,
                        image_urls,
                        title,
                    ) = await asyncio.get_running_loop().run_in_executor(
                        self.worker_pool.executor, scraper.scrape
                    )

                if len(content) < 100:
                    self.logger.warning(f"Content too short or empty for {link}")
                    return {
                        "url": link,
                        "raw_content": None,
                        "image_urls": [],
                        "title": title,
                    }

                # Log results
                self.logger.info(f"\nTitle: {title}")
                self.logger.info(
                    f"Content length: {len(content) if content else 0} characters"
                )
                self.logger.info(f"Number of images: {len(image_urls)}")
                self.logger.info(f"URL: {link}")
                self.logger.info("=" * 50)

                if not content or len(content) < 100:
                    self.logger.warning(f"Content too short or empty for {link}")
                    return {
                        "url": link,
                        "raw_content": None,
                        "image_urls": [],
                        "title": title,
                    }

                return {
                    "url": link,
                    "raw_content": content,
                    "image_urls": image_urls,
                    "title": title,
                }

            except Exception as e:
                self.logger.error(f"Error processing {link}: {str(e)}")
                return {"url": link, "raw_content": None, "image_urls": [], "title": ""}

    def get_scraper(self, link):
        """
        The function `get_scraper` determines the appropriate scraper class based on the provided link
        or a default scraper if none matches.

        Args:
          link: The `get_scraper` method takes a `link` parameter which is a URL link to a webpage or a
        PDF file. Based on the type of content the link points to, the method determines the appropriate
        scraper class to use for extracting data from that content.

        Returns:
          The `get_scraper` method returns the scraper class based on the provided link. The method
        checks the link to determine the appropriate scraper class to use based on predefined mappings
        in the `SCRAPER_CLASSES` dictionary. If the link ends with ".pdf", it selects the
        `PyMuPDFScraper` class. If the link contains "arxiv.org", it selects the `ArxivScraper
        """

        SCRAPER_CLASSES = {
            "pdf": PyMuPDFScraper,
            "arxiv": ArxivScraper,
            "bs": BeautifulSoupScraper,
            "web_base_loader": WebBaseLoaderScraper,
            "browser": BrowserScraper,
            "nodriver": NoDriverScraper,
            "tavily_extract": TavilyExtract,
            "firecrawl": FireCrawl,
        }

        scraper_key = None

        if link.endswith(".pdf"):
            scraper_key = "pdf"
        elif "arxiv.org" in link:
            scraper_key = "arxiv"
        else:
            scraper_key = self.scraper

        scraper_class = SCRAPER_CLASSES.get(scraper_key)
        if scraper_class is None:
            raise Exception("Scraper not found.")

        return scraper_class



================================================
FILE: gpt_researcher/scraper/utils.py
================================================
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import logging
import hashlib
import re
import bs4

def get_relevant_images(soup: BeautifulSoup, url: str) -> list:
    """Extract relevant images from the page"""
    image_urls = []
    
    try:
        # Find all img tags with src attribute
        all_images = soup.find_all('img', src=True)
        
        for img in all_images:
            img_src = urljoin(url, img['src'])
            if img_src.startswith(('http://', 'https://')):
                score = 0
                # Check for relevant classes
                if any(cls in img.get('class', []) for cls in ['header', 'featured', 'hero', 'thumbnail', 'main', 'content']):
                    score = 4  # Higher score
                # Check for size attributes
                elif img.get('width') and img.get('height'):
                    width = parse_dimension(img['width'])
                    height = parse_dimension(img['height'])
                    if width and height:
                        if width >= 2000 and height >= 1000:
                            score = 3  # Medium score (very large images)
                        elif width >= 1600 or height >= 800:
                            score = 2  # Lower score
                        elif width >= 800 or height >= 500:
                            score = 1  # Lowest score
                        elif width >= 500 or height >= 300:
                            score = 0  # Lowest score
                        else:
                            continue  # Skip small images
                
                image_urls.append({'url': img_src, 'score': score})
        
        # Sort images by score (highest first)
        sorted_images = sorted(image_urls, key=lambda x: x['score'], reverse=True)
        
        return sorted_images[:10]  # Ensure we don't return more than 10 images in total
    
    except Exception as e:
        logging.error(f"Error in get_relevant_images: {e}")
        return []

def parse_dimension(value: str) -> int:
    """Parse dimension value, handling px units"""
    if value.lower().endswith('px'):
        value = value[:-2]  # Remove 'px' suffix
    try:
        return int(value)  # Convert to float first to handle decimal values
    except ValueError as e:
        print(f"Error parsing dimension value {value}: {e}")
        return None

def extract_title(soup: BeautifulSoup) -> str:
    """Extract the title from the BeautifulSoup object"""
    return soup.title.string if soup.title else ""

def get_image_hash(image_url: str) -> str:
    """Calculate a simple hash based on the image filename and essential query parameters"""
    try:
        parsed_url = urlparse(image_url)
        
        # Extract the filename
        filename = parsed_url.path.split('/')[-1]
        
        # Extract essential query parameters (e.g., 'url' for CDN-served images)
        query_params = parse_qs(parsed_url.query)
        essential_params = query_params.get('url', [])
        
        # Combine filename and essential parameters
        image_identifier = filename + ''.join(essential_params)
        
        # Calculate hash
        return hashlib.md5(image_identifier.encode()).hexdigest()
    except Exception as e:
        logging.error(f"Error calculating image hash for {image_url}: {e}")
        return None


def clean_soup(soup: BeautifulSoup) -> BeautifulSoup:
    """Clean the soup by removing unwanted tags"""
    for tag in soup.find_all(
        [
            "script",
            "style",
            "footer",
            "header",
            "nav",
            "menu",
            "sidebar",
            "svg",
        ]
    ):
        tag.decompose()

    disallowed_class_set = {"nav", "menu", "sidebar", "footer"}

    # clean tags with certain classes
    def does_tag_have_disallowed_class(elem) -> bool:
        if not isinstance(elem, bs4.Tag):
            return False

        return any(
            cls_name in disallowed_class_set for cls_name in elem.get("class", [])
        )

    for tag in soup.find_all(does_tag_have_disallowed_class):
        tag.decompose()

    return soup


def get_text_from_soup(soup: BeautifulSoup) -> str:
    """Get the relevant text from the soup with improved filtering"""
    text = soup.get_text(strip=True, separator="\n")
    # Remove excess whitespace
    text = re.sub(r"\s{2,}", " ", text)
    return text


================================================
FILE: gpt_researcher/scraper/arxiv/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/arxiv/arxiv.py
================================================
from langchain_community.retrievers import ArxivRetriever


class ArxivScraper:

    def __init__(self, link, session=None):
        self.link = link
        self.session = session

    def scrape(self):
        """
        The function scrapes relevant documents from Arxiv based on a given link and returns the content
        of the first document.
        
        Returns:
          The code is returning the page content of the first document retrieved by the ArxivRetriever
        for a given query extracted from the link.
        """
        query = self.link.split("/")[-1]
        retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)
        docs = retriever.invoke(query)

        # Include the published date and author to provide additional context, 
        # aligning with APA-style formatting in the report.
        context = f"Published: {docs[0].metadata['Published']}; Author: {docs[0].metadata['Authors']}; Content: {docs[0].page_content}"
        image = []

        return context, image, docs[0].metadata["Title"]



================================================
FILE: gpt_researcher/scraper/beautiful_soup/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/beautiful_soup/beautiful_soup.py
================================================
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from ..utils import get_relevant_images, extract_title, get_text_from_soup, clean_soup

class BeautifulSoupScraper:

    def __init__(self, link, session=None):
        self.link = link
        self.session = session

    def scrape(self):
        """
        This function scrapes content from a webpage by making a GET request, parsing the HTML using
        BeautifulSoup, and extracting script and style elements before returning the cleaned content.
        
        Returns:
          The `scrape` method is returning the cleaned and extracted content from the webpage specified
        by the `self.link` attribute. The method fetches the webpage content, removes script and style
        tags, extracts the text content, and returns the cleaned content as a string. If any exception
        occurs during the process, an error message is printed and an empty string is returned.
        """
        try:
            response = self.session.get(self.link, timeout=4)
            soup = BeautifulSoup(
                response.content, "lxml", from_encoding=response.encoding
            )

            soup = clean_soup(soup)

            content = get_text_from_soup(soup)

            image_urls = get_relevant_images(soup, self.link)
            
            # Extract the title using the utility function
            title = extract_title(soup)

            return content, image_urls, title

        except Exception as e:
            print("Error! : " + str(e))
            return "", [], ""


================================================
FILE: gpt_researcher/scraper/browser/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/browser/browser.py
================================================
from __future__ import annotations

import traceback
import pickle
from pathlib import Path
from sys import platform
import time
import random
import string
import os

from bs4 import BeautifulSoup
from typing import Iterable, cast

from .processing.scrape_skills import (scrape_pdf_with_pymupdf,
                                       scrape_pdf_with_arxiv)

from urllib.parse import urljoin

from ..utils import get_relevant_images, extract_title, get_text_from_soup, clean_soup

FILE_DIR = Path(__file__).parent.parent

class BrowserScraper:
    def __init__(self, url: str, session=None):
        self.url = url
        self.session = session
        self.selenium_web_browser = "chrome"
        self.headless = False
        self.user_agent = ("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                           "AppleWebKit/537.36 (KHTML, like Gecko) "
                           "Chrome/128.0.0.0 Safari/537.36")
        self.driver = None
        self.use_browser_cookies = False
        self._import_selenium()  # Import only if used to avoid unnecessary dependencies
        self.cookie_filename = f"{self._generate_random_string(8)}.pkl"

    def scrape(self) -> tuple:
        if not self.url:
            print("URL not specified")
            return "A URL was not specified, cancelling request to browse website.", [], ""

        try:
            self.setup_driver()
            self._visit_google_and_save_cookies()
            self._load_saved_cookies()
            self._add_header()

            text, image_urls, title = self.scrape_text_with_selenium()
            return text, image_urls, title
        except Exception as e:
            print(f"An error occurred during scraping: {str(e)}")
            print("Full stack trace:")
            print(traceback.format_exc())
            return f"An error occurred: {str(e)}\n\nStack trace:\n{traceback.format_exc()}", [], ""
        finally:
            if self.driver:
                self.driver.quit()
            self._cleanup_cookie_file()

    def _import_selenium(self):
        try:
            global webdriver, By, EC, WebDriverWait, TimeoutException, WebDriverException
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.support.wait import WebDriverWait
            from selenium.common.exceptions import TimeoutException, WebDriverException

            global ChromeOptions, FirefoxOptions, SafariOptions
            from selenium.webdriver.chrome.options import Options as ChromeOptions
            from selenium.webdriver.firefox.options import Options as FirefoxOptions
            from selenium.webdriver.safari.options import Options as SafariOptions
        except ImportError as e:
            print(f"Failed to import Selenium: {str(e)}")
            print("Please install Selenium and its dependencies to use BrowserScraper.")
            print("You can install Selenium using pip:")
            print("    pip install selenium")
            print("If you're using a virtual environment, make sure it's activated.")
            raise ImportError(
                "Selenium is required but not installed. See error message above for installation instructions.") from e

    def setup_driver(self) -> None:
        # print(f"Setting up {self.selenium_web_browser} driver...")

        options_available = {
            "chrome": ChromeOptions,
            "firefox": FirefoxOptions,
            "safari": SafariOptions,
        }

        options = options_available[self.selenium_web_browser]()
        options.add_argument(f"user-agent={self.user_agent}")
        if self.headless:
            options.add_argument("--headless")
        options.add_argument("--enable-javascript")

        try:
            if self.selenium_web_browser == "firefox":
                self.driver = webdriver.Firefox(options=options)
            elif self.selenium_web_browser == "safari":
                self.driver = webdriver.Safari(options=options)
            else:  # chrome
                if platform == "linux" or platform == "linux2":
                    options.add_argument("--disable-dev-shm-usage")
                    options.add_argument("--remote-debugging-port=9222")
                options.add_argument("--no-sandbox")
                options.add_experimental_option("prefs", {"download_restrictions": 3})
                self.driver = webdriver.Chrome(options=options)

            if self.use_browser_cookies:
                self._load_browser_cookies()

            # print(f"{self.selenium_web_browser.capitalize()} driver set up successfully.")
        except Exception as e:
            print(f"Failed to set up {self.selenium_web_browser} driver: {str(e)}")
            print("Full stack trace:")
            print(traceback.format_exc())
            raise

    def _load_saved_cookies(self):
        """Load saved cookies before visiting the target URL"""
        cookie_file = Path(self.cookie_filename)
        if cookie_file.exists():
            cookies = pickle.load(open(self.cookie_filename, "rb"))
            for cookie in cookies:
                self.driver.add_cookie(cookie)
        else:
            print("No saved cookies found.")

    def _load_browser_cookies(self):
        """Load cookies directly from the browser"""
        try:
            import browser_cookie3
        except ImportError:
            print(
                "browser_cookie3 is not installed. Please install it using: pip install browser_cookie3"
            )
            return

        if self.selenium_web_browser == "chrome":
            cookies = browser_cookie3.chrome()
        elif self.selenium_web_browser == "firefox":
            cookies = browser_cookie3.firefox()
        else:
            print(f"Cookie loading not supported for {self.selenium_web_browser}")
            return

        for cookie in cookies:
            self.driver.add_cookie({'name': cookie.name, 'value': cookie.value, 'domain': cookie.domain})

    def _cleanup_cookie_file(self):
        """Remove the cookie file"""
        cookie_file = Path(self.cookie_filename)
        if cookie_file.exists():
            try:
                os.remove(self.cookie_filename)
            except Exception as e:
                print(f"Failed to remove cookie file: {str(e)}")
        else:
            print("No cookie file found to remove.")

    def _generate_random_string(self, length):
        """Generate a random string of specified length"""
        return "".join(random.choices(string.ascii_letters + string.digits, k=length))

    def _get_domain(self):
        """Extract domain from URL"""
        from urllib.parse import urlparse

        """Get domain from URL, removing 'www' if present"""
        domain = urlparse(self.url).netloc
        return domain[4:] if domain.startswith("www.") else domain

    def _visit_google_and_save_cookies(self):
        """Visit Google and save cookies before navigating to the target URL"""
        try:
            self.driver.get("https://www.google.com")
            time.sleep(2)  # Wait for cookies to be set

            # Save cookies to a file
            cookies = self.driver.get_cookies()
            pickle.dump(cookies, open(self.cookie_filename, "wb"))

            # print("Google cookies saved successfully.")
        except Exception as e:
            print(f"Failed to visit Google and save cookies: {str(e)}")
            print("Full stack trace:")
            print(traceback.format_exc())

    def scrape_text_with_selenium(self) -> tuple:
        self.driver.get(self.url)

        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except TimeoutException as e:
            print("Timed out waiting for page to load")
            print(f"Full stack trace:\n{traceback.format_exc()}")
            return "Page load timed out", [], ""

        self._scroll_to_bottom()

        if self.url.endswith(".pdf"):
            text = scrape_pdf_with_pymupdf(self.url)
            return text, [], ""
        elif "arxiv" in self.url:
            doc_num = self.url.split("/")[-1]
            text = scrape_pdf_with_arxiv(doc_num)
            return text, [], ""
        else:
            page_source = self.driver.execute_script(
                "return document.documentElement.outerHTML;"
            )
            soup = BeautifulSoup(page_source, "lxml")

            soup = clean_soup(soup)

            text = get_text_from_soup(soup)
            image_urls = get_relevant_images(soup, self.url)
            title = extract_title(soup)

        return text, image_urls, title

    def _scroll_to_bottom(self):
        """Scroll to the bottom of the page to load all content"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # Wait for content to load
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

    def _scroll_to_percentage(self, ratio: float) -> None:
        """Scroll to a percentage of the page"""
        if ratio < 0 or ratio > 1:
            raise ValueError("Percentage should be between 0 and 1")
        self.driver.execute_script(f"window.scrollTo(0, document.body.scrollHeight * {ratio});")

    def _add_header(self) -> None:
        """Add a header to the website"""
        self.driver.execute_script(open(f"{FILE_DIR}/browser/js/overlay.js", "r").read())



================================================
FILE: gpt_researcher/scraper/browser/nodriver_scraper.py
================================================
from contextlib import asynccontextmanager
import math
from pathlib import Path
import random
import traceback
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from typing import Dict, Literal, cast, Tuple, List
import requests
import asyncio
import logging

from ..utils import get_relevant_images, extract_title, get_text_from_soup, clean_soup


class NoDriverScraper:
    logger = logging.getLogger(__name__)
    max_browsers = 3
    browser_load_threshold = 5
    browsers: set["NoDriverScraper.Browser"] = set()
    browsers_lock = asyncio.Lock()

    @staticmethod
    def get_domain(url: str) -> str:
        domain = urlparse(url).netloc
        parts = domain.split(".")
        if len(parts) > 2:
            domain = ".".join(parts[-2:])
        return domain

    class Browser:
        def __init__(
            self,
            driver: "zendriver.Browser",
        ):
            self.driver = driver
            self.processing_count = 0
            self.has_blank_page = True
            self.allowed_requests_times = {}
            self.domain_semaphores: Dict[str, asyncio.Semaphore] = {}
            self.tab_mode = True
            self.max_scroll_percent = 500
            self.stopping = False

        async def get(self, url: str) -> "zendriver.Tab":
            self.processing_count += 1
            try:
                async with self.rate_limit_for_domain(url):
                    new_window = not self.has_blank_page
                    self.has_blank_page = False
                    if self.tab_mode:
                        return await self.driver.get(url, new_tab=new_window)
                    else:
                        return await self.driver.get(url, new_window=new_window)
            except Exception:
                self.processing_count -= 1
                raise

        async def scroll_page_to_bottom(self, page: "zendriver.Tab"):
            total_scroll_percent = 0
            while True:
                # in tab mode, we need to bring the tab to front before scrolling to load the page content properly
                if self.tab_mode:
                    await page.bring_to_front()
                scroll_percent = random.randrange(46, 97)
                total_scroll_percent += scroll_percent
                await page.scroll_down(scroll_percent)
                await self.wait_or_timeout(page, "idle", 2)
                await page.sleep(random.uniform(0.23, 0.56))

                if total_scroll_percent >= self.max_scroll_percent:
                    break

                if cast(
                    bool,
                    await page.evaluate(
                        "window.innerHeight + window.scrollY >= document.scrollingElement.scrollHeight"
                    ),
                ):
                    break

        async def wait_or_timeout(
            self,
            page: "zendriver.Tab",
            until: Literal["complete", "idle"] = "idle",
            timeout: float = 3,
        ):
            try:
                if until == "idle":
                    await asyncio.wait_for(page.wait(), timeout)
                else:
                    timeout = math.ceil(timeout)
                    await page.wait_for_ready_state(until, timeout=timeout)
            except asyncio.TimeoutError:
                NoDriverScraper.logger.debug(
                    f"timeout waiting for {until} after {timeout} seconds"
                )

        async def close_page(self, page: "zendriver.Tab"):
            try:
                await page.close()
            except Exception as e:
                NoDriverScraper.logger.error(f"Failed to close page: {e}")
            finally:
                self.processing_count -= 1

        @asynccontextmanager
        async def rate_limit_for_domain(self, url: str):
            semaphore = None
            try:
                domain = NoDriverScraper.get_domain(url)

                semaphore = self.domain_semaphores.get(domain)
                if not semaphore:
                    semaphore = asyncio.Semaphore(1)
                    self.domain_semaphores[domain] = semaphore

                was_locked = semaphore.locked()
                async with semaphore:
                    if was_locked:
                        await asyncio.sleep(random.uniform(0.6, 1.2))
                    yield

            except Exception as e:
                # Log error but don't block the request
                NoDriverScraper.logger.warning(
                    f"Rate limiting error for {url}: {str(e)}"
                )

        async def stop(self):
            if self.stopping:
                return
            self.stopping = True
            await self.driver.stop()

    @classmethod
    async def get_browser(cls, headless: bool = False) -> "NoDriverScraper.Browser":
        async def create_browser():
            try:
                global zendriver
                import zendriver
            except ImportError:
                raise ImportError(
                    "The zendriver package is required to use NoDriverScraper. "
                    "Please install it with: pip install zendriver"
                )

            config = zendriver.Config(
                headless=headless,
                browser_connection_timeout=1,
            )
            driver = await zendriver.start(config)
            browser = cls.Browser(driver)
            cls.browsers.add(browser)
            return browser

        async with cls.browsers_lock:
            if len(cls.browsers) == 0:
                # No browsers available, create new one
                return await create_browser()

            # Load balancing: Get browser with lowest number of tabs
            browser = min(cls.browsers, key=lambda b: b.processing_count)

            # If all browsers are heavily loaded and we can create more
            if (
                browser.processing_count >= cls.browser_load_threshold
                and len(cls.browsers) < cls.max_browsers
            ):
                return await create_browser()

            return browser

    @classmethod
    async def release_browser(cls, browser: Browser):
        async with cls.browsers_lock:
            if browser and browser.processing_count <= 0:
                try:
                    await browser.stop()
                except Exception as e:
                    NoDriverScraper.logger.error(f"Failed to release browser: {e}")
                finally:
                    cls.browsers.discard(browser)

    def __init__(self, url: str, session: requests.Session | None = None):
        self.url = url
        self.session = session
        self.debug = False

    async def scrape_async(self) -> Tuple[str, list[dict], str]:
        """Returns tuple of (text, image_urls, title)"""
        if not self.url:
            return (
                "A URL was not specified, cancelling request to browse website.",
                [],
                "",
            )

        browser: NoDriverScraper.Browser | None = None
        page = None
        try:
            try:
                browser = await self.get_browser()
            except ImportError as e:
                self.logger.error(f"Failed to initialize browser: {str(e)}")
                return str(e), [], ""

            page = await browser.get(self.url)
            await browser.wait_or_timeout(page, "complete", 2)
            # wait for potential redirection
            await page.sleep(random.uniform(0.3, 0.7))
            await browser.wait_or_timeout(page, "idle", 2)

            await browser.scroll_page_to_bottom(page)
            html = await page.get_content()
            soup = BeautifulSoup(html, "lxml")
            clean_soup(soup)
            text = get_text_from_soup(soup)
            image_urls = get_relevant_images(soup, self.url)
            title = extract_title(soup)

            if len(text) < 200:
                self.logger.warning(
                    f"Content is too short from {self.url}. Title: {title}, Text length: {len(text)},\n"
                    f"excerpt: {text}."
                )
                if self.debug:
                    screenshot_dir = Path("logs/screenshots")
                    screenshot_dir.mkdir(exist_ok=True)
                    screenshot_path = (
                        screenshot_dir
                        / f"screenshot-error-{NoDriverScraper.get_domain(self.url)}.jpeg"
                    )
                    await page.save_screenshot(screenshot_path)
                    self.logger.warning(
                        f"check screenshot at [{screenshot_path}] for more details."
                    )

            return text, image_urls, title
        except Exception as e:
            self.logger.error(
                f"An error occurred during scraping: {str(e)}\n"
                "Full stack trace:\n"
                f"{traceback.format_exc()}"
            )
            return str(e), [], ""
        finally:
            try:
                if page and browser:
                    await browser.close_page(page)
                if browser:
                    await self.release_browser(browser)
            except Exception as e:
                self.logger.error(e)



================================================
FILE: gpt_researcher/scraper/browser/js/overlay.js
================================================
const overlay = document.createElement('div');
Object.assign(overlay.style, {
    position: 'fixed',
    zIndex: 999999,
    top: 0,
    left: 0,
    width: '100%',
    height: '100%',
    background: 'rgba(0, 0, 0, 0.7)',
    color: '#fff',
    fontSize: '24px',
    fontWeight: 'bold',
    display: 'flex',
    justifyContent: 'center',
    alignItems: 'center',
});
const textContent = document.createElement('div');
Object.assign(textContent.style, {
    textAlign: 'center',
});
textContent.textContent = 'GPT Researcher: Analyzing Page';
overlay.appendChild(textContent);
document.body.append(overlay);
document.body.style.overflow = 'hidden';
let dotCount = 0;
setInterval(() => {
    textContent.textContent = 'GPT Researcher: Analyzing Page' + '.'.repeat(dotCount);
    dotCount = (dotCount + 1) % 4;
}, 1000);



================================================
FILE: gpt_researcher/scraper/browser/processing/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/browser/processing/html.py
================================================
"""HTML processing functions"""
from __future__ import annotations

from bs4 import BeautifulSoup
from requests.compat import urljoin


def extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:
    """Extract hyperlinks from a BeautifulSoup object

    Args:
        soup (BeautifulSoup): The BeautifulSoup object
        base_url (str): The base URL

    Returns:
        List[Tuple[str, str]]: The extracted hyperlinks
    """
    return [
        (link.text, urljoin(base_url, link["href"]))
        for link in soup.find_all("a", href=True)
    ]


def format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:
    """Format hyperlinks to be displayed to the user

    Args:
        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format

    Returns:
        List[str]: The formatted hyperlinks
    """
    return [f"{link_text} ({link_url})" for link_text, link_url in hyperlinks]



================================================
FILE: gpt_researcher/scraper/browser/processing/scrape_skills.py
================================================
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.retrievers import ArxivRetriever


def scrape_pdf_with_pymupdf(url) -> str:
    """Scrape a pdf with pymupdf

    Args:
        url (str): The url of the pdf to scrape

    Returns:
        str: The text scraped from the pdf
    """
    loader = PyMuPDFLoader(url)
    doc = loader.load()
    return str(doc)


def scrape_pdf_with_arxiv(query) -> str:
    """Scrape a pdf with arxiv
    default document length of 70000 about ~15 pages or None for no limit

    Args:
        query (str): The query to search for

    Returns:
        str: The text scraped from the pdf
    """
    retriever = ArxivRetriever(load_max_docs=2, doc_content_chars_max=None)
    docs = retriever.get_relevant_documents(query=query)
    return docs[0].page_content


================================================
FILE: gpt_researcher/scraper/firecrawl/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/firecrawl/firecrawl.py
================================================
from bs4 import BeautifulSoup
import os
from ..utils import get_relevant_images

class FireCrawl:

    def __init__(self, link, session=None):
        self.link = link
        self.session = session
        from firecrawl import FirecrawlApp
        self.firecrawl = FirecrawlApp(api_key=self.get_api_key(), api_url=self.get_server_url())

    def get_api_key(self) -> str:
        """
        Gets the FireCrawl API key
        Returns:
        Api key (str)
        """
        try:
            api_key = os.environ["FIRECRAWL_API_KEY"]
        except KeyError:
            raise Exception(
                "FireCrawl API key not found. Please set the FIRECRAWL_API_KEY environment variable.")
        return api_key

    def get_server_url(self) -> str:
        """
        Gets the FireCrawl server URL.
        Default to official FireCrawl server ('https://api.firecrawl.dev').
        Returns:
        server url (str)
        """
        try:
            server_url = os.environ["FIRECRAWL_SERVER_URL"]
        except KeyError:
            server_url = 'https://api.firecrawl.dev'
        return server_url

    def scrape(self) -> tuple:
        """
        This function extracts content and title from a specified link using the FireCrawl Python SDK,
        images from the link are extracted using the functions from `gpt_researcher/scraper/utils.py`.

        Returns:
          The `scrape` method returns a tuple containing the extracted content, a list of image URLs, and
        the title of the webpage specified by the `self.link` attribute. It uses the FireCrawl Python SDK to
        extract and clean content from the webpage. If any exception occurs during the process, an error
        message is printed and an empty result is returned.
        """

        try:
            response = self.firecrawl.scrape_url(url=self.link, formats=["markdown"])

            # Check if the page has been scraped success
            if "error" in response:
                print("Scrape failed! : " + str(response["error"]))
                return "", [], ""
            elif response["metadata"]["statusCode"] != 200:
                print("Scrape failed! : " + str(response))
                return "", [], ""

            # Extract the content (markdown) and title from FireCrawl response
            content = response.markdown
            title = response.metadata.get("title", "")

            # Parse the HTML content of the response to create a BeautifulSoup object for the utility functions
            response_bs = self.session.get(self.link, timeout=4)
            soup = BeautifulSoup(
                response_bs.content, "lxml", from_encoding=response_bs.encoding
            )

            # Get relevant images using the utility function
            image_urls = get_relevant_images(soup, self.link)

            return content, image_urls, title

        except Exception as e:
            print("Error! : " + str(e))
            return "", [], ""



================================================
FILE: gpt_researcher/scraper/pymupdf/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/pymupdf/pymupdf.py
================================================
import os
import requests
import tempfile
from urllib.parse import urlparse
from langchain_community.document_loaders import PyMuPDFLoader


class PyMuPDFScraper:

    def __init__(self, link, session=None):
        """
        Initialize the scraper with a link and an optional session.

        Args:
          link (str): The URL or local file path of the PDF document.
          session (requests.Session, optional): An optional session for making HTTP requests.
        """
        self.link = link
        self.session = session

    def is_url(self) -> bool:
        """
        Check if the provided `link` is a valid URL.

        Returns:
          bool: True if the link is a valid URL, False otherwise.
        """
        try:
            result = urlparse(self.link)
            return all([result.scheme, result.netloc])  # Check for valid scheme and network location
        except Exception:
            return False

    def scrape(self) -> tuple[str, list[str], str]:
        """
        The `scrape` function uses PyMuPDFLoader to load a document from the provided link (either URL or local file)
        and returns the document as a string.

        Returns:
          str: A string representation of the loaded document.
        """
        try:
            if self.is_url():
                response = requests.get(self.link, timeout=5, stream=True)
                response.raise_for_status()

                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
                    temp_filename = temp_file.name  # Get the temporary file name
                    for chunk in response.iter_content(chunk_size=8192):
                        temp_file.write(chunk)  # Write the downloaded content to the temporary file

                loader = PyMuPDFLoader(temp_filename)
                doc = loader.load()

                os.remove(temp_filename)
            else:
                loader = PyMuPDFLoader(self.link)
                doc = loader.load()

            # Extract the content, image (if any), and title from the document.
            image = []
            # Retrieve the content of the first page to minimize embedding costs.
            return doc[0].page_content, image, doc[0].metadata["title"]

        except requests.exceptions.Timeout:
            print(f"Download timed out. Please check the link : {self.link}")
            return "", [], ""
        except Exception as e:
            print(f"Error loading PDF : {self.link} {e}")
            return "", [], ""



================================================
FILE: gpt_researcher/scraper/tavily_extract/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/tavily_extract/tavily_extract.py
================================================
from bs4 import BeautifulSoup
import os
from ..utils import get_relevant_images, extract_title

class TavilyExtract:

    def __init__(self, link, session=None):
        self.link = link
        self.session = session
        from tavily import TavilyClient
        self.tavily_client = TavilyClient(api_key=self.get_api_key())

    def get_api_key(self) -> str:
        """
        Gets the Tavily API key
        Returns:
        Api key (str)
        """
        try:
            api_key = os.environ["TAVILY_API_KEY"]
        except KeyError:
            raise Exception(
                "Tavily API key not found. Please set the TAVILY_API_KEY environment variable.")
        return api_key

    def scrape(self) -> tuple:
        """
        This function extracts content from a specified link using the Tavily Python SDK, the title and
        images from the link are extracted using the functions from `gpt_researcher/scraper/utils.py`.

        Returns:
          The `scrape` method returns a tuple containing the extracted content, a list of image URLs, and
        the title of the webpage specified by the `self.link` attribute. It uses the Tavily Python SDK to
        extract and clean content from the webpage. If any exception occurs during the process, an error
        message is printed and an empty result is returned.
        """

        try:
            response = self.tavily_client.extract(urls=self.link)
            if response['failed_results']:
                return "", [], ""

            # Parse the HTML content of the response to create a BeautifulSoup object for the utility functions
            response_bs = self.session.get(self.link, timeout=4)
            soup = BeautifulSoup(
                response_bs.content, "lxml", from_encoding=response_bs.encoding
            )

            # Since only a single link is provided to tavily_client, the results will contain only one entry.
            content = response['results'][0]['raw_content']

            # Get relevant images using the utility function
            image_urls = get_relevant_images(soup, self.link)

            # Extract the title using the utility function
            title = extract_title(soup)

            return content, image_urls, title

        except Exception as e:
            print("Error! : " + str(e))
            return "", [], ""


================================================
FILE: gpt_researcher/scraper/web_base_loader/__init__.py
================================================



================================================
FILE: gpt_researcher/scraper/web_base_loader/web_base_loader.py
================================================
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import requests
from ..utils import get_relevant_images, extract_title

class WebBaseLoaderScraper:

    def __init__(self, link, session=None):
        self.link = link
        self.session = session or requests.Session()

    def scrape(self) -> tuple:
        """
        This Python function scrapes content from a webpage using a WebBaseLoader object and returns the
        concatenated page content.
        
        Returns:
          The `scrape` method is returning a string variable named `content` which contains the
        concatenated page content from the documents loaded by the `WebBaseLoader`. If an exception
        occurs during the process, an error message is printed and an empty string is returned.
        """
        try:
            from langchain_community.document_loaders import WebBaseLoader
            loader = WebBaseLoader(self.link)
            loader.requests_kwargs = {"verify": False}
            docs = loader.load()
            content = ""

            for doc in docs:
                content += doc.page_content

            response = self.session.get(self.link)
            soup = BeautifulSoup(response.content, 'html.parser')
            image_urls = get_relevant_images(soup, self.link)
            
            # Extract the title using the utility function
            title = extract_title(soup)

            return content, image_urls, title

        except Exception as e:
            print("Error! : " + str(e))
            return "", [], ""



================================================
FILE: gpt_researcher/skills/__init__.py
================================================
from .context_manager import ContextManager
from .researcher import ResearchConductor
from .writer import ReportGenerator
from .browser import BrowserManager
from .curator import SourceCurator

__all__ = [
    'ResearchConductor',
    'ReportGenerator',
    'ContextManager',
    'BrowserManager',
    'SourceCurator'
]



================================================
FILE: gpt_researcher/skills/browser.py
================================================
from gpt_researcher.utils.workers import WorkerPool

from ..actions.utils import stream_output
from ..actions.web_scraping import scrape_urls
from ..scraper.utils import get_image_hash


class BrowserManager:
    """Manages context for the researcher agent."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.worker_pool = WorkerPool(researcher.cfg.max_scraper_workers)

    async def browse_urls(self, urls: list[str]) -> list[dict]:
        """
        Scrape content from a list of URLs.

        Args:
            urls (list[str]): list of URLs to scrape.

        Returns:
            list[dict]: list of scraped content results.
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "scraping_urls",
                f"ğŸŒ Scraping content from {len(urls)} URLs...",
                self.researcher.websocket,
            )

        scraped_content, images = await scrape_urls(
            urls, self.researcher.cfg, self.worker_pool
        )
        self.researcher.add_research_sources(scraped_content)
        new_images = self.select_top_images(images, k=4)  # Select top 4 images
        self.researcher.add_research_images(new_images)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "scraping_content",
                f"ğŸ“„ Scraped {len(scraped_content)} pages of content",
                self.researcher.websocket,
            )
            await stream_output(
                "logs",
                "scraping_images",
                f"ğŸ–¼ï¸ Selected {len(new_images)} new images from {len(images)} total images",
                self.researcher.websocket,
                True,
                new_images,
            )
            await stream_output(
                "logs",
                "scraping_complete",
                f"ğŸŒ Scraping complete",
                self.researcher.websocket,
            )

        return scraped_content

    def select_top_images(self, images: list[dict], k: int = 2) -> list[str]:
        """
        Select most relevant images and remove duplicates based on image content.

        Args:
            images (list[dict]): list of image dictionaries with 'url' and 'score' keys.
            k (int): Number of top images to select if no high-score images are found.

        Returns:
            list[str]: list of selected image URLs.
        """
        unique_images = []
        seen_hashes = set()
        current_research_images = self.researcher.get_research_images()

        # Process images in descending order of their scores
        for img in sorted(images, key=lambda im: im["score"], reverse=True):
            img_hash = get_image_hash(img['url'])
            if (
                img_hash
                and img_hash not in seen_hashes
                and img['url'] not in current_research_images
            ):
                seen_hashes.add(img_hash)
                unique_images.append(img["url"])

                if len(unique_images) == k:
                    break

        return unique_images



================================================
FILE: gpt_researcher/skills/context_manager.py
================================================
import asyncio
from typing import List, Dict, Optional, Set

from ..context.compression import ContextCompressor, WrittenContentCompressor, VectorstoreCompressor
from ..actions.utils import stream_output


class ContextManager:
    """Manages context for the researcher agent."""

    def __init__(self, researcher):
        self.researcher = researcher

    async def get_similar_content_by_query(self, query, pages):
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "fetching_query_content",
                f"ğŸ“š Getting relevant content based on query: {query}...",
                self.researcher.websocket,
            )

        context_compressor = ContextCompressor(
            documents=pages,
            embeddings=self.researcher.memory.get_embeddings(),
            prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )
        return await context_compressor.async_get_context(
            query=query, max_results=10, cost_callback=self.researcher.add_costs
        )

    async def get_similar_content_by_query_with_vectorstore(self, query, filter):
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "fetching_query_format",
                f" Getting relevant content based on query: {query}...",
                self.researcher.websocket,
                )
        vectorstore_compressor = VectorstoreCompressor(
            self.researcher.vector_store, filter=filter, prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )
        return await vectorstore_compressor.async_get_context(query=query, max_results=8)

    async def get_similar_written_contents_by_draft_section_titles(
        self,
        current_subtopic: str,
        draft_section_titles: List[str],
        written_contents: List[Dict],
        max_results: int = 10
    ) -> List[str]:
        all_queries = [current_subtopic] + draft_section_titles

        async def process_query(query: str) -> Set[str]:
            return set(await self.__get_similar_written_contents_by_query(query, written_contents, **self.researcher.kwargs))

        results = await asyncio.gather(*[process_query(query) for query in all_queries])
        relevant_contents = set().union(*results)
        relevant_contents = list(relevant_contents)[:max_results]

        return relevant_contents

    async def __get_similar_written_contents_by_query(self,
                                                      query: str,
                                                      written_contents: List[Dict],
                                                      similarity_threshold: float = 0.5,
                                                      max_results: int = 10
                                                      ) -> List[str]:
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "fetching_relevant_written_content",
                f"ğŸ” Getting relevant written content based on query: {query}...",
                self.researcher.websocket,
            )

        written_content_compressor = WrittenContentCompressor(
            documents=written_contents,
            embeddings=self.researcher.memory.get_embeddings(),
            similarity_threshold=similarity_threshold,
            **self.researcher.kwargs
        )
        return await written_content_compressor.async_get_context(
            query=query, max_results=max_results, cost_callback=self.researcher.add_costs
        )



================================================
FILE: gpt_researcher/skills/curator.py
================================================
from typing import Dict, Optional, List
import json
from ..config.config import Config
from ..utils.llm import create_chat_completion
from ..actions import stream_output


class SourceCurator:
    """Ranks sources and curates data based on their relevance, credibility and reliability."""

    def __init__(self, researcher):
        self.researcher = researcher

    async def curate_sources(
        self,
        source_data: List,
        max_results: int = 10,
    ) -> List:
        """
        Rank sources based on research data and guidelines.

        Args:
            query: The research query/task
            source_data: List of source documents to rank
            max_results: Maximum number of top sources to return

        Returns:
            str: Ranked list of source URLs with reasoning
        """
        print(f"\n\nCurating {len(source_data)} sources: {source_data}")
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_plan",
                f"âš–ï¸ Evaluating and curating sources by credibility and relevance...",
                self.researcher.websocket,
            )

        response = ""
        try:
            response = await create_chat_completion(
                model=self.researcher.cfg.smart_llm_model,
                messages=[
                    {"role": "system", "content": f"{self.researcher.role}"},
                    {"role": "user", "content": self.researcher.prompt_family.curate_sources(
                        self.researcher.query, source_data, max_results)},
                ],
                temperature=0.2,
                max_tokens=8000,
                llm_provider=self.researcher.cfg.smart_llm_provider,
                llm_kwargs=self.researcher.cfg.llm_kwargs,
                cost_callback=self.researcher.add_costs,
            )

            curated_sources = json.loads(response)
            print(f"\n\nFinal Curated sources {len(source_data)} sources: {curated_sources}")

            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "research_plan",
                    f"ğŸ… Verified and ranked top {len(curated_sources)} most reliable sources",
                    self.researcher.websocket,
                )

            return curated_sources

        except Exception as e:
            print(f"Error in curate_sources from LLM response: {response}")
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "research_plan",
                    f"ğŸš« Source verification failed: {str(e)}",
                    self.researcher.websocket,
                )
            return source_data



================================================
FILE: gpt_researcher/skills/deep_research.py
================================================
from typing import List, Dict, Any, Optional, Set
import asyncio
import logging
import time
from datetime import datetime, timedelta

from gpt_researcher.llm_provider.generic.base import ReasoningEfforts
from ..utils.llm import create_chat_completion
from ..utils.enum import ReportType, ReportSource, Tone
from ..actions.query_processing import get_search_results

logger = logging.getLogger(__name__)

# Maximum words allowed in context (25k words for safety margin)
MAX_CONTEXT_WORDS = 25000

def count_words(text: str) -> int:
    """Count words in a text string"""
    return len(text.split())

def trim_context_to_word_limit(context_list: List[str], max_words: int = MAX_CONTEXT_WORDS) -> List[str]:
    """Trim context list to stay within word limit while preserving most recent/relevant items"""
    total_words = 0
    trimmed_context = []

    # Process in reverse to keep most recent items
    for item in reversed(context_list):
        words = count_words(item)
        if total_words + words <= max_words:
            trimmed_context.insert(0, item)  # Insert at start to maintain original order
            total_words += words
        else:
            break

    return trimmed_context

class ResearchProgress:
    def __init__(self, total_depth: int, total_breadth: int):
        self.current_depth = 1  # Start from 1 and increment up to total_depth
        self.total_depth = total_depth
        self.current_breadth = 0  # Start from 0 and count up to total_breadth as queries complete
        self.total_breadth = total_breadth
        self.current_query: Optional[str] = None
        self.total_queries = 0
        self.completed_queries = 0


class DeepResearchSkill:
    def __init__(self, researcher):
        self.researcher = researcher
        self.breadth = getattr(researcher.cfg, 'deep_research_breadth', 4)
        self.depth = getattr(researcher.cfg, 'deep_research_depth', 2)
        self.concurrency_limit = getattr(researcher.cfg, 'deep_research_concurrency', 2)
        self.websocket = researcher.websocket
        self.tone = researcher.tone
        self.config_path = researcher.cfg.config_path if hasattr(researcher.cfg, 'config_path') else None
        self.headers = researcher.headers or {}
        self.visited_urls = researcher.visited_urls
        self.learnings = []
        self.research_sources = []  # Track all research sources
        self.context = []  # Track all context

    async def generate_search_queries(self, query: str, num_queries: int = 3) -> List[Dict[str, str]]:
        """Generate SERP queries for research"""
        messages = [
            {"role": "system", "content": "You are an expert researcher generating search queries."},
            {"role": "user",
             "content": f"Given the following prompt, generate {num_queries} unique search queries to research the topic thoroughly. For each query, provide a research goal. Format as 'Query: <query>' followed by 'Goal: <goal>' for each pair: {query}"}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=self.researcher.cfg.strategic_llm_provider,
            model=self.researcher.cfg.strategic_llm_model,
            reasoning_effort=self.researcher.cfg.reasoning_effort,
            temperature=0.4
        )

        lines = response.split('\n')
        queries = []
        current_query = {}

        for line in lines:
            line = line.strip()
            if line.startswith('Query:'):
                if current_query:
                    queries.append(current_query)
                current_query = {'query': line.replace('Query:', '').strip()}
            elif line.startswith('Goal:') and current_query:
                current_query['researchGoal'] = line.replace('Goal:', '').strip()

        if current_query:
            queries.append(current_query)

        return queries[:num_queries]

    async def generate_research_plan(self, query: str, num_questions: int = 3) -> List[str]:
        """Generate follow-up questions to clarify research direction"""
        # Get initial search results to inform query generation
        search_results = await get_search_results(query, self.researcher.retrievers[0])
        logger.info(f"Initial web knowledge obtained: {len(search_results)} results")

        # Get current time for context
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        messages = [
            {"role": "system", "content": "You are an expert researcher. Your task is to analyze the original query and search results, then generate targeted questions that explore different aspects and time periods of the topic."},
            {"role": "user",
             "content": f"""Original query: {query}

Current time: {current_time}

Search results:
{search_results}

Based on these results, the original query, and the current time, generate {num_questions} unique questions. Each question should explore a different aspect or time period of the topic, considering recent developments up to {current_time}.

Format each question on a new line starting with 'Question: '"""}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=self.researcher.cfg.strategic_llm_provider,
            model=self.researcher.cfg.strategic_llm_model,
            reasoning_effort=ReasoningEfforts.High.value,
            temperature=0.4
        )

        questions = [q.replace('Question:', '').strip()
                     for q in response.split('\n')
                     if q.strip().startswith('Question:')]
        return questions[:num_questions]

    async def process_research_results(self, query: str, context: str, num_learnings: int = 3) -> Dict[str, List[str]]:
        """Process research results to extract learnings and follow-up questions"""
        messages = [
            {"role": "system", "content": "You are an expert researcher analyzing search results."},
            {"role": "user",
             "content": f"Given the following research results for the query '{query}', extract key learnings and suggest follow-up questions. For each learning, include a citation to the source URL if available. Format each learning as 'Learning [source_url]: <insight>' and each question as 'Question: <question>':\n\n{context}"}
        ]

        response = await create_chat_completion(
            messages=messages,
            llm_provider=self.researcher.cfg.strategic_llm_provider,
            model=self.researcher.cfg.strategic_llm_model,
            temperature=0.4,
            reasoning_effort=ReasoningEfforts.High.value,
            max_tokens=1000
        )

        lines = response.split('\n')
        learnings = []
        questions = []
        citations = {}

        for line in lines:
            line = line.strip()
            if line.startswith('Learning'):
                import re
                url_match = re.search(r'\[(.*?)\]:', line)
                if url_match:
                    url = url_match.group(1)
                    learning = line.split(':', 1)[1].strip()
                    learnings.append(learning)
                    citations[learning] = url
                else:
                    # Try to find URL in the line itself
                    url_match = re.search(
                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', line)
                    if url_match:
                        url = url_match.group(0)
                        learning = line.replace(url, '').replace('Learning:', '').strip()
                        learnings.append(learning)
                        citations[learning] = url
                    else:
                        learnings.append(line.replace('Learning:', '').strip())
            elif line.startswith('Question:'):
                questions.append(line.replace('Question:', '').strip())

        return {
            'learnings': learnings[:num_learnings],
            'followUpQuestions': questions[:num_learnings],
            'citations': citations
        }

    async def deep_research(
            self,
            query: str,
            breadth: int,
            depth: int,
            learnings: List[str] = None,
            citations: Dict[str, str] = None,
            visited_urls: Set[str] = None,
            on_progress=None
    ) -> Dict[str, Any]:
        """Conduct deep iterative research"""
        if learnings is None:
            learnings = []
        if citations is None:
            citations = {}
        if visited_urls is None:
            visited_urls = set()

        progress = ResearchProgress(depth, breadth)

        if on_progress:
            on_progress(progress)

        # Generate search queries
        serp_queries = await self.generate_search_queries(query, num_queries=breadth)
        progress.total_queries = len(serp_queries)

        all_learnings = learnings.copy()
        all_citations = citations.copy()
        all_visited_urls = visited_urls.copy()
        all_context = []
        all_sources = []

        # Process queries with concurrency limit
        semaphore = asyncio.Semaphore(self.concurrency_limit)

        async def process_query(serp_query: Dict[str, str]) -> Optional[Dict[str, Any]]:
            async with semaphore:
                try:
                    progress.current_query = serp_query['query']
                    if on_progress:
                        on_progress(progress)

                    from .. import GPTResearcher
                    researcher = GPTResearcher(
                        query=serp_query['query'],
                        report_type=ReportType.ResearchReport.value,
                        report_source=ReportSource.Web.value,
                        tone=self.tone,
                        websocket=self.websocket,
                        config_path=self.config_path,
                        headers=self.headers,
                        visited_urls=self.visited_urls
                    )

                    # Conduct research
                    context = await researcher.conduct_research()

                    # Get results and visited URLs
                    visited = researcher.visited_urls
                    sources = researcher.research_sources

                    # Process results to extract learnings and citations
                    results = await self.process_research_results(
                        query=serp_query['query'],
                        context=context
                    )

                    # Update progress
                    progress.completed_queries += 1
                    progress.current_breadth += 1
                    if on_progress:
                        on_progress(progress)

                    return {
                        'learnings': results['learnings'],
                        'visited_urls': list(visited),
                        'followUpQuestions': results['followUpQuestions'],
                        'researchGoal': serp_query['researchGoal'],
                        'citations': results['citations'],
                        'context': context if context else "",
                        'sources': sources if sources else []
                    }

                except Exception as e:
                    logger.error(f"Error processing query '{serp_query['query']}': {str(e)}")
                    return None

        # Process queries concurrently with limit
        tasks = [process_query(query) for query in serp_queries]
        results = await asyncio.gather(*tasks)
        results = [r for r in results if r is not None]

        # Update breadth progress based on successful queries
        progress.current_breadth = len(results)
        if on_progress:
            on_progress(progress)

        # Collect all results
        for result in results:
            all_learnings.extend(result['learnings'])
            all_visited_urls.update(result['visited_urls'])
            all_citations.update(result['citations'])
            if result['context']:
                all_context.append(result['context'])
            if result['sources']:
                all_sources.extend(result['sources'])

            # Continue deeper if needed
            if depth > 1:
                new_breadth = max(2, breadth // 2)
                new_depth = depth - 1
                progress.current_depth += 1

                # Create next query from research goal and follow-up questions
                next_query = f"""
                Previous research goal: {result['researchGoal']}
                Follow-up questions: {' '.join(result['followUpQuestions'])}
                """

                # Recursive research
                deeper_results = await self.deep_research(
                    query=next_query,
                    breadth=new_breadth,
                    depth=new_depth,
                    learnings=all_learnings,
                    citations=all_citations,
                    visited_urls=all_visited_urls,
                    on_progress=on_progress
                )

                all_learnings = deeper_results['learnings']
                all_visited_urls.update(deeper_results['visited_urls'])
                all_citations.update(deeper_results['citations'])
                if deeper_results.get('context'):
                    all_context.extend(deeper_results['context'])
                if deeper_results.get('sources'):
                    all_sources.extend(deeper_results['sources'])

        # Update class tracking
        self.context.extend(all_context)
        self.research_sources.extend(all_sources)

        # Trim context to stay within word limits
        trimmed_context = trim_context_to_word_limit(all_context)
        logger.info(f"Trimmed context from {len(all_context)} items to {len(trimmed_context)} items to stay within word limit")

        return {
            'learnings': list(set(all_learnings)),
            'visited_urls': list(all_visited_urls),
            'citations': all_citations,
            'context': trimmed_context,
            'sources': all_sources
        }

    async def run(self, on_progress=None) -> str:
        """Run the deep research process and generate final report"""
        start_time = time.time()

        # Log initial costs
        initial_costs = self.researcher.get_costs()

        follow_up_questions = await self.generate_research_plan(self.researcher.query)
        answers = ["Automatically proceeding with research"] * len(follow_up_questions)

        qa_pairs = [f"Q: {q}\nA: {a}" for q, a in zip(follow_up_questions, answers)]
        combined_query = f"""
        Initial Query: {self.researcher.query}\nFollow - up Questions and Answers:\n
        """ + "\n".join(qa_pairs)

        results = await self.deep_research(
            query=combined_query,
            breadth=self.breadth,
            depth=self.depth,
            on_progress=on_progress
        )

        # Get costs after deep research
        research_costs = self.researcher.get_costs() - initial_costs

        # Log research costs if we have a log handler
        if self.researcher.log_handler:
            await self.researcher._log_event("research", step="deep_research_costs", details={
                "research_costs": research_costs,
                "total_costs": self.researcher.get_costs()
            })

        # Prepare context with citations
        context_with_citations = []
        for learning in results['learnings']:
            citation = results['citations'].get(learning, '')
            if citation:
                context_with_citations.append(f"{learning} [Source: {citation}]")
            else:
                context_with_citations.append(learning)

        # Add all research context
        if results.get('context'):
            context_with_citations.extend(results['context'])

        # Trim final context to word limit
        final_context = trim_context_to_word_limit(context_with_citations)
        
        # Set enhanced context and visited URLs
        self.researcher.context = "\n".join(final_context)
        self.researcher.visited_urls = results['visited_urls']

        # Set research sources
        if results.get('sources'):
            self.researcher.research_sources = results['sources']

        # Log total execution time
        end_time = time.time()
        execution_time = timedelta(seconds=end_time - start_time)
        logger.info(f"Total research execution time: {execution_time}")
        logger.info(f"Total research costs: ${research_costs:.2f}")

        # Return the context - don't generate report here as it will be done by the main agent
        return self.researcher.context


================================================
FILE: gpt_researcher/skills/researcher.py
================================================
import asyncio
import random
import logging
import os
from ..actions.utils import stream_output
from ..actions.query_processing import plan_research_outline, get_search_results
from ..document import DocumentLoader, OnlineDocumentLoader, LangChainDocumentLoader
from ..utils.enum import ReportSource, ReportType
from ..utils.logging_config import get_json_handler
from ..actions.agent_creator import choose_agent


class ResearchConductor:
    """Manages and coordinates the research process."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.logger = logging.getLogger('research')
        self.json_handler = get_json_handler()
        # Add cache for MCP results to avoid redundant calls
        self._mcp_results_cache = None
        # Track MCP query count for balanced mode
        self._mcp_query_count = 0

    async def plan_research(self, query, query_domains=None):
        """Gets the sub-queries from the query
        Args:
            query: original query
        Returns:
            List of queries
        """
        await stream_output(
            "logs",
            "planning_research",
            f"ğŸŒ Browsing the web to learn more about the task: {query}...",
            self.researcher.websocket,
        )

        search_results = await get_search_results(query, self.researcher.retrievers[0], query_domains, researcher=self.researcher)
        self.logger.info(f"Initial search results obtained: {len(search_results)} results")

        await stream_output(
            "logs",
            "planning_research",
            f"ğŸ¤” Planning the research strategy and subtasks...",
            self.researcher.websocket,
        )

        retriever_names = [r.__name__ for r in self.researcher.retrievers]
        # Remove duplicate logging - this will be logged once in conduct_research instead

        outline = await plan_research_outline(
            query=query,
            search_results=search_results,
            agent_role_prompt=self.researcher.role,
            cfg=self.researcher.cfg,
            parent_query=self.researcher.parent_query,
            report_type=self.researcher.report_type,
            cost_callback=self.researcher.add_costs,
            retriever_names=retriever_names,  # Pass retriever names for MCP optimization
            **self.researcher.kwargs
        )
        self.logger.info(f"Research outline planned: {outline}")
        return outline

    async def conduct_research(self):
        """Runs the GPT Researcher to conduct research"""
        if self.json_handler:
            self.json_handler.update_content("query", self.researcher.query)
        
        self.logger.info(f"Starting research for query: {self.researcher.query}")
        
        # Log active retrievers once at the start of research
        retriever_names = [r.__name__ for r in self.researcher.retrievers]
        self.logger.info(f"Active retrievers: {retriever_names}")
        
        # Reset visited_urls and source_urls at the start of each research task
        self.researcher.visited_urls.clear()
        research_data = []

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "starting_research",
                f"ğŸ” Starting the research task for '{self.researcher.query}'...",
                self.researcher.websocket,
            )
            await stream_output(
                "logs",
                "agent_generated",
                self.researcher.agent,
                self.researcher.websocket
            )

        # Choose agent and role if not already defined
        if not (self.researcher.agent and self.researcher.role):
            self.researcher.agent, self.researcher.role = await choose_agent(
                query=self.researcher.query,
                cfg=self.researcher.cfg,
                parent_query=self.researcher.parent_query,
                cost_callback=self.researcher.add_costs,
                headers=self.researcher.headers,
                prompt_family=self.researcher.prompt_family
            )
                
        # Check if MCP retrievers are configured
        has_mcp_retriever = any("mcpretriever" in r.__name__.lower() for r in self.researcher.retrievers)
        if has_mcp_retriever:
            self.logger.info("MCP retrievers configured and will be used with standard research flow")

        # Conduct research based on the source type
        if self.researcher.source_urls:
            self.logger.info("Using provided source URLs")
            research_data = await self._get_context_by_urls(self.researcher.source_urls)
            if research_data and len(research_data) == 0 and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "answering_from_memory",
                    f"ğŸ§ I was unable to find relevant context in the provided sources...",
                    self.researcher.websocket,
                )
            if self.researcher.complement_source_urls:
                self.logger.info("Complementing with web search")
                additional_research = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
                research_data += ' '.join(additional_research)
        elif self.researcher.report_source == ReportSource.Web.value:
            self.logger.info("Using web search with all configured retrievers")
            research_data = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
        elif self.researcher.report_source == ReportSource.Local.value:
            self.logger.info("Using local search")
            document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            self.logger.info(f"Loaded {len(document_data)} documents")
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)

            research_data = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
        # Hybrid search including both local documents and web sources
        elif self.researcher.report_source == ReportSource.Hybrid.value:
            if self.researcher.document_urls:
                document_data = await OnlineDocumentLoader(self.researcher.document_urls).load()
            else:
                document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)
            docs_context = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
            web_context = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
            research_data = self.researcher.prompt_family.join_local_web_documents(docs_context, web_context)
        elif self.researcher.report_source == ReportSource.Azure.value:
            from ..document.azure_document_loader import AzureDocumentLoader
            azure_loader = AzureDocumentLoader(
                container_name=os.getenv("AZURE_CONTAINER_NAME"),
                connection_string=os.getenv("AZURE_CONNECTION_STRING")
            )
            azure_files = await azure_loader.load()
            document_data = await DocumentLoader(azure_files).load()  # Reuse existing loader
            research_data = await self._get_context_by_web_search(self.researcher.query, document_data)
            
        elif self.researcher.report_source == ReportSource.LangChainDocuments.value:
            langchain_documents_data = await LangChainDocumentLoader(
                self.researcher.documents
            ).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(langchain_documents_data)
            research_data = await self._get_context_by_web_search(
                self.researcher.query, langchain_documents_data, self.researcher.query_domains
            )
        elif self.researcher.report_source == ReportSource.LangChainVectorStore.value:
            research_data = await self._get_context_by_vectorstore(self.researcher.query, self.researcher.vector_store_filter)

        # Rank and curate the sources
        self.researcher.context = research_data
        if self.researcher.cfg.curate_sources:
            self.logger.info("Curating sources")
            self.researcher.context = await self.researcher.source_curator.curate_sources(research_data)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_step_finalized",
                f"Finalized research step.\nğŸ’¸ Total Research Costs: ${self.researcher.get_costs()}",
                self.researcher.websocket,
            )
            if self.json_handler:
                self.json_handler.update_content("costs", self.researcher.get_costs())
                self.json_handler.update_content("context", self.researcher.context)

        self.logger.info(f"Research completed. Context size: {len(str(self.researcher.context))}")
        return self.researcher.context

    async def _get_context_by_urls(self, urls):
        """Scrapes and compresses the context from the given urls"""
        self.logger.info(f"Getting context from URLs: {urls}")
        
        new_search_urls = await self._get_new_urls(urls)
        self.logger.info(f"New URLs to process: {new_search_urls}")

        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        self.logger.info(f"Scraped content from {len(scraped_content)} URLs")

        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        context = await self.researcher.context_manager.get_similar_content_by_query(
            self.researcher.query, scraped_content
        )
        return context

    # Add logging to other methods similarly...

    async def _get_context_by_vectorstore(self, query, filter: dict | None = None):
        """
        Generates the context for the research task by searching the vectorstore
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting vectorstore search for query: {query}")
        context = []
        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query)
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸  I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        context = await asyncio.gather(
            *[
                self._process_sub_query_with_vectorstore(sub_query, filter)
                for sub_query in sub_queries
            ]
        )
        return context

    async def _get_context_by_web_search(self, query, scraped_data: list | None = None, query_domains: list | None = None):
        """
        Generates the context for the research task by searching the query and scraping the results
        Returns:
            context: List of context
        """
        self.logger.info(f"Starting web search for query: {query}")
        
        if scraped_data is None:
            scraped_data = []
        if query_domains is None:
            query_domains = []

        # **CONFIGURABLE MCP OPTIMIZATION: Control MCP strategy**
        mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" in r.__name__.lower()]
        
        # Get MCP strategy configuration
        mcp_strategy = self._get_mcp_strategy()
        
        if mcp_retrievers and self._mcp_results_cache is None:
            if mcp_strategy == "disabled":
                # MCP disabled - skip MCP research entirely
                self.logger.info("MCP disabled by strategy, skipping MCP research")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_disabled",
                        f"âš¡ MCP research disabled by configuration",
                        self.researcher.websocket,
                    )
            elif mcp_strategy == "fast":
                # Fast: Run MCP once with original query
                self.logger.info("MCP fast strategy: Running once with original query")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_optimization",
                        f"ğŸš€ MCP Fast: Running once for main query (performance mode)",
                        self.researcher.websocket,
                    )
                
                # Execute MCP research once with the original query
                mcp_context = await self._execute_mcp_research_for_queries([query], mcp_retrievers)
                self._mcp_results_cache = mcp_context
                self.logger.info(f"MCP results cached: {len(mcp_context)} total context entries")
            elif mcp_strategy == "deep":
                # Deep: Will run MCP for all queries (original behavior) - defer to per-query execution
                self.logger.info("MCP deep strategy: Will run for all queries")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_comprehensive",
                        f"ğŸ” MCP Deep: Will run for each sub-query (thorough mode)",
                        self.researcher.websocket,
                    )
                # Don't cache - let each sub-query run MCP individually
            else:
                # Unknown strategy - default to fast
                self.logger.warning(f"Unknown MCP strategy '{mcp_strategy}', defaulting to fast")
                mcp_context = await self._execute_mcp_research_for_queries([query], mcp_retrievers)
                self._mcp_results_cache = mcp_context
                self.logger.info(f"MCP results cached: {len(mcp_context)} total context entries")

        # Generate Sub-Queries including original query
        sub_queries = await self.plan_research(query, query_domains)
        self.logger.info(f"Generated sub-queries: {sub_queries}")
        
        # If this is not part of a sub researcher, add original query to research for better results
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ I will conduct my research based on the following queries: {sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # Using asyncio.gather to process the sub_queries asynchronously
        try:
            context = await asyncio.gather(
                *[
                    self._process_sub_query(sub_query, scraped_data, query_domains)
                    for sub_query in sub_queries
                ]
            )
            self.logger.info(f"Gathered context from {len(context)} sub-queries")
            # Filter out empty results and join the context
            context = [c for c in context if c]
            if context:
                combined_context = " ".join(context)
                self.logger.info(f"Combined context size: {len(combined_context)}")
                return combined_context
            return []
        except Exception as e:
            self.logger.error(f"Error during web search: {e}", exc_info=True)
            return []

    def _get_mcp_strategy(self) -> str:
        """
        Get the MCP strategy configuration.
        
        Priority:
        1. Instance-level setting (self.researcher.mcp_strategy)
        2. Config file setting (self.researcher.cfg.mcp_strategy) 
        3. Default value ("fast")
        
        Returns:
            str: MCP strategy
                "disabled" = Skip MCP entirely
                "fast" = Run MCP once with original query (default)
                "deep" = Run MCP for all sub-queries
        """
        # Check instance-level setting first
        if hasattr(self.researcher, 'mcp_strategy') and self.researcher.mcp_strategy is not None:
            return self.researcher.mcp_strategy
        
        # Check config setting
        if hasattr(self.researcher.cfg, 'mcp_strategy'):
            return self.researcher.cfg.mcp_strategy
        
        # Default to fast mode
        return "fast"

    async def _execute_mcp_research_for_queries(self, queries: list, mcp_retrievers: list) -> list:
        """
        Execute MCP research for a list of queries.
        
        Args:
            queries: List of queries to research
            mcp_retrievers: List of MCP retriever classes
            
        Returns:
            list: Combined MCP context entries from all queries
        """
        all_mcp_context = []
        
        for i, query in enumerate(queries, 1):
            self.logger.info(f"Executing MCP research for query {i}/{len(queries)}: {query}")
            
            for retriever in mcp_retrievers:
                try:
                    mcp_results = await self._execute_mcp_research(retriever, query)
                    if mcp_results:
                        for result in mcp_results:
                            content = result.get("body", "")
                            url = result.get("href", "")
                            title = result.get("title", "")
                            
                            if content:
                                context_entry = {
                                    "content": content,
                                    "url": url,
                                    "title": title,
                                    "query": query,
                                    "source_type": "mcp"
                                }
                                all_mcp_context.append(context_entry)
                        
                        self.logger.info(f"Added {len(mcp_results)} MCP results for query: {query}")
                        
                        if self.researcher.verbose:
                            await stream_output(
                                "logs",
                                "mcp_results_cached",
                                f"âœ… Cached {len(mcp_results)} MCP results from query {i}/{len(queries)}",
                                self.researcher.websocket,
                            )
                except Exception as e:
                    self.logger.error(f"Error in MCP research for query '{query}': {e}")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_cache_error",
                            f"âš ï¸ MCP research error for query {i}, continuing with other sources",
                            self.researcher.websocket,
                        )
        
        return all_mcp_context

    async def _process_sub_query(self, sub_query: str, scraped_data: list = [], query_domains: list = []):
        """Takes in a sub query and scrapes urls based on it and gathers context."""
        if self.json_handler:
            self.json_handler.log_event("sub_query", {
                "query": sub_query,
                "scraped_data_size": len(scraped_data)
            })
        
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_research",
                f"\nğŸ” Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        try:
            # Identify MCP retrievers
            mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" in r.__name__.lower()]
            non_mcp_retrievers = [r for r in self.researcher.retrievers if "mcpretriever" not in r.__name__.lower()]
            
            # Initialize context components
            mcp_context = []
            web_context = ""
            
            # Get MCP strategy configuration
            mcp_strategy = self._get_mcp_strategy()
            
            # **CONFIGURABLE MCP PROCESSING**
            if mcp_retrievers:
                if mcp_strategy == "disabled":
                    # MCP disabled - skip entirely
                    self.logger.info(f"MCP disabled for sub-query: {sub_query}")
                elif mcp_strategy == "fast" and self._mcp_results_cache is not None:
                    # Fast: Use cached results
                    mcp_context = self._mcp_results_cache.copy()
                    
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_cache_reuse",
                            f"â™»ï¸ Reusing cached MCP results ({len(mcp_context)} sources) for: {sub_query}",
                            self.researcher.websocket,
                        )
                    
                    self.logger.info(f"Reused {len(mcp_context)} cached MCP results for sub-query: {sub_query}")
                elif mcp_strategy == "deep":
                    # Deep: Run MCP for every sub-query
                    self.logger.info(f"Running deep MCP research for: {sub_query}")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_comprehensive_run",
                            f"ğŸ” Running deep MCP research for: {sub_query}",
                            self.researcher.websocket,
                        )
                    
                    mcp_context = await self._execute_mcp_research_for_queries([sub_query], mcp_retrievers)
                else:
                    # Fallback: if no cache and not deep mode, run MCP for this query
                    self.logger.warning("MCP cache not available, falling back to per-sub-query execution")
                    if self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_fallback",
                            f"ğŸ”Œ MCP cache unavailable, running MCP research for: {sub_query}",
                            self.researcher.websocket,
                        )
                    
                    mcp_context = await self._execute_mcp_research_for_queries([sub_query], mcp_retrievers)
            
            # Get web search context using non-MCP retrievers (if no scraped data provided)
            if not scraped_data:
                scraped_data = await self._scrape_data_by_urls(sub_query, query_domains)
                self.logger.info(f"Scraped data size: {len(scraped_data)}")

            # Get similar content based on scraped data
            if scraped_data:
                web_context = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)
                self.logger.info(f"Web content found for sub-query: {len(str(web_context)) if web_context else 0} chars")

            # Combine MCP context with web context intelligently
            combined_context = self._combine_mcp_and_web_context(mcp_context, web_context, sub_query)
            
            # Log context combination results
            if combined_context:
                context_length = len(str(combined_context))
                self.logger.info(f"Combined context for '{sub_query}': {context_length} chars")
                
                if self.researcher.verbose:
                    mcp_count = len(mcp_context)
                    web_available = bool(web_context)
                    cache_used = self._mcp_results_cache is not None and mcp_retrievers and mcp_strategy != "deep"
                    cache_status = " (cached)" if cache_used else ""
                    await stream_output(
                        "logs",
                        "context_combined",
                        f"ğŸ“š Combined research context: {mcp_count} MCP sources{cache_status}, {'web content' if web_available else 'no web content'}",
                        self.researcher.websocket,
                    )
            else:
                self.logger.warning(f"No combined context found for sub-query: {sub_query}")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "subquery_context_not_found",
                        f"ğŸ¤· No content found for '{sub_query}'...",
                        self.researcher.websocket,
                    )
            
            if combined_context and self.json_handler:
                self.json_handler.log_event("content_found", {
                    "sub_query": sub_query,
                    "content_size": len(str(combined_context)),
                    "mcp_sources": len(mcp_context),
                    "web_content": bool(web_context)
                })
                
            return combined_context
            
        except Exception as e:
            self.logger.error(f"Error processing sub-query {sub_query}: {e}", exc_info=True)
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "subquery_error",
                    f"âŒ Error processing '{sub_query}': {str(e)}",
                    self.researcher.websocket,
                )
            return ""

    async def _execute_mcp_research(self, retriever, query):
        """
        Execute MCP research using the new two-stage approach.
        
        Args:
            retriever: The MCP retriever class
            query: The search query
            
        Returns:
            list: MCP research results
        """
        retriever_name = retriever.__name__
        
        self.logger.info(f"Executing MCP research with {retriever_name} for query: {query}")
        
        try:
            # Instantiate the MCP retriever with proper parameters
            # Pass the researcher instance (self.researcher) which contains both cfg and mcp_configs
            retriever_instance = retriever(
                query=query, 
                headers=self.researcher.headers,
                query_domains=self.researcher.query_domains,
                websocket=self.researcher.websocket,
                researcher=self.researcher  # Pass the entire researcher instance
            )
            
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_retrieval_stage1",
                    f"ğŸ§  Stage 1: Selecting optimal MCP tools for: {query}",
                    self.researcher.websocket,
                )
            
            # Execute the two-stage MCP search
            results = retriever_instance.search(
                max_results=self.researcher.cfg.max_search_results_per_query
            )
            
            if results:
                result_count = len(results)
                self.logger.info(f"MCP research completed: {result_count} results from {retriever_name}")
                
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_research_complete",
                        f"ğŸ¯ MCP research completed: {result_count} intelligent results obtained",
                        self.researcher.websocket,
                    )
                
                return results
            else:
                self.logger.info(f"No results returned from MCP research with {retriever_name}")
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "mcp_no_results",
                        f"â„¹ï¸ No relevant information found via MCP for: {query}",
                        self.researcher.websocket,
                    )
                return []
                
        except Exception as e:
            self.logger.error(f"Error in MCP research with {retriever_name}: {str(e)}")
            if self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_research_error",
                    f"âš ï¸ MCP research error: {str(e)} - continuing with other sources",
                    self.researcher.websocket,
                )
            return []

    def _combine_mcp_and_web_context(self, mcp_context: list, web_context: str, sub_query: str) -> str:
        """
        Intelligently combine MCP and web research context.
        
        Args:
            mcp_context: List of MCP context entries
            web_context: Web research context string  
            sub_query: The sub-query being processed
            
        Returns:
            str: Combined context string
        """
        combined_parts = []
        
        # Add web context first if available
        if web_context and web_context.strip():
            combined_parts.append(web_context.strip())
            self.logger.debug(f"Added web context: {len(web_context)} chars")
        
        # Add MCP context with proper formatting
        if mcp_context:
            mcp_formatted = []
            
            for i, item in enumerate(mcp_context):
                content = item.get("content", "")
                url = item.get("url", "")
                title = item.get("title", f"MCP Result {i+1}")
                
                if content and content.strip():
                    # Create a well-formatted context entry
                    if url and url != f"mcp://llm_analysis":
                        citation = f"\n\n*Source: {title} ({url})*"
                    else:
                        citation = f"\n\n*Source: {title}*"
                    
                    formatted_content = f"{content.strip()}{citation}"
                    mcp_formatted.append(formatted_content)
            
            if mcp_formatted:
                # Join MCP results with clear separation
                mcp_section = "\n\n---\n\n".join(mcp_formatted)
                combined_parts.append(mcp_section)
                self.logger.debug(f"Added {len(mcp_context)} MCP context entries")
        
        # Combine all parts
        if combined_parts:
            final_context = "\n\n".join(combined_parts)
            self.logger.info(f"Combined context for '{sub_query}': {len(final_context)} total chars")
            return final_context
        else:
            self.logger.warning(f"No context to combine for sub-query: {sub_query}")
            return ""

    async def _process_sub_query_with_vectorstore(self, sub_query: str, filter: dict | None = None):
        """Takes in a sub query and gathers context from the user provided vector store

        Args:
            sub_query (str): The sub-query generated from the original query

        Returns:
            str: The context gathered from search
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_with_vectorstore_research",
                f"\nğŸ” Running research for '{sub_query}'...",
                self.researcher.websocket,
            )

        context = await self.researcher.context_manager.get_similar_content_by_query_with_vectorstore(sub_query, filter)

        return context

    async def _get_new_urls(self, url_set_input):
        """Gets the new urls from the given url set.
        Args: url_set_input (set[str]): The url set to get the new urls from
        Returns: list[str]: The new urls from the given url set
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.researcher.visited_urls:
                self.researcher.visited_urls.add(url)
                new_urls.append(url)
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "added_source_url",
                        f"âœ… Added source url to research: {url}\n",
                        self.researcher.websocket,
                        True,
                        url,
                    )

        return new_urls

    async def _search_relevant_source_urls(self, query, query_domains: list | None = None):
        new_search_urls = []
        if query_domains is None:
            query_domains = []

        # Iterate through the currently set retrievers
        # This allows the method to work when retrievers are temporarily modified
        for retriever_class in self.researcher.retrievers:
            # Skip MCP retrievers as they don't provide URLs for scraping
            if "mcpretriever" in retriever_class.__name__.lower():
                continue
                
            try:
                # Instantiate the retriever with the sub-query
                retriever = retriever_class(query, query_domains=query_domains)

                # Perform the search using the current retriever
                search_results = await asyncio.to_thread(
                    retriever.search, max_results=self.researcher.cfg.max_search_results_per_query
                )

                # Collect new URLs from search results
                search_urls = [url.get("href") for url in search_results if url.get("href")]
                new_search_urls.extend(search_urls)
            except Exception as e:
                self.logger.error(f"Error searching with {retriever_class.__name__}: {e}")

        # Get unique URLs
        new_search_urls = await self._get_new_urls(new_search_urls)
        random.shuffle(new_search_urls)

        return new_search_urls

    async def _scrape_data_by_urls(self, sub_query, query_domains: list | None = None):
        """
        Runs a sub-query across multiple retrievers and scrapes the resulting URLs.

        Args:
            sub_query (str): The sub-query to search for.

        Returns:
            list: A list of scraped content results.
        """
        if query_domains is None:
            query_domains = []

        new_search_urls = await self._search_relevant_source_urls(sub_query, query_domains)

        # Log the research process if verbose mode is on
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "researching",
                f"ğŸ¤” Researching for relevant information across multiple sources...\n",
                self.researcher.websocket,
            )

        # Scrape the new URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)

        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        return scraped_content

    async def _search(self, retriever, query):
        """
        Perform a search using the specified retriever.
        
        Args:
            retriever: The retriever class to use
            query: The search query
            
        Returns:
            list: Search results
        """
        retriever_name = retriever.__name__
        is_mcp_retriever = "mcpretriever" in retriever_name.lower()
        
        self.logger.info(f"Searching with {retriever_name} for query: {query}")
        
        try:
            # Instantiate the retriever
            retriever_instance = retriever(
                query=query, 
                headers=self.researcher.headers,
                query_domains=self.researcher.query_domains,
                websocket=self.researcher.websocket if is_mcp_retriever else None,
                researcher=self.researcher if is_mcp_retriever else None
            )
            
            # Log MCP server configurations if using MCP retriever
            if is_mcp_retriever and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_retrieval",
                    f"ğŸ”Œ Consulting MCP server(s) for information on: {query}",
                    self.researcher.websocket,
                )
            
            # Perform the search
            if hasattr(retriever_instance, 'search'):
                results = retriever_instance.search(
                    max_results=self.researcher.cfg.max_search_results_per_query
                )
                
                # Log result information
                if results:
                    result_count = len(results)
                    self.logger.info(f"Received {result_count} results from {retriever_name}")
                    
                    # Special logging for MCP retriever
                    if is_mcp_retriever:
                        if self.researcher.verbose:
                            await stream_output(
                                "logs",
                                "mcp_results",
                                f"âœ“ Retrieved {result_count} results from MCP server",
                                self.researcher.websocket,
                            )
                        
                        # Log result details
                        for i, result in enumerate(results[:3]):  # Log first 3 results
                            title = result.get("title", "No title")
                            url = result.get("href", "No URL")
                            content_length = len(result.get("body", "")) if result.get("body") else 0
                            self.logger.info(f"MCP result {i+1}: '{title}' from {url} ({content_length} chars)")
                            
                        if result_count > 3:
                            self.logger.info(f"... and {result_count - 3} more MCP results")
                else:
                    self.logger.info(f"No results returned from {retriever_name}")
                    if is_mcp_retriever and self.researcher.verbose:
                        await stream_output(
                            "logs",
                            "mcp_no_results",
                            f"â„¹ï¸ No relevant information found from MCP server for: {query}",
                            self.researcher.websocket,
                        )
                
                return results
            else:
                self.logger.error(f"Retriever {retriever_name} does not have a search method")
                return []
        except Exception as e:
            self.logger.error(f"Error searching with {retriever_name}: {str(e)}")
            if is_mcp_retriever and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "mcp_error",
                    f"âŒ Error retrieving information from MCP server: {str(e)}",
                    self.researcher.websocket,
                )
            return []
            
    async def _extract_content(self, results):
        """
        Extract content from search results using the browser manager.
        
        Args:
            results: Search results
            
        Returns:
            list: Extracted content
        """
        self.logger.info(f"Extracting content from {len(results)} search results")
        
        # Get the URLs from the search results
        urls = []
        for result in results:
            if isinstance(result, dict) and "href" in result:
                urls.append(result["href"])
        
        # Skip if no URLs found
        if not urls:
            return []
            
        # Make sure we don't visit URLs we've already visited
        new_urls = [url for url in urls if url not in self.researcher.visited_urls]
        
        # Return empty if no new URLs
        if not new_urls:
            return []
            
        # Scrape the content from the URLs
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_urls)
        
        # Add the URLs to visited_urls
        self.researcher.visited_urls.update(new_urls)
        
        return scraped_content
        
    async def _summarize_content(self, query, content):
        """
        Summarize the extracted content.
        
        Args:
            query: The search query
            content: The extracted content
            
        Returns:
            str: Summarized content
        """
        self.logger.info(f"Summarizing content for query: {query}")
        
        # Skip if no content
        if not content:
            return ""
            
        # Summarize the content using the context manager
        summary = await self.researcher.context_manager.get_similar_content_by_query(
            query, content
        )
        
        return summary
        
    async def _update_search_progress(self, current, total):
        """
        Update the search progress.
        
        Args:
            current: Current number of sub-queries processed
            total: Total number of sub-queries
        """
        if self.researcher.verbose and self.researcher.websocket:
            progress = int((current / total) * 100)
            await stream_output(
                "logs",
                "research_progress",
                f"ğŸ“Š Research Progress: {progress}%",
                self.researcher.websocket,
                True,
                {
                    "current": current,
                    "total": total,
                    "progress": progress
                }
            )




================================================
FILE: gpt_researcher/skills/writer.py
================================================
from typing import Dict, Optional
import json

from ..utils.llm import construct_subtopics
from ..actions import (
    stream_output,
    generate_report,
    generate_draft_section_titles,
    write_report_introduction,
    write_conclusion
)


class ReportGenerator:
    """Generates reports based on research data."""

    def __init__(self, researcher):
        self.researcher = researcher
        self.research_params = {
            "query": self.researcher.query,
            "agent_role_prompt": self.researcher.cfg.agent_role or self.researcher.role,
            "report_type": self.researcher.report_type,
            "report_source": self.researcher.report_source,
            "tone": self.researcher.tone,
            "websocket": self.researcher.websocket,
            "cfg": self.researcher.cfg,
            "headers": self.researcher.headers,
        }

    async def write_report(self, existing_headers: list = [], relevant_written_contents: list = [], ext_context=None, custom_prompt="") -> str:
        """
        Write a report based on existing headers and relevant contents.

        Args:
            existing_headers (list): List of existing headers.
            relevant_written_contents (list): List of relevant written contents.
            ext_context (Optional): External context, if any.
            custom_prompt (str): Custom prompt for the report.

        Returns:
            str: The generated report.
        """
        # send the selected images prior to writing report
        research_images = self.researcher.get_research_images()
        if research_images:
            await stream_output(
                "images",
                "selected_images",
                json.dumps(research_images),
                self.researcher.websocket,
                True,
                research_images
            )

        context = ext_context or self.researcher.context
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "writing_report",
                f"âœï¸ Writing report for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        report_params = self.research_params.copy()
        report_params["context"] = context
        report_params["custom_prompt"] = custom_prompt

        if self.researcher.report_type == "subtopic_report":
            report_params.update({
                "main_topic": self.researcher.parent_query,
                "existing_headers": existing_headers,
                "relevant_written_contents": relevant_written_contents,
                "cost_callback": self.researcher.add_costs,
            })
        else:
            report_params["cost_callback"] = self.researcher.add_costs

        report = await generate_report(**report_params, **self.researcher.kwargs)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "report_written",
                f"ğŸ“ Report written for '{self.researcher.query}'",
                self.researcher.websocket,
            )

        return report

    async def write_report_conclusion(self, report_content: str) -> str:
        """
        Write the conclusion for the report.

        Args:
            report_content (str): The content of the report.

        Returns:
            str: The generated conclusion.
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "writing_conclusion",
                f"âœï¸ Writing conclusion for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        conclusion = await write_conclusion(
            query=self.researcher.query,
            context=report_content,
            config=self.researcher.cfg,
            agent_role_prompt=self.researcher.cfg.agent_role or self.researcher.role,
            cost_callback=self.researcher.add_costs,
            websocket=self.researcher.websocket,
            prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "conclusion_written",
                f"ğŸ“ Conclusion written for '{self.researcher.query}'",
                self.researcher.websocket,
            )

        return conclusion

    async def write_introduction(self):
        """Write the introduction section of the report."""
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "writing_introduction",
                f"âœï¸ Writing introduction for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        introduction = await write_report_introduction(
            query=self.researcher.query,
            context=self.researcher.context,
            agent_role_prompt=self.researcher.cfg.agent_role or self.researcher.role,
            config=self.researcher.cfg,
            websocket=self.researcher.websocket,
            cost_callback=self.researcher.add_costs,
            prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "introduction_written",
                f"ğŸ“ Introduction written for '{self.researcher.query}'",
                self.researcher.websocket,
            )

        return introduction

    async def get_subtopics(self):
        """Retrieve subtopics for the research."""
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "generating_subtopics",
                f"ğŸŒ³ Generating subtopics for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        subtopics = await construct_subtopics(
            task=self.researcher.query,
            data=self.researcher.context,
            config=self.researcher.cfg,
            subtopics=self.researcher.subtopics,
            prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subtopics_generated",
                f"ğŸ“Š Subtopics generated for '{self.researcher.query}'",
                self.researcher.websocket,
            )

        return subtopics

    async def get_draft_section_titles(self, current_subtopic: str):
        """Generate draft section titles for the report."""
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "generating_draft_sections",
                f"ğŸ“‘ Generating draft section titles for '{self.researcher.query}'...",
                self.researcher.websocket,
            )

        draft_section_titles = await generate_draft_section_titles(
            query=self.researcher.query,
            current_subtopic=current_subtopic,
            context=self.researcher.context,
            role=self.researcher.cfg.agent_role or self.researcher.role,
            websocket=self.researcher.websocket,
            config=self.researcher.cfg,
            cost_callback=self.researcher.add_costs,
            prompt_family=self.researcher.prompt_family,
            **self.researcher.kwargs
        )

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "draft_sections_generated",
                f"ğŸ—‚ï¸ Draft section titles generated for '{self.researcher.query}'",
                self.researcher.websocket,
            )

        return draft_section_titles



================================================
FILE: gpt_researcher/utils/__init__.py
================================================



================================================
FILE: gpt_researcher/utils/costs.py
================================================
import tiktoken

# Per OpenAI Pricing Page: https://openai.com/api/pricing/
ENCODING_MODEL = "o200k_base"
INPUT_COST_PER_TOKEN = 0.000005
OUTPUT_COST_PER_TOKEN = 0.000015
IMAGE_INFERENCE_COST = 0.003825
EMBEDDING_COST = 0.02 / 1000000 # Assumes new ada-3-small


# Cost estimation is via OpenAI libraries and models. May vary for other models
def estimate_llm_cost(input_content: str, output_content: str) -> float:
    encoding = tiktoken.get_encoding(ENCODING_MODEL)
    input_tokens = encoding.encode(input_content)
    output_tokens = encoding.encode(output_content)
    input_costs = len(input_tokens) * INPUT_COST_PER_TOKEN
    output_costs = len(output_tokens) * OUTPUT_COST_PER_TOKEN
    return input_costs + output_costs


def estimate_embedding_cost(model, docs):
    encoding = tiktoken.encoding_for_model(model)
    total_tokens = sum(len(encoding.encode(str(doc))) for doc in docs)
    return total_tokens * EMBEDDING_COST




================================================
FILE: gpt_researcher/utils/enum.py
================================================
from enum import Enum


class ReportType(Enum):
    ResearchReport = "research_report"
    ResourceReport = "resource_report"
    OutlineReport = "outline_report"
    CustomReport = "custom_report"
    DetailedReport = "detailed_report"
    SubtopicReport = "subtopic_report"
    DeepResearch = "deep"


class ReportSource(Enum):
    Web = "web"
    Local = "local"
    Azure = "azure"
    LangChainDocuments = "langchain_documents"
    LangChainVectorStore = "langchain_vectorstore"
    Static = "static"
    Hybrid = "hybrid"


class Tone(Enum):
    Objective = "Objective (impartial and unbiased presentation of facts and findings)"
    Formal = "Formal (adheres to academic standards with sophisticated language and structure)"
    Analytical = (
        "Analytical (critical evaluation and detailed examination of data and theories)"
    )
    Persuasive = (
        "Persuasive (convincing the audience of a particular viewpoint or argument)"
    )
    Informative = (
        "Informative (providing clear and comprehensive information on a topic)"
    )
    Explanatory = "Explanatory (clarifying complex concepts and processes)"
    Descriptive = (
        "Descriptive (detailed depiction of phenomena, experiments, or case studies)"
    )
    Critical = "Critical (judging the validity and relevance of the research and its conclusions)"
    Comparative = "Comparative (juxtaposing different theories, data, or methods to highlight differences and similarities)"
    Speculative = "Speculative (exploring hypotheses and potential implications or future research directions)"
    Reflective = "Reflective (considering the research process and personal insights or experiences)"
    Narrative = (
        "Narrative (telling a story to illustrate research findings or methodologies)"
    )
    Humorous = "Humorous (light-hearted and engaging, usually to make the content more relatable)"
    Optimistic = "Optimistic (highlighting positive findings and potential benefits)"
    Pessimistic = (
        "Pessimistic (focusing on limitations, challenges, or negative outcomes)"
    )
    Simple = "Simple (written for young readers, using basic vocabulary and clear explanations)"
    Casual = "Casual (conversational and relaxed style for easy, everyday reading)"


class PromptFamily(Enum):
    """Supported prompt families by name"""
    Default = "default"
    Granite = "granite"
    Granite3 = "granite3"
    Granite31 = "granite3.1"
    Granite32 = "granite3.2"
    Granite33 = "granite3.3"



================================================
FILE: gpt_researcher/utils/llm.py
================================================
# libraries
from __future__ import annotations

import logging
from typing import Any

from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate

from gpt_researcher.llm_provider.generic.base import NO_SUPPORT_TEMPERATURE_MODELS, SUPPORT_REASONING_EFFORT_MODELS, ReasoningEfforts

from ..prompts import PromptFamily
from .costs import estimate_llm_cost
from .validators import Subtopics
import os


def get_llm(llm_provider, **kwargs):
    from gpt_researcher.llm_provider import GenericLLMProvider
    return GenericLLMProvider.from_provider(llm_provider, **kwargs)


async def create_chat_completion(
        messages: list[dict[str, str]],
        model: str | None = None,
        temperature: float | None = 0.4,
        max_tokens: int | None = 4000,
        llm_provider: str | None = None,
        stream: bool = False,
        websocket: Any | None = None,
        llm_kwargs: dict[str, Any] | None = None,
        cost_callback: callable = None,
        reasoning_effort: str | None = ReasoningEfforts.Medium.value,
        **kwargs
) -> str:
    """Create a chat completion using the OpenAI API
    Args:
        messages (list[dict[str, str]]): The messages to send to the chat completion.
        model (str, optional): The model to use. Defaults to None.
        temperature (float, optional): The temperature to use. Defaults to 0.4.
        max_tokens (int, optional): The max tokens to use. Defaults to 4000.
        llm_provider (str, optional): The LLM Provider to use.
        stream (bool): Whether to stream the response. Defaults to False.
        webocket (WebSocket): The websocket used in the currect request,
        llm_kwargs (dict[str, Any], optional): Additional LLM keyword arguments. Defaults to None.
        cost_callback: Callback function for updating cost.
        reasoning_effort (str, optional): Reasoning effort for OpenAI's reasoning models. Defaults to 'low'.
        **kwargs: Additional keyword arguments.
    Returns:
        str: The response from the chat completion.
    """
    # validate input
    if model is None:
        raise ValueError("Model cannot be None")
    if max_tokens is not None and max_tokens > 32001:
        raise ValueError(
            f"Max tokens cannot be more than 32,000, but got {max_tokens}")

    # Get the provider from supported providers
    provider_kwargs = {'model': model}

    if llm_kwargs:
        provider_kwargs.update(llm_kwargs)

    if model in SUPPORT_REASONING_EFFORT_MODELS:
        provider_kwargs['reasoning_effort'] = reasoning_effort

    if model not in NO_SUPPORT_TEMPERATURE_MODELS:
        provider_kwargs['temperature'] = temperature
        provider_kwargs['max_tokens'] = max_tokens
    else:
        provider_kwargs['temperature'] = None
        provider_kwargs['max_tokens'] = None

    if llm_provider == "openai":
        base_url = os.environ.get("OPENAI_BASE_URL", None)
        if base_url:
            provider_kwargs['openai_api_base'] = base_url

    provider = get_llm(llm_provider, **provider_kwargs)
    response = ""
    # create response
    for _ in range(10):  # maximum of 10 attempts
        response = await provider.get_chat_response(
            messages, stream, websocket, **kwargs
        )

        if cost_callback:
            llm_costs = estimate_llm_cost(str(messages), response)
            cost_callback(llm_costs)

        return response

    logging.error(f"Failed to get response from {llm_provider} API")
    raise RuntimeError(f"Failed to get response from {llm_provider} API")


async def construct_subtopics(
    task: str,
    data: str,
    config,
    subtopics: list = [],
    prompt_family: type[PromptFamily] | PromptFamily = PromptFamily,
    **kwargs
) -> list:
    """
    Construct subtopics based on the given task and data.

    Args:
        task (str): The main task or topic.
        data (str): Additional data for context.
        config: Configuration settings.
        subtopics (list, optional): Existing subtopics. Defaults to [].
        prompt_family (PromptFamily): Family of prompts
        **kwargs: Additional keyword arguments.

    Returns:
        list: A list of constructed subtopics.
    """
    try:
        parser = PydanticOutputParser(pydantic_object=Subtopics)

        prompt = PromptTemplate(
            template=prompt_family.generate_subtopics_prompt(),
            input_variables=["task", "data", "subtopics", "max_subtopics"],
            partial_variables={
                "format_instructions": parser.get_format_instructions()},
        )

        provider_kwargs = {'model': config.smart_llm_model}

        if config.llm_kwargs:
            provider_kwargs.update(config.llm_kwargs)

        if config.smart_llm_model in SUPPORT_REASONING_EFFORT_MODELS:
            provider_kwargs['reasoning_effort'] = ReasoningEfforts.High.value
        else:
            provider_kwargs['temperature'] = config.temperature
            provider_kwargs['max_tokens'] = config.smart_token_limit

        provider = get_llm(config.smart_llm_provider, **provider_kwargs)

        model = provider.llm

        chain = prompt | model | parser

        output = await chain.ainvoke({
            "task": task,
            "data": data,
            "subtopics": subtopics,
            "max_subtopics": config.max_subtopics
        }, **kwargs)

        return output

    except Exception as e:
        print("Exception in parsing subtopics : ", e)
        logging.getLogger(__name__).error("Exception in parsing subtopics : \n {e}")
        return subtopics



================================================
FILE: gpt_researcher/utils/logger.py
================================================
import logging
import sys
from copy import copy
from typing import Literal

import click

TRACE_LOG_LEVEL = 5


def get_formatted_logger():
    """Return a formatted logger."""
    logger = logging.getLogger("scraper")
    # Set the logging level
    logger.setLevel(logging.INFO)

    # Check if the logger already has handlers to avoid duplicates
    if not logger.handlers:
        # Create a handler
        handler = logging.StreamHandler()

        # Create a formatter using DefaultFormatter
        formatter = DefaultFormatter(
            "%(levelprefix)s [%(asctime)s] %(message)s",
            datefmt="%H:%M:%S"
        )

        # Set the formatter for the handler
        handler.setFormatter(formatter)

        # Add the handler to the logger
        logger.addHandler(handler)

    # Disable propagation to prevent duplicate logging from parent loggers
    logger.propagate = False

    return logger


class ColourizedFormatter(logging.Formatter):
    """
    A custom log formatter class that:

    * Outputs the LOG_LEVEL with an appropriate color.
    * If a log call includes an `extras={"color_message": ...}` it will be used
      for formatting the output, instead of the plain text message.
    """

    level_name_colors = {
        TRACE_LOG_LEVEL: lambda level_name: click.style(str(level_name), fg="blue"),
        logging.DEBUG: lambda level_name: click.style(str(level_name), fg="cyan"),
        logging.INFO: lambda level_name: click.style(str(level_name), fg="green"),
        logging.WARNING: lambda level_name: click.style(str(level_name), fg="yellow"),
        logging.ERROR: lambda level_name: click.style(str(level_name), fg="red"),
        logging.CRITICAL: lambda level_name: click.style(str(level_name), fg="bright_red"),
    }

    def __init__(
        self,
        fmt: str | None = None,
        datefmt: str | None = None,
        style: Literal["%", "{", "$"] = "%",
        use_colors: bool | None = None,
    ):
        if use_colors in (True, False):
            self.use_colors = use_colors
        else:
            self.use_colors = sys.stdout.isatty()
        super().__init__(fmt=fmt, datefmt=datefmt, style=style)

    def color_level_name(self, level_name: str, level_no: int) -> str:
        def default(level_name: str) -> str:
            return str(level_name)  # pragma: no cover

        func = self.level_name_colors.get(level_no, default)
        return func(level_name)

    def should_use_colors(self) -> bool:
        return True  # pragma: no cover

    def formatMessage(self, record: logging.LogRecord) -> str:
        recordcopy = copy(record)
        levelname = recordcopy.levelname
        seperator = " " * (8 - len(recordcopy.levelname))
        if self.use_colors:
            levelname = self.color_level_name(levelname, recordcopy.levelno)
            if "color_message" in recordcopy.__dict__:
                recordcopy.msg = recordcopy.__dict__["color_message"]
                recordcopy.__dict__["message"] = recordcopy.getMessage()
        recordcopy.__dict__["levelprefix"] = levelname + ":" + seperator
        return super().formatMessage(recordcopy)


class DefaultFormatter(ColourizedFormatter):
    def should_use_colors(self) -> bool:
        return sys.stderr.isatty()  # pragma: no cover



================================================
FILE: gpt_researcher/utils/logging_config.py
================================================
import logging
import json
import os
from datetime import datetime
from pathlib import Path

class JSONResearchHandler:
    def __init__(self, json_file):
        self.json_file = json_file
        self.research_data = {
            "timestamp": datetime.now().isoformat(),
            "events": [],
            "content": {
                "query": "",
                "sources": [],
                "context": [],
                "report": "",
                "costs": 0.0
            }
        }

    def log_event(self, event_type: str, data: dict):
        self.research_data["events"].append({
            "timestamp": datetime.now().isoformat(),
            "type": event_type,
            "data": data
        })
        self._save_json()

    def update_content(self, key: str, value):
        self.research_data["content"][key] = value
        self._save_json()

    def _save_json(self):
        with open(self.json_file, 'w') as f:
            json.dump(self.research_data, f, indent=2)

def setup_research_logging():
    # Create logs directory if it doesn't exist
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    
    # Generate timestamp for log files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create log file paths
    log_file = logs_dir / f"research_{timestamp}.log"
    json_file = logs_dir / f"research_{timestamp}.json"
    
    # Configure file handler for research logs
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    
    # Get research logger and configure it
    research_logger = logging.getLogger('research')
    research_logger.setLevel(logging.INFO)
    
    # Remove any existing handlers to avoid duplicates
    research_logger.handlers.clear()
    
    # Add file handler
    research_logger.addHandler(file_handler)
    
    # Add stream handler for console output
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    research_logger.addHandler(console_handler)
    
    # Prevent propagation to root logger to avoid duplicate logs
    research_logger.propagate = False
    
    # Create JSON handler
    json_handler = JSONResearchHandler(json_file)
    
    return str(log_file), str(json_file), research_logger, json_handler

def get_research_logger():
    return logging.getLogger('research')

def get_json_handler():
    return getattr(logging.getLogger('research'), 'json_handler', None)



================================================
FILE: gpt_researcher/utils/tools.py
================================================
"""
Tool-enabled LLM utilities for GPT Researcher

This module provides provider-agnostic tool calling functionality using LangChain's
unified interface. It allows any LLM provider that supports function calling to use
tools seamlessly.
"""

import asyncio
import logging
from typing import Any, Dict, List, Tuple, Callable, Optional
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import tool

from .llm import create_chat_completion

logger = logging.getLogger(__name__)


async def create_chat_completion_with_tools(
    messages: List[Dict[str, str]],
    tools: List[Callable],
    model: str | None = None,
    temperature: float | None = 0.4,
    max_tokens: int | None = 4000,
    llm_provider: str | None = None,
    llm_kwargs: Dict[str, Any] | None = None,
    cost_callback: Callable = None,
    websocket: Any | None = None,
    **kwargs
) -> Tuple[str, List[Dict[str, Any]]]:
    """
    Create a chat completion with tool calling support across all LLM providers.
    
    This function uses LangChain's bind_tools() to enable function calling in a 
    provider-agnostic way. The AI decides autonomously when and how to use tools.
    
    Args:
        messages: List of chat messages with role and content
        tools: List of LangChain tool functions (decorated with @tool)
        model: The model to use (from config)
        temperature: Temperature for generation
        max_tokens: Maximum tokens to generate
        llm_provider: LLM provider name (from config)
        llm_kwargs: Additional LLM keyword arguments
        cost_callback: Callback function for cost tracking
        websocket: Optional websocket for streaming
        **kwargs: Additional arguments
        
    Returns:
        Tuple of (response_content, tool_calls_metadata)
        
    Raises:
        Exception: If tool-enabled completion fails, falls back to simple completion
    """
    try:
        from ..llm_provider.generic.base import GenericLLMProvider
        
        # Create LLM provider using the config
        provider_kwargs = {
            'model': model,
            **(llm_kwargs or {})
        }
        
        llm_provider_instance = GenericLLMProvider.from_provider(
            llm_provider, 
            **provider_kwargs
        )
        
        # Convert messages to LangChain format
        lc_messages = []
        for msg in messages:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                lc_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                lc_messages.append(AIMessage(content=msg["content"]))
        
        # Bind tools to the LLM - this works across all LangChain providers that support function calling
        llm_with_tools = llm_provider_instance.llm.bind_tools(tools)
        
        # Invoke the LLM with tools - this will handle the full conversation flow
        logger.info(f"Invoking LLM with {len(tools)} available tools")
        
        # For tool calling, we need to handle the full conversation including tool responses
        from langchain_core.messages import ToolMessage
        
        # First call to LLM
        response = await llm_with_tools.ainvoke(lc_messages)
        
        # Process tool calls if any were made
        tool_calls_metadata = []
        if hasattr(response, 'tool_calls') and response.tool_calls:
            logger.info(f"LLM made {len(response.tool_calls)} tool calls")
            
            # Add the assistant's response with tool calls to the conversation
            lc_messages.append(response)
            
            # Execute each tool call and add results to conversation
            for tool_call in response.tool_calls:
                tool_name = tool_call.get('name', 'unknown')
                tool_args = tool_call.get('args', {})
                tool_id = tool_call.get('id', '')
                
                logger.info(f"Tool called: {tool_name}")
                if tool_args:
                    args_str = ", ".join([f"{k}={v}" for k, v in tool_args.items()])
                    logger.debug(f"Tool arguments: {args_str}")
                
                # Find and execute the tool
                tool_result = "Tool execution failed"
                for tool in tools:
                    if tool.name == tool_name:
                        try:
                            if hasattr(tool, 'ainvoke'):
                                tool_result = await tool.ainvoke(tool_args)
                            elif hasattr(tool, 'invoke'):
                                tool_result = tool.invoke(tool_args)
                            else:
                                tool_result = await tool(**tool_args) if asyncio.iscoroutinefunction(tool) else tool(**tool_args)
                            break
                        except Exception as e:
                            logger.error(f"Error executing tool {tool_name}: {e}")
                            tool_result = f"Tool error: {str(e)}"
                
                # Add tool result to conversation
                tool_message = ToolMessage(content=str(tool_result), tool_call_id=tool_id)
                lc_messages.append(tool_message)
                
                # Add to metadata
                tool_calls_metadata.append({
                    "tool": tool_name,
                    "args": tool_args,
                    "call_id": tool_id,
                    "result": str(tool_result)[:200] + "..." if len(str(tool_result)) > 200 else str(tool_result)
                })
            
            # Get final response from LLM after tool execution
            logger.info("Getting final response from LLM after tool execution")
            final_response = await llm_with_tools.ainvoke(lc_messages)
            
            # Track costs if callback provided
            if cost_callback:
                from .costs import estimate_llm_cost
                # Calculate costs for both calls
                llm_costs = estimate_llm_cost(str(lc_messages), final_response.content or "")
                cost_callback(llm_costs)
            
            return final_response.content, tool_calls_metadata
        
        else:
            # No tool calls, return regular response
            if cost_callback:
                from .costs import estimate_llm_cost
                llm_costs = estimate_llm_cost(str(messages), response.content or "")
                cost_callback(llm_costs)
            
            return response.content, []
        
    except Exception as e:
        logger.error(f"Error in tool-enabled chat completion: {str(e)}")
        logger.info("Falling back to simple chat completion without tools")
        
        # Fallback to simple chat completion without tools
        response = await create_chat_completion(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            llm_provider=llm_provider,
            llm_kwargs=llm_kwargs,
            cost_callback=cost_callback,
            websocket=websocket,
            **kwargs
        )
        return response, []


def create_search_tool(search_function: Callable[[str], Dict]) -> Callable:
    """
    Create a standardized search tool for use with tool-enabled chat completions.
    
    Args:
        search_function: Function that takes a query string and returns search results
        
    Returns:
        LangChain tool function decorated with @tool
    """
    @tool
    def search_tool(query: str) -> str:
        """Search for current events or online information when you need new knowledge that doesn't exist in the current context"""
        try:
            results = search_function(query)
            if results and 'results' in results:
                search_content = f"Search results for '{query}':\n\n"
                for result in results['results'][:5]:
                    search_content += f"Title: {result.get('title', '')}\n"
                    search_content += f"Content: {result.get('content', '')[:300]}...\n"
                    search_content += f"URL: {result.get('url', '')}\n\n"
                return search_content
            else:
                return f"No search results found for: {query}"
        except Exception as e:
            logger.error(f"Search tool error: {str(e)}")
            return f"Search error: {str(e)}"
    
    return search_tool


def create_custom_tool(
    name: str,
    description: str, 
    function: Callable,
    parameter_schema: Optional[Dict] = None
) -> Callable:
    """
    Create a custom tool for use with tool-enabled chat completions.
    
    Args:
        name: Name of the tool
        description: Description of what the tool does
        function: The actual function to execute
        parameter_schema: Optional schema for function parameters
        
    Returns:
        LangChain tool function decorated with @tool
    """
    @tool
    def custom_tool(*args, **kwargs) -> str:
        try:
            result = function(*args, **kwargs)
            return str(result) if result is not None else "Tool executed successfully"
        except Exception as e:
            logger.error(f"Custom tool '{name}' error: {str(e)}")
            return f"Tool error: {str(e)}"
    
    # Set tool metadata
    custom_tool.name = name
    custom_tool.description = description
    
    return custom_tool


# Utility function for common tool patterns
def get_available_providers_with_tools() -> List[str]:
    """
    Get list of LLM providers that support tool calling.
    
    Returns:
        List of provider names that support function calling
    """
    # These are the providers known to support function calling in LangChain
    return [
        "openai",
        "anthropic", 
        "google_genai",
        "azure_openai",
        "fireworks",
        "groq",
        # Note: This list may expand as more providers add function calling support
    ]


def supports_tools(provider: str) -> bool:
    """
    Check if a given provider supports tool calling.
    
    Args:
        provider: LLM provider name
        
    Returns:
        True if provider supports tools, False otherwise
    """
    return provider in get_available_providers_with_tools()



================================================
FILE: gpt_researcher/utils/validators.py
================================================
from typing import List

from pydantic import BaseModel, Field

class Subtopic(BaseModel):
    task: str = Field(description="Task name", min_length=1)

class Subtopics(BaseModel):
    subtopics: List[Subtopic] = []



================================================
FILE: gpt_researcher/utils/workers.py
================================================
import asyncio
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager


class WorkerPool:
    def __init__(self, max_workers: int):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.semaphore = asyncio.Semaphore(max_workers)

    @asynccontextmanager
    async def throttle(self):
        async with self.semaphore:
            yield



================================================
FILE: gpt_researcher/vector_store/__init__.py
================================================
from .vector_store import VectorStoreWrapper

__all__ = ['VectorStoreWrapper']


================================================
FILE: gpt_researcher/vector_store/vector_store.py
================================================
"""
Wrapper for langchain vector store
"""
from typing import List, Dict

from langchain.docstore.document import Document
from langchain.vectorstores import VectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter

class VectorStoreWrapper:
    """
    A Wrapper for LangchainVectorStore to handle GPT-Researcher Document Type
    """
    def __init__(self, vector_store : VectorStore):
        self.vector_store = vector_store

    def load(self, documents):
        """
        Load the documents into vector_store
        Translate to langchain doc type, split to chunks then load
        """
        langchain_documents = self._create_langchain_documents(documents)
        splitted_documents = self._split_documents(langchain_documents)
        self.vector_store.add_documents(splitted_documents)
    
    def _create_langchain_documents(self, data: List[Dict[str, str]]) -> List[Document]:
        """Convert GPT Researcher Document to Langchain Document"""
        return [Document(page_content=item["raw_content"], metadata={"source": item["url"]}) for item in data]

    def _split_documents(self, documents: List[Document], chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Document]:
        """
        Split documents into smaller chunks
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        return text_splitter.split_documents(documents)

    async def asimilarity_search(self, query, k, filter):
        """Return query by vector store"""
        results = await self.vector_store.asimilarity_search(query=query, k=k, filter=filter)
        return results



================================================
FILE: mcp-server/README.md
================================================
# ğŸ” GPT Researcher MCP Server

> **Note:** This content has been moved to a dedicated repository: [https://github.com/assafelovic/gptr-mcp](https://github.com/assafelovic/gptr-mcp)

## Overview

The GPT Researcher MCP Server enables AI assistants like Claude to conduct comprehensive web research and generate detailed reports via the Machine Conversation Protocol (MCP).

## Why GPT Researcher MCP?

While LLM apps can access web search tools with MCP, **GPT Researcher MCP delivers deep research results.** Standard search tools return raw results requiring manual filtering, often containing irrelevant sources and wasting context window space.

GPT Researcher autonomously explores and validates numerous sources, focusing only on relevant, trusted and up-to-date information. Though slightly slower than standard search (~30 seconds wait), it delivers:

* âœ¨ Higher quality information
* ğŸ“Š Optimized context usage
* ğŸ” Comprehensive results
* ğŸ§  Better reasoning for LLMs

## Features

### Resources
* `research_resource`: Get web resources related to a given task via research.

### Primary Tools
* `deep_research`: Performs deep web research on a topic, finding reliable and relevant information
* `quick_search`: Performs a fast web search optimized for speed over quality 
* `write_report`: Generate a report based on research results
* `get_research_sources`: Get the sources used in the research
* `get_research_context`: Get the full context of the research

## Installation

For detailed installation and usage instructions, please visit the [official repository](https://github.com/assafelovic/gptr-mcp).

Quick start:

1. Clone the new repository:
   ```bash
   git clone https://github.com/assafelovic/gptr-mcp.git
   cd gptr-mcp
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Create a `.env` file with your API keys:
   ```
   OPENAI_API_KEY=your_openai_api_key
   TAVILY_API_KEY=your_tavily_api_key
   ```

4. Run the server:
   ```bash
   python server.py
   ```

For Docker deployment, Claude Desktop integration, example usage, and troubleshooting, please refer to the [full documentation](https://github.com/assafelovic/gptr-mcp).

## Support & Contact

* Website: [gptr.dev](https://gptr.dev)
* Email: assaf.elovic@gmail.com
* GitHub: [assafelovic/gptr-mcp](https://github.com/assafelovic/gptr-mcp) :-)


================================================
FILE: multi_agents/README.md
================================================
# LangGraph x GPT Researcher
[LangGraph](https://python.langchain.com/docs/langgraph) is a library for building stateful, multi-actor applications with LLMs. 
This example uses Langgraph to automate the process of an in depth research on any given topic.

## Use case
By using Langgraph, the research process can be significantly improved in depth and quality by leveraging multiple agents with specialized skills. 
Inspired by the recent [STORM](https://arxiv.org/abs/2402.14207) paper, this example showcases how a team of AI agents can work together to conduct research on a given topic, from planning to publication.

An average run generates a 5-6 page research report in multiple formats such as PDF, Docx and Markdown.

Please note: Multi-agents are utilizing the same configuration of models like GPT-Researcher does. However, only the SMART_LLM is used for the time being. Please refer to the [LLM config pages](https://docs.gptr.dev/docs/gpt-researcher/llms/llms).

## The Multi Agent Team
The research team is made up of 8 agents:
- **Human** - The human in the loop that oversees the process and provides feedback to the agents.
- **Chief Editor** - Oversees the research process and manages the team. This is the "master" agent that coordinates the other agents using Langgraph.
- **Researcher** (gpt-researcher) - A specialized autonomous agent that conducts in depth research on a given topic.
- **Editor** - Responsible for planning the research outline and structure.
- **Reviewer** - Validates the correctness of the research results given a set of criteria.
- **Revisor** - Revises the research results based on the feedback from the reviewer.
- **Writer** - Responsible for compiling and writing the final report.
- **Publisher** - Responsible for publishing the final report in various formats.

## How it works
Generally, the process is based on the following stages: 
1. Planning stage
2. Data collection and analysis
3. Review and revision
4. Writing and submission
5. Publication

### Architecture
<div align="center">
<img align="center" height="600" src="https://github.com/user-attachments/assets/ef561295-05f4-40a8-a57d-8178be687b18">
</div>
<br clear="all"/>

### Steps
More specifically (as seen in the architecture diagram) the process is as follows:
- Browser (gpt-researcher) - Browses the internet for initial research based on the given research task.
- Editor - Plans the report outline and structure based on the initial research.
- For each outline topic (in parallel):
  - Researcher (gpt-researcher) - Runs an in depth research on the subtopics and writes a draft.
  - Reviewer - Validates the correctness of the draft given a set of criteria and provides feedback.
  - Revisor - Revises the draft until it is satisfactory based on the reviewer feedback.
- Writer - Compiles and writes the final report including an introduction, conclusion and references section from the given research findings.
- Publisher - Publishes the final report to multi formats such as PDF, Docx, Markdown, etc.

## How to run
1. Install required packages found in this root folder including `langgraph`:
    ```bash
    pip install -r requirements.txt
    ```
3. Update env variables, see the [GPT-Researcher docs](https://docs.gptr.dev/docs/gpt-researcher/llms/llms) for more details.

2. Run the application:
    ```bash
    python main.py
    ```

## Usage
To change the research query and customize the report, edit the `task.json` file in the main directory.
#### Task.json contains the following fields:
- `query` - The research query or task.
- `model` - The OpenAI LLM to use for the agents.
- `max_sections` - The maximum number of sections in the report. Each section is a subtopic of the research query.
- `include_human_feedback` - If true, the user can provide feedback to the agents. If false, the agents will work autonomously.
- `publish_formats` - The formats to publish the report in. The reports will be written in the `output` directory.
- `source` - The location from which to conduct the research. Options: `web` or `local`. For local, please add `DOC_PATH` env var.
- `follow_guidelines` - If true, the research report will follow the guidelines below. It will take longer to complete. If false, the report will be generated faster but may not follow the guidelines.
- `guidelines` - A list of guidelines that the report must follow.
- `verbose` - If true, the application will print detailed logs to the console.

#### For example:
```json
{
  "query": "Is AI in a hype cycle?",
  "model": "gpt-4o",
  "max_sections": 3, 
  "publish_formats": { 
    "markdown": true,
    "pdf": true,
    "docx": true
  },
  "include_human_feedback": false,
  "source": "web",
  "follow_guidelines": true,
  "guidelines": [
    "The report MUST fully answer the original question",
    "The report MUST be written in apa format",
    "The report MUST be written in english"
  ],
  "verbose": true
}
```

## To Deploy

```shell
pip install langgraph-cli
langgraph up
```

From there, see documentation [here](https://github.com/langchain-ai/langgraph-example) on how to use the streaming and async endpoints, as well as the playground.



================================================
FILE: multi_agents/__init__.py
================================================
# multi_agents/__init__.py

from .agents import (
    ResearchAgent,
    WriterAgent,
    PublisherAgent,
    ReviserAgent,
    ReviewerAgent,
    EditorAgent,
    ChiefEditorAgent
)
from .memory import (
    DraftState,
    ResearchState
)

__all__ = [
    "ResearchAgent",
    "WriterAgent",
    "PublisherAgent",
    "ReviserAgent",
    "ReviewerAgent",
    "EditorAgent",
    "ChiefEditorAgent",
    "DraftState",
    "ResearchState"
]


================================================
FILE: multi_agents/agent.py
================================================
from multi_agents.agents import ChiefEditorAgent

chief_editor = ChiefEditorAgent({
  "query": "Is AI in a hype cycle?",
  "max_sections": 3,
  "follow_guidelines": False,
  "model": "gpt-4o",
  "guidelines": [
    "The report MUST be written in APA format",
    "Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section",
    "The report MUST be written in spanish"
  ],
  "verbose": False
}, websocket=None, stream_output=None)
graph = chief_editor.init_research_team()
graph = graph.compile()


================================================
FILE: multi_agents/langgraph.json
================================================
{
  "python_version": "3.11",
  "dependencies": [
    "."
  ],
  "graphs": {
    "agent": "./agent.py:graph"
  },
  "env": ".env"
}


================================================
FILE: multi_agents/main.py
================================================
from dotenv import load_dotenv
import sys
import os
import uuid
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from multi_agents.agents import ChiefEditorAgent
import asyncio
import json
from gpt_researcher.utils.enum import Tone

# Run with LangSmith if API key is set
if os.environ.get("LANGCHAIN_API_KEY"):
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
load_dotenv()

def open_task():
    # Get the directory of the current script
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # Construct the absolute path to task.json
    task_json_path = os.path.join(current_dir, 'task.json')
    
    with open(task_json_path, 'r') as f:
        task = json.load(f)

    if not task:
        raise Exception("No task found. Please ensure a valid task.json file is present in the multi_agents directory and contains the necessary task information.")

    # Override model with STRATEGIC_LLM if defined in environment
    strategic_llm = os.environ.get("STRATEGIC_LLM")
    if strategic_llm and ":" in strategic_llm:
        # Extract the model name (part after the first colon)
        model_name = strategic_llm.split(":", 1)[1]
        task["model"] = model_name
    elif strategic_llm:
        task["model"] = model_name

    return task

async def run_research_task(query, websocket=None, stream_output=None, tone=Tone.Objective, headers=None):
    task = open_task()
    task["query"] = query

    chief_editor = ChiefEditorAgent(task, websocket, stream_output, tone, headers)
    research_report = await chief_editor.run_research_task()

    if websocket and stream_output:
        await stream_output("logs", "research_report", research_report, websocket)

    return research_report

async def main():
    task = open_task()

    chief_editor = ChiefEditorAgent(task)
    research_report = await chief_editor.run_research_task(task_id=uuid.uuid4())

    return research_report

if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: multi_agents/package.json
================================================
{
  "name": "simple_js_test",
  "version": "1.0.0",
  "description": "",
  "main": "server.js",
  "type": "module",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "",
  "license": "ISC",
  "dependencies": {
    "@langchain/langgraph-sdk": "^0.0.1-rc.13"
  }
}



================================================
FILE: multi_agents/requirements.txt
================================================
json5
langgraph
json5
langgraph-cli
loguru
python-dotenv
weasyprint



================================================
FILE: multi_agents/task.json
================================================
{
  "query": "Is AI in a hype cycle?",
  "max_sections": 3,
  "publish_formats": {
    "markdown": true,
    "pdf": true,
    "docx": true
  },
  "include_human_feedback": false,
  "follow_guidelines": false,
  "model": "gpt-4o",
  "guidelines": [
    "The report MUST be written in APA format",
    "Each sub section MUST include supporting sources using hyperlinks. If none exist, erase the sub section or rewrite it to be a part of the previous section",
    "The report MUST be written in spanish"
  ],
  "verbose": true
}


================================================
FILE: multi_agents/agents/__init__.py
================================================
from .researcher import ResearchAgent
from .writer import WriterAgent
from .publisher import PublisherAgent
from .reviser import ReviserAgent
from .reviewer import ReviewerAgent
from .editor import EditorAgent
from .human import HumanAgent

# Below import should remain last since it imports all of the above
from .orchestrator import ChiefEditorAgent

__all__ = [
    "ChiefEditorAgent",
    "ResearchAgent",
    "WriterAgent",
    "EditorAgent",
    "PublisherAgent",
    "ReviserAgent",
    "ReviewerAgent",
    "HumanAgent"
]



================================================
FILE: multi_agents/agents/editor.py
================================================
from datetime import datetime
import asyncio
from typing import Dict, List, Optional

from langgraph.graph import StateGraph, END

from .utils.views import print_agent_output
from .utils.llms import call_model
from ..memory.draft import DraftState
from . import ResearchAgent, ReviewerAgent, ReviserAgent


class EditorAgent:
    """Agent responsible for editing and managing code."""

    def __init__(self, websocket=None, stream_output=None, tone=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.tone = tone
        self.headers = headers or {}

    async def plan_research(self, research_state: Dict[str, any]) -> Dict[str, any]:
        """
        Plan the research outline based on initial research and task parameters.

        :param research_state: Dictionary containing research state information
        :return: Dictionary with title, date, and planned sections
        """
        initial_research = research_state.get("initial_research")
        task = research_state.get("task")
        include_human_feedback = task.get("include_human_feedback")
        human_feedback = research_state.get("human_feedback")
        max_sections = task.get("max_sections")

        prompt = self._create_planning_prompt(
            initial_research, include_human_feedback, human_feedback, max_sections)

        print_agent_output(
            "Planning an outline layout based on initial research...", agent="EDITOR")
        plan = await call_model(
            prompt=prompt,
            model=task.get("model"),
            response_format="json",
        )

        return {
            "title": plan.get("title"),
            "date": plan.get("date"),
            "sections": plan.get("sections"),
        }

    async def run_parallel_research(self, research_state: Dict[str, any]) -> Dict[str, List[str]]:
        """
        Execute parallel research tasks for each section.

        :param research_state: Dictionary containing research state information
        :return: Dictionary with research results
        """
        agents = self._initialize_agents()
        workflow = self._create_workflow()
        chain = workflow.compile()

        queries = research_state.get("sections")
        title = research_state.get("title")

        self._log_parallel_research(queries)

        final_drafts = [
            chain.ainvoke(self._create_task_input(
                research_state, query, title))
            for query in queries
        ]
        research_results = [
            result["draft"] for result in await asyncio.gather(*final_drafts)
        ]

        return {"research_data": research_results}

    def _create_planning_prompt(self, initial_research: str, include_human_feedback: bool,
                                human_feedback: Optional[str], max_sections: int) -> List[Dict[str, str]]:
        """Create the prompt for research planning."""
        return [
            {
                "role": "system",
                "content": "You are a research editor. Your goal is to oversee the research project "
                           "from inception to completion. Your main task is to plan the article section "
                           "layout based on an initial research summary.\n ",
            },
            {
                "role": "user",
                "content": self._format_planning_instructions(initial_research, include_human_feedback,
                                                              human_feedback, max_sections),
            },
        ]

    def _format_planning_instructions(self, initial_research: str, include_human_feedback: bool,
                                      human_feedback: Optional[str], max_sections: int) -> str:
        """Format the instructions for research planning."""
        today = datetime.now().strftime('%d/%m/%Y')
        feedback_instruction = (
            f"Human feedback: {human_feedback}. You must plan the sections based on the human feedback."
            if include_human_feedback and human_feedback and human_feedback != 'no'
            else ''
        )

        return f"""Today's date is {today}
                   Research summary report: '{initial_research}'
                   {feedback_instruction}
                   \nYour task is to generate an outline of sections headers for the research project
                   based on the research summary report above.
                   You must generate a maximum of {max_sections} section headers.
                   You must focus ONLY on related research topics for subheaders and do NOT include introduction, conclusion and references.
                   You must return nothing but a JSON with the fields 'title' (str) and 
                   'sections' (maximum {max_sections} section headers) with the following structure:
                   '{{title: string research title, date: today's date, 
                   sections: ['section header 1', 'section header 2', 'section header 3' ...]}}'."""

    def _initialize_agents(self) -> Dict[str, any]:
        """Initialize the research, reviewer, and reviser skills."""
        return {
            "research": ResearchAgent(self.websocket, self.stream_output, self.tone, self.headers),
            "reviewer": ReviewerAgent(self.websocket, self.stream_output, self.headers),
            "reviser": ReviserAgent(self.websocket, self.stream_output, self.headers),
        }

    def _create_workflow(self) -> StateGraph:
        """Create the workflow for the research process."""
        agents = self._initialize_agents()
        workflow = StateGraph(DraftState)

        workflow.add_node("researcher", agents["research"].run_depth_research)
        workflow.add_node("reviewer", agents["reviewer"].run)
        workflow.add_node("reviser", agents["reviser"].run)

        workflow.set_entry_point("researcher")
        workflow.add_edge("researcher", "reviewer")
        workflow.add_edge("reviser", "reviewer")
        workflow.add_conditional_edges(
            "reviewer",
            lambda draft: "accept" if draft["review"] is None else "revise",
            {"accept": END, "revise": "reviser"},
        )

        return workflow

    def _log_parallel_research(self, queries: List[str]) -> None:
        """Log the start of parallel research tasks."""
        if self.websocket and self.stream_output:
            asyncio.create_task(self.stream_output(
                "logs",
                "parallel_research",
                f"Running parallel research for the following queries: {queries}",
                self.websocket,
            ))
        else:
            print_agent_output(
                f"Running the following research tasks in parallel: {queries}...",
                agent="EDITOR",
            )

    def _create_task_input(self, research_state: Dict[str, any], query: str, title: str) -> Dict[str, any]:
        """Create the input for a single research task."""
        return {
            "task": research_state.get("task"),
            "topic": query,
            "title": title,
            "headers": self.headers,
        }



================================================
FILE: multi_agents/agents/human.py
================================================
import json


class HumanAgent:
    def __init__(self, websocket=None, stream_output=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers or {}

    async def review_plan(self, research_state: dict):
        print(f"HumanAgent websocket: {self.websocket}")
        print(f"HumanAgent stream_output: {self.stream_output}")
        task = research_state.get("task")
        layout = research_state.get("sections")

        user_feedback = None

        if task.get("include_human_feedback"):
            # Stream response to the user if a websocket is provided (such as from web app)
            if self.websocket and self.stream_output:
                try:
                    await self.stream_output(
                        "human_feedback",
                        "request",
                        f"Any feedback on this plan of topics to research? {layout}? If not, please reply with 'no'.",
                        self.websocket,
                    )
                    # because websocket is wrapped inside a CustomLogsHandler in websocket_manager
                    response = await self.websocket.websocket.receive_text()
                    print(f"Received response: {response}", flush=True)
                    response_data = json.loads(response)
                    if response_data.get("type") == "human_feedback":
                        user_feedback = response_data.get("content")
                    else:
                        print(
                            f"Unexpected response type: {response_data.get('type')}",
                            flush=True,
                        )
                except Exception as e:
                    print(f"Error receiving human feedback: {e}", flush=True)
            # Otherwise, prompt the user for feedback in the console
            else:
                user_feedback = input(
                    f"Any feedback on this plan? {layout}? If not, please reply with 'no'.\n>> "
                )

        if user_feedback and "no" in user_feedback.strip().lower():
            user_feedback = None

        print(f"User feedback before return: {user_feedback}")

        return {"human_feedback": user_feedback}



================================================
FILE: multi_agents/agents/orchestrator.py
================================================
import os
import time
import datetime
from langgraph.graph import StateGraph, END
# from langgraph.checkpoint.memory import MemorySaver
from .utils.views import print_agent_output
from ..memory.research import ResearchState
from .utils.utils import sanitize_filename

# Import agent classes
from . import \
    WriterAgent, \
    EditorAgent, \
    PublisherAgent, \
    ResearchAgent, \
    HumanAgent


class ChiefEditorAgent:
    """Agent responsible for managing and coordinating editing tasks."""

    def __init__(self, task: dict, websocket=None, stream_output=None, tone=None, headers=None):
        self.task = task
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers or {}
        self.tone = tone
        self.task_id = self._generate_task_id()
        self.output_dir = self._create_output_directory()

    def _generate_task_id(self):
        # Currently time based, but can be any unique identifier
        return int(time.time())

    def _create_output_directory(self):
        output_dir = "./outputs/" + \
            sanitize_filename(
                f"run_{self.task_id}_{self.task.get('query')[0:40]}")

        os.makedirs(output_dir, exist_ok=True)
        return output_dir

    def _initialize_agents(self):
        return {
            "writer": WriterAgent(self.websocket, self.stream_output, self.headers),
            "editor": EditorAgent(self.websocket, self.stream_output, self.tone, self.headers),
            "research": ResearchAgent(self.websocket, self.stream_output, self.tone, self.headers),
            "publisher": PublisherAgent(self.output_dir, self.websocket, self.stream_output, self.headers),
            "human": HumanAgent(self.websocket, self.stream_output, self.headers)
        }

    def _create_workflow(self, agents):
        workflow = StateGraph(ResearchState)

        # Add nodes for each agent
        workflow.add_node("browser", agents["research"].run_initial_research)
        workflow.add_node("planner", agents["editor"].plan_research)
        workflow.add_node("researcher", agents["editor"].run_parallel_research)
        workflow.add_node("writer", agents["writer"].run)
        workflow.add_node("publisher", agents["publisher"].run)
        workflow.add_node("human", agents["human"].review_plan)

        # Add edges
        self._add_workflow_edges(workflow)

        return workflow

    def _add_workflow_edges(self, workflow):
        workflow.add_edge('browser', 'planner')
        workflow.add_edge('planner', 'human')
        workflow.add_edge('researcher', 'writer')
        workflow.add_edge('writer', 'publisher')
        workflow.set_entry_point("browser")
        workflow.add_edge('publisher', END)

        # Add human in the loop
        workflow.add_conditional_edges(
            'human',
            lambda review: "accept" if review['human_feedback'] is None else "revise",
            {"accept": "researcher", "revise": "planner"}
        )

    def init_research_team(self):
        """Initialize and create a workflow for the research team."""
        agents = self._initialize_agents()
        return self._create_workflow(agents)

    async def _log_research_start(self):
        message = f"Starting the research process for query '{self.task.get('query')}'..."
        if self.websocket and self.stream_output:
            await self.stream_output("logs", "starting_research", message, self.websocket)
        else:
            print_agent_output(message, "MASTER")

    async def run_research_task(self, task_id=None):
        """
        Run a research task with the initialized research team.

        Args:
            task_id (optional): The ID of the task to run.

        Returns:
            The result of the research task.
        """
        research_team = self.init_research_team()
        chain = research_team.compile()

        await self._log_research_start()

        config = {
            "configurable": {
                "thread_id": task_id,
                "thread_ts": datetime.datetime.utcnow()
            }
        }

        result = await chain.ainvoke({"task": self.task}, config=config)
        return result



================================================
FILE: multi_agents/agents/publisher.py
================================================
from .utils.file_formats import \
    write_md_to_pdf, \
    write_md_to_word, \
    write_text_to_md

from .utils.views import print_agent_output


class PublisherAgent:
    def __init__(self, output_dir: str, websocket=None, stream_output=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.output_dir = output_dir.strip()
        self.headers = headers or {}
        
    async def publish_research_report(self, research_state: dict, publish_formats: dict):
        layout = self.generate_layout(research_state)
        await self.write_report_by_formats(layout, publish_formats)

        return layout

    def generate_layout(self, research_state: dict):
        sections = []
        for subheader in research_state.get("research_data", []):
            if isinstance(subheader, dict):
                # Handle dictionary case
                for key, value in subheader.items():
                    sections.append(f"{value}")
            else:
                # Handle string case
                sections.append(f"{subheader}")
        
        sections_text = '\n\n'.join(sections)
        references = '\n'.join(f"{reference}" for reference in research_state.get("sources", []))
        headers = research_state.get("headers", {})
        layout = f"""# {headers.get('title')}
#### {headers.get("date")}: {research_state.get('date')}

## {headers.get("introduction")}
{research_state.get('introduction')}

## {headers.get("table_of_contents")}
{research_state.get('table_of_contents')}

{sections_text}

## {headers.get("conclusion")}
{research_state.get('conclusion')}

## {headers.get("references")}
{references}
"""
        return layout

    async def write_report_by_formats(self, layout:str, publish_formats: dict):
        if publish_formats.get("pdf"):
            await write_md_to_pdf(layout, self.output_dir)
        if publish_formats.get("docx"):
            await write_md_to_word(layout, self.output_dir)
        if publish_formats.get("markdown"):
            await write_text_to_md(layout, self.output_dir)

    async def run(self, research_state: dict):
        task = research_state.get("task")
        publish_formats = task.get("publish_formats")
        if self.websocket and self.stream_output:
            await self.stream_output("logs", "publishing", f"Publishing final research report based on retrieved data...", self.websocket)
        else:
            print_agent_output(output="Publishing final research report based on retrieved data...", agent="PUBLISHER")
        final_research_report = await self.publish_research_report(research_state, publish_formats)
        return {"report": final_research_report}



================================================
FILE: multi_agents/agents/researcher.py
================================================
from gpt_researcher import GPTResearcher
from colorama import Fore, Style
from .utils.views import print_agent_output


class ResearchAgent:
    def __init__(self, websocket=None, stream_output=None, tone=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers or {}
        self.tone = tone

    async def research(self, query: str, research_report: str = "research_report",
                       parent_query: str = "", verbose=True, source="web", tone=None, headers=None):
        # Initialize the researcher
        researcher = GPTResearcher(query=query, report_type=research_report, parent_query=parent_query,
                                   verbose=verbose, report_source=source, tone=tone, websocket=self.websocket, headers=self.headers)
        # Conduct research on the given query
        await researcher.conduct_research()
        # Write the report
        report = await researcher.write_report()

        return report

    async def run_subtopic_research(self, parent_query: str, subtopic: str, verbose: bool = True, source="web", headers=None):
        try:
            report = await self.research(parent_query=parent_query, query=subtopic,
                                         research_report="subtopic_report", verbose=verbose, source=source, tone=self.tone, headers=None)
        except Exception as e:
            print(f"{Fore.RED}Error in researching topic {subtopic}: {e}{Style.RESET_ALL}")
            report = None
        return {subtopic: report}

    async def run_initial_research(self, research_state: dict):
        task = research_state.get("task")
        query = task.get("query")
        source = task.get("source", "web")

        if self.websocket and self.stream_output:
            await self.stream_output("logs", "initial_research", f"Running initial research on the following query: {query}", self.websocket)
        else:
            print_agent_output(f"Running initial research on the following query: {query}", agent="RESEARCHER")
        return {"task": task, "initial_research": await self.research(query=query, verbose=task.get("verbose"),
                                                                      source=source, tone=self.tone, headers=self.headers)}

    async def run_depth_research(self, draft_state: dict):
        task = draft_state.get("task")
        topic = draft_state.get("topic")
        parent_query = task.get("query")
        source = task.get("source", "web")
        verbose = task.get("verbose")
        if self.websocket and self.stream_output:
            await self.stream_output("logs", "depth_research", f"Running in depth research on the following report topic: {topic}", self.websocket)
        else:
            print_agent_output(f"Running in depth research on the following report topic: {topic}", agent="RESEARCHER")
        research_draft = await self.run_subtopic_research(parent_query=parent_query, subtopic=topic,
                                                          verbose=verbose, source=source, headers=self.headers)
        return {"draft": research_draft}


================================================
FILE: multi_agents/agents/reviewer.py
================================================
from .utils.views import print_agent_output
from .utils.llms import call_model

TEMPLATE = """You are an expert research article reviewer. \
Your goal is to review research drafts and provide feedback to the reviser only based on specific guidelines. \
"""


class ReviewerAgent:
    def __init__(self, websocket=None, stream_output=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers or {}

    async def review_draft(self, draft_state: dict):
        """
        Review a draft article
        :param draft_state:
        :return:
        """
        task = draft_state.get("task")
        guidelines = "- ".join(guideline for guideline in task.get("guidelines"))
        revision_notes = draft_state.get("revision_notes")

        revise_prompt = f"""The reviser has already revised the draft based on your previous review notes with the following feedback:
{revision_notes}\n
Please provide additional feedback ONLY if critical since the reviser has already made changes based on your previous feedback.
If you think the article is sufficient or that non critical revisions are required, please aim to return None.
"""

        review_prompt = f"""You have been tasked with reviewing the draft which was written by a non-expert based on specific guidelines.
Please accept the draft if it is good enough to publish, or send it for revision, along with your notes to guide the revision.
If not all of the guideline criteria are met, you should send appropriate revision notes.
If the draft meets all the guidelines, please return None.
{revise_prompt if revision_notes else ""}

Guidelines: {guidelines}\nDraft: {draft_state.get("draft")}\n
"""
        prompt = [
            {"role": "system", "content": TEMPLATE},
            {"role": "user", "content": review_prompt},
        ]

        response = await call_model(prompt, model=task.get("model"))

        if task.get("verbose"):
            if self.websocket and self.stream_output:
                await self.stream_output(
                    "logs",
                    "review_feedback",
                    f"Review feedback is: {response}...",
                    self.websocket,
                )
            else:
                print_agent_output(
                    f"Review feedback is: {response}...", agent="REVIEWER"
                )

        if "None" in response:
            return None
        return response

    async def run(self, draft_state: dict):
        task = draft_state.get("task")
        guidelines = task.get("guidelines")
        to_follow_guidelines = task.get("follow_guidelines")
        review = None
        if to_follow_guidelines:
            print_agent_output(f"Reviewing draft...", agent="REVIEWER")

            if task.get("verbose"):
                print_agent_output(
                    f"Following guidelines {guidelines}...", agent="REVIEWER"
                )

            review = await self.review_draft(draft_state)
        else:
            print_agent_output(f"Ignoring guidelines...", agent="REVIEWER")
        return {"review": review}



================================================
FILE: multi_agents/agents/reviser.py
================================================
from .utils.views import print_agent_output
from .utils.llms import call_model
import json

sample_revision_notes = """
{
  "draft": { 
    draft title: The revised draft that you are submitting for review 
  },
  "revision_notes": Your message to the reviewer about the changes you made to the draft based on their feedback
}
"""


class ReviserAgent:
    def __init__(self, websocket=None, stream_output=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers or {}

    async def revise_draft(self, draft_state: dict):
        """
        Review a draft article
        :param draft_state:
        :return:
        """
        review = draft_state.get("review")
        task = draft_state.get("task")
        draft_report = draft_state.get("draft")
        prompt = [
            {
                "role": "system",
                "content": "You are an expert writer. Your goal is to revise drafts based on reviewer notes.",
            },
            {
                "role": "user",
                "content": f"""Draft:\n{draft_report}" + "Reviewer's notes:\n{review}\n\n
You have been tasked by your reviewer with revising the following draft, which was written by a non-expert.
If you decide to follow the reviewer's notes, please write a new draft and make sure to address all of the points they raised.
Please keep all other aspects of the draft the same.
You MUST return nothing but a JSON in the following format:
{sample_revision_notes}
""",
            },
        ]

        response = await call_model(
            prompt,
            model=task.get("model"),
            response_format="json",
        )
        return response

    async def run(self, draft_state: dict):
        print_agent_output(f"Rewriting draft based on feedback...", agent="REVISOR")
        revision = await self.revise_draft(draft_state)

        if draft_state.get("task").get("verbose"):
            if self.websocket and self.stream_output:
                await self.stream_output(
                    "logs",
                    "revision_notes",
                    f"Revision notes: {revision.get('revision_notes')}",
                    self.websocket,
                )
            else:
                print_agent_output(
                    f"Revision notes: {revision.get('revision_notes')}", agent="REVISOR"
                )

        return {
            "draft": revision.get("draft"),
            "revision_notes": revision.get("revision_notes"),
        }



================================================
FILE: multi_agents/agents/writer.py
================================================
from datetime import datetime
import json5 as json
from .utils.views import print_agent_output
from .utils.llms import call_model

sample_json = """
{
  "table_of_contents": A table of contents in markdown syntax (using '-') based on the research headers and subheaders,
  "introduction": An indepth introduction to the topic in markdown syntax and hyperlink references to relevant sources,
  "conclusion": A conclusion to the entire research based on all research data in markdown syntax and hyperlink references to relevant sources,
  "sources": A list with strings of all used source links in the entire research data in markdown syntax and apa citation format. For example: ['-  Title, year, Author [source url](source)', ...]
}
"""


class WriterAgent:
    def __init__(self, websocket=None, stream_output=None, headers=None):
        self.websocket = websocket
        self.stream_output = stream_output
        self.headers = headers

    def get_headers(self, research_state: dict):
        return {
            "title": research_state.get("title"),
            "date": "Date",
            "introduction": "Introduction",
            "table_of_contents": "Table of Contents",
            "conclusion": "Conclusion",
            "references": "References",
        }

    async def write_sections(self, research_state: dict):
        query = research_state.get("title")
        data = research_state.get("research_data")
        task = research_state.get("task")
        follow_guidelines = task.get("follow_guidelines")
        guidelines = task.get("guidelines")

        prompt = [
            {
                "role": "system",
                "content": "You are a research writer. Your sole purpose is to write a well-written "
                "research reports about a "
                "topic based on research findings and information.\n ",
            },
            {
                "role": "user",
                "content": f"Today's date is {datetime.now().strftime('%d/%m/%Y')}\n."
                f"Query or Topic: {query}\n"
                f"Research data: {str(data)}\n"
                f"Your task is to write an in depth, well written and detailed "
                f"introduction and conclusion to the research report based on the provided research data. "
                f"Do not include headers in the results.\n"
                f"You MUST include any relevant sources to the introduction and conclusion as markdown hyperlinks -"
                f"For example: 'This is a sample text. ([url website](url))'\n\n"
                f"{f'You must follow the guidelines provided: {guidelines}' if follow_guidelines else ''}\n"
                f"You MUST return nothing but a JSON in the following format (without json markdown):\n"
                f"{sample_json}\n\n",
            },
        ]

        response = await call_model(
            prompt,
            task.get("model"),
            response_format="json",
        )
        return response

    async def revise_headers(self, task: dict, headers: dict):
        prompt = [
            {
                "role": "system",
                "content": """You are a research writer. 
Your sole purpose is to revise the headers data based on the given guidelines.""",
            },
            {
                "role": "user",
                "content": f"""Your task is to revise the given headers JSON based on the guidelines given.
You are to follow the guidelines but the values should be in simple strings, ignoring all markdown syntax.
You must return nothing but a JSON in the same format as given in headers data.
Guidelines: {task.get("guidelines")}\n
Headers Data: {headers}\n
""",
            },
        ]

        response = await call_model(
            prompt,
            task.get("model"),
            response_format="json",
        )
        return {"headers": response}

    async def run(self, research_state: dict):
        if self.websocket and self.stream_output:
            await self.stream_output(
                "logs",
                "writing_report",
                f"Writing final research report based on research data...",
                self.websocket,
            )
        else:
            print_agent_output(
                f"Writing final research report based on research data...",
                agent="WRITER",
            )

        research_layout_content = await self.write_sections(research_state)

        if research_state.get("task").get("verbose"):
            if self.websocket and self.stream_output:
                research_layout_content_str = json.dumps(
                    research_layout_content, indent=2
                )
                await self.stream_output(
                    "logs",
                    "research_layout_content",
                    research_layout_content_str,
                    self.websocket,
                )
            else:
                print_agent_output(research_layout_content, agent="WRITER")

        headers = self.get_headers(research_state)
        if research_state.get("task").get("follow_guidelines"):
            if self.websocket and self.stream_output:
                await self.stream_output(
                    "logs",
                    "rewriting_layout",
                    "Rewriting layout based on guidelines...",
                    self.websocket,
                )
            else:
                print_agent_output(
                    "Rewriting layout based on guidelines...", agent="WRITER"
                )
            headers = await self.revise_headers(
                task=research_state.get("task"), headers=headers
            )
            headers = headers.get("headers")

        return {**research_layout_content, "headers": headers}



================================================
FILE: multi_agents/agents/utils/__init__.py
================================================



================================================
FILE: multi_agents/agents/utils/file_formats.py
================================================
import aiofiles
import urllib
import uuid
import mistune
import os

async def write_to_file(filename: str, text: str) -> None:
    """Asynchronously write text to a file in UTF-8 encoding.

    Args:
        filename (str): The filename to write to.
        text (str): The text to write.
    """
    # Convert text to UTF-8, replacing any problematic characters
    text_utf8 = text.encode('utf-8', errors='replace').decode('utf-8')

    async with aiofiles.open(filename, "w", encoding='utf-8') as file:
        await file.write(text_utf8)


async def write_text_to_md(text: str, path: str) -> str:
    """Writes text to a Markdown file and returns the file path.

    Args:
        text (str): Text to write to the Markdown file.

    Returns:
        str: The file path of the generated Markdown file.
    """
    task = uuid.uuid4().hex
    file_path = f"{path}/{task}.md"
    await write_to_file(file_path, text)
    print(f"Report written to {file_path}")
    return file_path


async def write_md_to_pdf(text: str, path: str) -> str:
    """Converts Markdown text to a PDF file and returns the file path.

    Args:
        text (str): Markdown text to convert.

    Returns:
        str: The encoded file path of the generated PDF.
    """
    task = uuid.uuid4().hex
    file_path = f"{path}/{task}.pdf"

    try:
        # Get the directory of the current file
        current_dir = os.path.dirname(os.path.abspath(__file__))
        css_path = os.path.join(current_dir, "pdf_styles.css")
        
        # Moved imports to inner function to avoid known import errors with gobject-2.0
        from md2pdf.core import md2pdf
        md2pdf(file_path,
               md_content=text,
               css_file_path=css_path,
               base_url=None)
        print(f"Report written to {file_path}")
    except Exception as e:
        print(f"Error in converting Markdown to PDF: {e}")
        return ""

    encoded_file_path = urllib.parse.quote(file_path)
    return encoded_file_path


async def write_md_to_word(text: str, path: str) -> str:
    """Converts Markdown text to a DOCX file and returns the file path.

    Args:
        text (str): Markdown text to convert.

    Returns:
        str: The encoded file path of the generated DOCX.
    """
    task = uuid.uuid4().hex
    file_path = f"{path}/{task}.docx"

    try:
        from htmldocx import HtmlToDocx
        from docx import Document
        # Convert report markdown to HTML
        html = mistune.html(text)
        # Create a document object
        doc = Document()
        # Convert the html generated from the report to document format
        HtmlToDocx().add_html_to_document(html, doc)

        # Saving the docx document to file_path
        doc.save(file_path)

        print(f"Report written to {file_path}")

        encoded_file_path = urllib.parse.quote(f"{file_path}.docx")
        return encoded_file_path

    except Exception as e:
        print(f"Error in converting Markdown to DOCX: {e}")
        return ""



================================================
FILE: multi_agents/agents/utils/llms.py
================================================
import json_repair
from langchain_community.adapters.openai import convert_openai_messages
from langchain_core.utils.json import parse_json_markdown
from loguru import logger

from gpt_researcher.config.config import Config
from gpt_researcher.utils.llm import create_chat_completion


async def call_model(
    prompt: list,
    model: str,
    response_format: str | None = None,
):

    cfg = Config()
    lc_messages = convert_openai_messages(prompt)

    try:
        response = await create_chat_completion(
            model=model,
            messages=lc_messages,
            temperature=0,
            llm_provider=cfg.smart_llm_provider,
            llm_kwargs=cfg.llm_kwargs,
            # cost_callback=cost_callback,
        )

        if response_format == "json":
            return parse_json_markdown(response, parser=json_repair.loads)

        return response

    except Exception as e:
        print("âš ï¸ Error in calling model")
        logger.error(f"Error in calling model: {e}")



================================================
FILE: multi_agents/agents/utils/pdf_styles.css
================================================
body {
    font-family: 'Libre Baskerville', serif;
    font-size: 12pt; /* standard size for academic papers */
    line-height: 1.6; /* for readability */
    color: #333; /* softer on the eyes than black */
    background-color: #fff; /* white background */
    margin: 0;
    padding: 0;
}

h1, h2, h3, h4, h5, h6 {
    font-family: 'Libre Baskerville', serif;
    color: #000; /* darker than the body text */
    margin-top: 1em; /* space above headers */
}

h1 {
    font-size: 2em; /* make h1 twice the size of the body text */
}

h2 {
    font-size: 1.5em;
}

/* Add some space between paragraphs */
p {
    margin-bottom: 1em;
}

/* Style for blockquotes, often used in academic papers */
blockquote {
    font-style: italic;
    margin: 1em 0;
    padding: 1em;
    background-color: #f9f9f9; /* a light grey background */
}

/* You might want to style tables, figures, etc. too */
table {
    border-collapse: collapse;
    width: 100%;
}

table, th, td {
    border: 1px solid #ddd;
    text-align: left;
    padding: 8px;
}

th {
    background-color: #f2f2f2;
    color: black;
}


================================================
FILE: multi_agents/agents/utils/utils.py
================================================
import re

def sanitize_filename(filename: str) -> str:
    """
    Sanitize a given filename by replacing characters that are invalid 
    in Windows file paths with an underscore ('_').

    This function ensures that the filename is compatible with all 
    operating systems by removing or replacing characters that are 
    not allowed in Windows file paths. Specifically, it replaces 
    the following characters: < > : " / \\ | ? *

    Parameters:
    filename (str): The original filename to be sanitized.

    Returns:
    str: The sanitized filename with invalid characters replaced by an underscore.
    
    Examples:
    >>> sanitize_filename('invalid:file/name*example?.txt')
    'invalid_file_name_example_.txt'
    
    >>> sanitize_filename('valid_filename.txt')
    'valid_filename.txt'
    """
    return re.sub(r'[<>:"/\\|?*]', '_', filename)



================================================
FILE: multi_agents/agents/utils/views.py
================================================
from colorama import Fore, Style
from enum import Enum


class AgentColor(Enum):
    RESEARCHER = Fore.LIGHTBLUE_EX
    EDITOR = Fore.YELLOW
    WRITER = Fore.LIGHTGREEN_EX
    PUBLISHER = Fore.MAGENTA
    REVIEWER = Fore.CYAN
    REVISOR = Fore.LIGHTWHITE_EX
    MASTER = Fore.LIGHTYELLOW_EX


def print_agent_output(output:str, agent: str="RESEARCHER"):
    print(f"{AgentColor[agent].value}{agent}: {output}{Style.RESET_ALL}")


================================================
FILE: multi_agents/memory/__init__.py
================================================
from .draft import DraftState
from .research import ResearchState

__all__ = [
    "DraftState",
    "ResearchState"
]


================================================
FILE: multi_agents/memory/draft.py
================================================
from typing import TypedDict, List, Annotated
import operator


class DraftState(TypedDict):
    task: dict
    topic: str
    draft: dict
    review: str
    revision_notes: str


================================================
FILE: multi_agents/memory/research.py
================================================
from typing import TypedDict, List, Annotated
import operator


class ResearchState(TypedDict):
    task: dict
    initial_research: str
    sections: List[str]
    research_data: List[dict]
    human_feedback: str
    # Report layout
    title: str
    headers: dict
    date: str
    table_of_contents: str
    introduction: str
    conclusion: str
    sources: List[str]
    report: str





================================================
FILE: tests/__init__.py
================================================



================================================
FILE: tests/documents-report-source.py
================================================
import os
import asyncio
import pytest
# Ensure this path is correct
from gpt_researcher import GPTResearcher
from dotenv import load_dotenv
load_dotenv()

# Define the report types to test
report_types = [
    "research_report",
    "custom_report",
    "subtopic_report",
    "summary_report",
    "detailed_report",
    "quick_report"
]

# Define a common query and sources for testing
query = "What can you tell me about myself based on my documents?"

# Define the output directory
output_dir = "./outputs"


@pytest.mark.asyncio
@pytest.mark.parametrize("report_type", report_types)
async def test_gpt_researcher(report_type):
    # Ensure the output directory exists
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Create an instance of GPTResearcher with report_source set to "documents"
    researcher = GPTResearcher(
        query=query, report_type=report_type, report_source="documents")

    # Conduct research and write the report
    await researcher.conduct_research()
    report = await researcher.write_report()

    # Define the expected output filenames
    pdf_filename = os.path.join(output_dir, f"{report_type}.pdf")
    docx_filename = os.path.join(output_dir, f"{report_type}.docx")

    # Check if the PDF and DOCX files are created
    # assert os.path.exists(pdf_filename), f"PDF file not found for report type: {report_type}"
    # assert os.path.exists(docx_filename), f"DOCX file not found for report type: {report_type}"

    # Clean up the generated files (optional)
    # os.remove(pdf_filename)
    # os.remove(docx_filename)

if __name__ == "__main__":
    pytest.main()



================================================
FILE: tests/gptr-logs-handler.py
================================================
import logging
from typing import List, Dict, Any
import asyncio
from gpt_researcher import GPTResearcher
from backend.server.server_utils import CustomLogsHandler  # Update import

async def run() -> None:
    """Run the research process and generate a report."""
    query = "What happened in the latest burning man floods?"
    report_type = "research_report"
    report_source = "online"
    tone = "informative"
    config_path = None

    custom_logs_handler = CustomLogsHandler(None, query)  # Pass query parameter

    researcher = GPTResearcher(
        query=query,
        report_type=report_type,
        report_source=report_source,
        tone=tone,
        config_path=config_path,
        websocket=custom_logs_handler
    )

    await researcher.conduct_research()  # Conduct the research
    report = await researcher.write_report()  # Write the research report
    logging.info("Report generated successfully.")  # Log report generation

    return report

# Run the asynchronous function using asyncio
if __name__ == "__main__":
    asyncio.run(run())



================================================
FILE: tests/report-types.py
================================================
import os
import asyncio
import pytest
from unittest.mock import AsyncMock
from gpt_researcher.agent import GPTResearcher
from backend.server.server_utils import CustomLogsHandler
from typing import List, Dict, Any

# Define the report types to test
report_types = ["research_report", "subtopic_report"]

# Define a common query and sources for testing
query = "what is gpt-researcher"


@pytest.mark.asyncio
@pytest.mark.parametrize("report_type", report_types)
async def test_gpt_researcher(report_type):
    mock_websocket = AsyncMock()
    custom_logs_handler = CustomLogsHandler(mock_websocket, query)
    # Create an instance of GPTResearcher
    researcher = GPTResearcher(
        query=query,
        query_domains=["github.com"],
        report_type=report_type,
        websocket=custom_logs_handler,
    )

    # Conduct research and write the report
    await researcher.conduct_research()
    report = await researcher.write_report()

    print(researcher.visited_urls)
    print(report)

    # Check if the report contains part of the query
    assert "gpt-researcher" in report

    # test if at least one url starts with "github.com" as it was limited to this domain
    matching_urls = [
        url for url in researcher.visited_urls if url.startswith("https://github.com")
    ]
    assert len(matching_urls) > 0


if __name__ == "__main__":
    pytest.main()



================================================
FILE: tests/research_test.py
================================================
"""
Hi! The following test cases are for the new parameter `complement_source_urls` and fix on the functional error with `source_urls` in GPTResearcher class.

The source_urls parameter was resetting each time in conduct_research function causing gptr to forget the given links. Now, that has been fixed and a new parameter is introduced.
This parameter named will `complement_source_urls` allow GPTR to research on sources other than the provided sources via source_urls if set to True. 
Default is False, i.e., no additional research will be conducted on newer sources.
"""

## Notes:
## Please uncomment the test case to run and comment the rest.
## Thanks!



#### Test case 1 (original test case as control from https://docs.gptr.dev/docs/gpt-researcher/tailored-research)

from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct
import asyncio
import logging
from typing import List, Dict, Any
from backend.server.server_utils import CustomLogsHandler  # Update import

async def get_report(query: str, report_type: str, sources: list) -> str:
    custom_logs_handler = CustomLogsHandler(None, query)  # Pass query parameter
    researcher = GPTResearcher(query=query, 
                               report_type=report_type, 
                               complement_source_urls=False,
                               websocket=custom_logs_handler)
    await researcher.conduct_research()
    report = await researcher.write_report()
    return report, researcher

if __name__ == "__main__":
    query = "Write an analysis on paul graham"
    report_type = "research_report"
    sources = ["https://www.paulgraham.com/when.html", "https://www.paulgraham.com/noob.html"]  # query is related

    report, researcher = asyncio.run(get_report(query, report_type, sources))
    print(report)

    print(f"\nLength of the context = {len(researcher.get_research_context())}") # Must say Non-zero value because the query is related to the contents of the page, so there will be relevant context present



#### Test case 2 (Illustrating the problem, i.e., source_urls are not scoured. Hence, no relevant context)

# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct
# import asyncio

# async def get_report(query: str, report_type: str, sources: list) -> str:
#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources)
#     await researcher.conduct_research()
#     report = await researcher.write_report()
#     return report, researcher

# if __name__ == "__main__":
#     query = "What is Microsoft's business model?"
#     report_type = "research_report"
#     sources = ["https://www.apple.com", "https://en.wikipedia.org/wiki/Olympic_Games"]  # query is UNRELATED.

#     report, researcher = asyncio.run(get_report(query, report_type, sources))
#     print(report)

#     print(f"\nLength of the context = {len(researcher.get_research_context())}") # Must say 0 (zero) value because the query is UNRELATED to the contents of the pages, so there will be NO relevant context present



#### Test case 3 (Suggested solution - complement_source_urls parameter allows GPTR to scour more of the web and not restrict to source_urls)

# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct
# import asyncio

# async def get_report(query: str, report_type: str, sources: list) -> str:
#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=True)
#     await researcher.conduct_research()
#     report = await researcher.write_report()
#     return report, researcher

# if __name__ == "__main__":
#     query = "What is Microsoft's business model?"
#     report_type = "research_report"
#     sources = ["https://www.apple.com", "https://en.wikipedia.org/wiki/Olympic_Games"]  # query is UNRELATED

#     report, researcher = asyncio.run(get_report(query, report_type, sources))
#     print(report)

#     print(f"\nLength of the context = {len(researcher.get_research_context())}") # Must say Non-zero value because the query is UNRELATED to the contents of the page, but the complement_source_urls is set which should make gptr do default web search to gather contexts
    


# #### Test case 4 (Furthermore, GPTR will create more context in addition to source_urls if the complement_source_urls parameter is set allowing for a larger research scope)

# from gpt_researcher.agent import GPTResearcher  # Ensure this path is correct
# import asyncio

# async def get_report(query: str, report_type: str, sources: list) -> str:
#     researcher = GPTResearcher(query=query, report_type=report_type, source_urls=sources, complement_source_urls=True)
#     await researcher.conduct_research()
#     report = await researcher.write_report()
#     return report, researcher

# if __name__ == "__main__":
#     query = "What are the latest advancements in AI?"
#     report_type = "research_report"
#     sources = ["https://en.wikipedia.org/wiki/Artificial_intelligence", "https://www.ibm.com/watson/ai"]  # query is related

#     report, researcher = asyncio.run(get_report(query, report_type, sources))
#     print(report)

#     print(f"\nLength of the context = {len(researcher.get_research_context())}") # Must say Non-zero value because the query is related to the contents of the page, and additionally the complement_source_urls is set which should make gptr do default web search to gather more contexts!



================================================
FILE: tests/test-loaders.py
================================================
from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredCSVLoader

# # Test PyMuPDFLoader
pdf_loader = PyMuPDFLoader("my-docs/Elisha - Coding Career.pdf")
try:
    pdf_data = pdf_loader.load()
    print("PDF Data:", pdf_data)
except Exception as e:
    print("Failed to load PDF:", e)

# Test UnstructuredCSVLoader
csv_loader = UnstructuredCSVLoader("my-docs/active_braze_protocols_from_bq.csv", mode="elements")
try:
    csv_data = csv_loader.load()
    print("CSV Data:", csv_data)
except Exception as e:
    print("Failed to load CSV:", e)


================================================
FILE: tests/test-openai-llm.py
================================================
import asyncio
from gpt_researcher.utils.llm import get_llm
from gpt_researcher import GPTResearcher
from dotenv import load_dotenv
load_dotenv()

async def main():

    # Example usage of get_llm function
    llm_provider = "openai"
    model = "gpt-3.5-turbo" 
    temperature = 0.7
    max_tokens = 1000

    llm = get_llm(llm_provider, model=model, temperature=temperature, max_tokens=max_tokens)
    print(f"LLM Provider: {llm_provider}, Model: {model}, Temperature: {temperature}, Max Tokens: {max_tokens}")
    print('llm: ',llm)
    await test_llm(llm=llm)


async def test_llm(llm):
    # Test the connection with a simple query
    messages = [{"role": "user", "content": "sup?"}]
    try:
        response = await llm.get_chat_response(messages, stream=False)
        print("LLM response:", response)
    except Exception as e:
        print(f"Error: {e}")

# Run the async function
asyncio.run(main())


================================================
FILE: tests/test-your-embeddings.py
================================================
from gpt_researcher.config.config import Config
from gpt_researcher.memory.embeddings import Memory
import asyncio
import os
from dotenv import load_dotenv
load_dotenv()

async def main():
    cfg = Config()
    
    print("Current embedding configuration:")
    print(f"EMBEDDING env var: {os.getenv('EMBEDDING', 'Not set')}")
    print(f"EMBEDDING_PROVIDER env var: {os.getenv('EMBEDDING_PROVIDER', 'Not set')}")
    
    try:
        # Check if embedding attributes are set
        print(f"cfg.embedding: {getattr(cfg, 'embedding', 'Not set')}")
        print(f"cfg.embedding_provider: {getattr(cfg, 'embedding_provider', 'Not set')}")
        print(f"cfg.embedding_model: {getattr(cfg, 'embedding_model', 'Not set')}")
        
        # If embedding_provider and embedding_model are not set, use defaults
        if not hasattr(cfg, 'embedding_provider') or not cfg.embedding_provider:
            print("Setting default embedding provider: openai")
            cfg.embedding_provider = "openai"
            
        if not hasattr(cfg, 'embedding_model') or not cfg.embedding_model:
            print("Setting default embedding model: text-embedding-3-small")
            cfg.embedding_model = "text-embedding-3-small"
        
        # Create a Memory instance using the configuration
        # Note: We're not passing embedding_kwargs since it's not properly initialized
        memory = Memory(
            embedding_provider=cfg.embedding_provider,
            model=cfg.embedding_model
        )
        
        # Get the embeddings object
        embeddings = memory.get_embeddings()
        
        # Test the embeddings with a simple text
        test_text = "This is a test sentence to verify embeddings are working correctly."
        embedding_vector = embeddings.embed_query(test_text)
        
        # Print information about the embedding
        print(f"\nSuccess! Generated embeddings using provider: {cfg.embedding_provider}")
        print(f"Model: {cfg.embedding_model}")
        print(f"Embedding vector length: {len(embedding_vector)}")
        print(f"First few values: {embedding_vector[:5]}")
        
    except Exception as e:
        print(f"Error testing embeddings: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())


================================================
FILE: tests/test-your-llm.py
================================================
from gpt_researcher.config.config import Config
from gpt_researcher.utils.llm import create_chat_completion
import asyncio
from dotenv import load_dotenv
load_dotenv()

async def main():
    cfg = Config()

    try:
        report = await create_chat_completion(
            model=cfg.smart_llm_model,
            messages = [{"role": "user", "content": "sup?"}],
            temperature=0.35,
            llm_provider=cfg.smart_llm_provider,
            stream=True,
            max_tokens=cfg.smart_token_limit,
            llm_kwargs=cfg.llm_kwargs
        )
    except Exception as e:
        print(f"Error in calling LLM: {e}")

# Run the async function
asyncio.run(main())


================================================
FILE: tests/test-your-retriever.py
================================================
import asyncio
from dotenv import load_dotenv
from gpt_researcher.config.config import Config
from gpt_researcher.actions.retriever import get_retrievers
from gpt_researcher.skills.researcher import ResearchConductor
import pprint
# Load environment variables from .env file
load_dotenv()

async def test_scrape_data_by_query():
    # Initialize the Config object
    config = Config()

    # Retrieve the retrievers based on the current configuration
    retrievers = get_retrievers({}, config)
    print("Retrievers:", retrievers)

    # Create a mock researcher object with necessary attributes
    class MockResearcher:
        def init(self):
            self.retrievers = retrievers
            self.cfg = config
            self.verbose = True
            self.websocket = None
            self.scraper_manager = None  # Mock or implement scraper manager
            self.vector_store = None  # Mock or implement vector store

    researcher = MockResearcher()
    research_conductor = ResearchConductor(researcher)
    # print('research_conductor',dir(research_conductor))
    # print('MockResearcher',dir(researcher))
    # Define a sub-query to test
    sub_query = "design patterns for autonomous ai agents"

    # Iterate through all retrievers
    for retriever_class in retrievers:
        # Instantiate the retriever with the sub-query
        retriever = retriever_class(sub_query)

        # Perform the search using the current retriever
        search_results = await asyncio.to_thread(
            retriever.search, max_results=10
        )

        print("\033[35mSearch results:\033[0m")
        pprint.pprint(search_results, indent=4, width=80)

if __name__ == "__main__":
    asyncio.run(test_scrape_data_by_query())


================================================
FILE: tests/test_logging.py
================================================
import pytest
from unittest.mock import AsyncMock
from fastapi import WebSocket
from backend.server.server_utils import CustomLogsHandler
import os
import json

@pytest.mark.asyncio
async def test_custom_logs_handler():
    # Mock websocket
    mock_websocket = AsyncMock()
    mock_websocket.send_json = AsyncMock()
    
    # Test initialization
    handler = CustomLogsHandler(mock_websocket, "test_query")
    
    # Verify log file creation
    assert os.path.exists(handler.log_file)
    
    # Test sending log data
    test_data = {
        "type": "logs",
        "message": "Test log message"
    }
    
    await handler.send_json(test_data)
    
    # Verify websocket was called with correct data
    mock_websocket.send_json.assert_called_once_with(test_data)
    
    # Verify log file contents
    with open(handler.log_file, 'r') as f:
        log_data = json.load(f)
        assert len(log_data['events']) == 1
        assert log_data['events'][0]['data'] == test_data 

@pytest.mark.asyncio
async def test_content_update():
    """Test handling of non-log type data that updates content"""
    mock_websocket = AsyncMock()
    mock_websocket.send_json = AsyncMock()
    
    handler = CustomLogsHandler(mock_websocket, "test_query")
    
    # Test content update
    content_data = {
        "query": "test query",
        "sources": ["source1", "source2"],
        "report": "test report"
    }
    
    await handler.send_json(content_data)
    
    mock_websocket.send_json.assert_called_once_with(content_data)
    
    # Verify log file contents
    with open(handler.log_file, 'r') as f:
        log_data = json.load(f)
        assert log_data['content']['query'] == "test query"
        assert log_data['content']['sources'] == ["source1", "source2"]
        assert log_data['content']['report'] == "test report"


================================================
FILE: tests/test_logging_output.py
================================================
import pytest
import asyncio
from pathlib import Path
import json
import logging
from fastapi import WebSocket
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestWebSocket(WebSocket):
    def __init__(self):
        self.events = []
        self.scope = {}

    def __bool__(self):
        return True

    async def accept(self):
        self.scope["type"] = "websocket"
        pass
        
    async def send_json(self, event):
        logger.info(f"WebSocket received event: {event}")
        self.events.append(event)

@pytest.mark.asyncio
async def test_log_output_file():
    """Test to verify logs are properly written to output file"""
    from gpt_researcher.agent import GPTResearcher
    from backend.server.server_utils import CustomLogsHandler
    
    # 1. Setup like the main app
    websocket = TestWebSocket()
    await websocket.accept()
    
    # 2. Initialize researcher like main app
    query = "What is the capital of France?"
    research_id = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hash(query)}"
    logs_handler = CustomLogsHandler(websocket=websocket, task=research_id)
    researcher = GPTResearcher(query=query, websocket=logs_handler)
    
    # 3. Run research
    await researcher.conduct_research()
    
    # 4. Verify events were captured
    logger.info(f"Events captured: {len(websocket.events)}")
    assert len(websocket.events) > 0, "No events were captured"
    
    # 5. Check output file
    output_dir = Path().joinpath(Path.cwd(), "outputs")
    output_files = list(output_dir.glob(f"task_*{research_id}*.json"))
    assert len(output_files) > 0, "No output file was created"
    
    with open(output_files[-1]) as f:
        data = json.load(f)
        assert len(data.get('events', [])) > 0, "No events in output file" 

    # Clean up the output files
    for output_file in output_files:
        output_file.unlink()
        logger.info(f"Deleted output file: {output_file}")


================================================
FILE: tests/test_logs.py
================================================
import os
from pathlib import Path
import sys

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from backend.server.server_utils import CustomLogsHandler

def test_logs_creation():
    # Print current working directory
    print(f"Current working directory: {os.getcwd()}")
    
    # Print project root
    print(f"Project root: {project_root}")
    
    # Try to create logs directory directly
    logs_dir = project_root / "logs"
    print(f"Attempting to create logs directory at: {logs_dir}")
    
    try:
        # Create directory with full permissions
        os.makedirs(logs_dir, mode=0o777, exist_ok=True)
        print(f"âœ“ Created directory: {logs_dir}")
        
        # Test file creation
        test_file = logs_dir / "test.txt"
        with open(test_file, 'w') as f:
            f.write("Test log entry")
        print(f"âœ“ Created test file: {test_file}")
        
        # Initialize the handler
        handler = CustomLogsHandler()
        print("âœ“ CustomLogsHandler initialized")
        
        # Test JSON logging
        handler.logs.append({"test": "message"})
        print("âœ“ Added test log entry")
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        print(f"Error type: {type(e)}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")

if __name__ == "__main__":
    test_logs_creation() 


================================================
FILE: tests/test_mcp.py
================================================
#!/usr/bin/env python3
"""
Test script for MCP integration in GPT Researcher

This script tests two MCP integration scenarios:
1. Web Search MCP (Tavily) - News and general web search queries
2. GitHub MCP - Code repository and technical documentation queries

Both tests verify:
- MCP server connection and tool usage
- Research execution with default optimal settings
- Report generation with MCP data

Prerequisites:
1. Install GPT Researcher: pip install gpt-researcher
2. Install MCP servers:
   - Web Search: npm install -g tavily-mcp
   - GitHub: npm install -g @modelcontextprotocol/server-github
3. Set up environment variables:
   - GITHUB_PERSONAL_ACCESS_TOKEN: Your GitHub Personal Access Token
   - OPENAI_API_KEY: Your OpenAI API key
   - TAVILY_API_KEY: Your Tavily API key
"""

import asyncio
import os
import logging
from typing import Dict, List, Any

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Get API keys from environment variables
GITHUB_TOKEN = os.environ.get("GITHUB_PERSONAL_ACCESS_TOKEN")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")

# Test configuration using environment variables
def get_mcp_config():
    """Get MCP configuration with environment variables."""
    return [
        {
            "name": "tavily",
            "command": "npx",
            "args": ["-y", "tavily-mcp@0.1.2"],
            "env": {
                "TAVILY_API_KEY": TAVILY_API_KEY
            }
        }
    ]

def get_github_mcp_config():
    """Get GitHub MCP configuration with environment variables."""
    return [
        {
            "name": "github",
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-github"],
            "env": {
                "GITHUB_PERSONAL_ACCESS_TOKEN": GITHUB_TOKEN
            }
        }
    ]

def setup_environment():
    """Validate required environment variables."""
    required_vars = {
        "GITHUB_PERSONAL_ACCESS_TOKEN": GITHUB_TOKEN,
        "OPENAI_API_KEY": OPENAI_API_KEY,
        "TAVILY_API_KEY": TAVILY_API_KEY
    }
    
    missing_vars = []
    
    for var_name, var_value in required_vars.items():
        if not var_value:
            missing_vars.append(var_name)
    
    if missing_vars:
        print("âŒ Missing required environment variables:")
        for var in missing_vars:
            print(f"   â€¢ {var}")
        print("\nPlease set these environment variables before running the test:")
        print("   export GITHUB_PERSONAL_ACCESS_TOKEN='your_github_token'")
        print("   export OPENAI_API_KEY='your_openai_key'")
        print("   export TAVILY_API_KEY='your_tavily_key'")
        return False
    
    print("âœ… All required environment variables are set")
    return True

async def test_web_search_mcp():
    """Test MCP integration with web search (Tavily) for news and general topics."""
    print("\nğŸŒ Testing Web Search MCP Integration")
    print("=" * 50)
    
    try:
        from gpt_researcher import GPTResearcher
        
        # Create web search MCP configuration
        mcp_configs = get_mcp_config()
        
        # Create researcher with web search query
        query = "What is the latest updates in the NBA playoffs?"
        researcher = GPTResearcher(
            query=query,
            mcp_configs=mcp_configs
        )
        
        print("âœ… GPTResearcher initialized with web search MCP")
        print(f"ğŸ”§ MCP servers configured: {len(mcp_configs)} (Tavily)")
        print(f"ğŸ“ Query: {query}")
        
        # Conduct research - should use fast strategy by default
        print("ğŸš€ Starting web search research...")
        context = await researcher.conduct_research()
        
        print(f"ğŸ“Š Web search research completed!")
        print(f"ğŸ“ˆ Context collected: {len(str(context)) if context else 0} chars")
        
        # Generate a brief report
        print("ğŸ“ Generating report...")
        report = await researcher.write_report()
        
        print(f"âœ… Report generated successfully!")
        print(f"ğŸ“„ Report length: {len(report)} characters")
        
        # Save test report
        filename = "../test_web_search_mcp_report.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"# Test Report: Web Search MCP Integration\n\n")
            f.write(f"**Query:** {researcher.query}\n\n")
            f.write(f"**MCP Server:** Tavily (Web Search)\n\n")
            f.write(f"**Generated Report:**\n\n")
            f.write(report)
        
        print(f"ğŸ’¾ Test report saved to: {filename}")
        
        # Print summary
        print(f"\nğŸ“‹ Web Search MCP Test Summary:")
        print(f"   â€¢ News query processed successfully")
        print(f"   â€¢ Context gathered: {len(str(context)):,} chars")
        print(f"   â€¢ Report generated: {len(report):,} chars")
        print(f"   â€¢ Cost: ${researcher.get_costs():.4f}")
        print(f"   â€¢ Saved to: {filename}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in web search MCP test: {e}")
        logger.exception("Web search MCP test error:")
        return False

async def test_github_mcp():
    """Test MCP integration with GitHub for code-related queries."""
    print("\nğŸ™ Testing GitHub MCP Integration")
    print("=" * 50)
    
    try:
        from gpt_researcher import GPTResearcher
        
        # Create GitHub MCP configuration
        mcp_configs = get_github_mcp_config()
        
        # Create researcher with code-related query
        query = "What are the key features and implementation of React's useState hook? How has it evolved in recent versions?"
        researcher = GPTResearcher(
            query=query,
            mcp_configs=mcp_configs
        )
        
        print("âœ… GPTResearcher initialized with GitHub MCP")
        print(f"ğŸ”§ MCP servers configured: {len(mcp_configs)} (GitHub)")
        print(f"ğŸ“ Query: {query}")
        
        # Conduct research - should use fast strategy by default
        print("ğŸš€ Starting GitHub code research...")
        context = await researcher.conduct_research()
        
        print(f"ğŸ“Š GitHub research completed!")
        print(f"ğŸ“ˆ Context collected: {len(str(context)) if context else 0} chars")
        
        # Generate a brief report
        print("ğŸ“ Generating report...")
        report = await researcher.write_report()
        
        print(f"âœ… Report generated successfully!")
        print(f"ğŸ“„ Report length: {len(report)} characters")
        
        # Save test report
        filename = "../test_github_mcp_report.md"
        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"# Test Report: GitHub MCP Integration\n\n")
            f.write(f"**Query:** {researcher.query}\n\n")
            f.write(f"**MCP Server:** GitHub (Code Repository)\n\n")
            f.write(f"**Generated Report:**\n\n")
            f.write(report)
        
        print(f"ğŸ’¾ Test report saved to: {filename}")
        
        # Print summary
        print(f"\nğŸ“‹ GitHub MCP Test Summary:")
        print(f"   â€¢ Code query processed successfully")
        print(f"   â€¢ Context gathered: {len(str(context)):,} chars")
        print(f"   â€¢ Report generated: {len(report):,} chars")
        print(f"   â€¢ Cost: ${researcher.get_costs():.4f}")
        print(f"   â€¢ Saved to: {filename}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error in GitHub MCP test: {e}")
        logger.exception("GitHub MCP test error:")
        return False

async def main():
    """Main test function."""
    print("ğŸš€ Testing MCP Integration with GPT Researcher")
    print("=" * 50)
    
    # Check environment setup
    if not setup_environment():
        print("\nâŒ Environment setup failed. Please check your configuration.")
        return
    
    print("âœ… Environment setup complete")
    
    # Track test results
    test_results = []
    
    # Run Web Search MCP test
    print("\nğŸŒ Running Web Search MCP Test (Tavily)")
    result1 = await test_web_search_mcp()
    test_results.append(("Web Search MCP", result1))
    
    # Run GitHub MCP test
    print("\nğŸ™ Running GitHub MCP Test")
    result2 = await test_github_mcp()
    test_results.append(("GitHub MCP", result2))
    
    # Summary
    print("\nğŸ“Š Test Results Summary")
    print("=" * 30)
    
    passed = 0
    total = len(test_results)
    
    for test_name, passed_test in test_results:
        status = "âœ… PASSED" if passed_test else "âŒ FAILED"
        print(f"  {test_name}: {status}")
        if passed_test:
            passed += 1
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All MCP integration tests completed successfully!")
        print("âš¡ Both Web Search (news) and GitHub (code) MCP servers work seamlessly!")
    else:
        print("âš ï¸ Some tests failed. Check the output above for details.")

if __name__ == "__main__":
    print("ğŸ”§ MCP Integration Tests")
    print("=" * 30)
    print("Testing Web Search (Tavily) and GitHub MCP integrations with optimal default settings.")
    print()
    
    asyncio.run(main()) 


================================================
FILE: tests/test_researcher_logging.py
================================================
import pytest
import asyncio
from pathlib import Path
import sys
import logging

# Add the project root to Python path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Configure basic logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@pytest.mark.asyncio
async def test_researcher_logging():  # Renamed function to be more specific
    """
    Test suite for verifying the researcher's logging infrastructure.
    Ensures proper creation and formatting of log files.
    """
    try:
        # Import here to catch any import errors
        from backend.server.server_utils import Researcher
        logger.info("Successfully imported Researcher class")
        
        # Create a researcher instance with a logging-focused query
        researcher = Researcher(
            query="Test query for logging verification",
            report_type="research_report"
        )
        logger.info("Created Researcher instance")
        
        # Run the research
        report = await researcher.research()
        logger.info("Research completed successfully!")
        logger.info(f"Report length: {len(report)}")
        
        # Basic report assertions
        assert report is not None
        assert len(report) > 0
        
        # Detailed log file verification
        logs_dir = Path(project_root) / "logs"
        log_files = list(logs_dir.glob("research_*.log"))
        json_files = list(logs_dir.glob("research_*.json"))
        
        # Verify log files exist
        assert len(log_files) > 0, "No log files were created"
        assert len(json_files) > 0, "No JSON files were created"
        
        # Log the findings
        logger.info(f"\nFound {len(log_files)} log files:")
        for log_file in log_files:
            logger.info(f"- {log_file.name}")
            # Could add additional checks for log file format/content here
            
        logger.info(f"\nFound {len(json_files)} JSON files:")
        for json_file in json_files:
            logger.info(f"- {json_file.name}")
            # Could add additional checks for JSON file structure here
            
    except ImportError as e:
        logger.error(f"Import error: {e}")
        logger.error("Make sure gpt_researcher is installed and in your PYTHONPATH")
        raise
    except Exception as e:
        logger.error(f"Error during research: {e}")
        raise

if __name__ == "__main__":
    pytest.main([__file__]) 


================================================
FILE: tests/test_security_fix.py
================================================
"""
Security tests for path traversal vulnerability fix.

This module tests the security improvements made to file upload and deletion
operations to prevent path traversal attacks.
"""

import pytest
import tempfile
import os
import shutil
from unittest.mock import Mock, MagicMock
from fastapi import HTTPException
from fastapi.responses import JSONResponse

# Import the functions we're testing
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.server.server_utils import (
    secure_filename, 
    validate_file_path, 
    handle_file_upload, 
    handle_file_deletion
)


class TestSecureFilename:
    """Test the secure_filename function against various attack vectors."""
    
    def test_basic_filename(self):
        """Test that normal filenames pass through unchanged."""
        assert secure_filename("document.pdf") == "document.pdf"
        assert secure_filename("report_2024.docx") == "report_2024.docx"
    
    def test_path_traversal_attacks(self):
        """Test that path traversal attempts are blocked."""
        with pytest.raises(ValueError):
            secure_filename("../../../etc/passwd")
        
        with pytest.raises(ValueError):
            secure_filename("..\\..\\windows\\system32\\config\\SAM")
        
        # Multiple traversal attempts
        assert secure_filename("....//....//etc/passwd") == "etcpasswd"
    
    def test_null_byte_injection(self):
        """Test that null byte injection is prevented."""
        # Null bytes should be removed
        result = secure_filename("test\x00.txt")
        assert "\x00" not in result
        assert result == "test.txt"
    
    def test_control_characters(self):
        """Test that control characters are removed."""
        # Test various control characters
        result = secure_filename("test\x01\x02\x03file.txt")
        assert result == "testfile.txt"
    
    def test_unicode_normalization(self):
        """Test that unicode attacks are prevented."""
        # Test unicode normalization
        filename = "test\u202e\u202dfile.txt"  # Right-to-left override
        result = secure_filename(filename)
        # Should be normalized and safe
        assert len(result) > 0
    
    def test_drive_letters_windows(self):
        """Test that Windows drive letters are removed."""
        assert secure_filename("C:sensitive.txt") == "sensitive.txt"
        assert secure_filename("D:important.doc") == "important.doc"
    
    def test_reserved_names_windows(self):
        """Test that Windows reserved names are blocked."""
        reserved_names = ['CON', 'PRN', 'AUX', 'NUL', 'COM1', 'LPT1']
        
        for name in reserved_names:
            with pytest.raises(ValueError, match="reserved name"):
                secure_filename(f"{name}.txt")
            
            with pytest.raises(ValueError, match="reserved name"):
                secure_filename(name.lower())
    
    def test_empty_filename(self):
        """Test that empty filenames are rejected."""
        with pytest.raises(ValueError, match="empty"):
            secure_filename("")
        
        with pytest.raises(ValueError, match="empty"):
            secure_filename("   ")  # Only spaces
        
        with pytest.raises(ValueError, match="empty"):
            secure_filename("...")  # Only dots
    
    def test_filename_length_limit(self):
        """Test that overly long filenames are rejected."""
        # Create a filename longer than 255 bytes
        long_filename = "a" * 300 + ".txt"
        with pytest.raises(ValueError, match="too long"):
            secure_filename(long_filename)
    
    def test_leading_dots_spaces(self):
        """Test that leading dots and spaces are removed."""
        assert secure_filename("...file.txt") == "file.txt"
        assert secure_filename("   file.txt") == "file.txt"
        assert secure_filename(". . .file.txt") == "file.txt"


class TestValidateFilePath:
    """Test the validate_file_path function."""
    
    def test_valid_path(self):
        """Test that valid paths within base directory are accepted."""
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = os.path.join(temp_dir, "test.txt")
            result = validate_file_path(file_path, temp_dir)
            assert result == os.path.abspath(file_path)
    
    def test_path_traversal_blocked(self):
        """Test that path traversal attempts are blocked."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Try to escape the directory
            malicious_path = os.path.join(temp_dir, "..", "..", "etc", "passwd")
            
            with pytest.raises(ValueError, match="outside allowed directory"):
                validate_file_path(malicious_path, temp_dir)
    
    def test_symlink_traversal_blocked(self):
        """Test that symlink-based traversal is blocked."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a symlink pointing outside the directory
            outside_file = "/tmp/test_target.txt"
            symlink_path = os.path.join(temp_dir, "malicious_link")
            
            try:
                os.symlink(outside_file, symlink_path)
                target_path = os.path.join(temp_dir, "malicious_link", "nested")
                
                with pytest.raises(ValueError, match="outside allowed directory"):
                    validate_file_path(target_path, temp_dir)
            except OSError:
                # Skip if symlinks aren't supported (e.g., Windows without admin)
                pytest.skip("Symlinks not supported in this environment")


class TestHandleFileUpload:
    """Test the secure file upload functionality."""
    
    @pytest.fixture
    def mock_file(self):
        """Create a mock file object for testing."""
        mock_file = Mock()
        mock_file.filename = "test.txt"
        mock_file.file = Mock()
        return mock_file
    
    @pytest.fixture
    def temp_doc_path(self):
        """Create a temporary directory for testing."""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    @pytest.mark.asyncio
    async def test_normal_file_upload(self, mock_file, temp_doc_path):
        """Test that normal file uploads work correctly."""
        # NOTE: Not fully tested with DocumentLoader due to automated environment limits
        # Manual testing recommended for: DocumentLoader integration
        
        # Mock the DocumentLoader to avoid dependency issues
        import backend.server.server_utils
        original_loader = backend.server.server_utils.DocumentLoader
        
        class MockDocumentLoader:
            def __init__(self, path):
                self.path = path
            async def load(self):
                pass
        
        backend.server.server_utils.DocumentLoader = MockDocumentLoader
        
        try:
            result = await handle_file_upload(mock_file, temp_doc_path)
            
            assert result["filename"] == "test.txt"
            assert temp_doc_path in result["path"]
            assert os.path.exists(result["path"])
        finally:
            # Restore original loader
            backend.server.server_utils.DocumentLoader = original_loader
    
    @pytest.mark.asyncio
    async def test_malicious_filename_upload(self, temp_doc_path):
        """Test that malicious filenames are rejected."""
        mock_file = Mock()
        mock_file.filename = "../../../etc/passwd"
        mock_file.file = Mock()
        
        with pytest.raises(HTTPException) as exc_info:
            await handle_file_upload(mock_file, temp_doc_path)
        
        assert exc_info.value.status_code == 400
        assert "Invalid file" in str(exc_info.value.detail)
    
    @pytest.mark.asyncio
    async def test_empty_filename_upload(self, temp_doc_path):
        """Test that empty filenames are rejected."""
        mock_file = Mock()
        mock_file.filename = ""
        mock_file.file = Mock()
        
        with pytest.raises(HTTPException) as exc_info:
            await handle_file_upload(mock_file, temp_doc_path)
        
        assert exc_info.value.status_code == 400
    
    @pytest.mark.asyncio
    async def test_file_conflict_handling(self, mock_file, temp_doc_path):
        """Test that file conflicts are handled by creating unique names."""
        # Create an existing file
        existing_path = os.path.join(temp_doc_path, "test.txt")
        os.makedirs(temp_doc_path, exist_ok=True)
        with open(existing_path, "w") as f:
            f.write("existing content")
        
        # Mock DocumentLoader
        import backend.server.server_utils
        original_loader = backend.server.server_utils.DocumentLoader
        
        class MockDocumentLoader:
            def __init__(self, path):
                pass
            async def load(self):
                pass
        
        backend.server.server_utils.DocumentLoader = MockDocumentLoader
        
        try:
            result = await handle_file_upload(mock_file, temp_doc_path)
            
            # Should create a unique filename
            assert result["filename"] == "test_1.txt"
            assert os.path.exists(result["path"])
        finally:
            backend.server.server_utils.DocumentLoader = original_loader


class TestHandleFileDeletion:
    """Test the secure file deletion functionality."""
    
    @pytest.fixture
    def temp_doc_path(self):
        """Create a temporary directory for testing."""
        temp_dir = tempfile.mkdtemp()
        yield temp_dir
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    @pytest.mark.asyncio
    async def test_normal_file_deletion(self, temp_doc_path):
        """Test that normal file deletion works correctly."""
        # Create a test file
        test_file = os.path.join(temp_doc_path, "test.txt")
        os.makedirs(temp_doc_path, exist_ok=True)
        with open(test_file, "w") as f:
            f.write("test content")
        
        result = await handle_file_deletion("test.txt", temp_doc_path)
        
        assert isinstance(result, JSONResponse)
        assert not os.path.exists(test_file)
    
    @pytest.mark.asyncio
    async def test_malicious_filename_deletion(self, temp_doc_path):
        """Test that malicious filenames are rejected for deletion."""
        result = await handle_file_deletion("../../../etc/passwd", temp_doc_path)
        
        assert isinstance(result, JSONResponse)
        assert result.status_code == 400
    
    @pytest.mark.asyncio
    async def test_nonexistent_file_deletion(self, temp_doc_path):
        """Test deletion of non-existent files."""
        result = await handle_file_deletion("nonexistent.txt", temp_doc_path)
        
        assert isinstance(result, JSONResponse)
        assert result.status_code == 404
    
    @pytest.mark.asyncio
    async def test_directory_deletion_blocked(self, temp_doc_path):
        """Test that directory deletion is blocked."""
        # Create a subdirectory
        subdir = os.path.join(temp_doc_path, "subdir")
        os.makedirs(subdir, exist_ok=True)
        
        result = await handle_file_deletion("subdir", temp_doc_path)
        
        assert isinstance(result, JSONResponse)
        assert result.status_code == 400
        assert "not a file" in str(result.body.decode())


class TestSecurityIntegration:
    """Integration tests for the complete security fix."""
    
    def test_attack_vectors_blocked(self):
        """Test that common attack vectors are blocked."""
        attack_vectors = [
            "../../../etc/passwd",
            "..\\..\\..\\windows\\system32\\config\\SAM",
            "test\x00.txt",
            "CON.txt",
            "PRN",
            "C:sensitive.txt",
            "....//....//sensitive",
            "\u202e\u202dmalicious.txt"  # Unicode RLO attack
        ]
        
        for attack in attack_vectors:
            try:
                result = secure_filename(attack)
                # If it doesn't raise an exception, ensure it's safe
                assert ".." not in result
                assert "/" not in result
                assert "\\" not in result
                assert "\x00" not in result
                assert not result.startswith(".")
            except ValueError:
                # This is expected for malicious inputs
                pass
    
    def test_legitimate_files_allowed(self):
        """Test that legitimate files are still allowed."""
        legitimate_files = [
            "document.pdf",
            "report_2024.docx",
            "data.csv",
            "image.jpg",
            "script.py",
            "config.json",
            "README.md",
            "file-with-dashes.txt",
            "file_with_underscores.txt"
        ]
        
        for filename in legitimate_files:
            result = secure_filename(filename)
            assert result == filename  # Should pass through unchanged


if __name__ == "__main__":
    # Run tests if executed directly
    pytest.main([__file__, "-v"]) 


================================================
FILE: tests/vector-store.py
================================================
import asyncio
import pytest
from typing import List
from gpt_researcher import GPTResearcher

from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS, InMemoryVectorStore
from langchain_core.documents import Document


# taken from https://paulgraham.com/persistence.html
essay = """
The right kind of Stubborn

July 2024

Successful people tend to be persistent. New ideas often don't work at first, but they're not deterred. They keep trying and eventually find something that does.

Mere obstinacy, on the other hand, is a recipe for failure. Obstinate people are so annoying. They won't listen. They beat their heads against a wall and get nowhere.

But is there any real difference between these two cases? Are persistent and obstinate people actually behaving differently? Or are they doing the same thing, and we just label them later as persistent or obstinate depending on whether they turned out to be right or not?

If that's the only difference then there's nothing to be learned from the distinction. Telling someone to be persistent rather than obstinate would just be telling them to be right rather than wrong, and they already know that. Whereas if persistence and obstinacy are actually different kinds of behavior, it would be worthwhile to tease them apart. [1]

I've talked to a lot of determined people, and it seems to me that they're different kinds of behavior. I've often walked away from a conversation thinking either "Wow, that guy is determined" or "Damn, that guy is stubborn," and I don't think I'm just talking about whether they seemed right or not. That's part of it, but not all of it.

There's something annoying about the obstinate that's not simply due to being mistaken. They won't listen. And that's not true of all determined people. I can't think of anyone more determined than the Collison brothers, and when you point out a problem to them, they not only listen, but listen with an almost predatory intensity. Is there a hole in the bottom of their boat? Probably not, but if there is, they want to know about it.

It's the same with most successful people. They're never more engaged than when you disagree with them. Whereas the obstinate don't want to hear you. When you point out problems, their eyes glaze over, and their replies sound like ideologues talking about matters of doctrine. [2]

The reason the persistent and the obstinate seem similar is that they're both hard to stop. But they're hard to stop in different senses. The persistent are like boats whose engines can't be throttled back. The obstinate are like boats whose rudders can't be turned. [3]

In the degenerate case they're indistinguishable: when there's only one way to solve a problem, your only choice is whether to give up or not, and persistence and obstinacy both say no. This is presumably why the two are so often conflated in popular culture. It assumes simple problems. But as problems get more complicated, we can see the difference between them. The persistent are much more attached to points high in the decision tree than to minor ones lower down, while the obstinate spray "don't give up" indiscriminately over the whole tree.

The persistent are attached to the goal. The obstinate are attached to their ideas about how to reach it.

Worse still, that means they'll tend to be attached to their first ideas about how to solve a problem, even though these are the least informed by the experience of working on it. So the obstinate aren't merely attached to details, but disproportionately likely to be attached to wrong ones.



Why are they like this? Why are the obstinate obstinate? One possibility is that they're overwhelmed. They're not very capable. They take on a hard problem. They're immediately in over their head. So they grab onto ideas the way someone on the deck of a rolling ship might grab onto the nearest handhold.

That was my initial theory, but on examination it doesn't hold up. If being obstinate were simply a consequence of being in over one's head, you could make persistent people become obstinate by making them solve harder problems. But that's not what happens. If you handed the Collisons an extremely hard problem to solve, they wouldn't become obstinate. If anything they'd become less obstinate. They'd know they had to be open to anything.

Similarly, if obstinacy were caused by the situation, the obstinate would stop being obstinate when solving easier problems. But they don't. And if obstinacy isn't caused by the situation, it must come from within. It must be a feature of one's personality.

Obstinacy is a reflexive resistance to changing one's ideas. This is not identical with stupidity, but they're closely related. A reflexive resistance to changing one's ideas becomes a sort of induced stupidity as contrary evidence mounts. And obstinacy is a form of not giving up that's easily practiced by the stupid. You don't have to consider complicated tradeoffs; you just dig in your heels. It even works, up to a point.

The fact that obstinacy works for simple problems is an important clue. Persistence and obstinacy aren't opposites. The relationship between them is more like the relationship between the two kinds of respiration we can do: aerobic respiration, and the anaerobic respiration we inherited from our most distant ancestors. Anaerobic respiration is a more primitive process, but it has its uses. When you leap suddenly away from a threat, that's what you're using.

The optimal amount of obstinacy is not zero. It can be good if your initial reaction to a setback is an unthinking "I won't give up," because this helps prevent panic. But unthinking only gets you so far. The further someone is toward the obstinate end of the continuum, the less likely they are to succeed in solving hard problems. [4]



Obstinacy is a simple thing. Animals have it. But persistence turns out to have a fairly complicated internal structure.

One thing that distinguishes the persistent is their energy. At the risk of putting too much weight on words, they persist rather than merely resisting. They keep trying things. Which means the persistent must also be imaginative. To keep trying things, you have to keep thinking of things to try.

Energy and imagination make a wonderful combination. Each gets the best out of the other. Energy creates demand for the ideas produced by imagination, which thus produces more, and imagination gives energy somewhere to go. [5]

Merely having energy and imagination is quite rare. But to solve hard problems you need three more qualities: resilience, good judgement, and a focus on some kind of goal.

Resilience means not having one's morale destroyed by setbacks. Setbacks are inevitable once problems reach a certain size, so if you can't bounce back from them, you can only do good work on a small scale. But resilience is not the same as obstinacy. Resilience means setbacks can't change your morale, not that they can't change your mind.

Indeed, persistence often requires that one change one's mind. That's where good judgement comes in. The persistent are quite rational. They focus on expected value. It's this, not recklessness, that lets them work on things that are unlikely to succeed.

There is one point at which the persistent are often irrational though: at the very top of the decision tree. When they choose between two problems of roughly equal expected value, the choice usually comes down to personal preference. Indeed, they'll often classify projects into deliberately wide bands of expected value in order to ensure that the one they want to work on still qualifies.

Empirically this doesn't seem to be a problem. It's ok to be irrational near the top of the decision tree. One reason is that we humans will work harder on a problem we love. But there's another more subtle factor involved as well: our preferences among problems aren't random. When we love a problem that other people don't, it's often because we've unconsciously noticed that it's more important than they realize.

Which leads to our fifth quality: there needs to be some overall goal. If you're like me you began, as a kid, merely with the desire to do something great. In theory that should be the most powerful motivator of all, since it includes everything that could possibly be done. But in practice it's not much use, precisely because it includes too much. It doesn't tell you what to do at this moment.

So in practice your energy and imagination and resilience and good judgement have to be directed toward some fairly specific goal. Not too specific, or you might miss a great discovery adjacent to what you're searching for, but not too general, or it won't work to motivate you. [6]

When you look at the internal structure of persistence, it doesn't resemble obstinacy at all. It's so much more complex. Five distinct qualities â€” energy, imagination, resilience, good judgement, and focus on a goal â€” combine to produce a phenomenon that seems a bit like obstinacy in the sense that it causes you not to give up. But the way you don't give up is completely different. Instead of merely resisting change, you're driven toward a goal by energy and resilience, through paths discovered by imagination and optimized by judgement. You'll give way on any point low down in the decision tree, if its expected value drops sufficiently, but energy and resilience keep pushing you toward whatever you choose higher up.

Considering what it's made of, it's not surprising that the right kind of stubbornness is so much rarer than the wrong kind, or that it gets so much better results. Anyone can do obstinacy. Indeed, kids and drunks and fools are best at it. Whereas very few people have enough of all five of the qualities that produce the right kind of stubbornness, but when they do the results are magical.







Notes

[1] I'm going to use "persistent" for the good kind of stubborn and "obstinate" for the bad kind, but I can't claim I'm simply following current usage. Conventional opinion barely distinguishes between good and bad kinds of stubbornness, and usage is correspondingly promiscuous. I could have invented a new word for the good kind, but it seemed better just to stretch "persistent."

[2] There are some domains where one can succeed by being obstinate. Some political leaders have been notorious for it. But it won't work in situations where you have to pass external tests. And indeed the political leaders who are famous for being obstinate are famous for getting power, not for using it well.

[3] There will be some resistance to turning the rudder of a persistent person, because there's some cost to changing direction.

[4] The obstinate do sometimes succeed in solving hard problems. One way is through luck: like the stopped clock that's right twice a day, they seize onto some arbitrary idea, and it turns out to be right. Another is when their obstinacy cancels out some other form of error. For example, if a leader has overcautious subordinates, their estimates of the probability of success will always be off in the same direction. So if he mindlessly says "push ahead regardless" in every borderline case, he'll usually turn out to be right.

[5] If you stop there, at just energy and imagination, you get the conventional caricature of an artist or poet.

[6] Start by erring on the small side. If you're inexperienced you'll inevitably err on one side or the other, and if you err on the side of making the goal too broad, you won't get anywhere. Whereas if you err on the small side you'll at least be moving forward. Then, once you're moving, you expand the goal.
"""


def load_document():
    document = [Document(page_content=essay)]
    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=30, separator="\n")
    return text_splitter.split_documents(documents=document)


def create_vectorstore(documents: List[Document]):
    embeddings = OpenAIEmbeddings()
    return FAISS.from_documents(documents, embeddings)

@pytest.mark.asyncio
async def test_gpt_researcher_with_vector_store():
    docs = load_document()
    vectorstore = create_vectorstore(docs)

    query = """
        Summarize the essay into 3 or 4 succint sections.
        Make sure to include key points regarding the differences between
        persistance vs obstinate.

        Include some recommendations for entrepeneurs in the conclusion.
        Recommend some ways to increase persistance in a healthy way.
    """


    # Create an instance of GPTResearcher
    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        report_source="langchain_vectorstore",
        vector_store=vectorstore,
    )

    # Conduct research and write the report
    await researcher.conduct_research()
    report = await researcher.write_report()

    assert report is not None

@pytest.mark.asyncio
async def test_store_in_vector_store_web():
    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
    query = "Which one is the best LLM"

    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        report_source="web",
        vector_store=vector_store,
    )

    await researcher.conduct_research()

    related_contexts = await vector_store.asimilarity_search("GPT-4", k=2)

    assert len(related_contexts) == 2
    # Add more assertions as needed to verify the results


@pytest.mark.asyncio
async def test_store_in_vector_store_urls():
    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
    query = "Who won the world cup in 2022"

    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        vector_store=vector_store,
        source_urls=["https://en.wikipedia.org/wiki/FIFA_World_Cup"]
    )

    await researcher.conduct_research()

    related_contexts = await vector_store.asimilarity_search("GPT-4", k=2)

    assert len(related_contexts) == 2


@pytest.mark.asyncio
async def test_store_in_vector_store_langchain_docs():
    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
    docs = load_document()
    query = "What does successful people tend to do?"

    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        vector_store=vector_store,
        report_source="langchain_documents",
        documents=docs
    )

    await researcher.conduct_research()

    related_contexts = await vector_store.asimilarity_search("GPT-4", k=2)

    assert len(related_contexts) == 2

@pytest.mark.asyncio
async def test_store_in_vector_store_locals():
    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
    query = "What is transformer?"

    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        vector_store=vector_store,
        report_source="local",
        config_path= "test_local"
    )

    await researcher.conduct_research()

    related_contexts = await vector_store.asimilarity_search("GPT-4", k=2)

    assert len(related_contexts) == 2

@pytest.mark.asyncio
async def test_store_in_vector_store_hybrids():
    vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
    query = "What is transformer?"
    
    researcher = GPTResearcher(
        query=query,
        report_type="research_report",
        vector_store=vector_store,
        report_source="hybrid",
        config_path= "test_local"
    )
    
    await researcher.conduct_research()
    
    related_contexts = await vector_store.asimilarity_search("GPT-4", k=2)
    
    assert len(related_contexts) == 2



================================================
FILE: .github/dependabot.yml
================================================
# To get started with Dependabot version updates, you'll need to specify which
# package ecosystems to update and where the package manifests are located.
# Please see the documentation for all configuration options:
# https://docs.github.com/github/administering-a-repository/configuration-options-for-dependency-updates

version: 2
updates:
  - package-ecosystem: "pip" # See documentation for possible values
    directory: "/" # Location of package manifests
    schedule:
      interval: "weekly"
  - package-ecosystem: "docker"
    directory: "/"
    schedule:
      interval: "weekly"



================================================
FILE: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: Create a report to help us improve
title: ''
labels: ''
assignees: ''

---

**Describe the bug**
A clear and concise description of what the bug is.

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Desktop (please complete the following information):**
 - OS: [e.g. iOS]
 - Browser [e.g. chrome, safari]
 - Version [e.g. 22]

**Smartphone (please complete the following information):**
 - Device: [e.g. iPhone6]
 - OS: [e.g. iOS8.1]
 - Browser [e.g. stock browser, safari]
 - Version [e.g. 22]

**Additional context**
Add any other context about the problem here.



================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for this project
title: ''
labels: ''
assignees: ''

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.



================================================
FILE: .github/workflows/docker-build.yml
================================================
name: GPTR tests
run-name: ${{ github.actor }} ran the GPTR tests flow
permissions:
  contents: read
  pull-requests: write
on:
  workflow_dispatch:  # Add this line to enable manual triggering
  # pull_request:
  #   types: [opened, synchronize]

jobs:
  docker:
    runs-on: ubuntu-latest
    environment: tests  # Specify the environment to use for this job
    env:
      # Ensure these environment variables are set for the entire job
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
      LANGCHAIN_API_KEY: ${{ secrets.LANGCHAIN_API_KEY }}
    steps:
      - name: Git checkout
        uses: actions/checkout@v3

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
        with:
          driver: docker

      # - name: Build Docker images
      #   uses: docker/build-push-action@v4
      #   with:
      #     push: false
      #     tags: gptresearcher/gpt-researcher:latest
      #     file: Dockerfile          

      - name: Set up Docker Compose
        run: |
          sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
      - name: Run tests with Docker Compose
        run: |
          docker-compose --profile test run --rm gpt-researcher-tests


================================================
FILE: .github/workflows/docker-push.yml
================================================
# yaml-language-server: $schema=https://json.schemastore.org/github-workflow.json

name: "Docker Multi-Arch Build and Push â€“ matrix"
run-name: ${{ github.actor }} - building and pushing Docker images â€“ matrix

concurrency:
  group: "${{ github.workflow }}-${{ github.ref }}"
  cancel-in-progress: true

permissions:
  contents: "read"
  packages: "write"
  pull-requests: "read"

on:
  push:
    branches: ["*"] #  [ 'main', 'master' ]
    tags: ["*"] # [ 'v*' ]
  pull_request:
    branches: ["main", "master"]
  workflow_dispatch:

env:
  IMAGE_NAME: "${{ github.repository_owner }}/gpt-researcher"
  REGISTRY_IMAGE: "ghcr.io/${{ github.repository }}"

jobs:
  build-push-root:
    name: "Build Root Image - ${{ matrix.platform_slash }}"
    # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
    if: ${{ success() || failure() }}
    runs-on: "${{ matrix.runner_label }}"
    env:
      # Variables for Docker tokens and authentication
      IMAGE_NAME_GHCR_SUFFIXED: "${{ github.event.repository.name }}"
      IMAGE_NAME_DOCKERHUB_SUFFIXED: "${{ github.event.repository.name }}"
      DOCKERFILE_PATH: "./Dockerfile"
      BUILD_CONTEXT: "."
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform_slash: "linux/amd64"
            runner_label: "ubuntu-latest"
          - platform_slash: "linux/arm64"
            runner_label: "ubuntu-22.04-arm"
    steps:
      - name: "Checkout repository"
        uses: "actions/checkout@v4"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          fetch-depth: 0
      - name: "Set up Docker Buildx"
        uses: "docker/setup-buildx-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
      # Set token variables with fallbacks
      - name: "Set Docker token variables"
        id: set-tokens
        run: |
          # GitHub token with fallbacks
          echo "GITHUB_AUTH_TOKEN=${{ secrets.GITHUB_TOKEN || secrets.GITHUB_PERSONAL_ACCESS_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker token with fallbacks
          echo "DOCKER_TOKEN=${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker username with fallbacks
          echo "DOCKER_USERNAME=${{ secrets.DOCKER_USERNAME || github.repository_owner }}" >> $GITHUB_ENV

          # Docker auth flag
          if [[ -n "${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" ]]; then
            echo "DOCKERHUB_AUTH=true" >> $GITHUB_ENV
          else
            echo "DOCKERHUB_AUTH=false" >> $GITHUB_ENV
          fi
      - name: "Log in to GitHub Container Registry"
        uses: "docker/login-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          registry: "ghcr.io"
          username: "${{ github.repository_owner }}"
          password: "${{ env.GITHUB_AUTH_TOKEN }}"
      - name: "Log in to Docker Hub (if secrets present)"
        if: env.DOCKERHUB_AUTH == 'true'
        uses: "docker/login-action@v3"
        with:
          username: "${{ env.DOCKER_USERNAME }}"
          password: "${{ env.DOCKER_TOKEN }}"
      - name: "Extract Docker metadata"
        id: "meta"
        uses: "docker/metadata-action@v5"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          images: |
            ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME_GHCR_SUFFIXED }}
            ${{ env.DOCKER_USERNAME }}/${{ env.IMAGE_NAME_DOCKERHUB_SUFFIXED }}
          tags: |
            # For release tags: apply the version tag and mark as latest
            type=semver,pattern={{version}}
            type=semver,pattern=latest
            # For default branch (main/master): apply dev tag
            type=raw,value=dev,enable=${{ github.event_name == 'push' && github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
            # For branch pushes: apply the branch name as tag
            type=ref,event=branch,prefix=
            # For PRs: apply pr-{number} tag
            type=ref,event=pr
      - name: "Build and push Docker image"
        uses: "docker/build-push-action@v6"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          context: "${{ env.BUILD_CONTEXT }}"
          file: "${{ env.DOCKERFILE_PATH }}"
          platforms: "${{ matrix.platform_slash }}"
          tags: "${{ steps.meta.outputs.tags }}"
          labels: "${{ steps.meta.outputs.labels }}"
          push: true
          provenance: false
          sbom: false
          cache-from: "type=gha"
          cache-to: "type=gha,mode=max"

  build-push-frontend-nextjs:
    name: "Build Frontend Next.js Image - ${{ matrix.platform_slash }}"
    # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
    if: ${{ success() || failure() }}
    runs-on: "${{ matrix.runner_label }}"
    env:
      # Variables for Docker tokens and authentication
      IMAGE_NAME_GHCR_SUFFIXED: "${{ github.event.repository.name }}-frontend-nextjs"
      IMAGE_NAME_DOCKERHUB_SUFFIXED: "${{ github.event.repository.name }}-frontend-nextjs"
      DOCKERFILE_PATH: "./frontend/nextjs/Dockerfile.dev"
      BUILD_CONTEXT: "./frontend/nextjs"
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform_slash: "linux/amd64"
            runner_label: "ubuntu-latest"
          - platform_slash: "linux/arm64"
            runner_label: "ubuntu-22.04-arm"
    steps:
      - name: "Checkout repository"
        uses: "actions/checkout@v4"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          fetch-depth: 0
      - name: "Set up Docker Buildx"
        uses: "docker/setup-buildx-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
      # Set token variables with fallbacks
      - name: "Set Docker token variables"
        id: set-tokens
        run: |
          # GitHub token with fallbacks
          echo "GITHUB_AUTH_TOKEN=${{ secrets.GITHUB_TOKEN || secrets.GITHUB_PERSONAL_ACCESS_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker token with fallbacks
          echo "DOCKER_TOKEN=${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker username with fallbacks
          echo "DOCKER_USERNAME=${{ secrets.DOCKER_USERNAME || github.repository_owner }}" >> $GITHUB_ENV

          # Docker auth flag
          if [[ -n "${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" ]]; then
            echo "DOCKERHUB_AUTH=true" >> $GITHUB_ENV
          else
            echo "DOCKERHUB_AUTH=false" >> $GITHUB_ENV
          fi
      - name: "Log in to GitHub Container Registry"
        uses: "docker/login-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          registry: "ghcr.io"
          username: "${{ github.repository_owner }}"
          password: "${{ env.GITHUB_AUTH_TOKEN }}"
      - name: "Log in to Docker Hub (if secrets present)"
        if: env.DOCKERHUB_AUTH == 'true'
        uses: "docker/login-action@v3"
        with:
          username: "${{ env.DOCKER_USERNAME }}"
          password: "${{ env.DOCKER_TOKEN }}"
      - name: "Extract Docker metadata"
        id: "meta"
        uses: "docker/metadata-action@v5"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          images: |
            ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME_GHCR_SUFFIXED }}
            ${{ env.DOCKER_USERNAME }}/${{ env.IMAGE_NAME_DOCKERHUB_SUFFIXED }}
          tags: |
            # For release tags: apply the version tag and mark as latest
            type=semver,pattern={{version}}
            type=semver,pattern=latest
            # For default branch (main/master): apply dev tag
            type=raw,value=dev,enable=${{ github.event_name == 'push' && github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
            # For branch pushes: apply the branch name as tag
            type=ref,event=branch,prefix=
            # For PRs: apply pr-{number} tag
            type=ref,event=pr
      - name: "Build and push Docker image"
        uses: "docker/build-push-action@v6"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          context: "${{ env.BUILD_CONTEXT }}"
          file: "${{ env.DOCKERFILE_PATH }}"
          platforms: "${{ matrix.platform_slash }}"
          tags: "${{ steps.meta.outputs.tags }}"
          labels: "${{ steps.meta.outputs.labels }}"
          push: true
          provenance: false
          sbom: false
          cache-from: "type=gha"
          cache-to: "type=gha,mode=max"

  build-push-mcp-server:
    name: "Build MCP Server Image - ${{ matrix.platform_slash }}"
    # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
    if: ${{ success() || failure() }}
    runs-on: "${{ matrix.runner_label }}"
    env:
      # Variables for Docker tokens and authentication
      IMAGE_NAME_GHCR_SUFFIXED: "${{ github.event.repository.name }}-mcp-server"
      IMAGE_NAME_DOCKERHUB_SUFFIXED: "${{ github.event.repository.name }}-mcp-server"
      DOCKERFILE_PATH: "./mcp-server/Dockerfile"
      BUILD_CONTEXT: "./mcp-server"
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform_slash: "linux/amd64"
            runner_label: "ubuntu-latest"
          - platform_slash: "linux/arm64"
            runner_label: "ubuntu-22.04-arm"
    steps:
      - name: "Checkout repository"
        uses: "actions/checkout@v4"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          fetch-depth: 0
      - name: "Set up Docker Buildx"
        uses: "docker/setup-buildx-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
      # Set token variables with fallbacks
      - name: "Set Docker token variables"
        id: set-tokens
        run: |
          # GitHub token with fallbacks
          echo "GITHUB_AUTH_TOKEN=${{ secrets.GITHUB_TOKEN || secrets.GITHUB_PERSONAL_ACCESS_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker token with fallbacks
          echo "DOCKER_TOKEN=${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker username with fallbacks
          echo "DOCKER_USERNAME=${{ secrets.DOCKER_USERNAME || github.repository_owner }}" >> $GITHUB_ENV

          # Docker auth flag
          if [[ -n "${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" ]]; then
            echo "DOCKERHUB_AUTH=true" >> $GITHUB_ENV
          else
            echo "DOCKERHUB_AUTH=false" >> $GITHUB_ENV
          fi
      - name: "Log in to GitHub Container Registry"
        uses: "docker/login-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          registry: "ghcr.io"
          username: "${{ github.repository_owner }}"
          password: "${{ env.GITHUB_AUTH_TOKEN }}"
      - name: "Log in to Docker Hub (if secrets present)"
        if: env.DOCKERHUB_AUTH == 'true'
        uses: "docker/login-action@v3"
        with:
          username: "${{ env.DOCKER_USERNAME }}"
          password: "${{ env.DOCKER_TOKEN }}"
      - name: "Extract Docker metadata"
        id: "meta"
        uses: "docker/metadata-action@v5"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          images: |
            ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME_GHCR_SUFFIXED }}
            ${{ env.DOCKER_USERNAME }}/${{ env.IMAGE_NAME_DOCKERHUB_SUFFIXED }}
          tags: |
            # For release tags: apply the version tag and mark as latest
            type=semver,pattern={{version}}
            type=semver,pattern=latest
            # For default branch (main/master): apply dev tag
            type=raw,value=dev,enable=${{ github.event_name == 'push' && github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
            # For branch pushes: apply the branch name as tag
            type=ref,event=branch,prefix=
            # For PRs: apply pr-{number} tag
            type=ref,event=pr
      - name: "Build and push Docker image"
        uses: "docker/build-push-action@v6"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          context: "${{ env.BUILD_CONTEXT }}"
          file: "${{ env.DOCKERFILE_PATH }}"
          platforms: "${{ matrix.platform_slash }}"
          tags: "${{ steps.meta.outputs.tags }}"
          labels: "${{ steps.meta.outputs.labels }}"
          push: true
          provenance: false
          sbom: false
          cache-from: "type=gha"
          cache-to: "type=gha,mode=max"

  build-push-docs-discord-bot:
    name: "Build Docs Discord Bot Image - ${{ matrix.platform_slash }}"
    # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
    if: ${{ success() || failure() }}
    runs-on: "${{ matrix.runner_label }}"
    env:
      # Variables for Docker tokens and authentication
      IMAGE_NAME_GHCR_SUFFIXED: "${{ github.event.repository.name }}-docs-discord-bot"
      IMAGE_NAME_DOCKERHUB_SUFFIXED: "${{ github.event.repository.name }}-docs-discord-bot"
      DOCKERFILE_PATH: "./docs/discord-bot/Dockerfile"
      BUILD_CONTEXT: "./docs/discord-bot"
    strategy:
      fail-fast: false
      matrix:
        include:
          - platform_slash: "linux/amd64"
            runner_label: "ubuntu-latest"
          - platform_slash: "linux/arm64"
            runner_label: "ubuntu-22.04-arm"
    steps:
      - name: "Checkout repository"
        uses: "actions/checkout@v4"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          fetch-depth: 0
      - name: "Set up Docker Buildx"
        uses: "docker/setup-buildx-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
      # Set token variables with fallbacks
      - name: "Set Docker token variables"
        id: set-tokens
        run: |
          # GitHub token with fallbacks
          echo "GITHUB_AUTH_TOKEN=${{ secrets.GITHUB_TOKEN || secrets.GITHUB_PERSONAL_ACCESS_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker token with fallbacks
          echo "DOCKER_TOKEN=${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" >> $GITHUB_ENV

          # Docker username with fallbacks
          echo "DOCKER_USERNAME=${{ secrets.DOCKER_USERNAME || github.repository_owner }}" >> $GITHUB_ENV

          # Docker auth flag
          if [[ -n "${{ secrets.DOCKERHUB_TOKEN || secrets.DOCKER_ACCESS_TOKEN || secrets.DOCKER_HUB_TOKEN || secrets.DOCKER_TOKEN || '' }}" ]]; then
            echo "DOCKERHUB_AUTH=true" >> $GITHUB_ENV
          else
            echo "DOCKERHUB_AUTH=false" >> $GITHUB_ENV
          fi
      - name: "Log in to GitHub Container Registry"
        uses: "docker/login-action@v3"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          registry: "ghcr.io"
          username: "${{ github.repository_owner }}"
          password: "${{ env.GITHUB_AUTH_TOKEN }}"
      - name: "Log in to Docker Hub (if secrets present)"
        if: env.DOCKERHUB_AUTH == 'true'
        uses: "docker/login-action@v3"
        with:
          username: "${{ env.DOCKER_USERNAME }}"
          password: "${{ env.DOCKER_TOKEN }}"
      - name: "Extract Docker metadata"
        id: "meta"
        uses: "docker/metadata-action@v5"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          images: |
            ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME_GHCR_SUFFIXED }}
            ${{ env.DOCKER_USERNAME }}/${{ env.IMAGE_NAME_DOCKERHUB_SUFFIXED }}
          tags: |
            # For release tags: apply the version tag and mark as latest
            type=semver,pattern={{version}}
            type=semver,pattern=latest
            # For default branch (main/master): apply dev tag
            type=raw,value=dev,enable=${{ github.event_name == 'push' && github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
            # For branch pushes: apply the branch name as tag
            type=ref,event=branch,prefix=
            # For PRs: apply pr-{number} tag
            type=ref,event=pr
      - name: "Build and push Docker image"
        uses: "docker/build-push-action@v6"
        # This line makes it possible to cancel a workflow in the middle of a step. Otherwise the step needs to complete first.
        if: ${{ success() || failure() }}
        with:
          context: "${{ env.BUILD_CONTEXT }}"
          file: "${{ env.DOCKERFILE_PATH }}"
          platforms: "${{ matrix.platform_slash }}"
          tags: "${{ steps.meta.outputs.tags }}"
          labels: "${{ steps.meta.outputs.labels }}"
          push: true
          provenance: false
          sbom: false
          cache-from: "type=gha"
          cache-to: "type=gha,mode=max"


